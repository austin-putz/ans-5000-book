[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science & Statistics",
    "section": "",
    "text": "Welcome\nWelcome to Introduction to Data Science & Statistics (AnS 500) for Fall 2025!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Introduction to Data Science & Statistics",
    "section": "Course Overview",
    "text": "Course Overview\nThis comprehensive 16-week course is divided into two parts, providing a complete foundation in data science and statistical methods for animal and agricultural sciences. All examples and exercises use R and Quarto for reproducible analysis.\nDuration: 16 weeks (8 weeks per part) Format: 2 hours lecture per week + homework assignments Tools: R + Quarto exclusively Prerequisites: None (we start from the basics!)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#part-1-introduction-to-data-science-weeks-1-8",
    "href": "index.html#part-1-introduction-to-data-science-weeks-1-8",
    "title": "Introduction to Data Science & Statistics",
    "section": "Part 1: Introduction to Data Science (Weeks 1-8)",
    "text": "Part 1: Introduction to Data Science (Weeks 1-8)\nThe first half introduces you to modern data science practices using R, RStudio, and the tidyverse ecosystem. You’ll learn to import, clean, transform, visualize, and communicate data effectively.\n\nWhat You’ll Learn in Part 1\n\nFoundations: Data science workflow, best practices, project organization\nR & RStudio: Getting started, reading data (CSV, Excel)\nData Manipulation: dplyr for transforming and summarizing data\nVisualization: ggplot2 for creating publication-quality plots\nAdvanced Topics: Reshaping data, joining datasets, iteration with purrr\n\n\n\nPart 1 Chapters\n\nFoundations of Data Science - What is data science? Best practices, R/RStudio/Quarto intro, Git/GitHub\nR, RStudio & Reading Data - Interface, projects, packages, importing CSV/Excel\nData Types & Strings - Types, string manipulation, regex, intro to dplyr\nData Manipulation with dplyr - mutate, arrange, group_by, summarise, handling NAs\nIntroduction to ggplot2 - Grammar of Graphics, essential plot types, themes\nAdvanced ggplot2 - Faceting, statistical layers, combining plots\nReshaping & Joining - Tidy data, pivoting, joins, functional programming\nSpecial Formats & Wrap-up - SAS/SPSS/Stata, Excel, janitor, complete workflow",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#part-2-introduction-to-statistics-weeks-9-16",
    "href": "index.html#part-2-introduction-to-statistics-weeks-9-16",
    "title": "Introduction to Data Science & Statistics",
    "section": "Part 2: Introduction to Statistics (Weeks 9-16)",
    "text": "Part 2: Introduction to Statistics (Weeks 9-16)\nThe second half builds on your data science skills to conduct statistical analyses. You’ll learn frequentist inference, hypothesis testing, and regression modeling.\n\nWhat You’ll Learn in Part 2\n\nStatistical Foundations: P-values, study design, experimental vs observational\nDescriptive Statistics: Measures of central tendency and variability, EDA\nProbability & Inference: Normal distribution, Central Limit Theorem, confidence intervals\nHypothesis Testing: t-tests, ANOVA, power, Type I/II errors\nRegression: Simple and multiple linear regression, model diagnostics\n\n\n\nPart 2 Chapters\n\nStatistical Foundations - Frequentist vs Bayesian, p-values, RCTs, confounding\nDescriptive Statistics - Mean, median, variance, SD, visualization, outliers\nProbability Distributions - Normal distribution, CLT, sampling distributions\nHypothesis Testing - t-tests, null/alternative hypotheses, power\nANOVA - One-way ANOVA, post-hoc tests, multiple comparisons\nCategorical Data - Chi-square, Fisher’s exact, odds ratios\nSimple Linear Regression - Correlation, least squares, diagnostics\nMultiple Regression - Multiple predictors, model comparison, variable selection",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#r-packages-used",
    "href": "index.html#r-packages-used",
    "title": "Introduction to Data Science & Statistics",
    "section": "R Packages Used",
    "text": "R Packages Used\nThis course makes extensive use of R packages from the tidyverse ecosystem and beyond.\n\nPart 1 Packages (Data Science)\n# Core tidyverse\ninstall.packages(\"tidyverse\")\n\n# Additional Part 1 packages\ninstall.packages(c(\n  \"readxl\",      # Read Excel files\n  \"writexl\",     # Write Excel files\n  \"haven\",       # Read SAS/SPSS/Stata files\n  \"janitor\",     # Data cleaning\n  \"lubridate\",   # Date/time manipulation\n  \"cowplot\",     # Combining plots\n  \"patchwork\",   # Combining plots (alternative)\n  \"here\",        # Project-relative paths\n  \"glue\"         # String interpolation\n))\n\n\nPart 2 Packages (Statistics)\ninstall.packages(c(\n  \"broom\",       # Tidy statistical output\n  \"car\",         # Companion to Applied Regression\n  \"effsize\",     # Effect size calculations\n  \"ggpubr\",      # Publication-ready plots\n  \"rstatix\",     # Pipe-friendly statistical tests\n  \"gt\",          # Grammar of Tables\n  \"emmeans\"      # Estimated marginal means\n))",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-philosophy",
    "href": "index.html#course-philosophy",
    "title": "Introduction to Data Science & Statistics",
    "section": "Course Philosophy",
    "text": "Course Philosophy\nThis course emphasizes:\n\nTidyverse-first approach: Modern R practices from day one\nReproducible research: Every analysis in Quarto\nReal-world data: Animal and agricultural science datasets with real messiness\nBest practices early: Project organization, naming conventions, version control\nStatistical thinking: Understanding over button-pushing\nBuild incrementally: Each week builds on previous skills",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Introduction to Data Science & Statistics",
    "section": "Getting Started",
    "text": "Getting Started\nNavigate through the chapters using the sidebar on the left. Each chapter includes:\n\nLecture content with conceptual explanations\nR code examples you can run and modify\nVisualizations to build intuition\nPractice exercises and homework assignments\n\nLet’s begin your statistical journey!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html",
    "href": "chapters/part1-ch01-foundations.html",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "",
    "text": "1.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#learning-objectives",
    "href": "chapters/part1-ch01-foundations.html#learning-objectives",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "",
    "text": "Define data science and understand its key components\nDescribe the data science workflow from data collection to communication\nExplain why statistics is essential for data science\nDistinguish between observational and experimental study designs\nCompare different programming languages for data science (R, Python, Julia)\nUnderstand when to use (and avoid) Excel for data analysis\nOrganize data science projects with proper folder structure and naming conventions\nApply best practices for data formatting, column naming, and documentation\nUnderstand database basics and the difference between long and wide data formats\nRecognize the role of version control (Git/GitHub) in reproducible research\nGet started with R, RStudio, and Quarto\nUnderstand career paths and skills needed for data science",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#what-is-data-science",
    "href": "chapters/part1-ch01-foundations.html#what-is-data-science",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.2 What is Data Science?",
    "text": "1.2 What is Data Science?\n\n1.2.1 Definition and Scope\nData Science is the interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\nData science combines:\n\nComputer Science: Programming, algorithms, data structures\nStatistics/Mathematics: Probability, inference, modeling\nDomain Expertise: Subject matter knowledge (in our case, animal science!)\n\n\n\n\n\n\n\nNoteData Science vs Statistics vs Machine Learning\n\n\n\n\nStatistics: Focuses on inference and understanding relationships\nMachine Learning: Emphasizes prediction and automation\nData Science: Encompasses both, plus data engineering, visualization, and communication\n\n\n\n\n\n1.2.2 The Data Science Workflow\nEvery data science project follows a similar workflow:\n\n\n\n\n\nflowchart LR\n    A[Collect] --&gt; B[Clean]\n    B --&gt; C[Explore]\n    C --&gt; D[Analyze]\n    D --&gt; E[Communicate]\n    E -.-&gt; A\n\n\n\n\n\n\n\nCollect: Import or gather data from various sources\nClean: Handle missing values, fix errors, standardize formats\nExplore: Visualize and summarize to understand patterns\nAnalyze: Apply statistical methods or models\nCommunicate: Present findings clearly to stakeholders\n\nThis workflow is iterative—you’ll often cycle back to earlier steps as you learn more about your data.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#why-statistics-matters",
    "href": "chapters/part1-ch01-foundations.html#why-statistics-matters",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.3 Why Statistics Matters",
    "text": "1.3 Why Statistics Matters\nWhile Part 1 focuses on data manipulation and visualization, statistics (Part 2 of this course) is essential because it allows us to:\n\nQuantify uncertainty: How confident are we in our findings?\nMake inferences: Can we generalize from a sample to a population?\nTest hypotheses: Is this treatment effect real or due to chance?\nBuild models: What factors predict our outcome of interest?\n\n\n\n\n\n\n\nImportantData Science Without Statistics is Dangerous\n\n\n\nBeautiful visualizations and complex models are useless if we don’t understand:\n\nWhether our sample is representative\nWhether differences are statistically significant\nWhether our assumptions are violated\nWhether our conclusions are justified\n\n\n\nWe’ll preview some statistical concepts here, but Part 2 will dive deep into hypothesis testing, ANOVA, regression, and more.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#study-design-observational-vs-experimental",
    "href": "chapters/part1-ch01-foundations.html#study-design-observational-vs-experimental",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.4 Study Design: Observational vs Experimental",
    "text": "1.4 Study Design: Observational vs Experimental\nUnderstanding study design is crucial for interpreting data correctly.\n\n1.4.1 Observational Studies\nIn observational studies, researchers observe without manipulating variables.\nCharacteristics:\n\nNo treatment assignment by researcher\nCannot control for all confounding variables\nCan show associations but not causation\n\nExamples:\n\nSurveying farmers about feeding practices and milk production\nTracking health outcomes in animals over time\nComparing herds that naturally differ in management\n\nAdvantages: Often cheaper, faster, and more ethical\nDisadvantages: Confounding variables make causal inference difficult\n\n\n1.4.2 Experimental Studies\nIn experimental studies, researchers actively manipulate variables (treatments).\nCharacteristics:\n\nResearcher assigns treatments\nRandomization balances confounders\nCan establish causation (under proper design)\n\nExamples:\n\nFeed trial: Randomly assign calves to different diets\nDrug efficacy: Randomly assign animals to treatment vs placebo\nBreeding trial: Randomly assign mating pairs\n\nAdvantages: Can establish cause-and-effect\nDisadvantages: Can be expensive, time-consuming, or unethical\n\n\n\n\n\n\nTipGold Standard: Randomized Controlled Trials (RCTs)\n\n\n\nThe strongest experimental design involves:\n\nRandomization: Random assignment to treatment/control\nControl group: Baseline for comparison\nReplication: Multiple subjects per treatment\nBlinding: When possible, subjects/observers don’t know treatment\n\nWe’ll explore RCTs in detail in Part 2, Week 1.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#programming-languages-for-data-science",
    "href": "chapters/part1-ch01-foundations.html#programming-languages-for-data-science",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.5 Programming Languages for Data Science",
    "text": "1.5 Programming Languages for Data Science\nData science can be done in many languages. Let’s compare the three most popular:\n\n1.5.1 R\nR is a programming language designed specifically for statistical computing and graphics.\nStrengths:\n\nExcellent for statistics and data visualization\nHuge ecosystem of packages (CRAN, Bioconductor)\nStrong in academia, especially biological/agricultural sciences\nFree and open-source\nRStudio IDE is fantastic\nQuarto for reproducible reports\n\nWeaknesses:\n\nSlower than compiled languages for some tasks\nInconsistent syntax (though tidyverse helps!)\nSteeper learning curve for general programming\n\nBest for: Statistical analysis, data visualization, academic research\n\n\n1.5.2 Python\nPython is a general-purpose programming language with strong data science libraries.\nStrengths:\n\nVersatile (web dev, automation, machine learning, data science)\nHuge community and job market\nExcellent for machine learning (scikit-learn, TensorFlow, PyTorch)\nPandas for data manipulation\nClean, readable syntax\n\nWeaknesses:\n\nData visualization not as elegant as R’s ggplot2\nStatistical methods less comprehensive than R\nMultiple competing tools for the same task\n\nBest for: Machine learning, automation, production systems, general programming\n\n\n1.5.3 Julia\nJulia is a newer language designed for high-performance numerical computing.\nStrengths:\n\nFast (C-like speed with Python-like syntax)\nDesigned for scientific computing\nEasy to write high-performance code\nMultiple dispatch system is powerful\n\nWeaknesses:\n\nSmaller ecosystem than R or Python\nLess mature tooling\nSmaller community\nFewer learning resources\n\nBest for: High-performance computing, complex simulations, numerical optimization\n\n\n1.5.4 Which Should You Learn?\nFor animal science and agricultural research, we recommend starting with R because:\n\nMost statistics textbooks and courses use R\nSuperior data visualization with ggplot2\nExcellent for reproducible research with Quarto\nStrong community in biological sciences\nEasier for non-programmers to learn\n\nBut: Python is valuable for machine learning and automation. Many data scientists know both!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#why-avoid-excel-for-analysis",
    "href": "chapters/part1-ch01-foundations.html#why-avoid-excel-for-analysis",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.6 Why Avoid Excel for Analysis?",
    "text": "1.6 Why Avoid Excel for Analysis?\nMicrosoft Excel is ubiquitous and useful for simple tasks, but problematic for data analysis.\n\n1.6.1 Problems with Excel\n\nNot Reproducible:\n\nPoint-and-click operations aren’t documented\nCan’t easily share analysis steps\nHard to audit or replicate\n\nError-Prone:\n\nEasy to accidentally modify data\nNo validation of formulas\nCopy-paste errors are common\n\nLimited Capabilities:\n\nPoor for large datasets (1M row limit)\nLimited statistical functions\nVisualization options are restrictive\n\nAutomatic Conversions (notorious):\n\nConverts gene names to dates (SEPT1 → Sep-1)\nChanges scientific notation unpredictably\nAlters leading zeros\n\nVersion Control Nightmare:\n\nBinary format, can’t use Git effectively\nMultiple versions proliferate (analysis_final_v3_FINAL.xlsx)\n\n\n\n\n\n\n\n\nWarningExcel Has Cost Researchers Dearly\n\n\n\n\nEconomics paper retracted due to Excel error (Reinhart-Rogoff, 2013)\nThousands of genomics papers have errors from Excel gene name conversions\nCOVID-19 cases lost in UK due to Excel row limit (2020)\n\n\n\n\n\n1.6.2 When Excel is Okay\nExcel is fine for:\n\nInitial data entry (but export to CSV immediately!)\nQuick visual inspection of small datasets\nSharing final results with non-technical collaborators\nSimple calculations that don’t need to be reproduced\n\n\n\n1.6.3 The Better Approach\nUse R + Quarto for analysis because:\n\nEvery step is documented in code\nAnalysis is fully reproducible\nEasy to update when data changes\nVersion control with Git\nProfessional, publication-quality output",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#project-organization-folder-structure",
    "href": "chapters/part1-ch01-foundations.html#project-organization-folder-structure",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.7 Project Organization: Folder Structure",
    "text": "1.7 Project Organization: Folder Structure\nConsistent project organization makes collaboration easier and reduces errors.\n\n1.7.1 Recommended Folder Structure\nproject_name/\n│\n├── data/\n│   ├── raw/              # Original, untouched data\n│   ├── processed/        # Cleaned data ready for analysis\n│   └── README.md         # Metadata documentation\n│\n├── code/\n│   ├── 01_import.R       # Data import and initial cleaning\n│   ├── 02_clean.R        # Data cleaning\n│   ├── 03_analyze.R      # Statistical analysis\n│   └── 04_visualize.R    # Create plots\n│\n├── output/\n│   ├── figures/          # Plots and visualizations\n│   ├── tables/           # Summary tables\n│   └── reports/          # Rendered Quarto documents\n│\n├── docs/                 # Documentation, notes\n│\n├── project_name.Rproj    # RStudio project file\n└── README.md             # Project overview\n\n\n\n\n\n\nTipKey Principles\n\n\n\n\nNever modify raw data: Keep originals untouched in data/raw/\nNumber your scripts: Use prefixes like 01_, 02_ for execution order\nSeparate data from code: Don’t mix data files and scripts\nDocument everything: README files explain what’s what\nUse relative paths: R Projects make this easy",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#file-naming-conventions",
    "href": "chapters/part1-ch01-foundations.html#file-naming-conventions",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.8 File Naming Conventions",
    "text": "1.8 File Naming Conventions\nGood file names are:\n\nMachine-readable: No spaces, special characters\nHuman-readable: Descriptive, not cryptic\nSortable: Use ISO dates (YYYY-MM-DD), leading zeros\n\n\n1.8.1 Examples\n\nBad NamesGood Names\n\n\nmy data.xlsx\nfinal.R\nfig1.png\nanalysis version 2 (old).R\nreport_10_1_23.docx\nProblems: spaces, not descriptive, ambiguous dates, “final” lies\n\n\npig_weights_2024-11-14.csv\n01_import_data.R\n02_clean_data.R\nfigure_01_weight_by_treatment.png\nreport_feed_trial_2024-11-14.html\nBenefits: ISO dates, descriptive, machine-readable, sortable\n\n\n\n\n\n1.8.2 Naming Best Practices\n# Use underscores or hyphens (not spaces)\npig_growth_data.csv      # good\npig-growth-data.csv      # good\npig growth data.csv      # bad\n\n# Use ISO 8601 dates: YYYY-MM-DD\ndata_2024-11-14.csv      # good\ndata_11-14-24.csv        # ambiguous\ndata_Nov_14_2024.csv     # not sortable\n\n# Include important metadata\ncattle_weights_iowa_2024.csv       # good\ndata.csv                           # too vague\n\n# Use leading zeros for sorting\nfile_01.R, file_02.R, file_10.R    # good\nfile_1.R, file_2.R, file_10.R      # file_10.R sorts before file_2.R!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#data-best-practices",
    "href": "chapters/part1-ch01-foundations.html#data-best-practices",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.9 Data Best Practices",
    "text": "1.9 Data Best Practices\n\n1.9.1 Column Naming\nGood column names are crucial for analysis.\nRules:\n\nNo spaces: Use snake_case or camelCase\nNo special characters: Avoid #, $, %, @, etc.\nStart with letters: Not numbers or symbols\nBe descriptive: body_weight_kg not bw\nUse consistent units: weight_kg, height_cm, temp_c\nLowercase preferred: Easier to type, less error-prone\n\n\nBad Column NamesGood Column Names\n\n\nAnimal #\nBody Weight (kg)\nDate-of-Birth\ntreatment$group\n2024_weight\n\n\nanimal_id\nbody_weight_kg\ndate_of_birth\ntreatment_group\nweight_2024_kg\n\n\n\n\n\n1.9.2 Data Format: CSV vs Excel\n\n\n\n\n\n\n\n\n\nFormat\nPros\nCons\nWhen to Use\n\n\n\n\nCSV\nPlain text, universal, version control friendly, fast\nNo formatting, one sheet only\nPreferred for analysis data\n\n\nExcel (.xlsx)\nMultiple sheets, formatting, formulas\nBinary format, Excel errors, version control issues\nData entry, sharing with non-technical users\n\n\n\n\n\n\n\n\n\nImportantGolden Rule\n\n\n\nAlways export to CSV for analysis, even if data was entered in Excel.\n# In R, read CSV files with:\nlibrary(readr)\ndata &lt;- read_csv(\"data/raw/animal_weights.csv\")\n\n\n\n\n1.9.3 Tidy Data Principles\nTidy data (Hadley Wickham) has a consistent structure that makes analysis easier:\n\nEach variable is a column\nEach observation is a row\nEach type of observational unit is a table\n\nExample:\n\nTidy FormatMessy Format (Wide)\n\n\n\n\n\nanimal_id\ndate\nweight_kg\ntreatment\n\n\n\n\nA01\n2024-01-01\n250\nControl\n\n\nA01\n2024-02-01\n275\nControl\n\n\nA02\n2024-01-01\n245\nTreatment\n\n\nA02\n2024-02-01\n285\nTreatment\n\n\n\n\n\n\n\n\nanimal_id\ntreatment\nweight_jan\nweight_feb\n\n\n\n\nA01\nControl\n250\n275\n\n\nA02\nTreatment\n245\n285\n\n\n\n\n\n\nThe tidy format is easier to filter, group, and plot. We’ll learn to reshape data in Week 7.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#database-basics",
    "href": "chapters/part1-ch01-foundations.html#database-basics",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.10 Database Basics",
    "text": "1.10 Database Basics\n\n1.10.1 Long vs Wide Format\nData can be organized in two ways:\nLong Format (Tidy):\n\nEach row is one observation\nMultiple rows per subject\nBetter for analysis and plotting\n\nWide Format:\n\nEach row is one subject\nMultiple columns for repeated measures\nMore compact, easier for humans to read\n\nWe’ll use tidyr (Week 7) to convert between formats.\n\n\n1.10.2 Relational Databases\nIn complex projects, data often exists in multiple related tables.\nExample: Feed trial study\nTable 1: animals\n\n\n\nanimal_id\nbirth_date\nbreed\nfarm_id\n\n\n\n\nA001\n2023-05-10\nAngus\nF01\n\n\nA002\n2023-05-12\nHolstein\nF02\n\n\n\nTable 2: weights\n\n\n\nanimal_id\ndate\nweight_kg\n\n\n\n\nA001\n2024-01-01\n250\n\n\nA001\n2024-02-01\n275\n\n\nA002\n2024-01-01\n320\n\n\n\nTable 3: farms\n\n\n\nfarm_id\nfarm_name\nstate\n\n\n\n\nF01\nSmith Ranch\nIA\n\n\nF02\nJohnson Dairy\nWI\n\n\n\nThese tables are linked by keys (animal_id, farm_id). We’ll learn to join them in Week 7.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#metadata-document-your-data",
    "href": "chapters/part1-ch01-foundations.html#metadata-document-your-data",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.11 Metadata: Document Your Data",
    "text": "1.11 Metadata: Document Your Data\nMetadata is “data about data”—it explains what your data means.\n\n1.11.1 Create a Data Dictionary\nEvery dataset should have a README or data dictionary:\n# Pig Growth Trial Data Dictionary\n\n**File**: pig_weights_2024.csv\n**Date Created**: 2024-11-14\n**Author**: Dr. Jane Smith\n**Description**: Weekly weights for 100 pigs in feed trial\n\n## Variables\n\n- `pig_id`: Unique identifier (P001-P100)\n- `treatment`: Feed type (A, B, or Control)\n- `date`: Date of measurement (YYYY-MM-DD)\n- `weight_kg`: Body weight in kilograms\n- `pen`: Pen number (1-10, 10 pigs per pen)\n\n## Notes\n\n- Missing weights indicate pig was sick that week\n- Treatment A = high protein diet\n- Treatment B = standard diet with supplements\n- Control = standard commercial diet\nThis documentation is crucial for:\n\nCollaborators understanding your data\nFuture you remembering what you did\nJournal reviewers checking your work",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#version-control-git-and-github",
    "href": "chapters/part1-ch01-foundations.html#version-control-git-and-github",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.12 Version Control: Git and GitHub",
    "text": "1.12 Version Control: Git and GitHub\n\n1.12.1 Why Version Control?\nWithout version control, you end up with:\nanalysis.R\nanalysis_final.R\nanalysis_final2.R\nanalysis_final_FINAL.R\nanalysis_final_FINAL_v2_USE_THIS_ONE.R\nVersion control tracks changes over time so you have:\nanalysis.R  (with full change history)\n\n\n1.12.2 Git\nGit is the most popular version control system.\nWhat Git does:\n\nTracks all changes to files over time\nAllows you to revert to previous versions\nShows who changed what and when\nEnables branching for experiments\nFacilitates collaboration\n\nBasic Git workflow:\ngit add file.R        # Stage changes\ngit commit -m \"Updated analysis\"  # Save snapshot\ngit push              # Upload to GitHub\n\n\n1.12.3 GitHub\nGitHub is a cloud platform for hosting Git repositories.\nBenefits:\n\nBackup your code online\nCollaborate with others\nShare code publicly or privately\nTrack issues and projects\nHost websites (GitHub Pages)\n\n\n\n\n\n\n\nNoteWe’ll Use Git Lightly in This Course\n\n\n\nVersion control is important, but can be overwhelming for beginners. We’ll introduce basic concepts, but won’t require mastery.\nAs you become more serious about data science, invest time in learning Git properly!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#introduction-to-r-rstudio-and-quarto",
    "href": "chapters/part1-ch01-foundations.html#introduction-to-r-rstudio-and-quarto",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.13 Introduction to R, RStudio, and Quarto",
    "text": "1.13 Introduction to R, RStudio, and Quarto\n\n1.13.1 What is R?\nR is a free, open-source programming language designed for statistical computing and graphics.\nInstall R:\n\nGo to https://www.r-project.org/\nClick “download R”\nChoose a CRAN mirror (any US mirror works)\nDownload for your operating system (Windows, Mac, Linux)\n\n\n\n1.13.2 What is RStudio?\nRStudio (now called Posit) is an Integrated Development Environment (IDE) for R—it makes R much easier to use.\nInstall RStudio:\n\nGo to https://posit.co/downloads/\nDownload RStudio Desktop (free version)\nInstall (requires R to be installed first)\n\n\n\n\n\n\n\nImportantInstall Order Matters!\n\n\n\n\nInstall R first\nThen install RStudio\n\n\n\n\n\n1.13.3 What is Quarto?\nQuarto is an open-source publishing system that lets you combine code, text, and output in a single document.\nWhy Quarto?\n\nWrite analysis and explanation together\nCode runs automatically when rendered\nOutputs to HTML, PDF, Word, slides, etc.\nReproducible reports with one click\nSuccessor to R Markdown with more features\n\nQuarto is included in RStudio, no separate installation needed!\n\n\n1.13.4 Your First R Session\nOpen RStudio and try some basic commands in the Console:\n\n# R as a calculator\n2 + 2\n\n[1] 4\n\n10 * 5\n\n[1] 50\n\nsqrt(16)\n\n[1] 4\n\n# Create variables\nx &lt;- 5\ny &lt;- 10\nx + y\n\n[1] 15\n\n# Create a vector\nweights &lt;- c(250, 275, 290, 310)\nweights\n\n[1] 250 275 290 310\n\n# Calculate mean\nmean(weights)\n\n[1] 281.25\n\n\nCongratulations, you’ve written R code!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#what-does-it-take-to-be-a-data-scientist",
    "href": "chapters/part1-ch01-foundations.html#what-does-it-take-to-be-a-data-scientist",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.14 What Does It Take to Be a Data Scientist?",
    "text": "1.14 What Does It Take to Be a Data Scientist?\n\n1.14.1 Core Skills\n\nProgramming: R or Python for data manipulation\nStatistics: Hypothesis testing, regression, experimental design\nVisualization: Communicating patterns clearly\nDomain Knowledge: Understanding the field (animal science!)\nCommunication: Explaining technical results to non-technical audiences\n\n\n\n1.14.2 Useful Additional Skills\n\nSQL: Querying databases\nMachine Learning: Predictive modeling\nCloud Computing: AWS, Google Cloud, Azure\nCommand Line: Bash scripting\nVersion Control: Git/GitHub\n\n\n\n1.14.3 Career Paths\nData science skills open many doors:\n\nResearch Scientist: University or industry R&D\nData Analyst: Descriptive analytics, reporting\nData Scientist: Predictive modeling, machine learning\nBioinformatician: Genomics and computational biology\nQuantitative Geneticist: Breeding programs, genomic selection\nStatistical Consultant: Supporting researchers\n\n\n\n\n\n\n\nTipYou Don’t Need to Master Everything!\n\n\n\nStart with the fundamentals (this course), then specialize based on your interests and career goals.\nEvery data scientist started as a beginner!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#summary",
    "href": "chapters/part1-ch01-foundations.html#summary",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.15 Summary",
    "text": "1.15 Summary\nThis chapter introduced the foundations of data science:\n\nData science combines programming, statistics, and domain expertise\nThe data science workflow is iterative: collect → clean → explore → analyze → communicate\nStudy design matters: experimental studies can establish causation, observational studies show associations\nR is excellent for statistics and visualization; Python for machine learning; Julia for speed\nAvoid Excel for analysis; use R + Quarto for reproducibility\nOrganize projects with consistent folder structure and naming conventions\nData best practices: tidy format, good column names, CSV for analysis, metadata documentation\nVersion control (Git/GitHub) tracks changes and enables collaboration\nR, RStudio, and Quarto are our tools for reproducible analysis",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#homework-assignment",
    "href": "chapters/part1-ch01-foundations.html#homework-assignment",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.16 Homework Assignment",
    "text": "1.16 Homework Assignment\n\n1.16.1 Assignment: Getting Started with Data Science\nDue: Before Week 2\n\n1.16.1.1 Part 1: Installation (30 points)\n\nInstall R from https://www.r-project.org/\nInstall RStudio from https://posit.co/downloads/\nInstall the tidyverse package in R:\ninstall.packages(\"tidyverse\")\nTake a screenshot of RStudio with the message showing tidyverse installed successfully\n\n\n\n1.16.1.2 Part 2: Project Setup (30 points)\n\nCreate a new RStudio Project called “ans500_firstname_lastname”\nWithin the project folder, create the following subfolders:\n\ndata/raw\ndata/processed\ncode\noutput/figures\n\nCreate a README.md file describing the project\nTake screenshots of your folder structure\n\n\n\n1.16.1.3 Part 3: First Quarto Document (40 points)\nCreate a Quarto document called week1_homework.qmd with:\n\nYAML header with title, author, and date\nIntroduction section explaining:\n\nWhat is data science?\nWhy are you taking this course?\nWhat do you hope to learn?\n\nR code chunk that:\n\nCreates a vector of animal weights (make up 10 values)\nCalculates the mean and standard deviation\nCreates a simple histogram\n\nReflection section (200-300 words) on:\n\nOne thing that surprised you about data science\nHow you currently manage data (Excel? Paper? Other?)\nWhat bad habits you might need to break\n\n\nRender to HTML and submit both .qmd and .html files.\n\n\n\n1.16.2 Recommended YAML\n---\ntitle: \"Week 1 Homework: Foundations of Data Science\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    code-fold: false\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n1.16.3 Grading Rubric\n\nInstallation (30%): All software installed, tidyverse working\nProject Setup (30%): Proper folder structure, README created\nQuarto Document (40%):\n\nCode runs without errors (10%)\nProper formatting and organization (10%)\nThoughtful reflection (20%)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch01-foundations.html#additional-resources",
    "href": "chapters/part1-ch01-foundations.html#additional-resources",
    "title": "1  Foundations of Data Science & Best Practices",
    "section": "1.17 Additional Resources",
    "text": "1.17 Additional Resources\n\n1.17.1 Required Reading\n\nR for Data Science (2e) - Chapters 1-3: Introduction, Data Visualization, Workflow Basics\nQuarto Getting Started\n\n\n\n1.17.2 Optional Reading\n\nWickham, H. (2014). “Tidy Data.” Journal of Statistical Software, 59(10). Link\nWilson, G. et al. (2017). “Good enough practices in scientific computing.” PLOS Computational Biology. Link\n\n\n\n1.17.3 Videos\n\n“What is Data Science?” by StatQuest\n“Introduction to R and RStudio” by RStudio\n\n\n\n1.17.4 Cheat Sheets\n\nRStudio IDE Cheat Sheet\nR Markdown (Quarto predecessor) Cheat Sheet\n\n\nNext Chapter: Getting Started with R, RStudio, and Reading Data",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch02-r_rstudio_reading_data.html",
    "href": "chapters/part1-ch02-r_rstudio_reading_data.html",
    "title": "2  Getting Started with R, RStudio, and Reading Data",
    "section": "",
    "text": "2.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Data Types, Strings, and Introduction to dplyr",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch02-r_rstudio_reading_data.html#learning-objectives",
    "href": "chapters/part1-ch02-r_rstudio_reading_data.html#learning-objectives",
    "title": "2  Getting Started with R, RStudio, and Reading Data",
    "section": "",
    "text": "Navigate the RStudio interface and understand its main components\nCreate and manage R Projects for organized workflows\nInstall and load R packages\nCreate and render Quarto documents\nRead CSV and Excel files into R\nPerform basic data exploration\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch03-data_types_strings_dplyr.html",
    "href": "chapters/part1-ch03-data_types_strings_dplyr.html",
    "title": "3  Data Types, Strings, and Introduction to dplyr",
    "section": "",
    "text": "3.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Data Manipulation with dplyr",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch03-data_types_strings_dplyr.html#learning-objectives",
    "href": "chapters/part1-ch03-data_types_strings_dplyr.html#learning-objectives",
    "title": "3  Data Types, Strings, and Introduction to dplyr",
    "section": "",
    "text": "Understand and work with different R data types\nManipulate strings using the stringr package\nApply basic regular expressions for pattern matching\nUse the pipe operator for readable code\nFilter and select data using dplyr\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch04-dplyr.html",
    "href": "chapters/part1-ch04-dplyr.html",
    "title": "4  Data Manipulation with dplyr",
    "section": "",
    "text": "4.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Introduction to Data Visualization with ggplot2",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch04-dplyr.html#learning-objectives",
    "href": "chapters/part1-ch04-dplyr.html#learning-objectives",
    "title": "4  Data Manipulation with dplyr",
    "section": "",
    "text": "Create and modify variables with mutate()\nSort data with arrange()\nCreate grouped summaries with group_by() and summarise()\nHandle missing data effectively\nApply conditional logic with if_else() and case_when()\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch05-ggplot2_intro.html",
    "href": "chapters/part1-ch05-ggplot2_intro.html",
    "title": "5  Introduction to Data Visualization with ggplot2",
    "section": "",
    "text": "5.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Advanced ggplot2 and Multi-Panel Plots",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch05-ggplot2_intro.html#learning-objectives",
    "href": "chapters/part1-ch05-ggplot2_intro.html#learning-objectives",
    "title": "5  Introduction to Data Visualization with ggplot2",
    "section": "",
    "text": "Understand the Grammar of Graphics philosophy\nCreate basic plots: scatter, line, bar, histogram, boxplot\nMap variables to aesthetic properties\nApply themes and customize plot appearance\nSave plots for publication\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch06-ggplot2_advanced.html",
    "href": "chapters/part1-ch06-ggplot2_advanced.html",
    "title": "6  Advanced ggplot2 and Multi-Panel Plots",
    "section": "",
    "text": "6.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Data Reshaping, Joining, and Iteration",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch06-ggplot2_advanced.html#learning-objectives",
    "href": "chapters/part1-ch06-ggplot2_advanced.html#learning-objectives",
    "title": "6  Advanced ggplot2 and Multi-Panel Plots",
    "section": "",
    "text": "Create faceted plots with facet_wrap() and facet_grid()\nAdd statistical layers and trend lines\nCustomize themes extensively\nWork with color palettes\nCombine multiple plots using cowplot and patchwork\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch07-tidyr_joins_purrr.html",
    "href": "chapters/part1-ch07-tidyr_joins_purrr.html",
    "title": "7  Data Reshaping, Joining, and Iteration",
    "section": "",
    "text": "7.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Chapter: Special Data Formats and Course Wrap-up",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch07-tidyr_joins_purrr.html#learning-objectives",
    "href": "chapters/part1-ch07-tidyr_joins_purrr.html#learning-objectives",
    "title": "7  Data Reshaping, Joining, and Iteration",
    "section": "",
    "text": "Apply tidy data principles\nReshape data with pivot_longer() and pivot_wider()\nJoin multiple datasets using various join types\nUse functional programming with purrr\nWork with nested data structures\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch08-special_formats.html",
    "href": "chapters/part1-ch08-special_formats.html",
    "title": "8  Special Data Formats and Course Wrap-up",
    "section": "",
    "text": "8.1 Learning Objectives\nBy the end of this chapter, you will be able to:\nNext Section: Part 2: Introduction to Statistics",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Special Data Formats and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/part1-ch08-special_formats.html#learning-objectives",
    "href": "chapters/part1-ch08-special_formats.html#learning-objectives",
    "title": "8  Special Data Formats and Course Wrap-up",
    "section": "",
    "text": "Read data from SAS, SPSS, and Stata files\nWork with Excel files for reading and writing\nUse helpful packages like janitor and lubridate\nApply the complete data science workflow\nCreate reproducible analysis pipelines\n\n\n\n\n\n\n\nNoteComing Soon\n\n\n\nThis chapter is under development. Content will be added shortly.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Special Data Formats and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html",
    "href": "chapters/part2-ch01-statistical_foundations.html",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "",
    "text": "10 Introduction: Why Statistics Matter in Animal Science\nImagine you’re a swine nutritionist testing a new feed additive that claims to improve growth rates. After a 90-day trial, you observe that pigs fed the new additive weigh an average of 5 kg more than the control group. Is this difference real, or just random variation? Should you recommend this expensive additive to producers?\nOr perhaps you’re a beef geneticist comparing two breeding programs. Bulls from Program A seem to produce offspring with slightly better marbling scores. But is the difference large enough to justify changing breeding protocols?\nThese are the types of questions statistics helps us answer. Statistics is fundamentally about making decisions in the presence of uncertainty. In animal science, we deal with biological variation constantly—no two animals are exactly alike, even if they’re raised identically. Statistics gives us a framework to:\nIn this course, we’ll build your statistical toolkit step by step, always connecting concepts back to real problems in animal agriculture.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-frequentist",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-frequentist",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "11.1 Frequentist Statistics",
    "text": "11.1 Frequentist Statistics\nThe frequentist approach defines probability as long-run frequency. If we say “the probability of getting heads is 0.5,” we mean that if we flipped a coin infinitely many times, about half would be heads.\n\n11.1.1 Key Principles\n\nParameters are fixed but unknown: The true average weight of pigs on a new diet is a fixed number—we just don’t know it. Our job is to estimate it from data.\nProbability describes data, not hypotheses: We calculate “the probability of observing data this extreme if the null hypothesis were true,” NOT “the probability that the hypothesis is true.”\nRepetition is key: Frequentist inference imagines repeating the same experiment many times. Confidence intervals and p-values only make sense in this framework of repeated sampling.\n\n\n\n11.1.2 Example: Feed Trial\nSuppose we test a new feed supplement in pigs. The frequentist asks:\n\n“If this supplement truly had no effect (null hypothesis), what’s the probability we’d observe a difference this large just by chance?”\n\nIf that probability is very small (say, p &lt; 0.05), we conclude the data are incompatible with the null hypothesis, and we reject it in favor of the alternative (the supplement does have an effect).\n\n\n\n\n\n\nImportantCritical Point\n\n\n\nA p-value of 0.03 does NOT mean “there’s a 3% chance the null hypothesis is true.” It means “if the null were true, we’d see data this extreme only 3% of the time by chance alone.”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-bayesian",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-bayesian",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "11.2 Bayesian Statistics",
    "text": "11.2 Bayesian Statistics\nThe Bayesian approach defines probability as degree of belief. It explicitly incorporates prior knowledge and updates that knowledge with data.\n\n11.2.1 Key Principles\n\nParameters have probability distributions: Instead of saying “the true effect is unknown,” Bayesians say “our belief about the effect can be described by a probability distribution.”\nPrior + Data = Posterior: Bayesian analysis combines:\n\nPrior: What we believed before seeing the data\nLikelihood: What the data tell us\nPosterior: Updated beliefs after seeing the data\n\nDirect probability statements about hypotheses: Bayesians can say things like “there’s an 85% probability the effect is positive” or “the treatment effect is between 2 and 8 kg with 95% probability.”\n\n\n\n11.2.2 Bayes’ Theorem\nThe mathematical foundation of Bayesian statistics is Bayes’ Theorem:\n\\[\nP(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\times P(\\theta)}{P(\\text{data})}\n\\]\nWhere:\n\n\\(P(\\theta | \\text{data})\\) = Posterior: Our updated belief about parameter \\(\\theta\\) after seeing the data\n\\(P(\\text{data} | \\theta)\\) = Likelihood: How probable our data are under different values of \\(\\theta\\)\n\\(P(\\theta)\\) = Prior: Our belief about \\(\\theta\\) before seeing the data\n\\(P(\\text{data})\\) = Marginal likelihood: A normalizing constant (probability of data across all possible \\(\\theta\\))\n\n\n\n11.2.3 Example: Same Feed Trial, Bayesian Perspective\nA Bayesian might start with prior knowledge: “Previous studies suggest feed supplements increase growth by 0-10 kg, with most around 3-5 kg.” After seeing the data, they update this prior to a posterior distribution and can make statements like:\n\n“Based on our data, there’s a 92% probability that the supplement increases weight by at least 2 kg, and a 70% probability the increase is between 4 and 8 kg.”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#comparing-the-approaches",
    "href": "chapters/part2-ch01-statistical_foundations.html#comparing-the-approaches",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "11.3 Comparing the Approaches",
    "text": "11.3 Comparing the Approaches\n\n\n\n\n\nAspect\nFrequentist\nBayesian\n\n\n\n\nDefinition of Probability\nLong-run frequency\nDegree of belief\n\n\nParameters\nFixed, unknown constants\nRandom variables with distributions\n\n\nPrior Knowledge\nNot formally incorporated\nExplicitly incorporated via priors\n\n\nOutput\np-values, confidence intervals\nPosterior distributions, credible intervals\n\n\nInterpretation\nBased on hypothetical repetition\nDirect probability statements\n\n\n\n\n\n\n\n\n\n\n\nTipWhy Frequentist in This Course?\n\n\n\nFrequentist methods are:\n\nMore commonly used in animal science journals\nRequired by many regulatory bodies (FDA, EPA)\nComputationally simpler for basic analyses\nThe foundation for most statistical software defaults\n\nHowever, Bayesian methods are growing in popularity, especially for complex models. Being fluent in frequentist thinking first makes learning Bayesian approaches easier later.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-definition",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-definition",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "12.1 Definition and Meaning",
    "text": "12.1 Definition and Meaning\nA p-value is defined as:\n\\[\np = P(\\text{data as extreme or more extreme} \\mid H_0 \\text{ is true})\n\\]\nWhere:\n\n\\(P(\\cdot)\\) = Probability\n\\(\\mid\\) = “given that” or “conditional on”\n\\(H_0\\) = The null hypothesis (typically “no effect” or “no difference”)\n\nIn plain English: The p-value is the probability of observing results at least as extreme as what we actually observed, assuming the null hypothesis is true.\n\n12.1.1 Breaking Down the Definition\nLet’s unpack each part:\n\n“Probability of observing results…” – We’re talking about data, not hypotheses\n“…at least as extreme…” – Not just exactly what we saw, but anything further from what we’d expect under the null\n“…assuming the null hypothesis is true” – This is a conditional probability; we’re starting with an assumption\n\n\n\n\n\n\n\nWarningThe p-value is NOT:\n\n\n\n\n❌ The probability that the null hypothesis is true: \\(P(H_0 | \\text{data})\\)\n❌ The probability that the result occurred by chance\n❌ The probability of making a wrong decision\n❌ The size or importance of an effect\n❌ The probability that replicating the study would give the same result",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-visual",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-visual",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "12.2 Visualizing What P-Values Mean",
    "text": "12.2 Visualizing What P-Values Mean\nLet’s simulate a situation to build intuition. Suppose we’re comparing two groups of beef cattle (Control vs Treatment), and in reality, there’s no true difference between them (null hypothesis is true).\n\n\nCode\n# Simulation parameters\nn_per_group &lt;- 25\ntrue_mean &lt;- 600  # kg, both groups\ntrue_sd &lt;- 40\n\n# Generate ONE sample where null is true\nset.seed(123)\ncontrol &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\ntreatment &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n\n# Combine into data frame\nsample_data &lt;- tibble(\n  weight = c(control, treatment),\n  group = rep(c(\"Control\", \"Treatment\"), each = n_per_group)\n)\n\n# Visualize\np1 &lt;- ggplot(sample_data, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2) +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Treatment\" = \"#56B4E9\")) +\n  labs(\n    title = \"One Sample: Weights Under the Null (No True Difference)\",\n    subtitle = sprintf(\"Control mean: %.1f kg | Treatment mean: %.1f kg\",\n                      mean(control), mean(treatment)),\n    y = \"Final Weight (kg)\",\n    x = \"Group\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(500, 700)\n\nprint(p1)\n\n\n\n\n\n\n\n\n\nCode\n# Run t-test\ntest_result &lt;- t.test(control, treatment)\ncat(sprintf(\"\\nObserved difference: %.1f kg\\n\", mean(treatment) - mean(control)))\n\n\n\nObserved difference: 5.4 kg\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", test_result$p.value))\n\n\nP-value: 0.6100\n\n\nEven though the null hypothesis is true (both groups have the same mean), we observe a difference just due to random sampling. The p-value tells us how “surprising” this observed difference would be if the null were true.\n\n12.2.1 The Distribution of P-Values Under the Null\nNow, what happens if we repeat this experiment 1,000 times, always with no true difference?\n\n\nCode\n# Simulate 1000 experiments where null is true\nn_simulations &lt;- 1000\n\nsimulate_study &lt;- function() {\n  control &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n  treatment &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n  t.test(control, treatment)$p.value\n}\n\np_values &lt;- replicate(n_simulations, simulate_study())\n\n# Visualize distribution\np2 &lt;- tibble(p_value = p_values) %&gt;%\n  ggplot(aes(x = p_value)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0.05, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n  annotate(\"text\", x = 0.05, y = 70, label = \"α = 0.05\",\n           color = \"red\", hjust = -0.1, size = 5) +\n  labs(\n    title = \"Distribution of P-Values When the Null Hypothesis is TRUE\",\n    subtitle = sprintf(\"%d simulations: each time, both groups truly have mean = %d kg\",\n                      n_simulations, true_mean),\n    x = \"P-value\",\n    y = \"Count (out of 1,000 studies)\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 13)\n\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate proportion \"significant\"\nprop_sig &lt;- mean(p_values &lt; 0.05)\ncat(sprintf(\"\\nProportion of p-values &lt; 0.05: %.3f (expected: 0.05)\\n\", prop_sig))\n\n\n\nProportion of p-values &lt; 0.05: 0.062 (expected: 0.05)\n\n\nCode\ncat(sprintf(\"Out of %d studies where null is TRUE, %d (%.1f%%) would be \\\"significant\\\" at p &lt; 0.05\\n\",\n            n_simulations, sum(p_values &lt; 0.05), 100 * prop_sig))\n\n\nOut of 1000 studies where null is TRUE, 62 (6.2%) would be \"significant\" at p &lt; 0.05\n\n\n\n\n\n\n\n\nImportantCritical Insight\n\n\n\nWhen the null hypothesis is true, p-values are uniformly distributed between 0 and 1. This means about 5% of studies will produce p &lt; 0.05 purely by chance—this is the Type I error rate (false positive rate).\nIf you use α = 0.05 as your threshold, you’re accepting that 5% of the time, you’ll incorrectly reject a true null hypothesis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-misconceptions",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-pvalue-misconceptions",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "12.3 Common P-Value Misconceptions",
    "text": "12.3 Common P-Value Misconceptions\nLet’s address the most common misinterpretations with specific examples from animal science.\n\n12.3.1 Misconception 1: “p = 0.03 means 3% chance null is true”\nWRONG. The p-value is \\(P(\\text{data} \\mid H_0)\\), not \\(P(H_0 \\mid \\text{data})\\).\nExample: In a swine growth study, you find p = 0.03 when comparing two diets. This means:\n\n✅ Correct: “If both diets were truly identical, we’d see a difference this large in only 3% of similar studies, just by chance.”\n❌ Incorrect: “There’s a 3% chance the diets are really the same.”\n\nTo know \\(P(H_0 \\mid \\text{data})\\), you’d need to know the prior probability that \\(H_0\\) is true—that requires Bayesian analysis.\n\n\n12.3.2 Misconception 2: “p = 0.06 means no effect”\nWRONG. Absence of evidence is not evidence of absence.\nExample: You test a new probiotic in beef cattle and get p = 0.06 for weight gain.\n\n❌ Incorrect: “The probiotic doesn’t work.”\n✅ Correct: “Our data don’t provide strong evidence against the null hypothesis. The effect might be real but small, or our sample size might be too small to detect it.”\n\nConsider these two scenarios that both give p = 0.06:\n\n\nCode\nset.seed(456)\n\n# Scenario A: Large sample, small effect\nn_large &lt;- 100\neffect_small &lt;- 3  # kg difference\ncattle_a_control &lt;- rnorm(n_large, mean = 600, sd = 40)\ncattle_a_treat &lt;- rnorm(n_large, mean = 600 + effect_small, sd = 40)\n\n# Scenario B: Small sample, large effect\nn_small &lt;- 15\neffect_large &lt;- 15  # kg difference\ncattle_b_control &lt;- rnorm(n_small, mean = 600, sd = 40)\ncattle_b_treat &lt;- rnorm(n_small, mean = 600 + effect_large, sd = 40)\n\n# T-tests\np_a &lt;- t.test(cattle_a_treat, cattle_a_control)$p.value\np_b &lt;- t.test(cattle_b_treat, cattle_b_control)$p.value\n\n# Visualize\ndata_a &lt;- tibble(weight = c(cattle_a_control, cattle_a_treat),\n                 group = rep(c(\"Control\", \"Probiotic\"), each = n_large),\n                 scenario = \"A\")\ndata_b &lt;- tibble(weight = c(cattle_b_control, cattle_b_treat),\n                 group = rep(c(\"Control\", \"Probiotic\"), each = n_small),\n                 scenario = \"B\")\n\nplot_a &lt;- ggplot(data_a, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 1.5) +\n  labs(title = sprintf(\"Scenario A: Large Sample, Small Effect\\nn=%d per group, p=%.3f\",\n                       n_large, p_a),\n       y = \"Weight (kg)\", x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(450, 750)\n\nplot_b &lt;- ggplot(data_b, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 2) +\n  labs(title = sprintf(\"Scenario B: Small Sample, Large Effect\\nn=%d per group, p=%.3f\",\n                       n_small, p_b),\n       y = \"Weight (kg)\", x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(450, 750)\n\nplot_a + plot_b\n\n\n\n\n\n\n\n\n\nBoth studies have p ≈ 0.05-0.07, but they tell very different stories! Always report effect sizes and confidence intervals, not just p-values.\n\n\n12.3.3 Misconception 3: “p &lt; 0.001 means a large/important effect”\nWRONG. Statistical significance ≠ practical significance.\nExample: In a massive database of 10,000 pigs, you find that pigs born on Mondays weigh 0.3 kg less at market than pigs born on other days (p &lt; 0.001).\n\nStatistically significant: Yes! With huge sample sizes, even tiny effects become “significant.”\nPractically significant: Probably not. A 0.3 kg difference is unlikely to matter economically.\n\n\n\nCode\n# Simulation: huge sample, tiny effect\nset.seed(789)\nn_huge &lt;- 5000\ntiny_effect &lt;- 0.3  # kg\n\nmonday_pigs &lt;- rnorm(n_huge, mean = 280, sd = 20)\nother_pigs &lt;- rnorm(n_huge, mean = 280 + tiny_effect, sd = 20)\n\ntest_huge &lt;- t.test(other_pigs, monday_pigs)\n\ncat(sprintf(\"Sample size: %d per group\\n\", n_huge))\n\n\nSample size: 5000 per group\n\n\nCode\ncat(sprintf(\"Mean difference: %.2f kg\\n\", mean(other_pigs) - mean(monday_pigs)))\n\n\nMean difference: -0.17 kg\n\n\nCode\ncat(sprintf(\"P-value: %.2e (highly significant!)\\n\", test_huge$p.value))\n\n\nP-value: 6.67e-01 (highly significant!)\n\n\nCode\ncat(sprintf(\"But effect size: %.2f kg (%.1f%% of mean weight)\\n\",\n            tiny_effect, 100 * tiny_effect / 280))\n\n\nBut effect size: 0.30 kg (0.1% of mean weight)\n\n\n\n\n\n\n\n\nTipAlways Ask Two Questions\n\n\n\n\nIs it statistically significant? (p-value)\nIs it practically significant? (effect size, confidence intervals, domain knowledge)\n\nA difference can be statistically significant without being biologically or economically meaningful.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-threshold",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-threshold",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "12.4 The Arbitrary Nature of p < 0.05",
    "text": "12.4 The Arbitrary Nature of p &lt; 0.05\nWhere did p &lt; 0.05 come from? It was popularized by statistician R.A. Fisher in the 1920s as a convenient convention, not a law of nature. He even cautioned against treating it as a bright-line rule.\n\n12.4.1 The Problem with Bright Lines\nConsider three studies comparing the same feed additive:\n\nStudy A: p = 0.049 → “Significant! The additive works!”\nStudy B: p = 0.051 → “Not significant. No evidence it works.”\nStudy C: p = 0.048 → “Significant! Definitely works!”\n\nDoes it really make sense that Study A and C lead to completely different conclusions than Study B, when the p-values are nearly identical?\n\n\nCode\n# Visualize the arbitrary threshold\ntibble(\n  study = c(\"A\", \"B\", \"C\"),\n  p_value = c(0.049, 0.051, 0.048),\n  significant = p_value &lt; 0.05\n) %&gt;%\n  ggplot(aes(x = study, y = p_value, fill = significant)) +\n  geom_col(alpha = 0.7) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_text(aes(label = sprintf(\"p = %.3f\", p_value)), vjust = -0.5, size = 5) +\n  annotate(\"text\", x = 2, y = 0.05, label = \"α = 0.05 threshold\",\n           color = \"red\", vjust = -0.5, size = 4) +\n  scale_fill_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"gray50\"),\n                    labels = c(\"TRUE\" = \"Significant\", \"FALSE\" = \"Not Significant\")) +\n  labs(\n    title = \"The Arbitrary Nature of p &lt; 0.05\",\n    subtitle = \"Should Study B really lead to a completely different conclusion?\",\n    x = \"Study\",\n    y = \"P-value\",\n    fill = \"\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Modern Perspectives\nMany scientific fields are moving away from rigid thresholds:\n\nReport exact p-values (e.g., p = 0.03, not just “p &lt; 0.05”)\nFocus on effect sizes and confidence intervals more than p-values\nConsider p-values as continuous measures of evidence, not binary decisions\nSome journals now ban the term “statistically significant” entirely\n\n\n\n\n\n\n\nNoteWhat We’ll Do in This Course\n\n\n\nWe’ll calculate p-values because they’re standard in animal science, but we’ll always interpret them alongside:\n\nEffect sizes (how big is the difference?)\nConfidence intervals (what’s the range of plausible values?)\nPractical significance (does the effect size matter in the real world?)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-causation",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-causation",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "13.1 The Gold Standard: Causation vs Association",
    "text": "13.1 The Gold Standard: Causation vs Association\n\nAssociation (correlation): Two variables change together, but we don’t know if one causes the other\nCausation: Changing one variable causes changes in the other\n\nThe single most important question when reading research: Can this study establish causation, or only association?\n\n13.1.1 Observational Studies\nIn an observational study, the researcher simply observes and records data without manipulating any variables. You measure what’s already happening naturally.\nExamples in animal science:\n\nCross-sectional survey: Measure backfat thickness in pigs across different farms at one point in time\nCohort study: Follow beef cattle over time and record which ones develop health issues\nCase-control study: Compare diet history of cattle with vs without liver abscesses\n\nStrengths:\n\nCan study things we can’t (or shouldn’t) experimentally manipulate\nOften cheaper and faster than experiments\nReflects real-world conditions\nGood for exploratory research and hypothesis generation\n\nLimitations:\n\nCannot establish causation (only association)\nConfounding variables can bias results (more on this below)\nDifficult to control for all alternative explanations\n\n\n13.1.1.1 Example: Farm Size and Pig Health\nImagine you survey 100 swine farms and find that larger farms have lower mortality rates.\nCan you conclude that increasing farm size causes better health outcomes?\nNo! Many confounding variables could explain this:\n\nLarger farms might have better veterinary care\nThey might use better biosecurity protocols\nThey might have more experienced managers\nThey might be in regions with different disease pressures\nHealthier farms might have expanded to become larger (reverse causation!)\n\n\n\nCode\n# Simulate observational data with confounding\nset.seed(321)\nn_farms &lt;- 100\n\n# Management quality is a confounder\nmanagement_quality &lt;- rnorm(n_farms, mean = 50, sd = 15)\n\n# Better-managed farms tend to be larger (confounding)\nfarm_size &lt;- 500 + 8 * management_quality + rnorm(n_farms, mean = 0, sd = 200)\n\n# Mortality is affected by management quality, NOT farm size directly\nmortality_rate &lt;- 8 - 0.08 * management_quality + rnorm(n_farms, mean = 0, sd = 1.5)\nmortality_rate &lt;- pmax(0, mortality_rate)  # Can't be negative\n\nfarm_data &lt;- tibble(\n  farm_id = 1:n_farms,\n  size = farm_size,\n  mortality = mortality_rate,\n  management = management_quality\n)\n\n# Naive analysis (ignoring confounding)\np3 &lt;- ggplot(farm_data, aes(x = size, y = mortality)) +\n  geom_point(alpha = 0.6, size = 3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", linewidth = 1.2) +\n  labs(\n    title = \"Observational Study: Farm Size vs Mortality Rate\",\n    subtitle = \"Appears that larger farms have lower mortality - but is this causal?\",\n    x = \"Farm Size (number of sows)\",\n    y = \"Mortality Rate (%)\"\n  )\n\nprint(p3)\n\n\n\n\n\n\n\n\n\nCode\ncor_size_mort &lt;- cor(farm_data$size, farm_data$mortality)\ncat(sprintf(\"\\nCorrelation between farm size and mortality: %.3f\\n\", cor_size_mort))\n\n\n\nCorrelation between farm size and mortality: -0.383\n\n\nCode\ncat(\"But this is driven by a confounder: management quality!\\n\")\n\n\nBut this is driven by a confounder: management quality!\n\n\nThis is association, not causation. To establish that farm size itself affects mortality, you’d need an experimental design.\n\n\n\n\n13.1.2 Experimental Studies\nIn an experimental study, the researcher actively manipulates one or more variables (the “treatment” or “intervention”) and measures the effect on an outcome.\nKey features:\n\nResearcher controls who receives which treatment\nIdeally uses randomization to assign treatments\nControls other variables to isolate the effect of the treatment\nCan establish causation (if designed properly)\n\nExamples in animal science:\n\nFeed trial: Randomly assign piglets to Diet A vs Diet B, measure growth\nDrug efficacy trial: Randomly assign cattle to antibiotic vs placebo, measure recovery\nBreeding experiment: Randomly assign boars to breeding groups, compare offspring traits\n\n\n13.1.2.1 Example: Does Lysine Supplementation Improve Growth?\nStudy design: Take 60 pigs, randomly assign 30 to a control diet and 30 to a lysine-supplemented diet. Raise them identically otherwise. Measure final weight.\n\n\nCode\n# Simulate experimental data\nset.seed(654)\nn_pigs &lt;- 60\n\n# Randomly assign treatment\npig_data &lt;- tibble(\n  pig_id = 1:n_pigs,\n  treatment = rep(c(\"Control\", \"Lysine\"), each = n_pigs/2),\n  # Lysine truly increases weight by ~8 kg\n  final_weight = ifelse(treatment == \"Control\",\n                       rnorm(n_pigs/2, mean = 115, sd = 12),\n                       rnorm(n_pigs/2, mean = 115 + 8, sd = 12))\n)\n\n# Visualize\np4 &lt;- ggplot(pig_data, aes(x = treatment, y = final_weight, fill = treatment)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, fill = \"red\") +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Lysine\" = \"#009E73\")) +\n  labs(\n    title = \"Experimental Study: Effect of Lysine Supplementation on Pig Growth\",\n    subtitle = \"Random assignment allows causal inference\",\n    y = \"Final Weight (kg)\",\n    x = \"Treatment Group\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(p4)\n\n\n\n\n\n\n\n\n\nCode\n# Test for difference\nexp_test &lt;- t.test(final_weight ~ treatment, data = pig_data)\ncat(sprintf(\"\\nMean difference: %.2f kg\\n\",\n            mean(pig_data$final_weight[pig_data$treatment == \"Lysine\"]) -\n            mean(pig_data$final_weight[pig_data$treatment == \"Control\"])))\n\n\n\nMean difference: 6.86 kg\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", exp_test$p.value))\n\n\nP-value: 0.0044\n\n\nCode\ncat(\"\\nBecause we RANDOMLY assigned treatments, we can conclude:\\n\")\n\n\n\nBecause we RANDOMLY assigned treatments, we can conclude:\n\n\nCode\ncat(\"Lysine supplementation CAUSES increased growth in pigs.\\n\")\n\n\nLysine supplementation CAUSES increased growth in pigs.\n\n\nWhy can we claim causation here?\nBecause of randomization (discussed in detail in the next section). Random assignment ensures that the two groups are equivalent on average at the start—any difference at the end must be due to the treatment.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-confounding",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-confounding",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "13.2 Confounding Variables",
    "text": "13.2 Confounding Variables\nA confounding variable (or confounder) is a variable that:\n\nIs associated with the treatment/exposure\nIndependently affects the outcome\nIs not on the causal pathway between treatment and outcome\n\nConfounding creates spurious associations—relationships that appear causal but aren’t.\n\n13.2.1 The Classic Example: Ice Cream and Drowning\nThis non-agricultural example illustrates confounding perfectly:\nObservation: Ice cream sales are strongly correlated with drowning deaths.\nConclusion: Ice cream causes drowning?! Should we ban ice cream to save lives?\nReality: Both are caused by a confounder: temperature/summer season\n\nHot weather → people buy ice cream\nHot weather → people go swimming → more drownings\n\nIce cream and drowning are associated but not causally related.\n\n\n\n\n\n\n\n\n\n\n\n13.2.2 Agricultural Example: Pasture Type and Weight Gain\nScenario: You visit 20 beef farms. Some use Pasture A (fescue), others use Pasture B (mixed grass). You record average daily gain (ADG) for cattle on each farm.\nObservation: Cattle on Pasture A have higher ADG.\nCan you conclude Pasture A is better?\nProbably not! Possible confounders:\n\nFarm quality: Better-managed farms might choose Pasture A (and also have better nutrition, genetics, health)\nSoil quality: Farms with better soil grow Pasture A, but soil quality also affects other forages\nRegion: Pasture A might be used in regions with better climate for cattle\nGenetics: Farms using Pasture A might also use superior genetics\n\n\n\nCode\n# Simulate pasture study with confounding\nset.seed(987)\nn_farms &lt;- 20\n\npasture_data &lt;- tibble(\n  farm = 1:n_farms,\n  pasture_type = rep(c(\"Fescue\", \"Mixed Grass\"), each = 10)\n) %&gt;%\n  mutate(\n    # Farm quality is confounder: better farms choose fescue\n    farm_quality = ifelse(pasture_type == \"Fescue\",\n                         rnorm(n_farms/2, mean = 75, sd = 8),\n                         rnorm(n_farms/2, mean = 60, sd = 8)),\n    # ADG depends on farm quality, NOT pasture type!\n    adg = 1.2 + 0.012 * farm_quality + rnorm(n_farms, mean = 0, sd = 0.15)\n  )\n\n# Visualize the confounding\np6 &lt;- ggplot(pasture_data, aes(x = farm_quality, y = adg, color = pasture_type, shape = pasture_type)) +\n  geom_point(size = 4, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.2) +\n  scale_color_manual(values = c(\"Fescue\" = \"#D55E00\", \"Mixed Grass\" = \"#0072B2\")) +\n  labs(\n    title = \"Confounding Example: Farm Quality Affects Both Pasture Choice and ADG\",\n    subtitle = \"Better farms choose fescue AND have higher ADG (but pasture isn't the cause)\",\n    x = \"Farm Quality Score\",\n    y = \"Average Daily Gain (kg/day)\",\n    color = \"Pasture Type\",\n    shape = \"Pasture Type\"\n  ) +\n  theme(legend.position = \"top\")\n\nprint(p6)\n\n\n\n\n\n\n\n\n\nCode\n# Naive comparison\npasture_data %&gt;%\n  group_by(pasture_type) %&gt;%\n  summarise(mean_adg = mean(adg), .groups = 'drop') %&gt;%\n  knitr::kable(digits = 3, col.names = c(\"Pasture Type\", \"Mean ADG (kg/day)\"))\n\n\n\n\n\nPasture Type\nMean ADG (kg/day)\n\n\n\n\nFescue\n2.028\n\n\nMixed Grass\n2.026\n\n\n\n\n\nThe fescue group has higher ADG, but it’s because better farms choose fescue—not because fescue itself is superior.\n\n\n\n\n\n\nWarningHow to Address Confounding\n\n\n\nIn observational studies:\n\nStatistical adjustment (multiple regression, matching, stratification)\nCareful measurement of potential confounders\nAcknowledge limitations in conclusions\n\nIn experimental studies:\n\nRandomization (the gold standard—discussed next!)\nBlocking/stratification\nStandardizing all other conditions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-randomization",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-randomization",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "14.1 Why Randomization is Powerful",
    "text": "14.1 Why Randomization is Powerful\nRandom assignment ensures that treatment groups are balanced on all variables—both measured and unmeasured—on average.\nThis is crucial because:\n\nYou can’t measure every potential confounder\nYou don’t always know what the confounders are\nRandomization balances them automatically (in expectation)\n\n\n14.1.1 Mathematical Intuition\nWhen you randomly assign \\(n\\) animals to groups, every animal has an equal probability of being in any group. This means:\n\\[\nE[\\text{Confounder}_{\\text{Treatment}}] = E[\\text{Confounder}_{\\text{Control}}]\n\\]\nWhere \\(E[\\cdot]\\) denotes expected value (average across many repetitions).\nIn plain English: On average, the treatment and control groups will have the same distribution of age, weight, genetics, health status, etc.—even if you don’t measure these variables!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-rct-features",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-rct-features",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "14.2 Key Features of RCTs",
    "text": "14.2 Key Features of RCTs\n\n14.2.1 1. Random Assignment\nNot “haphazard” or “arbitrary”—random using a chance mechanism (coin flip, random number generator, etc.).\nExample: 60 pigs, 30 to each group\n\n\nCode\nset.seed(2025)\n\n# Start with 60 pigs with various characteristics\npigs &lt;- tibble(\n  pig_id = 1:60,\n  initial_weight = rnorm(60, mean = 25, sd = 4),\n  age_days = round(runif(60, min = 50, max = 70)),\n  sex = sample(c(\"Male\", \"Female\"), 60, replace = TRUE),\n  litter = sample(1:15, 60, replace = TRUE)\n)\n\n# RANDOMLY assign to treatment\npigs &lt;- pigs %&gt;%\n  mutate(treatment = sample(rep(c(\"Control\", \"Probiotic\"), each = 30)))\n\n# Check balance\npigs %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = mean(initial_weight),\n    mean_age = mean(age_days),\n    prop_male = mean(sex == \"Male\"),\n    .groups = 'drop'\n  ) %&gt;%\n  knitr::kable(digits = 2,\n               col.names = c(\"Treatment\", \"N\", \"Mean Weight (kg)\",\n                            \"Mean Age (days)\", \"Proportion Male\"))\n\n\n\n\n\nTreatment\nN\nMean Weight (kg)\nMean Age (days)\nProportion Male\n\n\n\n\nControl\n30\n25.19\n60.97\n0.47\n\n\nProbiotic\n30\n25.89\n59.77\n0.37\n\n\n\n\n\nNotice how the groups are similar on all measured characteristics—that’s randomization working!\n\n\n14.2.2 2. Control Group\nThe control group provides the counterfactual: what would have happened without the treatment?\nTypes of controls:\n\nNegative control: No treatment (or placebo)\nPositive control: Standard treatment (if testing a new alternative)\nMultiple controls: Compare several treatments\n\n\n\n14.2.3 3. Blinding (when possible)\nBlinding means keeping the treatment assignment hidden to reduce bias:\n\nSingle-blind: Animals (or caretakers) don’t know which group receives which treatment\nDouble-blind: Neither caretakers nor researchers analyzing data know\n\nExample: In a drug trial for cattle, identical-looking pills (one with drug, one placebo) prevent the farm workers from treating groups differently.\nNote: Blinding isn’t always possible in animal science (e.g., you can’t hide which diet an animal is eating), but controlling for observer bias is still important.\n\n\n14.2.4 4. Standardization\nKeep all other conditions identical between groups:\n\nSame housing\nSame feeding schedule\nSame environmental conditions\nSame outcome measurement procedures",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-rct-example",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-rct-example",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "14.3 Example RCT: Feed Additive Trial in Swine",
    "text": "14.3 Example RCT: Feed Additive Trial in Swine\nResearch question: Does a novel feed additive improve average daily gain (ADG) in growing pigs?\nDesign:\n\nPopulation: 120 pigs (60-day-old, weaned)\nRandomization: Randomly assign 60 to control diet, 60 to additive diet\nControl: Standard corn-soybean diet\nTreatment: Same diet + 0.5% additive\nBlinding: Farm workers don’t know which pens get which diet (feed is labeled A/B)\nStandardization: All pigs housed in identical pens, same schedule, same health protocols\nDuration: 90 days\nOutcome: Average daily gain (kg/day)\n\n\n\nCode\nset.seed(111)\n\n# Simulate RCT data\nrct_pigs &lt;- tibble(\n  pig_id = 1:120,\n  # Randomize first\n  treatment = sample(rep(c(\"Control\", \"Additive\"), each = 60)),\n  # Baseline characteristics are balanced (due to randomization)\n  initial_weight = rnorm(120, mean = 20, sd = 3),\n  # Outcome: additive truly improves ADG by 0.05 kg/day\n  adg = ifelse(treatment == \"Control\",\n              rnorm(60, mean = 0.75, sd = 0.10),\n              rnorm(60, mean = 0.75 + 0.05, sd = 0.10))\n)\n\n# Visualize\np7 &lt;- ggplot(rct_pigs, aes(x = treatment, y = adg, fill = treatment)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 5, fill = \"white\", color = \"black\") +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Additive\" = \"#56B4E9\")) +\n  labs(\n    title = \"RCT Results: Feed Additive Effect on Average Daily Gain\",\n    subtitle = \"White diamond = group mean\",\n    y = \"Average Daily Gain (kg/day)\",\n    x = \"Treatment Group\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(p7)\n\n\n\n\n\n\n\n\n\nCode\n# Statistical test\nrct_test &lt;- t.test(adg ~ treatment, data = rct_pigs)\neffect_size &lt;- mean(rct_pigs$adg[rct_pigs$treatment == \"Additive\"]) -\n               mean(rct_pigs$adg[rct_pigs$treatment == \"Control\"])\n\ncat(sprintf(\"\\nControl mean ADG: %.3f kg/day\\n\",\n            mean(rct_pigs$adg[rct_pigs$treatment == \"Control\"])))\n\n\n\nControl mean ADG: 0.767 kg/day\n\n\nCode\ncat(sprintf(\"Additive mean ADG: %.3f kg/day\\n\",\n            mean(rct_pigs$adg[rct_pigs$treatment == \"Additive\"])))\n\n\nAdditive mean ADG: 0.786 kg/day\n\n\nCode\ncat(sprintf(\"Difference: %.3f kg/day\\n\", effect_size))\n\n\nDifference: 0.019 kg/day\n\n\nCode\ncat(sprintf(\"95%% CI: [%.3f, %.3f]\\n\", rct_test$conf.int[1], rct_test$conf.int[2]))\n\n\n95% CI: [-0.019, 0.056]\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", rct_test$p.value))\n\n\nP-value: 0.3196\n\n\nCode\ncat(\"\\n✓ Because of RANDOM ASSIGNMENT, we can conclude:\\n\")\n\n\n\n✓ Because of RANDOM ASSIGNMENT, we can conclude:\n\n\nCode\ncat(\"  The additive CAUSES a ~0.05 kg/day increase in growth rate.\\n\")\n\n\n  The additive CAUSES a ~0.05 kg/day increase in growth rate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#sec-rct-limitations",
    "href": "chapters/part2-ch01-statistical_foundations.html#sec-rct-limitations",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "14.4 Limitations of RCTs",
    "text": "14.4 Limitations of RCTs\nDespite being the gold standard, RCTs have limitations:\n\nCost: Experiments are expensive and time-consuming\nEthics: Some treatments can’t be tested experimentally (e.g., exposing animals to disease)\nPracticality: Long-term outcomes (years) may be infeasible\nExternal validity: Controlled conditions may not reflect real-world settings\nSample size: May need large numbers to detect small effects\n\nWhen RCTs aren’t possible, observational studies remain valuable—but we must be cautious about causal claims and carefully consider confounding.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#main-concepts",
    "href": "chapters/part2-ch01-statistical_foundations.html#main-concepts",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "15.1 Main Concepts",
    "text": "15.1 Main Concepts\n\n\n\n\n\n\nTip1. Two Statistical Philosophies\n\n\n\n\nFrequentist: Probability as long-run frequency; focus on \\(P(\\text{data} \\mid H_0)\\)\nBayesian: Probability as degree of belief; focus on \\(P(H_0 \\mid \\text{data})\\)\nThis course uses frequentist methods (standard in animal science)\n\n\n\n\n\n\n\n\n\nTip2. P-Values Are Widely Misunderstood\n\n\n\n\nDefinition: \\(p = P(\\text{data as extreme or more} \\mid H_0 \\text{ true})\\)\nNOT the probability the null hypothesis is true\nNOT the size or importance of an effect\np &lt; 0.05 is an arbitrary convention, not a law of nature\nAlways report effect sizes and confidence intervals alongside p-values\n\n\n\n\n\n\n\n\n\nTip3. Study Design Determines What You Can Conclude\n\n\n\n\nObservational studies: Can show association, NOT causation (confounding!)\nExperimental studies: Can establish causation (if designed properly)\nConfounding variables create spurious associations in observational data\n\n\n\n\n\n\n\n\n\nTip4. Randomization is Powerful\n\n\n\n\nRCTs are the gold standard for causal inference\nRandom assignment balances confounders automatically (on average)\nControl groups provide the counterfactual\nBut RCTs have limitations (cost, ethics, practicality)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#looking-ahead",
    "href": "chapters/part2-ch01-statistical_foundations.html#looking-ahead",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "15.2 Looking Ahead",
    "text": "15.2 Looking Ahead\nNext week, we’ll move from big-picture philosophy to practical tools: how to describe and summarize data using descriptive statistics and exploratory data analysis. We’ll learn to:\n\nCalculate and interpret measures of central tendency and variability\nVisualize distributions effectively\nIdentify outliers and unusual patterns\nCreate publication-quality summary tables\n\nThese foundational skills will prepare us for inferential statistics (hypothesis testing, confidence intervals, regression) in subsequent weeks.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#reflection-questions",
    "href": "chapters/part2-ch01-statistical_foundations.html#reflection-questions",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "15.3 Reflection Questions",
    "text": "15.3 Reflection Questions\nBefore next week’s class, think about:\n\nFind a recent paper in your area of animal science. Is it observational or experimental? If observational, what are potential confounders?\nLook at the p-values reported in the paper. Are effect sizes and confidence intervals also reported? If not, what information is missing?\nIf the paper claims causation, is that claim justified by the study design?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#additional-resources",
    "href": "chapters/part2-ch01-statistical_foundations.html#additional-resources",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "15.4 Additional Resources",
    "text": "15.4 Additional Resources\n\n15.4.1 Recommended Reading\n\nASA Statement on P-values and Statistical Significance (2016) – required reading\nGreenland et al. (2016): “Statistical tests, P values, confidence intervals, and power: a guide to misinterpretations” – comprehensive list of common errors\nIoannidis (2005): “Why Most Published Research Findings Are False” – provocative but important\n\n\n\n15.4.2 Videos\n\nStatQuest by Josh Starmer (YouTube): “P-values, clearly explained”\n“Dance of the p-values” (YouTube): Visual demonstration of p-value behavior\n\n\n\n15.4.3 Books\n\nThe Lady Tasting Tea by David Salsburg – history of statistics, very readable\nNaked Statistics by Charles Wheelan – conceptual introduction, no equations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch01-statistical_foundations.html#session-info",
    "href": "chapters/part2-ch01-statistical_foundations.html#session-info",
    "title": "9  Week 1: Statistical Foundations and Study Design",
    "section": "15.5 Session Info",
    "text": "15.5 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.4.0    patchwork_1.3.2 broom_1.0.7     lubridate_1.9.3\n [5] forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4     purrr_1.0.4    \n [9] readr_2.1.5     tidyr_1.3.1     tibble_3.2.1    ggplot2_4.0.0  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     xml2_1.3.6         lattice_0.22-6    \n [5] stringi_1.8.4      hms_1.1.3          digest_0.6.37      magrittr_2.0.3    \n [9] evaluate_1.0.1     grid_4.4.2         timechange_0.3.0   RColorBrewer_1.1-3\n[13] fastmap_1.2.0      Matrix_1.7-1       jsonlite_1.8.9     backports_1.5.0   \n[17] mgcv_1.9-1         fansi_1.0.6        viridisLite_0.4.2  textshaping_0.4.0 \n[21] cli_3.6.4          rlang_1.1.6        splines_4.4.2      withr_3.0.2       \n[25] yaml_2.3.10        tools_4.4.2        tzdb_0.4.0         kableExtra_1.4.0  \n[29] vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4    htmlwidgets_1.6.4 \n[33] pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.6       glue_1.8.0        \n[37] systemfonts_1.3.1  xfun_0.53          tidyselect_1.2.1   rstudioapi_0.17.1 \n[41] knitr_1.49         farver_2.1.2       nlme_3.1-166       htmltools_0.5.8.1 \n[45] labeling_0.4.3     rmarkdown_2.29     svglite_2.2.1      compiler_4.4.2    \n[49] S7_0.2.0          \n\n\n\nEnd of Week 1: Statistical Foundations and Study Design",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Week 1: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html",
    "href": "chapters/part2-ch02-descriptive_statistics.html",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "",
    "text": "11 Introduction: The Foundation of Data Analysis\nBefore you can run sophisticated statistical tests or build complex models, you must understand your data. This seemingly simple step is where many analyses go wrong. Jumping straight to hypothesis tests without thoroughly exploring your data is like performing surgery without examining the patient first.\nConsider a swine nutritionist who receives data from a 12-week growth trial involving 200 pigs across four different diets. What should be the first step? Running an ANOVA? No! The first step is exploratory data analysis (EDA): looking at the data, understanding its structure, identifying patterns, and checking for potential issues.\nDescriptive statistics help us summarize data with numbers (means, standard deviations, percentiles), while exploratory data analysis uses visualization and summary techniques to understand patterns, spot outliers, and generate hypotheses.\nIn this chapter, we’ll learn to:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-mean",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-mean",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.1 The Mean (Arithmetic Average)",
    "text": "12.1 The Mean (Arithmetic Average)\nThe mean is the sum of all values divided by the number of observations. It’s the most commonly used measure of central tendency.\n\n12.1.1 Mathematical Definition\nFor a sample of \\(n\\) observations \\(x_1, x_2, \\ldots, x_n\\), the sample mean is:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nWhere:\n\n\\(\\bar{x}\\) (pronounced “x-bar”) = the sample mean\n\\(\\sum\\) = summation symbol (add up all values)\n\\(i=1\\) to \\(n\\) = index from the first to the \\(n\\)-th observation\n\\(x_i\\) = the \\(i\\)-th observation\n\n\n\n12.1.2 Example: Weaning Weights in Pigs\nSuppose we have 8 piglets with the following weaning weights (kg):\n\n\nCode\n# Weaning weights of 8 piglets\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# Calculate mean manually\nmean_manual &lt;- sum(weights) / length(weights)\n\n# Calculate mean using R function\nmean_r &lt;- mean(weights)\n\ncat(\"Piglet weights (kg):\", paste(weights, collapse = \", \"), \"\\n\")\n\n\nPiglet weights (kg): 6.2, 5.8, 6.5, 6, 5.9, 6.3, 6.1, 5.7 \n\n\nCode\ncat(sprintf(\"Manual calculation: (%.1f + %.1f + ... + %.1f) / 8 = %.2f kg\\n\",\n            weights[1], weights[2], weights[8], mean_manual))\n\n\nManual calculation: (6.2 + 5.8 + ... + 5.7) / 8 = 6.06 kg\n\n\nCode\ncat(sprintf(\"Using mean(): %.2f kg\\n\", mean_r))\n\n\nUsing mean(): 6.06 kg\n\n\n\n\n12.1.3 Properties of the Mean\nStrengths:\n\nUses all data points (every value contributes)\nAlgebraically convenient (works well in formulas)\nFamiliar and widely understood\n\nWeaknesses:\n\nSensitive to outliers: One extreme value can dramatically shift the mean\nRequires numerical data: Can’t use with categorical data\nNot robust: May not represent “typical” value if distribution is skewed\n\n\n\n12.1.4 The Mean and Outliers\nLet’s see how outliers affect the mean:\n\n\nCode\n# Normal piglet weights\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# One piglet has a data entry error (67 kg instead of 6.7 kg!)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean = 6.06 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(weights_with_outlier)))\n\n\n  Mean = 13.72 kg\n\n\nCode\ncat(sprintf(\"  Difference: %.2f kg\\n\\n\",\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Difference: 7.66 kg\n\n\nCode\ncat(\"The outlier increased the mean by ~7.5 kg!\\n\")\n\n\nThe outlier increased the mean by ~7.5 kg!\n\n\nCode\ncat(\"This clearly doesn't represent the 'typical' piglet weight.\\n\")\n\n\nThis clearly doesn't represent the 'typical' piglet weight.\n\n\nThis is why we need other measures of central tendency that are more robust to outliers.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-median",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-median",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.2 The Median",
    "text": "12.2 The Median\nThe median is the middle value when data are ordered from smallest to largest. Half the observations are below the median, half are above.\n\n12.2.1 How to Calculate the Median\n\nSort the data from smallest to largest\nIf \\(n\\) is odd: median = the middle value\nIf \\(n\\) is even: median = average of the two middle values\n\nMathematically, for sorted data \\(x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\):\n\\[\n\\text{Median} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd} \\\\\n\\frac{x_{(n/2)} + x_{(n/2+1)}}{2} & \\text{if } n \\text{ is even}\n\\end{cases}\n\\]\n\n\n12.2.2 Example: Median Calculation\n\n\nCode\n# Odd number of observations (9 pigs)\nweights_odd &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7, 6.4)\n\n# Even number of observations (8 pigs)\nweights_even &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\ncat(\"Odd sample (n=9):\\n\")\n\n\nOdd sample (n=9):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_odd), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.4, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (5th value): %.1f kg\\n\", median(weights_odd)))\n\n\n  Median (5th value): 6.1 kg\n\n\nCode\ncat(\"\\nEven sample (n=8):\\n\")\n\n\n\nEven sample (n=8):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_even), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (average of 4th and 5th): (%.1f + %.1f)/2 = %.2f kg\\n\",\n            sort(weights_even)[4], sort(weights_even)[5], median(weights_even)))\n\n\n  Median (average of 4th and 5th): (6.0 + 6.1)/2 = 6.05 kg\n\n\n\n\n12.2.3 The Median is Robust to Outliers\nLet’s revisit our outlier example:\n\n\nCode\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean:   6.06 kg\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg\\n\", median(normal_weights)))\n\n\n  Median: 6.05 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg (changed by %.2f kg)\\n\",\n            mean(weights_with_outlier),\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Mean:   13.72 kg (changed by 7.66 kg)\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg (changed by %.2f kg)\\n\",\n            median(weights_with_outlier),\n            median(weights_with_outlier) - median(normal_weights)))\n\n\n  Median: 6.15 kg (changed by 0.10 kg)\n\n\nCode\ncat(\"\\nThe median barely changed, while the mean shifted dramatically!\\n\")\n\n\n\nThe median barely changed, while the mean shifted dramatically!\n\n\n\n\n\n\n\n\nTipWhen to Use Mean vs Median\n\n\n\nUse the mean when:\n\nData are roughly symmetric (no strong skew)\nNo extreme outliers\nYou need the mathematical properties of the mean (e.g., for further calculations)\n\nUse the median when:\n\nData are skewed (right-skewed or left-skewed)\nOutliers are present\nYou want a measure resistant to extreme values\nReporting income, house prices, or other variables with long tails",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-mode",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-mode",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.3 The Mode",
    "text": "12.3 The Mode\nThe mode is the most frequently occurring value in a dataset. Unlike mean and median, the mode can be used with categorical data.\n\n12.3.1 Example: Mode in Categorical Data\n\n\nCode\n# Breed types in a beef herd\nbreeds &lt;- c(\"Angus\", \"Angus\", \"Hereford\", \"Angus\", \"Charolais\",\n            \"Angus\", \"Hereford\", \"Angus\", \"Angus\")\n\n# Find mode (most common breed)\nbreed_counts &lt;- table(breeds)\nmode_breed &lt;- names(breed_counts)[which.max(breed_counts)]\n\ncat(\"Breed counts:\\n\")\n\n\nBreed counts:\n\n\nCode\nprint(breed_counts)\n\n\nbreeds\n    Angus Charolais  Hereford \n        6         1         2 \n\n\nCode\ncat(sprintf(\"\\nMode: %s (most common breed)\\n\", mode_breed))\n\n\n\nMode: Angus (most common breed)\n\n\n\n\n12.3.2 Mode in Continuous Data\nFor continuous data, the mode is less useful because values rarely repeat exactly. Instead, we look at the peak of the distribution using histograms or density plots.\n\n\nCode\n# Birth weights of 100 calves\nset.seed(123)\ncalf_weights &lt;- rnorm(100, mean = 40, sd = 5)\n\n# Visualize\nggplot(tibble(weight = calf_weights), aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(calf_weights), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = mean(calf_weights) + 2, y = 0.07,\n           label = sprintf(\"Mean = %.1f kg\", mean(calf_weights)),\n           color = \"darkgreen\", hjust = 0) +\n  labs(\n    title = \"Distribution of Calf Birth Weights\",\n    subtitle = \"Red curve shows density; mode is near the peak\",\n    x = \"Birth Weight (kg)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Point: Mode\n\n\n\nThe mode is most useful for:\n\nCategorical variables (breed, sex, treatment group)\nDiscrete counts (number of piglets per litter)\nMultimodal distributions (distributions with multiple peaks, suggesting subgroups)\n\nFor continuous measurements, mean and median are usually more informative.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-skewness",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-skewness",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.4 Comparing Measures in Skewed Distributions",
    "text": "12.4 Comparing Measures in Skewed Distributions\nThe relationship between mean, median, and mode reveals the shape of the distribution.\n\n12.4.1 Symmetric Distribution\nWhen data are symmetric (like a normal distribution):\n\\[\n\\text{Mean} \\approx \\text{Median} \\approx \\text{Mode}\n\\]\n\n\nCode\nset.seed(456)\nsymmetric_data &lt;- rnorm(500, mean = 100, sd = 15)\n\np_sym &lt;- ggplot(tibble(x = symmetric_data), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(symmetric_data), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(symmetric_data), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  labs(title = \"Symmetric Distribution\",\n       subtitle = sprintf(\"Mean (green) = %.1f | Median (purple) = %.1f\",\n                         mean(symmetric_data), median(symmetric_data)),\n       x = \"Value\", y = \"Density\")\n\nprint(p_sym)\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Right-Skewed Distribution\nWhen data have a long tail to the right (positive skew):\n\\[\n\\text{Mean} &gt; \\text{Median} &gt; \\text{Mode}\n\\]\nThe mean is “pulled” toward the tail by extreme high values.\nExample: Days to market for pigs (most finish quickly, some take much longer)\n\n\nCode\nset.seed(789)\n# Simulate right-skewed data (e.g., days to market)\nright_skewed &lt;- rgamma(500, shape = 2, rate = 0.02)\n\np_right &lt;- ggplot(tibble(x = right_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(right_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(right_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(right_skewed), y = 0.012,\n           label = sprintf(\"Mean = %.1f\", mean(right_skewed)),\n           color = \"darkgreen\", hjust = -0.1, size = 3.5) +\n  annotate(\"text\", x = median(right_skewed), y = 0.011,\n           label = sprintf(\"Median = %.1f\", median(right_skewed)),\n           color = \"purple\", hjust = 1.1, size = 3.5) +\n  labs(title = \"Right-Skewed Distribution (Positive Skew)\",\n       subtitle = \"Mean &gt; Median (mean pulled toward long right tail)\",\n       x = \"Days to Market\", y = \"Density\")\n\nprint(p_right)\n\n\n\n\n\n\n\n\n\n\n\n12.4.3 Left-Skewed Distribution\nWhen data have a long tail to the left (negative skew):\n\\[\n\\text{Mean} &lt; \\text{Median} &lt; \\text{Mode}\n\\]\nExample: Carcass yield percentage (most are high, some are unusually low)\n\n\nCode\nset.seed(321)\n# Simulate left-skewed data\nleft_skewed &lt;- 100 - rgamma(500, shape = 2, rate = 0.2)\n\np_left &lt;- ggplot(tibble(x = left_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(left_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(left_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(left_skewed), y = 0.04,\n           label = sprintf(\"Mean = %.1f\", mean(left_skewed)),\n           color = \"darkgreen\", hjust = 1.1, size = 3.5) +\n  annotate(\"text\", x = median(left_skewed), y = 0.042,\n           label = sprintf(\"Median = %.1f\", median(left_skewed)),\n           color = \"purple\", hjust = -0.1, size = 3.5) +\n  labs(title = \"Left-Skewed Distribution (Negative Skew)\",\n       subtitle = \"Mean &lt; Median (mean pulled toward long left tail)\",\n       x = \"Carcass Yield (%)\", y = \"Density\")\n\nprint(p_left)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantRemember\n\n\n\n\nSymmetric: Mean ≈ Median\nRight-skewed: Mean &gt; Median (use median to describe center)\nLeft-skewed: Mean &lt; Median (use median to describe center)\n\nAlways visualize your data to understand its shape before choosing which measure to report!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-range",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-range",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.1 Range",
    "text": "13.1 Range\nThe range is the simplest measure of spread:\n\\[\n\\text{Range} = \\text{Maximum} - \\text{Minimum}\n\\]\n\n\nCode\n# Two herds with same mean, different range\nherd_a &lt;- c(5.8, 5.9, 6.0, 6.1, 6.2)\nherd_b &lt;- c(3.5, 5.0, 6.0, 7.0, 8.5)\n\ncat(\"Herd A:\", paste(herd_a, collapse = \", \"), \"\\n\")\n\n\nHerd A: 5.8, 5.9, 6, 6.1, 6.2 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_a), min(herd_a), max(herd_a), max(herd_a) - min(herd_a)))\n\n\n  Mean: 6.0 kg | Range: 5.8 - 6.2 kg (0.4 kg)\n\n\nCode\ncat(\"\\nHerd B:\", paste(herd_b, collapse = \", \"), \"\\n\")\n\n\n\nHerd B: 3.5, 5, 6, 7, 8.5 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_b), min(herd_b), max(herd_b), max(herd_b) - min(herd_b)))\n\n\n  Mean: 6.0 kg | Range: 3.5 - 8.5 kg (5.0 kg)\n\n\nLimitation: The range uses only two values (min and max) and is extremely sensitive to outliers. We need better measures.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-variance-sd",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-variance-sd",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.2 Variance and Standard Deviation",
    "text": "13.2 Variance and Standard Deviation\nThe variance and standard deviation are the most important measures of variability in statistics.\n\n13.2.1 Variance\nThe variance measures the average squared deviation from the mean:\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nWhere:\n\n\\(s^2\\) = sample variance\n\\(n\\) = sample size\n\\(x_i\\) = each observation\n\\(\\bar{x}\\) = sample mean\n\\((x_i - \\bar{x})\\) = deviation of observation \\(i\\) from the mean\n\\(n-1\\) = degrees of freedom (we use \\(n-1\\) instead of \\(n\\) for sample variance)\n\nWhy \\(n-1\\) instead of \\(n\\)? This is called Bessel’s correction. Using \\(n-1\\) makes the sample variance an unbiased estimator of the population variance. Since we estimated the mean from the same data, we “lose” one degree of freedom.\n\n\n13.2.2 Standard Deviation\nThe standard deviation is the square root of the variance:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\nWhy take the square root? Variance is in squared units (e.g., kg²), which is hard to interpret. Standard deviation is in the original units (kg), making it much more intuitive.\n\n\n13.2.3 Calculating by Hand\nLet’s calculate variance and SD step by step:\n\n\nCode\n# Piglet weights\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9)\n\n# Step 1: Calculate mean\nmean_w &lt;- mean(weights)\n\n# Step 2: Calculate deviations from mean\ndeviations &lt;- weights - mean_w\n\n# Step 3: Square the deviations\nsquared_devs &lt;- deviations^2\n\n# Step 4: Sum squared deviations\nsum_sq_devs &lt;- sum(squared_devs)\n\n# Step 5: Divide by n-1\nvariance &lt;- sum_sq_devs / (length(weights) - 1)\n\n# Step 6: Take square root for SD\nstd_dev &lt;- sqrt(variance)\n\n# Create summary table\ntibble(\n  Weight = weights,\n  Deviation = round(deviations, 2),\n  `Squared Dev` = round(squared_devs, 3)\n) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Variance Calculation: Step by Step\") %&gt;%\n  tab_source_note(sprintf(\"Mean = %.2f kg\", mean_w)) %&gt;%\n  tab_source_note(sprintf(\"Sum of squared deviations = %.3f\", sum_sq_devs)) %&gt;%\n  tab_source_note(sprintf(\"Variance = %.3f / %d = %.3f kg²\",\n                         sum_sq_devs, length(weights)-1, variance)) %&gt;%\n  tab_source_note(sprintf(\"Standard Deviation = √%.3f = %.3f kg\",\n                         variance, std_dev))\n\n\n\n\n\n\n\n\nVariance Calculation: Step by Step\n\n\nWeight\nDeviation\nSquared Dev\n\n\n\n\n6.2\n0.12\n0.014\n\n\n5.8\n-0.28\n0.078\n\n\n6.5\n0.42\n0.176\n\n\n6.0\n-0.08\n0.006\n\n\n5.9\n-0.18\n0.032\n\n\n\nMean = 6.08 kg\n\n\nSum of squared deviations = 0.308\n\n\nVariance = 0.308 / 4 = 0.077 kg²\n\n\nStandard Deviation = √0.077 = 0.277 kg\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare to R's built-in functions\ncat(sprintf(\"\\nUsing R functions:\\n\"))\n\n\n\nUsing R functions:\n\n\nCode\ncat(sprintf(\"  Variance: %.3f kg²\\n\", var(weights)))\n\n\n  Variance: 0.077 kg²\n\n\nCode\ncat(sprintf(\"  Standard Deviation: %.3f kg\\n\", sd(weights)))\n\n\n  Standard Deviation: 0.277 kg\n\n\n\n\n13.2.4 Interpreting Standard Deviation\nStandard deviation tells us, on average, how far observations deviate from the mean.\n\nSmall SD: Data are clustered tightly around the mean (low variability)\nLarge SD: Data are spread out widely (high variability)\n\n\n\nCode\nset.seed(999)\n\n# Generate two datasets with same mean, different SD\nlow_sd &lt;- rnorm(500, mean = 100, sd = 5)\nhigh_sd &lt;- rnorm(500, mean = 100, sd = 20)\n\np_low &lt;- ggplot(tibble(x = low_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(low_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"Low Variability: SD = %.1f\", sd(low_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_high &lt;- ggplot(tibble(x = high_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"darkorange\", alpha = 0.7) +\n  geom_vline(xintercept = mean(high_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"High Variability: SD = %.1f\", sd(high_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_low + p_high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipThe Empirical Rule (68-95-99.7 Rule)\n\n\n\nFor data that are approximately normally distributed:\n\nAbout 68% of values fall within 1 SD of the mean\nAbout 95% of values fall within 2 SD of the mean\nAbout 99.7% of values fall within 3 SD of the mean\n\nThis rule helps you quickly assess whether an observation is unusual.\n\n\n\n\nCode\n# Demonstrate empirical rule\nset.seed(2025)\ndata_norm &lt;- rnorm(10000, mean = 100, sd = 15)\nmean_val &lt;- 100\nsd_val &lt;- 15\n\nggplot(tibble(x = data_norm), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50,\n                 fill = \"lightblue\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1.5) +\n  # Mark mean\n  geom_vline(xintercept = mean_val, color = \"red\",\n             linetype = \"solid\", linewidth = 1.2) +\n  # Mark ±1 SD\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ±2 SD\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val),\n             color = \"orange\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ±3 SD\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val),\n             color = \"purple\", linetype = \"dashed\", linewidth = 1) +\n  # Annotations\n  annotate(\"text\", x = mean_val, y = 0.025, label = \"Mean\",\n           color = \"red\", fontface = \"bold\") +\n  annotate(\"text\", x = mean_val + sd_val, y = 0.020,\n           label = \"±1 SD\\n(68%)\", color = \"darkgreen\", size = 3) +\n  annotate(\"text\", x = mean_val + 2*sd_val, y = 0.015,\n           label = \"±2 SD\\n(95%)\", color = \"orange\", size = 3) +\n  annotate(\"text\", x = mean_val + 3*sd_val, y = 0.010,\n           label = \"±3 SD\\n(99.7%)\", color = \"purple\", size = 3) +\n  labs(\n    title = \"The Empirical Rule (68-95-99.7 Rule)\",\n    subtitle = \"For normal distributions: most data fall within 3 SD of the mean\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  xlim(25, 175)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-iqr",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-iqr",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.3 Interquartile Range (IQR)",
    "text": "13.3 Interquartile Range (IQR)\nThe interquartile range (IQR) is a robust measure of spread that’s not affected by outliers.\n\n13.3.1 Quartiles\nQuartiles divide data into four equal parts:\n\nQ1 (First quartile / 25th percentile): 25% of data are below this value\nQ2 (Second quartile / 50th percentile): The median\nQ3 (Third quartile / 75th percentile): 75% of data are below this value\n\n\\[\n\\text{IQR} = Q3 - Q1\n\\]\nThe IQR represents the range containing the middle 50% of the data.\n\n\nCode\n# Beef cattle weights\ncattle_weights &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\nq1 &lt;- quantile(cattle_weights, 0.25)\nq2 &lt;- quantile(cattle_weights, 0.50)  # Median\nq3 &lt;- quantile(cattle_weights, 0.75)\niqr_val &lt;- IQR(cattle_weights)\n\ncat(\"Cattle weights (kg):\", paste(cattle_weights, collapse = \", \"), \"\\n\\n\")\n\n\nCattle weights (kg): 450, 480, 490, 510, 520, 530, 540, 560, 580, 620 \n\n\nCode\ncat(sprintf(\"Q1 (25th percentile): %.1f kg\\n\", q1))\n\n\nQ1 (25th percentile): 495.0 kg\n\n\nCode\ncat(sprintf(\"Q2 (Median):          %.1f kg\\n\", q2))\n\n\nQ2 (Median):          525.0 kg\n\n\nCode\ncat(sprintf(\"Q3 (75th percentile): %.1f kg\\n\", q3))\n\n\nQ3 (75th percentile): 555.0 kg\n\n\nCode\ncat(sprintf(\"\\nIQR = Q3 - Q1 = %.1f - %.1f = %.1f kg\\n\", q3, q1, iqr_val))\n\n\n\nIQR = Q3 - Q1 = 555.0 - 495.0 = 60.0 kg\n\n\nCode\ncat(\"\\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\\n\")\n\n\n\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\n\n\n\n\n13.3.2 IQR is Robust to Outliers\n\n\nCode\n# Normal data\nnormal_data &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\n# Add extreme outlier\nwith_outlier &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 900)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg\\n\", sd(normal_data)))\n\n\n  SD:  50.1 kg\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg\\n\", IQR(normal_data)))\n\n\n  IQR: 60.0 kg\n\n\nCode\ncat(\"\\nWith outlier (900 kg):\\n\")\n\n\n\nWith outlier (900 kg):\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg (increased by %.1f kg)\\n\",\n            sd(with_outlier), sd(with_outlier) - sd(normal_data)))\n\n\n  SD:  126.8 kg (increased by 76.7 kg)\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg (increased by %.1f kg)\\n\",\n            IQR(with_outlier), IQR(with_outlier) - IQR(normal_data)))\n\n\n  IQR: 60.0 kg (increased by 0.0 kg)\n\n\nCode\ncat(\"\\nIQR is much more stable in the presence of outliers!\\n\")\n\n\n\nIQR is much more stable in the presence of outliers!\n\n\n\n\n\n\n\n\nTipWhen to Use SD vs IQR\n\n\n\nUse Standard Deviation when:\n\nData are approximately normal\nNo extreme outliers\nYou need mathematical properties of variance (e.g., for further statistical tests)\n\nUse IQR when:\n\nData are skewed\nOutliers are present\nYou want a robust, resistant measure of spread",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-histograms",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-histograms",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.1 Histograms",
    "text": "14.1 Histograms\nA histogram shows the distribution of a continuous variable by dividing the range into bins and counting observations in each bin.\n\n\nCode\n# Generate pig growth data\nset.seed(12345)\npig_weights &lt;- tibble(\n  weight = rnorm(200, mean = 115, sd = 18),\n  diet = sample(c(\"Control\", \"High Protein\"), 200, replace = TRUE)\n)\n\n# Basic histogram\np_hist1 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Pig Weights\",\n    subtitle = \"20 bins\",\n    x = \"Final Weight (kg)\",\n    y = \"Count\"\n  )\n\n# Histogram with density overlay\np_hist2 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20,\n                 fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(pig_weights$weight),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Histogram with Density Curve\",\n    subtitle = \"Red = density curve | Green = mean\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\"\n  )\n\np_hist1 + p_hist2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningChoosing the Number of Bins\n\n\n\nThe number of bins affects how the distribution looks:\n\nToo few bins: May hide important features\nToo many bins: May show too much noise\n\nCommon rules:\n\nSturges’ rule: \\(\\text{bins} \\approx \\log_2(n) + 1\\)\nSquare root rule: \\(\\text{bins} \\approx \\sqrt{n}\\)\nOr just experiment! Try different bin numbers and see what reveals patterns best\n\n\n\n\n\nCode\n# Show effect of bin number\np_few &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 5, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Few Bins (5)\", x = \"Weight (kg)\", y = \"Count\")\n\np_many &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Many Bins (50)\", x = \"Weight (kg)\", y = \"Count\")\n\np_few + p_many",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-boxplots",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-boxplots",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.2 Boxplots",
    "text": "14.2 Boxplots\nA boxplot (box-and-whisker plot) displays the distribution using five-number summary: minimum, Q1, median, Q3, and maximum.\n\n14.2.1 Anatomy of a Boxplot\n\n\nCode\n# Create sample data\nset.seed(456)\nsample_data &lt;- rnorm(100, mean = 100, sd = 15)\n\n# Calculate components\nq1 &lt;- quantile(sample_data, 0.25)\nmedian_val &lt;- median(sample_data)\nq3 &lt;- quantile(sample_data, 0.75)\niqr_val &lt;- IQR(sample_data)\nlower_whisker &lt;- max(min(sample_data), q1 - 1.5 * iqr_val)\nupper_whisker &lt;- min(max(sample_data), q3 + 1.5 * iqr_val)\n\n# Find outliers\noutliers &lt;- sample_data[sample_data &lt; lower_whisker | sample_data &gt; upper_whisker]\n\n# Create boxplot\nggplot(tibble(x = \"Data\", y = sample_data), aes(x = x, y = y)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 3) +\n  # Add labels\n  annotate(\"text\", x = 1.35, y = q1, label = sprintf(\"Q1 = %.1f\", q1),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = median_val, label = sprintf(\"Median = %.1f\", median_val),\n           hjust = 0, size = 4, color = \"darkgreen\", fontface = \"bold\") +\n  annotate(\"text\", x = 1.35, y = q3, label = sprintf(\"Q3 = %.1f\", q3),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = upper_whisker,\n           label = sprintf(\"Upper whisker = %.1f\", upper_whisker),\n           hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 1.35, y = lower_whisker,\n           label = sprintf(\"Lower whisker = %.1f\", lower_whisker),\n           hjust = 0, size = 3.5) +\n  # Add IQR bracket\n  annotate(\"segment\", x = 0.7, xend = 0.7, y = q1, yend = q3,\n           color = \"purple\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.65, y = (q1 + q3)/2,\n           label = sprintf(\"IQR = %.1f\", iqr_val),\n           angle = 90, vjust = 1, color = \"purple\", fontface = \"bold\") +\n  labs(\n    title = \"Anatomy of a Boxplot\",\n    subtitle = \"Red points are outliers (&gt; 1.5 × IQR from Q1 or Q3)\",\n    y = \"Value\",\n    x = \"\"\n  ) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n14.2.2 Comparing Groups with Boxplots\nBoxplots are excellent for comparing distributions across groups:\n\n\nCode\n# Simulate beef cattle data from different breeding programs\nset.seed(789)\ncattle_data &lt;- tibble(\n  program = rep(c(\"Program A\", \"Program B\", \"Program C\"), each = 60),\n  weight = c(\n    rnorm(60, mean = 580, sd = 45),  # Program A\n    rnorm(60, mean = 610, sd = 50),  # Program B\n    rnorm(60, mean = 595, sd = 35)   # Program C\n  )\n)\n\n# Boxplot comparison\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Final Weights by Breeding Program\",\n    subtitle = \"Red diamond = mean | Bold line = median | Box = IQR\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-density",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-density",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.3 Density Plots",
    "text": "14.3 Density Plots\nA density plot is a smoothed version of a histogram, showing the probability density function.\n\n\nCode\n# Compare distributions across groups\nggplot(cattle_data, aes(x = weight, fill = program)) +\n  geom_density(alpha = 0.5, linewidth = 1) +\n  geom_vline(data = cattle_data %&gt;% group_by(program) %&gt;%\n               summarise(mean_wt = mean(weight), .groups = 'drop'),\n             aes(xintercept = mean_wt, color = program),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Density Plots: Comparing Weight Distributions\",\n    subtitle = \"Dashed lines show group means\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\",\n    fill = \"Program\",\n    color = \"Program\"\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-violin",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-violin",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.4 Violin Plots",
    "text": "14.4 Violin Plots\nA violin plot combines a boxplot with a density plot, showing both summary statistics and the full distribution shape.\n\n\nCode\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_violin(alpha = 0.6, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.8, outlier.shape = NA) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Violin Plots: Distribution Shape + Boxplot\",\n    subtitle = \"Width shows density | Box shows quartiles | Red = mean\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-iqr-method",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-iqr-method",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.1 The IQR Method",
    "text": "15.1 The IQR Method\nThe most common method defines outliers as observations beyond:\n\\[\n\\begin{align}\n\\text{Lower fence} &= Q1 - 1.5 \\times \\text{IQR} \\\\\n\\text{Upper fence} &= Q3 + 1.5 \\times \\text{IQR}\n\\end{align}\n\\]\nThis is the definition used by boxplots.\n\n\nCode\n# Cattle weights with some outliers\nset.seed(111)\nweights_with_outliers &lt;- c(\n  rnorm(45, mean = 550, sd = 40),  # Normal cattle\n  c(350, 420, 720)                  # Outliers\n)\n\n# Calculate fences\nq1 &lt;- quantile(weights_with_outliers, 0.25)\nq3 &lt;- quantile(weights_with_outliers, 0.75)\niqr &lt;- IQR(weights_with_outliers)\nlower_fence &lt;- q1 - 1.5 * iqr\nupper_fence &lt;- q3 + 1.5 * iqr\n\n# Identify outliers\noutliers &lt;- weights_with_outliers[weights_with_outliers &lt; lower_fence |\n                                   weights_with_outliers &gt; upper_fence]\n\ncat(\"Outlier Detection Using IQR Method\\n\")\n\n\nOutlier Detection Using IQR Method\n\n\nCode\ncat(\"===================================\\n\")\n\n\n===================================\n\n\nCode\ncat(sprintf(\"Q1 = %.1f kg\\n\", q1))\n\n\nQ1 = 501.2 kg\n\n\nCode\ncat(sprintf(\"Q3 = %.1f kg\\n\", q3))\n\n\nQ3 = 564.0 kg\n\n\nCode\ncat(sprintf(\"IQR = %.1f kg\\n\", iqr))\n\n\nIQR = 62.8 kg\n\n\nCode\ncat(sprintf(\"\\nLower fence = Q1 - 1.5×IQR = %.1f - %.1f = %.1f kg\\n\",\n            q1, 1.5*iqr, lower_fence))\n\n\n\nLower fence = Q1 - 1.5×IQR = 501.2 - 94.3 = 406.9 kg\n\n\nCode\ncat(sprintf(\"Upper fence = Q3 + 1.5×IQR = %.1f + %.1f = %.1f kg\\n\",\n            q3, 1.5*iqr, upper_fence))\n\n\nUpper fence = Q3 + 1.5×IQR = 564.0 + 94.3 = 658.3 kg\n\n\nCode\ncat(sprintf(\"\\nOutliers detected: %s\\n\", paste(round(outliers, 1), collapse = \", \")))\n\n\n\nOutliers detected: 658.7, 350, 720\n\n\n\n\nCode\n# Visualize outliers\noutlier_data &lt;- tibble(\n  weight = weights_with_outliers,\n  is_outlier = weight &lt; lower_fence | weight &gt; upper_fence\n)\n\np_box_outlier &lt;- ggplot(outlier_data, aes(x = \"Cattle\", y = weight)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 4) +\n  geom_hline(yintercept = c(lower_fence, upper_fence),\n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = lower_fence,\n           label = sprintf(\"Lower fence = %.0f\", lower_fence),\n           color = \"red\", hjust = 0) +\n  annotate(\"text\", x = 1.3, y = upper_fence,\n           label = sprintf(\"Upper fence = %.0f\", upper_fence),\n           color = \"red\", hjust = 0) +\n  labs(title = \"Boxplot: Outliers in Red\",\n       y = \"Weight (kg)\", x = \"\") +\n  theme(axis.text.x = element_blank())\n\np_hist_outlier &lt;- ggplot(outlier_data, aes(x = weight, fill = is_outlier)) +\n  geom_histogram(bins = 20, color = \"white\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                    labels = c(\"Normal\", \"Outlier\")) +\n  labs(title = \"Histogram: Outliers Highlighted\",\n       x = \"Weight (kg)\", y = \"Count\", fill = \"\")\n\np_box_outlier + p_hist_outlier",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#sec-handling-outliers",
    "href": "chapters/part2-ch02-descriptive_statistics.html#sec-handling-outliers",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.2 What to Do with Outliers?",
    "text": "15.2 What to Do with Outliers?\nNEVER automatically delete outliers! Instead:\n\nInvestigate: Is it a data entry error? Measurement error? Legitimate extreme value?\nDocument: Record what you find and what you decide\nConsider:\n\nIf error: Correct if possible, or remove and document\nIf legitimate: Keep it! Report results with and without outliers if it’s influential\nIf different population: Analyze separately\n\n\n\n\n\n\n\n\nWarningImportant\n\n\n\nRemoving outliers just to get “better” p-values is data manipulation and scientifically dishonest. Always have a principled reason for any data exclusions and report them transparently.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#example-swine-growth-trial-summary",
    "href": "chapters/part2-ch02-descriptive_statistics.html#example-swine-growth-trial-summary",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "16.1 Example: Swine Growth Trial Summary",
    "text": "16.1 Example: Swine Growth Trial Summary\n\n\nCode\n# Simulate swine growth data\nset.seed(2025)\nswine_data &lt;- tibble(\n  diet = rep(c(\"Control\", \"High Protein\", \"High Energy\", \"Balanced\"), each = 50),\n  initial_weight = rnorm(200, mean = 25, sd = 3),\n  final_weight = initial_weight + rnorm(200, mean = 90, sd = 12) +\n    case_when(\n      diet == \"Control\" ~ 0,\n      diet == \"High Protein\" ~ 5,\n      diet == \"High Energy\" ~ 3,\n      diet == \"Balanced\" ~ 7\n    )\n) %&gt;%\n  mutate(weight_gain = final_weight - initial_weight)\n\n# Create comprehensive summary table\nsummary_table &lt;- swine_data %&gt;%\n  group_by(diet) %&gt;%\n  summarise(\n    N = n(),\n    Mean = mean(weight_gain),\n    SD = sd(weight_gain),\n    Median = median(weight_gain),\n    IQR = IQR(weight_gain),\n    Min = min(weight_gain),\n    Max = max(weight_gain),\n    .groups = 'drop'\n  )\n\n# Display with gt package\nsummary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Summary Statistics: Weight Gain by Diet\",\n    subtitle = \"12-week growth trial (n=200 pigs)\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Mean, SD, Median, IQR, Min, Max),\n    decimals = 1\n  ) %&gt;%\n  cols_label(\n    diet = \"Diet Treatment\",\n    N = \"n\",\n    Mean = \"Mean (kg)\",\n    SD = \"SD (kg)\",\n    Median = \"Median (kg)\",\n    IQR = \"IQR (kg)\",\n    Min = \"Min (kg)\",\n    Max = \"Max (kg)\"\n  ) %&gt;%\n  tab_source_note(\"SD = Standard Deviation; IQR = Interquartile Range\") %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.background.color = \"#f0f0f0\"\n  )\n\n\n\n\n\n\n\n\nSummary Statistics: Weight Gain by Diet\n\n\n12-week growth trial (n=200 pigs)\n\n\nDiet Treatment\nn\nMean (kg)\nSD (kg)\nMedian (kg)\nIQR (kg)\nMin (kg)\nMax (kg)\n\n\n\n\nBalanced\n50\n96.5\n11.8\n96.1\n12.0\n66.4\n131.0\n\n\nControl\n50\n89.0\n11.6\n89.0\n15.3\n55.8\n112.6\n\n\nHigh Energy\n50\n92.1\n12.1\n92.8\n15.6\n63.5\n116.6\n\n\nHigh Protein\n50\n96.2\n12.0\n96.6\n16.3\n63.5\n124.9\n\n\n\nSD = Standard Deviation; IQR = Interquartile Range",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#step-1-overall-summary-statistics",
    "href": "chapters/part2-ch02-descriptive_statistics.html#step-1-overall-summary-statistics",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.1 Step 1: Overall Summary Statistics",
    "text": "17.1 Step 1: Overall Summary Statistics\n\n\nCode\n# Overall summary of key variables\noverall_summary &lt;- feedlot_data %&gt;%\n  summarise(\n    `Sample Size` = n(),\n    across(c(initial_weight, final_weight, weight_gain, adg),\n           list(\n             Mean = ~mean(.),\n             SD = ~sd(.),\n             Median = ~median(.),\n             IQR = ~IQR(.),\n             Min = ~min(.),\n             Max = ~max(.)\n           ),\n           .names = \"{.col}_{.fn}\")\n  )\n\n# Reshape for display\noverall_summary %&gt;%\n  pivot_longer(-`Sample Size`, names_to = \"stat\", values_to = \"value\") %&gt;%\n  separate(stat, into = c(\"variable\", \"measure\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = measure, values_from = value) %&gt;%\n  mutate(variable = case_when(\n    variable == \"initial\" ~ \"Initial Weight (kg)\",\n    variable == \"final\" ~ \"Final Weight (kg)\",\n    variable == \"weight\" ~ \"Weight Gain (kg)\",\n    variable == \"adg\" ~ \"ADG (kg/day)\"\n  )) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Overall Summary Statistics\",\n             subtitle = sprintf(\"n = %d cattle\", overall_summary$`Sample Size`)) %&gt;%\n  fmt_number(columns = -variable, decimals = 2) %&gt;%\n  cols_label(variable = \"Variable\")\n\n\n\n\n\n\n\n\nOverall Summary Statistics\n\n\nn = 180 cattle\n\n\nSample Size\nVariable\nweight\ngain\nMean\nSD\nMedian\nIQR\nMin\nMax\n\n\n\n\n180.00\nInitial Weight (kg)\n298.21808, 35.59733, 301.28524, 49.67098, 176.78933, 381.05291\n\n\n\n\n\n\n\n\n\n180.00\nFinal Weight (kg)\n543.05239, 48.61542, 545.12486, 61.43552, 400.31501, 684.27656\n\n\n\n\n\n\n\n\n\n180.00\nWeight Gain (kg)\n\n244.83430, 34.25836, 245.22667, 42.03852, 139.47315, 331.21941\n\n\n\n\n\n\n\n\n180.00\nADG (kg/day)\n\n\n1.360191\n0.1903242\n1.36237\n0.2335473\n0.7748508\n1.840108",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#step-2-visualize-distributions",
    "href": "chapters/part2-ch02-descriptive_statistics.html#step-2-visualize-distributions",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.2 Step 2: Visualize Distributions",
    "text": "17.2 Step 2: Visualize Distributions\n\n\nCode\n# Create multiple visualizations\np1 &lt;- ggplot(feedlot_data, aes(x = initial_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"steelblue\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$initial_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Initial Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np2 &lt;- ggplot(feedlot_data, aes(x = final_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkorange\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$final_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Final Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np3 &lt;- ggplot(feedlot_data, aes(x = weight_gain)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkgreen\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$weight_gain),\n             linetype = \"dashed\", color = \"darkblue\", linewidth = 1) +\n  labs(title = \"Weight Gain Distribution\", x = \"Weight Gain (kg)\", y = \"Density\")\n\np4 &lt;- ggplot(feedlot_data, aes(x = adg)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"purple\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$adg),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"ADG Distribution\", x = \"ADG (kg/day)\", y = \"Density\")\n\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Distribution of Weight Variables\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#step-3-compare-groups",
    "href": "chapters/part2-ch02-descriptive_statistics.html#step-3-compare-groups",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.3 Step 3: Compare Groups",
    "text": "17.3 Step 3: Compare Groups\n\n\nCode\n# Summary by feed program\nfeed_summary &lt;- feedlot_data %&gt;%\n  group_by(feed_program) %&gt;%\n  summarise(\n    n = n(),\n    Mean_ADG = mean(adg),\n    SD_ADG = sd(adg),\n    Median_ADG = median(adg),\n    IQR_ADG = IQR(adg),\n    .groups = 'drop'\n  )\n\nprint(\"Summary by Feed Program:\")\n\n\n[1] \"Summary by Feed Program:\"\n\n\nCode\nfeed_summary %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = -c(feed_program, n), decimals = 3) %&gt;%\n  cols_label(feed_program = \"Feed Program\",\n             n = \"n\",\n             Mean_ADG = \"Mean ADG\",\n             SD_ADG = \"SD\",\n             Median_ADG = \"Median ADG\",\n             IQR_ADG = \"IQR\")\n\n\n\n\n\n\n\n\nFeed Program\nn\nMean ADG\nSD\nMedian ADG\nIQR\n\n\n\n\nEnhanced\n90\n1.377\n0.188\n1.368\n0.231\n\n\nStandard\n90\n1.344\n0.192\n1.353\n0.250\n\n\n\n\n\n\n\nCode\n# Visualize comparisons\np_feed &lt;- ggplot(feedlot_data, aes(x = feed_program, y = adg, fill = feed_program)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"ADG by Feed Program\", x = \"Feed Program\",\n       y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_sex &lt;- ggplot(feedlot_data, aes(x = sex, y = adg, fill = sex)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ADG by Sex\", x = \"Sex\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_breed &lt;- ggplot(feedlot_data, aes(x = breed, y = adg, fill = breed)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"ADG by Breed\", x = \"Breed\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_feed + p_sex + p_breed +\n  plot_annotation(title = \"Comparing ADG Across Groups\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#step-4-check-for-outliers",
    "href": "chapters/part2-ch02-descriptive_statistics.html#step-4-check-for-outliers",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.4 Step 4: Check for Outliers",
    "text": "17.4 Step 4: Check for Outliers\n\n\nCode\n# Identify outliers in ADG\nq1_adg &lt;- quantile(feedlot_data$adg, 0.25)\nq3_adg &lt;- quantile(feedlot_data$adg, 0.75)\niqr_adg &lt;- IQR(feedlot_data$adg)\nlower_adg &lt;- q1_adg - 1.5 * iqr_adg\nupper_adg &lt;- q3_adg + 1.5 * iqr_adg\n\noutliers_adg &lt;- feedlot_data %&gt;%\n  filter(adg &lt; lower_adg | adg &gt; upper_adg)\n\ncat(sprintf(\"Outlier Detection for ADG (kg/day)\\n\"))\n\n\nOutlier Detection for ADG (kg/day)\n\n\nCode\ncat(sprintf(\"Lower fence: %.3f\\n\", lower_adg))\n\n\nLower fence: 0.891\n\n\nCode\ncat(sprintf(\"Upper fence: %.3f\\n\", upper_adg))\n\n\nUpper fence: 1.825\n\n\nCode\ncat(sprintf(\"\\nNumber of outliers: %d out of %d (%.1f%%)\\n\",\n            nrow(outliers_adg), nrow(feedlot_data),\n            100 * nrow(outliers_adg) / nrow(feedlot_data)))\n\n\n\nNumber of outliers: 4 out of 180 (2.2%)\n\n\nCode\nif(nrow(outliers_adg) &gt; 0) {\n  cat(\"\\nOutlier animals:\\n\")\n  print(outliers_adg %&gt;%\n          select(animal_id, breed, sex, feed_program, adg) %&gt;%\n          arrange(adg))\n}\n\n\n\nOutlier animals:\n# A tibble: 4 × 5\n  animal_id breed     sex    feed_program   adg\n      &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1        66 Hereford  Heifer Standard     0.775\n2       107 Hereford  Heifer Enhanced     0.859\n3       170 Angus     Steer  Enhanced     1.82 \n4        70 Charolais Steer  Standard     1.84",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#looking-ahead",
    "href": "chapters/part2-ch02-descriptive_statistics.html#looking-ahead",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.1 Looking Ahead",
    "text": "18.1 Looking Ahead\nNext week, we’ll build on these foundations by learning about:\n\nProbability distributions (especially the normal distribution)\nThe Central Limit Theorem (why means are normally distributed)\nStandard error vs standard deviation\nConfidence intervals (quantifying uncertainty)\nIntroduction to sampling distributions\n\nThese concepts will bridge descriptive statistics to inferential statistics, allowing us to make conclusions about populations based on samples.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#reflection-questions",
    "href": "chapters/part2-ch02-descriptive_statistics.html#reflection-questions",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.2 Reflection Questions",
    "text": "18.2 Reflection Questions\nBefore next week, consider:\n\nFind a dataset from your research (or use one from class). Perform a complete EDA following the steps in this chapter.\nIn published papers from your field, are both mean/SD and median/IQR reported? Are visualizations included?\nThink about a variable you measure in your work. What would you consider an outlier? What would you do if you found one?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#additional-resources",
    "href": "chapters/part2-ch02-descriptive_statistics.html#additional-resources",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.3 Additional Resources",
    "text": "18.3 Additional Resources\n\n18.3.1 R Packages for Descriptive Statistics\n\nskimr: Quick, comprehensive summaries of datasets\nsummarytools: Detailed univariate and bivariate summaries\npsych: Descriptive statistics for psychological/survey data\nDataExplorer: Automated EDA reports\n\n\n\n18.3.2 Recommended Reading\n\n“Exploratory Data Analysis” by John Tukey (1977) - the classic text\n“Data Visualization: A Practical Introduction” by Kieran Healy\n“Fundamentals of Biostatistics” by Bernard Rosner - Chapters 2-3\n\n\n\n18.3.3 Online Resources\n\nR for Data Science (2e): Chapters on data transformation and visualization\nggplot2 book by Hadley Wickham: Comprehensive guide to data visualization",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch02-descriptive_statistics.html#session-info",
    "href": "chapters/part2-ch02-descriptive_statistics.html#session-info",
    "title": "10  Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.4 Session Info",
    "text": "18.4 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.4.0    gt_1.1.0        patchwork_1.3.2 broom_1.0.7    \n [5] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [9] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n[13] ggplot2_4.0.0   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] xml2_1.3.6         yaml_2.3.10        fastmap_1.2.0      R6_2.5.1          \n [9] labeling_0.4.3     generics_0.1.3     knitr_1.49         backports_1.5.0   \n[13] htmlwidgets_1.6.4  pillar_1.9.0       RColorBrewer_1.1-3 tzdb_0.4.0        \n[17] rlang_1.1.6        utf8_1.2.4         stringi_1.8.4      xfun_0.53         \n[21] sass_0.4.9         fs_1.6.5           S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.4          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.4.2         hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.1     glue_1.8.0         farver_2.1.2       fansi_1.0.6       \n[37] rmarkdown_2.29     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\nEnd of Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 2: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html",
    "href": "chapters/part2-ch03-probability_distributions.html",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "",
    "text": "12 Introduction: From Description to Inference\nIn the first two weeks, we learned to describe data: calculating means, standard deviations, creating visualizations, and understanding distributions. But animal scientists rarely just describe their sample—we want to make inferences about the broader population.\nImagine you’re testing a new feed additive in dairy cattle. You can’t test every dairy cow in the world, so you select a sample of 50 cows, randomly assign 25 to each treatment, and measure milk production. Your key questions are:\nThese are questions of statistical inference, and they all rely on understanding probability distributions.\nThis week, we bridge the gap between descriptive and inferential statistics by exploring:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#sec-normal-properties",
    "href": "chapters/part2-ch03-probability_distributions.html#sec-normal-properties",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "13.1 Properties of the Normal Distribution",
    "text": "13.1 Properties of the Normal Distribution\nA normal distribution is characterized by:\n\nSymmetric bell-shaped curve\nDefined by two parameters:\n\n\\(\\mu\\) (mu): The population mean (center of the distribution)\n\\(\\sigma\\) (sigma): The population standard deviation (spread of the distribution)\n\nNotation: \\(X \\sim N(\\mu, \\sigma^2)\\) means “X follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)”\n\n\n13.1.1 Mathematical Formula\nThe probability density function (PDF) of the normal distribution is:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\nDon’t worry about memorizing this formula! R will do all calculations for us. What matters is understanding the properties.\n\n\n13.1.2 Key Properties\n\n\nCode\n# Generate normal distributions with different parameters\nx &lt;- seq(-10, 20, length.out = 500)\n\n# Different means, same SD\ndist1 &lt;- dnorm(x, mean = 0, sd = 2)\ndist2 &lt;- dnorm(x, mean = 5, sd = 2)\ndist3 &lt;- dnorm(x, mean = 10, sd = 2)\n\n# Same mean, different SDs\ndist4 &lt;- dnorm(x, mean = 5, sd = 1)\ndist5 &lt;- dnorm(x, mean = 5, sd = 2)\ndist6 &lt;- dnorm(x, mean = 5, sd = 3)\n\n# Create data frames for plotting\ndf_means &lt;- bind_rows(\n  tibble(x = x, density = dist1, distribution = \"N(0, 2²)\"),\n  tibble(x = x, density = dist2, distribution = \"N(5, 2²)\"),\n  tibble(x = x, density = dist3, distribution = \"N(10, 2²)\")\n)\n\ndf_sds &lt;- bind_rows(\n  tibble(x = x, density = dist4, distribution = \"N(5, 1²)\"),\n  tibble(x = x, density = dist5, distribution = \"N(5, 2²)\"),\n  tibble(x = x, density = dist6, distribution = \"N(5, 3²)\")\n)\n\np1 &lt;- ggplot(df_means, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Effect of Changing Mean (μ)\",\n    subtitle = \"Same spread (σ = 2), different centers\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme(legend.position = \"top\")\n\np2 &lt;- ggplot(df_sds, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Effect of Changing Standard Deviation (σ)\",\n    subtitle = \"Same center (μ = 5), different spreads\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme(legend.position = \"top\")\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\n\nChanging μ shifts the distribution left or right\nChanging σ changes the spread (wider or narrower)\nThe area under the entire curve always equals 1 (total probability = 100%)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#sec-empirical-rule",
    "href": "chapters/part2-ch03-probability_distributions.html#sec-empirical-rule",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "13.2 The Empirical Rule (68-95-99.7 Rule)",
    "text": "13.2 The Empirical Rule (68-95-99.7 Rule)\nFor any normal distribution:\n\nApproximately 68% of values fall within 1 SD of the mean (\\(\\mu \\pm 1\\sigma\\))\nApproximately 95% of values fall within 2 SD of the mean (\\(\\mu \\pm 2\\sigma\\))\nApproximately 99.7% of values fall within 3 SD of the mean (\\(\\mu \\pm 3\\sigma\\))\n\nThis rule is incredibly useful for quick mental calculations!\n\n\nCode\n# Create visualization of empirical rule\nmu &lt;- 500  # Mean birth weight (kg) for beef calves\nsigma &lt;- 40\n\nx_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 500)\ny_vals &lt;- dnorm(x_vals, mean = mu, sd = sigma)\n\n# Create shaded regions\ndf_norm &lt;- tibble(x = x_vals, y = y_vals)\n\n# Base plot\np &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\")\n\n# Add shaded regions\np &lt;- p +\n  # 1 SD (68%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - sigma & x &lt;= mu + sigma),\n            aes(x = x, y = y), fill = \"darkgreen\", alpha = 0.3) +\n  # 2 SD (95%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - 2*sigma & x &lt;= mu + 2*sigma),\n            aes(x = x, y = y), fill = \"orange\", alpha = 0.2) +\n  # 3 SD (99.7%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - 3*sigma & x &lt;= mu + 3*sigma),\n            aes(x = x, y = y), fill = \"purple\", alpha = 0.15)\n\n# Add vertical lines\np &lt;- p +\n  geom_vline(xintercept = mu, linetype = \"solid\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mu + c(-1, 1) * sigma, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 0.8) +\n  geom_vline(xintercept = mu + c(-2, 2) * sigma, linetype = \"dashed\",\n             color = \"orange\", linewidth = 0.8) +\n  geom_vline(xintercept = mu + c(-3, 3) * sigma, linetype = \"dashed\",\n             color = \"purple\", linewidth = 0.8)\n\n# Add annotations\np &lt;- p +\n  annotate(\"text\", x = mu, y = max(y_vals) * 1.05, label = \"μ = 500\",\n           color = \"red\", fontface = \"bold\", size = 5) +\n  annotate(\"text\", x = mu, y = max(y_vals) * 0.5, label = \"68%\",\n           color = \"darkgreen\", fontface = \"bold\", size = 6) +\n  annotate(\"text\", x = mu + 1.5*sigma, y = max(y_vals) * 0.3, label = \"95%\",\n           color = \"orange\", fontface = \"bold\", size = 5) +\n  annotate(\"text\", x = mu + 2.5*sigma, y = max(y_vals) * 0.15, label = \"99.7%\",\n           color = \"purple\", fontface = \"bold\", size = 4)\n\np + labs(\n  title = \"The Empirical Rule for Normal Distributions\",\n  subtitle = \"Birth weights of beef calves: μ = 500 kg, σ = 40 kg\",\n  x = \"Birth Weight (kg)\",\n  y = \"Probability Density\"\n)\n\n\n\n\n\n\n\n\n\n\n13.2.1 Example Application\nQuestion: If birth weights are normally distributed with mean 500 kg and SD 40 kg, what range contains 95% of birth weights?\nAnswer: Using the empirical rule:\n\\[\n\\mu \\pm 2\\sigma = 500 \\pm 2(40) = 500 \\pm 80 = [420, 580] \\text{ kg}\n\\]\n\n\nCode\n# Calculate exactly\nmu &lt;- 500\nsigma &lt;- 40\n\ncat(\"Empirical Rule: 95% of birth weights fall within:\\n\")\n\n\nEmpirical Rule: 95% of birth weights fall within:\n\n\nCode\ncat(sprintf(\"  [%.0f, %.0f] kg\\n\", mu - 2*sigma, mu + 2*sigma))\n\n\n  [420, 580] kg\n\n\nCode\ncat(\"\\nThis means:\\n\")\n\n\n\nThis means:\n\n\nCode\ncat(sprintf(\"  - About 95%% of calves weigh between %.0f and %.0f kg at birth\\n\",\n            mu - 2*sigma, mu + 2*sigma))\n\n\n  - About 95% of calves weigh between 420 and 580 kg at birth\n\n\nCode\ncat(sprintf(\"  - Only ~2.5%% weigh less than %.0f kg\\n\", mu - 2*sigma))\n\n\n  - Only ~2.5% weigh less than 420 kg\n\n\nCode\ncat(sprintf(\"  - Only ~2.5%% weigh more than %.0f kg\\n\", mu + 2*sigma))\n\n\n  - Only ~2.5% weigh more than 580 kg",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#sec-normal-r",
    "href": "chapters/part2-ch03-probability_distributions.html#sec-normal-r",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "13.3 Working with the Normal Distribution in R",
    "text": "13.3 Working with the Normal Distribution in R\nR provides four functions for working with the normal distribution:\n\ndnorm(): Probability density function (height of the curve)\npnorm(): Cumulative distribution function (probability below a value)\nqnorm(): Quantile function (inverse of pnorm())\nrnorm(): Random number generation\n\n\n13.3.1 rnorm(): Generate Random Normal Values\n\n\nCode\n# Generate 10 random values from N(100, 15²)\nset.seed(123)\nrandom_values &lt;- rnorm(n = 10, mean = 100, sd = 15)\n\ncat(\"10 random values from N(100, 15²):\\n\")\n\n\n10 random values from N(100, 15²):\n\n\nCode\nprint(round(random_values, 1))\n\n\n [1]  91.6  96.5 123.4 101.1 101.9 125.7 106.9  81.0  89.7  93.3\n\n\nCode\n# Generate 1000 values and visualize\nlarge_sample &lt;- rnorm(n = 1000, mean = 100, sd = 15)\n\nggplot(tibble(x = large_sample), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = 100, linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(\n    title = \"1000 Random Values from N(100, 15²)\",\n    subtitle = \"Histogram + density overlay | Green line = mean\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 pnorm(): Calculate Probabilities (Area Under Curve)\nQuestion: What proportion of values in N(100, 15) are less than 110?\n\n\nCode\n# Calculate probability\nprob &lt;- pnorm(q = 110, mean = 100, sd = 15)\n\ncat(sprintf(\"P(X ≤ 110) = %.4f (%.2f%%)\\n\", prob, prob * 100))\n\n\nP(X ≤ 110) = 0.7475 (74.75%)\n\n\nCode\n# Visualize\nx &lt;- seq(50, 150, length.out = 500)\ny &lt;- dnorm(x, mean = 100, sd = 15)\n\ndf_viz &lt;- tibble(x = x, y = y)\n\nggplot(df_viz, aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(data = df_viz %&gt;% filter(x &lt;= 110),\n            aes(x = x, y = y), fill = \"red\", alpha = 0.4) +\n  geom_vline(xintercept = 110, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 110, y = max(y) * 0.8,\n           label = sprintf(\"P(X ≤ 110) = %.2f%%\", prob * 100),\n           hjust = -0.1, size = 5, color = \"red\") +\n  labs(\n    title = \"Using pnorm(): Calculate Area to the Left\",\n    subtitle = \"N(100, 15) distribution\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 More pnorm() Examples\n\n\nCode\n# Example 1: P(X &gt; 115)\nprob_above &lt;- 1 - pnorm(115, mean = 100, sd = 15)\n# OR use lower.tail = FALSE\nprob_above2 &lt;- pnorm(115, mean = 100, sd = 15, lower.tail = FALSE)\n\ncat(\"Example 1: What proportion of values are ABOVE 115?\\n\")\n\n\nExample 1: What proportion of values are ABOVE 115?\n\n\nCode\ncat(sprintf(\"  P(X &gt; 115) = %.4f (%.2f%%)\\n\\n\", prob_above, prob_above * 100))\n\n\n  P(X &gt; 115) = 0.1587 (15.87%)\n\n\nCode\n# Example 2: P(90 &lt; X &lt; 110)\nprob_between &lt;- pnorm(110, mean = 100, sd = 15) - pnorm(90, mean = 100, sd = 15)\n\ncat(\"Example 2: What proportion fall BETWEEN 90 and 110?\\n\")\n\n\nExample 2: What proportion fall BETWEEN 90 and 110?\n\n\nCode\ncat(sprintf(\"  P(90 &lt; X &lt; 110) = %.4f (%.2f%%)\\n\\n\", prob_between, prob_between * 100))\n\n\n  P(90 &lt; X &lt; 110) = 0.4950 (49.50%)\n\n\nCode\n# Example 3: Application to pig weights\n# Pig market weights: N(120, 12²) kg\n# Pigs below 100 kg get a price penalty\n\nprob_penalty &lt;- pnorm(100, mean = 120, sd = 12)\n\ncat(\"Example 3: Pig market weights N(120, 12²)\\n\")\n\n\nExample 3: Pig market weights N(120, 12²)\n\n\nCode\ncat(sprintf(\"  Proportion below 100 kg (penalty): %.4f (%.2f%%)\\n\",\n            prob_penalty, prob_penalty * 100))\n\n\n  Proportion below 100 kg (penalty): 0.0478 (4.78%)\n\n\n\n\n13.3.4 qnorm(): Find Values from Probabilities\nInverse question: What weight cuts off the bottom 10% of pigs?\n\n\nCode\n# Find the 10th percentile (bottom 10%)\ncutoff_10 &lt;- qnorm(p = 0.10, mean = 120, sd = 12)\n\ncat(\"Question: What weight cuts off the bottom 10% of pigs?\\n\")\n\n\nQuestion: What weight cuts off the bottom 10% of pigs?\n\n\nCode\ncat(sprintf(\"Answer: %.2f kg\\n\\n\", cutoff_10))\n\n\nAnswer: 104.62 kg\n\n\nCode\ncat(\"Interpretation: 10% of pigs weigh less than this value\\n\")\n\n\nInterpretation: 10% of pigs weigh less than this value\n\n\nCode\n# Other useful quantiles\nq25 &lt;- qnorm(0.25, mean = 120, sd = 12)  # Q1\nq50 &lt;- qnorm(0.50, mean = 120, sd = 12)  # Median\nq75 &lt;- qnorm(0.75, mean = 120, sd = 12)  # Q3\n\ncat(\"\\nQuartiles of pig weights N(120, 12²):\\n\")\n\n\n\nQuartiles of pig weights N(120, 12²):\n\n\nCode\ncat(sprintf(\"  Q1 (25th percentile): %.2f kg\\n\", q25))\n\n\n  Q1 (25th percentile): 111.91 kg\n\n\nCode\ncat(sprintf(\"  Q2 (50th percentile/median): %.2f kg\\n\", q50))\n\n\n  Q2 (50th percentile/median): 120.00 kg\n\n\nCode\ncat(sprintf(\"  Q3 (75th percentile): %.2f kg\\n\", q75))\n\n\n  Q3 (75th percentile): 128.09 kg",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#the-z-score-formula",
    "href": "chapters/part2-ch03-probability_distributions.html#the-z-score-formula",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "14.1 The Z-Score Formula",
    "text": "14.1 The Z-Score Formula\nFor a value \\(x\\) from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nInterpretation:\n\n\\(z = 0\\): The value equals the mean\n\\(z = 1\\): The value is 1 SD above the mean\n\\(z = -2\\): The value is 2 SD below the mean\n\\(z &gt; 3\\) or \\(z &lt; -3\\): Unusual/outlier (beyond 99.7% of values)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#example-birth-weights-in-beef-cattle",
    "href": "chapters/part2-ch03-probability_distributions.html#example-birth-weights-in-beef-cattle",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "14.2 Example: Birth Weights in Beef Cattle",
    "text": "14.2 Example: Birth Weights in Beef Cattle\n\n\nCode\n# Population parameters\nmu_birth &lt;- 40  # kg\nsigma_birth &lt;- 5\n\n# Individual calf weights\ncalf_weights &lt;- c(35, 40, 42, 48, 52, 30)\n\n# Calculate z-scores\nz_scores &lt;- (calf_weights - mu_birth) / sigma_birth\n\n# Create summary table\ntibble(\n  `Calf ID` = 1:6,\n  `Weight (kg)` = calf_weights,\n  `Z-score` = round(z_scores, 2),\n  Interpretation = case_when(\n    z_scores &lt; -2 ~ \"Unusually light (&lt; -2 SD)\",\n    z_scores &lt; -1 ~ \"Below average\",\n    z_scores &lt; 1 ~ \"Near average\",\n    z_scores &lt; 2 ~ \"Above average\",\n    TRUE ~ \"Unusually heavy (&gt; 2 SD)\"\n  )\n) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Birth Weights and Z-scores\",\n    subtitle = sprintf(\"Population: μ = %.0f kg, σ = %.0f kg\", mu_birth, sigma_birth)\n  ) %&gt;%\n  data_color(\n    columns = `Z-score`,\n    colors = scales::col_numeric(\n      palette = c(\"red\", \"yellow\", \"lightgreen\", \"yellow\", \"red\"),\n      domain = c(-3, 3)\n    )\n  )\n\n\n\n\n\n\n\n\nBirth Weights and Z-scores\n\n\nPopulation: μ = 40 kg, σ = 5 kg\n\n\nCalf ID\nWeight (kg)\nZ-score\nInterpretation\n\n\n\n\n1\n35\n-1.0\nNear average\n\n\n2\n40\n0.0\nNear average\n\n\n3\n42\n0.4\nNear average\n\n\n4\n48\n1.6\nAbove average\n\n\n5\n52\n2.4\nUnusually heavy (&gt; 2 SD)\n\n\n6\n30\n-2.0\nBelow average",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#sec-standard-normal",
    "href": "chapters/part2-ch03-probability_distributions.html#sec-standard-normal",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "14.3 The Standard Normal Distribution",
    "text": "14.3 The Standard Normal Distribution\nWhen we calculate z-scores, we’re standardizing the distribution to have:\n\nMean = 0\nStandard deviation = 1\n\nThis is called the standard normal distribution, denoted \\(N(0, 1)\\) or \\(Z \\sim N(0, 1)\\).\n\n\nCode\n# Compare original and standardized distributions\nx_original &lt;- seq(25, 55, length.out = 500)\ny_original &lt;- dnorm(x_original, mean = 40, sd = 5)\n\nz_values &lt;- seq(-3, 3, length.out = 500)\ny_standard &lt;- dnorm(z_values, mean = 0, sd = 1)\n\np_orig &lt;- ggplot(tibble(x = x_original, y = y_original), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_vline(xintercept = 40, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Original: N(40, 5²)\",\n    subtitle = \"Birth weights in kg\",\n    x = \"Weight (kg)\",\n    y = \"Density\"\n  )\n\np_std &lt;- ggplot(tibble(x = z_values, y = y_standard), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkgreen\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Standardized: N(0, 1)\",\n    subtitle = \"Z-scores (standard normal)\",\n    x = \"Z-score\",\n    y = \"Density\"\n  )\n\np_orig + p_std\n\n\n\n\n\n\n\n\n\n\n14.3.1 Why Standardize?\nStandardization allows us to:\n\nCompare values from different distributions (e.g., compare a pig’s weight to a cow’s weight)\nUse standard normal tables (historically important, less so now with computers)\nIdentify outliers consistently (|z| &gt; 2 or 3)\nCalculate probabilities easily\n\n\n\nCode\n# Compare animals from different species\npig_weight &lt;- 110  # kg\npig_mean &lt;- 120\npig_sd &lt;- 12\n\ncow_weight &lt;- 550  # kg\ncow_mean &lt;- 600\ncow_sd &lt;- 50\n\n# Calculate z-scores\nz_pig &lt;- (pig_weight - pig_mean) / pig_sd\nz_cow &lt;- (cow_weight - cow_mean) / cow_sd\n\ncat(\"Question: Which animal is further below average for its species?\\n\\n\")\n\n\nQuestion: Which animal is further below average for its species?\n\n\nCode\ncat(\"Pig:\\n\")\n\n\nPig:\n\n\nCode\ncat(sprintf(\"  Weight: %.0f kg (population mean: %.0f kg, SD: %.0f kg)\\n\",\n            pig_weight, pig_mean, pig_sd))\n\n\n  Weight: 110 kg (population mean: 120 kg, SD: 12 kg)\n\n\nCode\ncat(sprintf(\"  Z-score: %.2f\\n\\n\", z_pig))\n\n\n  Z-score: -0.83\n\n\nCode\ncat(\"Cow:\\n\")\n\n\nCow:\n\n\nCode\ncat(sprintf(\"  Weight: %.0f kg (population mean: %.0f kg, SD: %.0f kg)\\n\",\n            cow_weight, cow_mean, cow_sd))\n\n\n  Weight: 550 kg (population mean: 600 kg, SD: 50 kg)\n\n\nCode\ncat(sprintf(\"  Z-score: %.2f\\n\\n\", z_cow))\n\n\n  Z-score: -1.00\n\n\nCode\nif (abs(z_pig) &gt; abs(z_cow)) {\n  cat(\"Answer: The pig is further below average (more unusual) for its species.\\n\")\n} else {\n  cat(\"Answer: The cow is further below average (more unusual) for its species.\\n\")\n}\n\n\nAnswer: The cow is further below average (more unusual) for its species.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#statement-of-the-central-limit-theorem",
    "href": "chapters/part2-ch03-probability_distributions.html#statement-of-the-central-limit-theorem",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "15.1 Statement of the Central Limit Theorem",
    "text": "15.1 Statement of the Central Limit Theorem\n\n\n\n\n\n\nImportantCentral Limit Theorem\n\n\n\nFor a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), if we:\n\nTake random samples of size \\(n\\) from the population\nCalculate the sample mean \\(\\bar{x}\\) for each sample\nRepeat this process many times\n\nThen the distribution of sample means (\\(\\bar{x}\\)) will be approximately normal with:\n\nMean: \\(\\mu_{\\bar{x}} = \\mu\\) (same as population mean)\nStandard deviation: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) (called the standard error)\n\nThe amazing part: This is true regardless of the shape of the original population distribution, as long as \\(n\\) is “large enough” (typically \\(n \\geq 30\\)).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#sec-clt-simulation",
    "href": "chapters/part2-ch03-probability_distributions.html#sec-clt-simulation",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "15.2 Demonstrating the CLT with Simulation",
    "text": "15.2 Demonstrating the CLT with Simulation\nLet’s prove the CLT works using simulation. We’ll start with a decidedly non-normal population (uniform distribution) and show that sample means become normal.\n\n15.2.1 Population: Uniform Distribution\n\n\nCode\n# Population: uniformly distributed pig birth weights between 20 and 60 kg\n# This is NOT normal (flat distribution)\nset.seed(42)\n\npopulation_size &lt;- 100000\npopulation &lt;- runif(population_size, min = 20, max = 60)\n\npop_mean &lt;- mean(population)\npop_sd &lt;- sd(population)\n\nggplot(tibble(weight = population), aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = pop_mean, linetype = \"dashed\",\n             color = \"red\", linewidth = 1.2) +\n  annotate(\"text\", x = pop_mean + 2, y = 4000,\n           label = sprintf(\"μ = %.1f\\nσ = %.1f\", pop_mean, pop_sd),\n           hjust = 0, size = 5, color = \"red\") +\n  labs(\n    title = \"Population Distribution: Uniform (NOT Normal)\",\n    subtitle = \"Birth weights uniformly distributed between 20 and 60 kg\",\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n15.2.2 Sampling Distribution for Different Sample Sizes\nNow let’s draw many samples of different sizes and plot the distribution of sample means:\n\n\nCode\n# Function to simulate sampling distribution\nsimulate_sampling_dist &lt;- function(pop, n_samples, sample_size) {\n  replicate(n_samples, mean(sample(pop, size = sample_size, replace = TRUE)))\n}\n\n# Simulate for different sample sizes\nn_samples &lt;- 1000\n\nsample_means_n5 &lt;- simulate_sampling_dist(population, n_samples, 5)\nsample_means_n10 &lt;- simulate_sampling_dist(population, n_samples, 10)\nsample_means_n30 &lt;- simulate_sampling_dist(population, n_samples, 30)\nsample_means_n100 &lt;- simulate_sampling_dist(population, n_samples, 100)\n\n# Expected standard error\nse_n5 &lt;- pop_sd / sqrt(5)\nse_n10 &lt;- pop_sd / sqrt(10)\nse_n30 &lt;- pop_sd / sqrt(30)\nse_n100 &lt;- pop_sd / sqrt(100)\n\n# Create plots\ndf_n5 &lt;- tibble(mean = sample_means_n5, n = \"n = 5\")\ndf_n10 &lt;- tibble(mean = sample_means_n10, n = \"n = 10\")\ndf_n30 &lt;- tibble(mean = sample_means_n30, n = \"n = 30\")\ndf_n100 &lt;- tibble(mean = sample_means_n100, n = \"n = 100\")\n\ndf_all &lt;- bind_rows(df_n5, df_n10, df_n30, df_n100) %&gt;%\n  mutate(n = factor(n, levels = c(\"n = 5\", \"n = 10\", \"n = 30\", \"n = 100\")))\n\nggplot(df_all, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = pop_mean, linetype = \"dashed\",\n             color = \"blue\", linewidth = 1) +\n  facet_wrap(~n, ncol = 2, scales = \"free_y\") +\n  labs(\n    title = \"Central Limit Theorem in Action\",\n    subtitle = \"Distribution of sample means for different sample sizes (1000 samples each)\",\n    x = \"Sample Mean Weight (kg)\",\n    y = \"Density\"\n  ) +\n  theme(strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n15.2.3 Summary Statistics of Sampling Distributions\n\n\nCode\n# Calculate statistics for each sampling distribution\nsummary_clt &lt;- tibble(\n  `Sample Size (n)` = c(5, 10, 30, 100),\n  `Mean of Sample Means` = c(mean(sample_means_n5), mean(sample_means_n10),\n                             mean(sample_means_n30), mean(sample_means_n100)),\n  `SD of Sample Means (observed)` = c(sd(sample_means_n5), sd(sample_means_n10),\n                                      sd(sample_means_n30), sd(sample_means_n100)),\n  `Standard Error (theoretical)` = c(se_n5, se_n10, se_n30, se_n100)\n)\n\nsummary_clt %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Central Limit Theorem: Observed vs Theoretical\",\n    subtitle = sprintf(\"Population: μ = %.2f, σ = %.2f\", pop_mean, pop_sd)\n  ) %&gt;%\n  fmt_number(columns = -`Sample Size (n)`, decimals = 3) %&gt;%\n  tab_source_note(\"Standard Error (SE) = σ / √n\") %&gt;%\n  tab_source_note(\"Notice: Observed SD ≈ Theoretical SE\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"lightyellow\"),\n    locations = cells_body(columns = `Standard Error (theoretical)`)\n  )\n\n\n\n\n\n\n\n\nCentral Limit Theorem: Observed vs Theoretical\n\n\nPopulation: μ = 40.03, σ = 11.58\n\n\nSample Size (n)\nMean of Sample Means\nSD of Sample Means (observed)\nStandard Error (theoretical)\n\n\n\n\n5\n40.027\n5.249\n5.179\n\n\n10\n40.008\n3.610\n3.662\n\n\n30\n39.986\n2.106\n2.114\n\n\n100\n40.082\n1.143\n1.158\n\n\n\nStandard Error (SE) = σ / √n\n\n\nNotice: Observed SD ≈ Theoretical SE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Observations from the CLT\n\n\n\n\nMean stays the same: The mean of sample means equals the population mean (40 kg)\nSpread decreases: As \\(n\\) increases, the spread of sample means decreases (SE = \\(\\sigma/\\sqrt{n}\\))\nShape becomes normal: Even though the population was uniform, sample means become approximately normal, especially for \\(n \\geq 30\\)\nObserved ≈ Theoretical: The observed SD of sample means matches the theoretical standard error",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#why-the-clt-matters",
    "href": "chapters/part2-ch03-probability_distributions.html#why-the-clt-matters",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "15.3 Why the CLT Matters",
    "text": "15.3 Why the CLT Matters\nThe Central Limit Theorem is profound because it:\n\nJustifies using normal distributions for inference, even when data aren’t perfectly normal\nExplains why sample means are reliable: Larger samples → smaller standard error → more precise estimates\nUnderpins t-tests, ANOVA, regression: All assume sampling distributions are approximately normal\nGuides sample size decisions: Tells us how much precision improves with larger \\(n\\)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#three-types-of-distributions-dont-confuse-them",
    "href": "chapters/part2-ch03-probability_distributions.html#three-types-of-distributions-dont-confuse-them",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "16.1 Three Types of Distributions: Don’t Confuse Them!",
    "text": "16.1 Three Types of Distributions: Don’t Confuse Them!\n\n\n\n\n\n\nWarningThree Different Distributions\n\n\n\n\nPopulation distribution: Distribution of all values in the entire population\nSample distribution: Distribution of values in one sample\nSampling distribution: Distribution of a statistic (e.g., mean) calculated from many repeated samples\n\nMost common confusion: Mixing up the sample distribution with the sampling distribution!\n\n\n\n16.1.1 Visual Comparison\n\n\nCode\n# Population\nset.seed(999)\npopulation &lt;- rnorm(100000, mean = 550, sd = 60)\n\n# One sample\none_sample &lt;- sample(population, size = 40)\n\n# Many sample means\nn_samples &lt;- 1000\nsample_means &lt;- replicate(n_samples, mean(sample(population, size = 40)))\n\n# Create plots\np_pop &lt;- ggplot(tibble(x = sample(population, 10000)), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"purple\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(population), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"1. Population Distribution\",\n    subtitle = sprintf(\"All animals: μ = 550, σ = 60\\n(showing 10,000 for visualization)\"),\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\np_sample &lt;- ggplot(tibble(x = one_sample), aes(x = x)) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(one_sample), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"2. One Sample Distribution\",\n    subtitle = sprintf(\"One sample: n = 40, x̄ = %.1f, s = %.1f\",\n                      mean(one_sample), sd(one_sample)),\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\np_sampling &lt;- ggplot(tibble(x = sample_means), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_means), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"3. Sampling Distribution of the Mean\",\n    subtitle = sprintf(\"1000 samples of n=40 each:\\nMean of x̄'s = %.1f, SD(x̄) = %.2f\",\n                      mean(sample_means), sd(sample_means)),\n    x = \"Sample Mean Weight (kg)\",\n    y = \"Count\"\n  )\n\np_pop / p_sample / p_sampling +\n  plot_annotation(\n    title = \"Three Different Distributions: Know the Difference!\",\n    theme = theme(plot.title = element_text(size = 16, face = \"bold\"))\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#definitions",
    "href": "chapters/part2-ch03-probability_distributions.html#definitions",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "17.1 Definitions",
    "text": "17.1 Definitions\n\n\n\n\n\n\nImportantStandard Deviation vs Standard Error\n\n\n\nStandard Deviation (SD or \\(s\\)):\n\nMeasures variability in the data itself\nDescribes the spread of individual observations\nFormula: \\(s = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\\)\nDoes not decrease with larger sample size (it estimates the population SD)\n\nStandard Error (SE):\n\nMeasures variability in the sample mean (or other statistic)\nDescribes how much the sample mean varies from sample to sample\nFormula: \\(SE = \\frac{s}{\\sqrt{n}}\\) (for the mean)\nDecreases with larger sample size: More data → more precise estimate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#visualizing-the-difference",
    "href": "chapters/part2-ch03-probability_distributions.html#visualizing-the-difference",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "17.2 Visualizing the Difference",
    "text": "17.2 Visualizing the Difference\n\n\nCode\n# Simulate samples of different sizes\nset.seed(123)\npop_mean &lt;- 600\npop_sd &lt;- 50\n\n# Small sample\nn_small &lt;- 10\nsample_small &lt;- rnorm(n_small, mean = pop_mean, sd = pop_sd)\nse_small &lt;- sd(sample_small) / sqrt(n_small)\n\n# Large sample\nn_large &lt;- 100\nsample_large &lt;- rnorm(n_large, mean = pop_mean, sd = pop_sd)\nse_large &lt;- sd(sample_large) / sqrt(n_large)\n\n# Print results\ncat(\"Small Sample (n = 10):\\n\")\n\n\nSmall Sample (n = 10):\n\n\nCode\ncat(sprintf(\"  Mean: %.2f kg\\n\", mean(sample_small)))\n\n\n  Mean: 603.73 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg (measures spread of individual weights)\\n\", sd(sample_small)))\n\n\n  SD: 47.69 kg (measures spread of individual weights)\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg (measures uncertainty in the mean)\\n\\n\", se_small))\n\n\n  SE: 15.08 kg (measures uncertainty in the mean)\n\n\nCode\ncat(\"Large Sample (n = 100):\\n\")\n\n\nLarge Sample (n = 100):\n\n\nCode\ncat(sprintf(\"  Mean: %.2f kg\\n\", mean(sample_large)))\n\n\n  Mean: 602.17 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg (similar to small sample - estimates population SD)\\n\", sd(sample_large)))\n\n\n  SD: 45.21 kg (similar to small sample - estimates population SD)\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg (much smaller - mean is more precise!)\\n\", se_large))\n\n\n  SE: 4.52 kg (much smaller - mean is more precise!)\n\n\nCode\n# Visualize\np1 &lt;- ggplot(tibble(x = sample_small), aes(x = x)) +\n  geom_histogram(bins = 8, fill = \"steelblue\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_small), color = \"red\", linewidth = 1.2) +\n  geom_segment(aes(x = mean(sample_small) - sd(sample_small),\n                   xend = mean(sample_small) + sd(sample_small),\n                   y = 0, yend = 0),\n               color = \"purple\", linewidth = 2, arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = mean(sample_small), y = 1.5,\n           label = sprintf(\"SD = %.1f\", sd(sample_small)),\n           color = \"purple\", fontface = \"bold\") +\n  labs(title = \"Small Sample (n=10)\",\n       subtitle = sprintf(\"Mean = %.1f, SE = %.2f\", mean(sample_small), se_small),\n       x = \"Weight (kg)\", y = \"Count\")\n\np2 &lt;- ggplot(tibble(x = sample_large), aes(x = x)) +\n  geom_histogram(bins = 20, fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_large), color = \"red\", linewidth = 1.2) +\n  geom_segment(aes(x = mean(sample_large) - sd(sample_large),\n                   xend = mean(sample_large) + sd(sample_large),\n                   y = 0, yend = 0),\n               color = \"purple\", linewidth = 2, arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = mean(sample_large), y = 8,\n           label = sprintf(\"SD = %.1f\", sd(sample_large)),\n           color = \"purple\", fontface = \"bold\") +\n  labs(title = \"Large Sample (n=100)\",\n       subtitle = sprintf(\"Mean = %.1f, SE = %.2f\", mean(sample_large), se_large),\n       x = \"Weight (kg)\", y = \"Count\")\n\np1 + p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#how-se-decreases-with-sample-size",
    "href": "chapters/part2-ch03-probability_distributions.html#how-se-decreases-with-sample-size",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "17.3 How SE Decreases with Sample Size",
    "text": "17.3 How SE Decreases with Sample Size\n\n\nCode\n# Show how SE decreases as n increases\npop_sd &lt;- 50\nsample_sizes &lt;- c(5, 10, 20, 30, 50, 100, 200, 500)\nstandard_errors &lt;- pop_sd / sqrt(sample_sizes)\n\ndf_se &lt;- tibble(\n  n = sample_sizes,\n  SE = standard_errors\n)\n\nggplot(df_se, aes(x = n, y = SE)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_point(size = 4, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  annotate(\"text\", x = 250, y = pop_sd / sqrt(30),\n           label = \"SE = σ / √n\",\n           size = 6, color = \"darkblue\", fontface = \"bold\") +\n  labs(\n    title = \"Standard Error Decreases with Sample Size\",\n    subtitle = \"Population SD = 50 kg\",\n    x = \"Sample Size (n)\",\n    y = \"Standard Error (kg)\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nCode\n# Show specific values\ndf_se %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Standard Error by Sample Size\",\n    subtitle = \"Population σ = 50 kg\"\n  ) %&gt;%\n  fmt_number(columns = SE, decimals = 2) %&gt;%\n  cols_label(n = \"Sample Size\", SE = \"Standard Error (kg)\")\n\n\n\n\n\n\n\n\nStandard Error by Sample Size\n\n\nPopulation σ = 50 kg\n\n\nSample Size\nStandard Error (kg)\n\n\n\n\n5\n22.36\n\n\n10\n15.81\n\n\n20\n11.18\n\n\n30\n9.13\n\n\n50\n7.07\n\n\n100\n5.00\n\n\n200\n3.54\n\n\n500\n2.24\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipWhen to Report SD vs SE\n\n\n\nReport SD when:\n\nDescribing the variability in your sample or population\nExample: “Pig weights ranged from 100 to 140 kg, with mean 120 kg (SD = 12 kg)”\n\nReport SE when:\n\nDescribing uncertainty in an estimate (usually the mean)\nExample: “Mean pig weight was 120 kg (SE = 1.7 kg)”\nOr better yet, report a confidence interval (next section!)\n\nBest practice: Report both! E.g., “Mean = 120 kg (SD = 12, SE = 1.7, n = 50)”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#concept-and-interpretation",
    "href": "chapters/part2-ch03-probability_distributions.html#concept-and-interpretation",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "18.1 Concept and Interpretation",
    "text": "18.1 Concept and Interpretation\nA 95% confidence interval for the population mean is calculated as:\n\\[\n\\bar{x} \\pm 1.96 \\times SE\n\\]\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\\(SE = s / \\sqrt{n}\\) = standard error of the mean\n1.96 = critical value from the standard normal distribution (for 95% confidence)\n\n\n18.1.1 What Does “95% Confidence” Mean?\n\n\n\n\n\n\nImportantCorrect Interpretation\n\n\n\n“If we repeated this study many times and calculated a 95% CI each time, about 95% of those intervals would contain the true population mean.”\nNOT: “There’s a 95% probability that the true mean is in this specific interval.”\nThe true mean either is or isn’t in the interval—we just don’t know which. The 95% refers to the procedure, not a specific interval.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#calculating-a-confidence-interval",
    "href": "chapters/part2-ch03-probability_distributions.html#calculating-a-confidence-interval",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "18.2 Calculating a Confidence Interval",
    "text": "18.2 Calculating a Confidence Interval\n\n18.2.1 Example: Beef Cattle Finishing Weights\n\n\nCode\n# Sample data: finishing weights of 35 beef cattle\nset.seed(456)\nn &lt;- 35\nweights &lt;- rnorm(n, mean = 580, sd = 60)\n\n# Calculate statistics\nxbar &lt;- mean(weights)\ns &lt;- sd(weights)\nse &lt;- s / sqrt(n)\n\n# 95% CI using z = 1.96 (we'll use t-distribution properly in Week 4)\nci_lower &lt;- xbar - 1.96 * se\nci_upper &lt;- xbar + 1.96 * se\n\ncat(\"Sample Statistics:\\n\")\n\n\nSample Statistics:\n\n\nCode\ncat(sprintf(\"  n = %d cattle\\n\", n))\n\n\n  n = 35 cattle\n\n\nCode\ncat(sprintf(\"  Mean weight: %.2f kg\\n\", xbar))\n\n\n  Mean weight: 587.37 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg\\n\", s))\n\n\n  SD: 68.72 kg\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg\\n\\n\", se))\n\n\n  SE: 11.62 kg\n\n\nCode\ncat(\"95% Confidence Interval:\\n\")\n\n\n95% Confidence Interval:\n\n\nCode\ncat(sprintf(\"  [%.2f, %.2f] kg\\n\\n\", ci_lower, ci_upper))\n\n\n  [564.60, 610.13] kg\n\n\nCode\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"We are 95% confident that the true mean finishing weight\\n\")\n\n\nWe are 95% confident that the true mean finishing weight\n\n\nCode\ncat(sprintf(\"of the population is between %.1f and %.1f kg.\\n\", ci_lower, ci_upper))\n\n\nof the population is between 564.6 and 610.1 kg.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#visualizing-confidence-intervals",
    "href": "chapters/part2-ch03-probability_distributions.html#visualizing-confidence-intervals",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "18.3 Visualizing Confidence Intervals",
    "text": "18.3 Visualizing Confidence Intervals\n\n\nCode\n# Create visualization\nci_data &lt;- tibble(\n  estimate = xbar,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper\n)\n\nggplot(ci_data, aes(x = 1, y = estimate)) +\n  geom_point(size = 5, color = \"red\") +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, linewidth = 1.2, color = \"blue\") +\n  geom_hline(yintercept = 580, linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = xbar,\n           label = sprintf(\"x̄ = %.1f kg\", xbar),\n           color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.3, y = ci_lower,\n           label = sprintf(\"Lower: %.1f\", ci_lower),\n           color = \"blue\", size = 4) +\n  annotate(\"text\", x = 1.3, y = ci_upper,\n           label = sprintf(\"Upper: %.1f\", ci_upper),\n           color = \"blue\", size = 4) +\n  annotate(\"text\", x = 1.3, y = 580,\n           label = \"True mean = 580\",\n           color = \"darkgreen\", size = 4, hjust = 0) +\n  labs(\n    title = \"95% Confidence Interval for Mean Weight\",\n    subtitle = \"Blue bars show range of plausible values for population mean\",\n    y = \"Weight (kg)\",\n    x = \"\"\n  ) +\n  coord_cartesian(ylim = c(550, 610)) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#what-does-95-mean-a-simulation",
    "href": "chapters/part2-ch03-probability_distributions.html#what-does-95-mean-a-simulation",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "18.4 What Does “95%” Mean? A Simulation",
    "text": "18.4 What Does “95%” Mean? A Simulation\nLet’s demonstrate what “95% confidence” actually means by simulating 100 studies:\n\n\nCode\n# Simulate 100 studies\nset.seed(789)\nn_studies &lt;- 100\nn_per_study &lt;- 35\ntrue_mean &lt;- 580\ntrue_sd &lt;- 60\n\n# Function to calculate CI for one study\ncalc_ci &lt;- function(study_id) {\n  sample_data &lt;- rnorm(n_per_study, mean = true_mean, sd = true_sd)\n  xbar &lt;- mean(sample_data)\n  se &lt;- sd(sample_data) / sqrt(n_per_study)\n\n  tibble(\n    study = study_id,\n    estimate = xbar,\n    ci_lower = xbar - 1.96 * se,\n    ci_upper = xbar + 1.96 * se,\n    captures_true = ci_lower &lt;= true_mean & ci_upper &gt;= true_mean\n  )\n}\n\n# Run simulation\nci_results &lt;- map_df(1:n_studies, calc_ci)\n\n# Count how many capture the true mean\nn_captured &lt;- sum(ci_results$captures_true)\ncapture_rate &lt;- mean(ci_results$captures_true)\n\ncat(sprintf(\"Out of %d studies:\\n\", n_studies))\n\n\nOut of 100 studies:\n\n\nCode\ncat(sprintf(\"  %d CIs (%.1f%%) captured the true mean\\n\", n_captured, capture_rate * 100))\n\n\n  92 CIs (92.0%) captured the true mean\n\n\nCode\ncat(sprintf(\"  %d CIs (%.1f%%) did NOT capture the true mean\\n\",\n            n_studies - n_captured, (1 - capture_rate) * 100))\n\n\n  8 CIs (8.0%) did NOT capture the true mean\n\n\nCode\ncat(\"\\nThis is very close to the expected 95%!\\n\")\n\n\n\nThis is very close to the expected 95%!\n\n\nCode\n# Visualize (show first 50 for clarity)\nggplot(ci_results %&gt;% slice(1:50),\n       aes(x = study, y = estimate, color = captures_true)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +\n  geom_hline(yintercept = true_mean, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1.2) +\n  scale_color_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"),\n                     labels = c(\"TRUE\" = \"Captures true mean\", \"FALSE\" = \"Misses true mean\")) +\n  annotate(\"text\", x = 45, y = true_mean + 15,\n           label = sprintf(\"True mean = %.0f kg\", true_mean),\n           color = \"darkgreen\", fontface = \"bold\", size = 4) +\n  labs(\n    title = \"95% Confidence Intervals from 50 Simulated Studies\",\n    subtitle = sprintf(\"Red intervals (%.0f%%) miss the true mean - this is expected!\",\n                      (1 - capture_rate) * 100),\n    x = \"Study Number\",\n    y = \"Mean Weight (kg)\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#factors-affecting-ci-width",
    "href": "chapters/part2-ch03-probability_distributions.html#factors-affecting-ci-width",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "18.5 Factors Affecting CI Width",
    "text": "18.5 Factors Affecting CI Width\nThree factors determine how wide a confidence interval is:\n\nSample size (\\(n\\)): Larger \\(n\\) → narrower CI (more precision)\nVariability (\\(\\sigma\\)): More variable data → wider CI (less precision)\nConfidence level: Higher confidence (e.g., 99% vs 95%) → wider CI (trade precision for confidence)\n\n\n\nCode\n# Demonstrate effect of sample size\ntrue_mean &lt;- 120\ntrue_sd &lt;- 15\nsample_sizes &lt;- c(10, 30, 50, 100, 200)\n\nci_by_n &lt;- map_df(sample_sizes, function(n) {\n  set.seed(123)\n  sample_data &lt;- rnorm(n, mean = true_mean, sd = true_sd)\n  xbar &lt;- mean(sample_data)\n  se &lt;- sd(sample_data) / sqrt(n)\n\n  tibble(\n    n = n,\n    estimate = xbar,\n    ci_lower = xbar - 1.96 * se,\n    ci_upper = xbar + 1.96 * se,\n    width = ci_upper - ci_lower\n  )\n})\n\n# Plot\nggplot(ci_by_n, aes(x = factor(n), y = estimate)) +\n  geom_point(size = 4, color = \"red\") +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, linewidth = 1.2, color = \"blue\") +\n  geom_hline(yintercept = true_mean, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1) +\n  geom_text(aes(label = sprintf(\"Width: %.1f\", width)),\n            vjust = -1, size = 3.5) +\n  labs(\n    title = \"Effect of Sample Size on Confidence Interval Width\",\n    subtitle = \"Larger samples → narrower CIs → more precise estimates\",\n    x = \"Sample Size (n)\",\n    y = \"Weight (kg)\"\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#scenario",
    "href": "chapters/part2-ch03-probability_distributions.html#scenario",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.1 Scenario",
    "text": "19.1 Scenario\nA dairy researcher wants to estimate the average daily milk production of a new Holstein strain. She samples 40 cows and records their daily production.\n\n\nCode\n# Generate realistic data\nset.seed(2025)\nn_cows &lt;- 40\nmilk_production &lt;- rnorm(n_cows, mean = 35, sd = 6)  # liters per day\n\n# Calculate summary statistics\nxbar &lt;- mean(milk_production)\ns &lt;- sd(milk_production)\nse &lt;- s / sqrt(n_cows)\n\n# Show first 10 observations\nhead(milk_production, 10) %&gt;%\n  round(1) %&gt;%\n  matrix(nrow = 2, byrow = TRUE) %&gt;%\n  as_tibble(.name_repair = \"minimal\") %&gt;%\n  setNames(paste(\"Cow\", 1:5)) %&gt;%\n  mutate(Row = c(\"1-5\", \"6-10\"), .before = 1) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Daily Milk Production (liters)\",\n             subtitle = \"First 10 cows (n = 40 total)\")\n\n\n\n\n\n\n\n\nDaily Milk Production (liters)\n\n\nFirst 10 cows (n = 40 total)\n\n\nRow\nCow 1\nCow 2\nCow 3\nCow 4\nCow 5\n\n\n\n\n1-5\n38.7\n35.2\n39.6\n42.6\n37.2\n\n\n6-10\n34.0\n37.4\n34.5\n32.9\n39.2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#step-1-descriptive-statistics",
    "href": "chapters/part2-ch03-probability_distributions.html#step-1-descriptive-statistics",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.2 Step 1: Descriptive Statistics",
    "text": "19.2 Step 1: Descriptive Statistics\n\n\nCode\n# Comprehensive summary\nsummary_stats &lt;- tibble(\n  Statistic = c(\"Sample size\", \"Mean\", \"Standard Deviation\", \"Standard Error\",\n                \"Minimum\", \"Q1\", \"Median\", \"Q3\", \"Maximum\", \"Range\"),\n  Value = c(n_cows, xbar, s, se, min(milk_production),\n            quantile(milk_production, 0.25), median(milk_production),\n            quantile(milk_production, 0.75), max(milk_production),\n            max(milk_production) - min(milk_production))\n)\n\nsummary_stats %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Summary Statistics: Daily Milk Production\") %&gt;%\n  fmt_number(columns = Value, rows = 2:10, decimals = 2) %&gt;%\n  fmt_number(columns = Value, rows = 1, decimals = 0) %&gt;%\n  tab_source_note(\"All values in liters per day (except n)\")\n\n\n\n\n\n\n\n\nSummary Statistics: Daily Milk Production\n\n\nStatistic\nValue\n\n\n\n\nSample size\n40\n\n\nMean\n35.77\n\n\nStandard Deviation\n6.13\n\n\nStandard Error\n0.97\n\n\nMinimum\n24.47\n\n\nQ1\n32.59\n\n\nMedian\n35.26\n\n\nQ3\n39.35\n\n\nMaximum\n52.12\n\n\nRange\n27.65\n\n\n\nAll values in liters per day (except n)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#step-2-visualize-the-data",
    "href": "chapters/part2-ch03-probability_distributions.html#step-2-visualize-the-data",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.3 Step 2: Visualize the Data",
    "text": "19.3 Step 2: Visualize the Data\n\n\nCode\np1 &lt;- ggplot(tibble(production = milk_production), aes(x = production)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 12,\n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = xbar, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1.2) +\n  annotate(\"text\", x = xbar + 1, y = 0.06,\n           label = sprintf(\"Mean = %.1f L\", xbar),\n           color = \"darkgreen\", hjust = 0, size = 4) +\n  labs(title = \"Distribution of Daily Milk Production\",\n       x = \"Liters per Day\", y = \"Density\")\n\np2 &lt;- ggplot(tibble(production = milk_production), aes(x = \"\", y = production)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\", outlier.size = 3) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2, color = \"darkblue\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 23,\n               size = 4, fill = \"red\", color = \"black\") +\n  labs(title = \"Boxplot with Individual Points\",\n       y = \"Liters per Day\", x = \"\") +\n  theme(axis.text.x = element_blank())\n\np1 + p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#step-3-check-normality-for-inference",
    "href": "chapters/part2-ch03-probability_distributions.html#step-3-check-normality-for-inference",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.4 Step 3: Check Normality (for inference)",
    "text": "19.4 Step 3: Check Normality (for inference)\n\n\nCode\n# Q-Q plot to assess normality\nqqnorm(milk_production, main = \"Q-Q Plot: Assessing Normality\",\n       pch = 19, col = \"steelblue\", cex = 1.5)\nqqline(milk_production, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nAssessment: Points fall approximately along the line,\\n\")\n\n\n\nAssessment: Points fall approximately along the line,\n\n\nCode\ncat(\"suggesting the data are reasonably normal.\\n\")\n\n\nsuggesting the data are reasonably normal.\n\n\nCode\ncat(\"This supports using normal-based inference methods.\\n\")\n\n\nThis supports using normal-based inference methods.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#step-4-calculate-confidence-interval",
    "href": "chapters/part2-ch03-probability_distributions.html#step-4-calculate-confidence-interval",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.5 Step 4: Calculate Confidence Interval",
    "text": "19.5 Step 4: Calculate Confidence Interval\n\n\nCode\n# 95% Confidence Interval\nci_lower &lt;- xbar - 1.96 * se\nci_upper &lt;- xbar + 1.96 * se\n\ncat(\"95% Confidence Interval for Mean Daily Production:\\n\")\n\n\n95% Confidence Interval for Mean Daily Production:\n\n\nCode\ncat(sprintf(\"  [%.2f, %.2f] liters per day\\n\\n\", ci_lower, ci_upper))\n\n\n  [33.87, 37.66] liters per day\n\n\nCode\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"We are 95% confident that the true mean daily milk production\\n\")\n\n\nWe are 95% confident that the true mean daily milk production\n\n\nCode\ncat(sprintf(\"for this Holstein strain is between %.1f and %.1f liters.\\n\\n\", ci_lower, ci_upper))\n\n\nfor this Holstein strain is between 33.9 and 37.7 liters.\n\n\nCode\ncat(\"In practical terms:\\n\")\n\n\nIn practical terms:\n\n\nCode\ncat(sprintf(\"  Best estimate (mean): %.1f liters/day\\n\", xbar))\n\n\n  Best estimate (mean): 35.8 liters/day\n\n\nCode\ncat(sprintf(\"  Margin of error: ±%.1f liters (1.96 × SE)\\n\", 1.96 * se))\n\n\n  Margin of error: ±1.9 liters (1.96 × SE)\n\n\nCode\ncat(sprintf(\"  Variability among cows (SD): %.1f liters\\n\", s))\n\n\n  Variability among cows (SD): 6.1 liters",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#step-5-standardize-observations-z-scores",
    "href": "chapters/part2-ch03-probability_distributions.html#step-5-standardize-observations-z-scores",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "19.6 Step 5: Standardize Observations (Z-scores)",
    "text": "19.6 Step 5: Standardize Observations (Z-scores)\n\n\nCode\n# Calculate z-scores\nz_scores &lt;- (milk_production - xbar) / s\n\n# Identify unusual observations\nunusual &lt;- abs(z_scores) &gt; 2\n\n# Create table of extreme cows\nextreme_cows &lt;- tibble(\n  Cow_ID = which(unusual),\n  Production = milk_production[unusual],\n  Z_score = z_scores[unusual],\n  Classification = ifelse(z_scores[unusual] &gt; 0, \"High producer\", \"Low producer\")\n) %&gt;%\n  arrange(desc(abs(Z_score)))\n\nif (nrow(extreme_cows) &gt; 0) {\n  cat(\"Cows with unusual production (|z| &gt; 2):\\n\")\n  extreme_cows %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = c(Production, Z_score), decimals = 2) %&gt;%\n    tab_header(title = \"Outlier Analysis\") %&gt;%\n    data_color(\n      columns = Z_score,\n      colors = scales::col_numeric(\n        palette = c(\"blue\", \"white\", \"red\"),\n        domain = c(-3, 3)\n      )\n    )\n} else {\n  cat(\"No cows with unusual production (all |z| &lt; 2)\\n\")\n}\n\n\nCows with unusual production (|z| &gt; 2):\n\n\n\n\n\n\n\n\nOutlier Analysis\n\n\nCow_ID\nProduction\nZ_score\nClassification\n\n\n\n\n23\n52.12\n2.67\nHigh producer\n\n\n20\n49.57\n2.25\nHigh producer",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#looking-ahead",
    "href": "chapters/part2-ch03-probability_distributions.html#looking-ahead",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "20.1 Looking Ahead",
    "text": "20.1 Looking Ahead\nNext week, we’ll use these foundational concepts to perform hypothesis testing:\n\nNull and alternative hypotheses\nType I and Type II errors\nOne-sample and two-sample t-tests\nP-values in the context of sampling distributions\nMaking decisions with confidence\n\nWith your understanding of sampling distributions and confidence intervals, hypothesis testing will make much more sense!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#reflection-questions",
    "href": "chapters/part2-ch03-probability_distributions.html#reflection-questions",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "20.2 Reflection Questions",
    "text": "20.2 Reflection Questions\nBefore next week, consider:\n\nFind a paper in your field. Do the authors report confidence intervals? If so, how do they interpret them?\nThink about a measurement you collect (e.g., animal weights, feed intake, milk yield). What would you estimate the population mean and SD to be? How large a sample would you need for SE &lt; 5% of the mean?\nSimulate your own CLT demonstration: Start with a non-normal distribution (e.g., exponential or uniform) and verify that sample means become normal.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#additional-resources",
    "href": "chapters/part2-ch03-probability_distributions.html#additional-resources",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "20.3 Additional Resources",
    "text": "20.3 Additional Resources\n\n20.3.1 R Packages\n\ndistributional: Modern tools for working with probability distributions\nggdist: Visualizing distributions and uncertainty\ninfer: Tidy framework for statistical inference\n\n\n\n20.3.2 Recommended Reading\n\n“OpenIntro Statistics” (free online) - Chapters 3-4 on probability and distributions\n“The Lady Tasting Tea” by David Salsburg - History of the CLT and its importance\n“Seeing Theory”: Interactive visualizations of probability and statistics (https://seeing-theory.brown.edu/)\n\n\n\n20.3.3 Videos\n\nStatQuest: “Normal Distribution, Clearly Explained” and “Central Limit Theorem”\n3Blue1Brown: “But what is the Central Limit Theorem?” (YouTube)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#practice-problems",
    "href": "chapters/part2-ch03-probability_distributions.html#practice-problems",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "20.4 Practice Problems",
    "text": "20.4 Practice Problems\nTry these on your own:\n\nCattle weights are N(650, 80²) kg. What proportion weigh:\n\nLess than 600 kg?\nBetween 640 and 700 kg?\nMore than 750 kg?\n\nSample means: If you take samples of n=25 from the population in #1, what are the mean and SE of the sampling distribution?\nConfidence intervals: A sample of 50 pigs has mean weight 110 kg and SD 14 kg. Calculate and interpret a 95% CI for the population mean.\nZ-scores: A pig weighs 125 kg. The population mean is 115 kg with SD 10 kg. Calculate the z-score and determine if this pig is unusual.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch03-probability_distributions.html#session-info",
    "href": "chapters/part2-ch03-probability_distributions.html#session-info",
    "title": "11  Week 3: Probability Distributions and Introduction to Inference",
    "section": "20.5 Session Info",
    "text": "20.5 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gt_1.1.0        scales_1.4.0    patchwork_1.3.2 broom_1.0.7    \n [5] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [9] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n[13] ggplot2_4.0.0   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] xml2_1.3.6         yaml_2.3.10        fastmap_1.2.0      R6_2.5.1          \n [9] labeling_0.4.3     generics_0.1.3     knitr_1.49         backports_1.5.0   \n[13] htmlwidgets_1.6.4  pillar_1.9.0       RColorBrewer_1.1-3 tzdb_0.4.0        \n[17] rlang_1.1.6        utf8_1.2.4         stringi_1.8.4      xfun_0.53         \n[21] sass_0.4.9         fs_1.6.5           S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.4          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.4.2         hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.1     glue_1.8.0         farver_2.1.2       fansi_1.0.6       \n[37] rmarkdown_2.29     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\nEnd of Week 3: Probability Distributions and Introduction to Inference",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Week 3: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html",
    "href": "chapters/part2-ch04-hypothesis_testing.html",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "",
    "text": "13 Introduction\nYou’re an animal nutritionist testing a new feed additive that’s supposed to increase daily weight gain in finishing pigs. You’ve conducted a trial with 30 pigs on the new feed and observed an average daily gain of 0.85 kg/day, compared to the historical average of 0.78 kg/day. The difference looks promising, but is it real, or could it have occurred by chance?\nThis is the fundamental question that hypothesis testing helps us answer. This week, we’ll develop a formal framework for making decisions about whether observed differences in data represent true effects or simply random variation.\nKey Questions We’ll Address:\nBy the end of this lecture, you’ll be able to conduct and interpret t-tests appropriately, check assumptions, calculate effect sizes, and understand the limitations of hypothesis testing.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-framework",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-framework",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "14.1 The Hypothesis Testing Framework",
    "text": "14.1 The Hypothesis Testing Framework\nHypothesis testing follows a structured approach:\n\nState hypotheses: Define null (H₀) and alternative (H₁) hypotheses\nChoose significance level: Typically α = 0.05\nCollect data and calculate a test statistic\nCalculate p-value: Probability of observing data this extreme under H₀\nMake decision: Reject H₀ if p &lt; α, otherwise fail to reject H₀\nInterpret in context with effect sizes and confidence intervals\n\n\n\n\n\n\n\nImportantHypothesis Testing is NOT Absolute Truth\n\n\n\nHypothesis testing doesn’t tell us:\n\nWhether H₀ is true\nWhether H₁ is true\nThe probability we’ve made a mistake\n\nIt tells us: How compatible our data is with the null hypothesis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-hypotheses",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-hypotheses",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "14.2 Null and Alternative Hypotheses",
    "text": "14.2 Null and Alternative Hypotheses\nThe null hypothesis (H₀) represents the status quo, no effect, or no difference. It’s the hypothesis we try to find evidence against.\nThe alternative hypothesis (H₁ or Hₐ) represents what we’re trying to find evidence for—usually that there is an effect or difference.\nExample: Feed Additive Trial\n\nH₀: The new feed additive has no effect on daily weight gain (μ = 0.78 kg/day)\nH₁: The new feed additive changes daily weight gain (μ ≠ 0.78 kg/day)\n\nThis is a two-sided test because we’re open to the additive increasing or decreasing weight gain.\nWe could also formulate one-sided tests:\n\nH₁: μ &gt; 0.78 (additive increases gain)\nH₁: μ &lt; 0.78 (additive decreases gain)\n\n\n\n\n\n\n\nWarningOne-Sided vs Two-Sided Tests\n\n\n\nUse two-sided tests by default. One-sided tests are only appropriate when:\n\nYou have strong a priori reasons to test in only one direction\nA difference in the other direction would be meaningless or impossible\nYou specified the direction before collecting data\n\nDon’t choose one-sided tests just to get p &lt; 0.05!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-visual-intuition",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-visual-intuition",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "14.3 Visual Intuition: Is There a Difference?",
    "text": "14.3 Visual Intuition: Is There a Difference?\nLet’s visualize what hypothesis testing is trying to determine:\n\n\nCode\n# Simulate two scenarios\nset.seed(123)\n\n# Scenario A: No real difference (H0 is true)\nscenario_a &lt;- tibble(\n  group = rep(c(\"Control\", \"Treatment\"), each = 30),\n  weight_gain = c(rnorm(30, mean = 0.78, sd = 0.12),\n                  rnorm(30, mean = 0.78, sd = 0.12))\n)\n\n# Scenario B: Real difference (H0 is false)\nscenario_b &lt;- tibble(\n  group = rep(c(\"Control\", \"Treatment\"), each = 30),\n  weight_gain = c(rnorm(30, mean = 0.78, sd = 0.12),\n                  rnorm(30, mean = 0.88, sd = 0.12))\n)\n\n# Plot both scenarios\np1 &lt;- ggplot(scenario_a, aes(x = group, y = weight_gain, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Scenario A: No True Difference\",\n       subtitle = \"Both groups sampled from same distribution\",\n       y = \"Daily Weight Gain (kg)\",\n       x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(0.4, 1.2)\n\np2 &lt;- ggplot(scenario_b, aes(x = group, y = weight_gain, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Scenario B: Real Difference\",\n       subtitle = \"Treatment group has higher mean\",\n       y = \"Daily Weight Gain (kg)\",\n       x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(0.4, 1.2)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe challenge: Even when there’s no real difference (Scenario A), we still see some difference in sample means due to random variation. Hypothesis testing helps us quantify whether the observed difference is larger than we’d expect by chance alone.\n\n\nCode\n# Calculate means and run t-tests\nscenario_a_summary &lt;- scenario_a %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_gain = mean(weight_gain), .groups = 'drop')\n\nscenario_b_summary &lt;- scenario_b %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_gain = mean(weight_gain), .groups = 'drop')\n\ntest_a &lt;- t.test(weight_gain ~ group, data = scenario_a)\ntest_b &lt;- t.test(weight_gain ~ group, data = scenario_b)\n\ncat(\"Scenario A (no true difference):\\n\")\n\n\nScenario A (no true difference):\n\n\nCode\ncat(sprintf(\"  Control: %.3f kg/day, Treatment: %.3f kg/day\\n\",\n            scenario_a_summary$mean_gain[1], scenario_a_summary$mean_gain[2]))\n\n\n  Control: 0.774 kg/day, Treatment: 0.801 kg/day\n\n\nCode\ncat(sprintf(\"  Difference: %.3f kg/day, p-value: %.3f\\n\\n\",\n            diff(scenario_a_summary$mean_gain), test_a$p.value))\n\n\n  Difference: 0.027 kg/day, p-value: 0.342\n\n\nCode\ncat(\"Scenario B (true difference = 0.10 kg/day):\\n\")\n\n\nScenario B (true difference = 0.10 kg/day):\n\n\nCode\ncat(sprintf(\"  Control: %.3f kg/day, Treatment: %.3f kg/day\\n\",\n            scenario_b_summary$mean_gain[1], scenario_b_summary$mean_gain[2]))\n\n\n  Control: 0.783 kg/day, Treatment: 0.869 kg/day\n\n\nCode\ncat(sprintf(\"  Difference: %.3f kg/day, p-value: %.4f\\n\",\n            diff(scenario_b_summary$mean_gain), test_b$p.value))\n\n\n  Difference: 0.086 kg/day, p-value: 0.0028\n\n\nIn Scenario A, we correctly fail to reject H₀ (p &gt; 0.05). In Scenario B, we correctly reject H₀ (p &lt; 0.05) and conclude there’s evidence of a real difference.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-error-definitions",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-error-definitions",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "15.1 Definitions",
    "text": "15.1 Definitions\nType I Error (False Positive, α): Rejecting H₀ when it’s actually true\n\nConcluding there’s an effect when there isn’t one\nThe probability of Type I error is α (significance level)\nTypically set at α = 0.05 (5% false positive rate)\n\nType II Error (False Negative, β): Failing to reject H₀ when it’s actually false\n\nConcluding there’s no effect when there is one\nThe probability of Type II error is β\nPower = 1 - β (probability of correctly rejecting false H₀)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-truth-table",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-truth-table",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "15.2 The Truth Table",
    "text": "15.2 The Truth Table\n\n\n\n\nH₀ is True\nH₀ is False\n\n\n\n\nReject H₀\nType I Error (α)\n✓ Correct (Power)\n\n\nFail to Reject H₀\n✓ Correct (1-α)\nType II Error (β)\n\n\n\n\n\n\n\n\n\nNoteWhy α = 0.05?\n\n\n\nThe 0.05 threshold is a convention, not a law of nature. R.A. Fisher suggested it as a convenient benchmark, but it’s arbitrary. Some fields use:\n\nα = 0.01 for more stringent control of false positives\nα = 0.10 for exploratory research where false negatives are more costly\n\nFocus on effect sizes and confidence intervals, not just whether p &lt; 0.05.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-error-consequences",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-error-consequences",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "15.3 Consequences in Animal Science",
    "text": "15.3 Consequences in Animal Science\nThe consequences of these errors depend on context:\nExample 1: New Feed Additive\n\nType I Error: Conclude additive works when it doesn’t → waste money on ineffective product\nType II Error: Conclude additive doesn’t work when it does → miss opportunity to improve productivity\n\nExample 2: Disease Screening\n\nType I Error: Conclude animal is diseased when it’s healthy → unnecessary treatment, stress, cost\nType II Error: Conclude animal is healthy when it’s diseased → disease spreads, welfare issues\n\nThe relative costs of these errors should inform your choice of α and sample size (which affects β).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-simulate-type-i",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-simulate-type-i",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "15.4 Simulating Type I Errors",
    "text": "15.4 Simulating Type I Errors\nLet’s demonstrate that when H₀ is true, we still get p &lt; 0.05 about 5% of the time:\n\n\nCode\n# Function to run one trial where H0 is TRUE\nrun_null_true_trial &lt;- function() {\n  # Both groups from same distribution (no real difference)\n  control &lt;- rnorm(25, mean = 100, sd = 15)\n  treatment &lt;- rnorm(25, mean = 100, sd = 15)  # Same mean!\n\n  test_result &lt;- t.test(treatment, control)\n  test_result$p.value\n}\n\n# Run 1000 trials\nn_sims &lt;- 1000\np_values_null_true &lt;- replicate(n_sims, run_null_true_trial())\n\n# Visualize\ntibble(p_value = p_values_null_true) %&gt;%\n  ggplot(aes(x = p_value)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0.05, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Distribution of P-values When H₀ is True\",\n       subtitle = sprintf(\"Proportion with p &lt; 0.05: %.3f (expected: 0.05)\",\n                         mean(p_values_null_true &lt; 0.05)),\n       x = \"P-value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntype_i_rate &lt;- mean(p_values_null_true &lt; 0.05)\ncat(sprintf(\"Type I error rate: %.3f (%.1f%%)\\n\", type_i_rate, type_i_rate * 100))\n\n\nType I error rate: 0.040 (4.0%)\n\n\nCode\ncat(sprintf(\"Out of %d trials: %d false positives\\n\",\n            n_sims, sum(p_values_null_true &lt; 0.05)))\n\n\nOut of 1000 trials: 40 false positives\n\n\nKey insight: Even when there’s no real effect, we’ll get “significant” results about 5% of the time. This is why replication is crucial in science!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-simulate-type-ii",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-simulate-type-ii",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "15.5 Simulating Type II Errors",
    "text": "15.5 Simulating Type II Errors\nNow let’s see Type II errors—when there IS a real effect, but we fail to detect it:\n\n\nCode\n# Function to run one trial where H0 is FALSE (real effect exists)\nrun_null_false_trial &lt;- function(true_effect = 10, sample_size = 20, sd = 15) {\n  control &lt;- rnorm(sample_size, mean = 100, sd = sd)\n  treatment &lt;- rnorm(sample_size, mean = 100 + true_effect, sd = sd)\n\n  test_result &lt;- t.test(treatment, control)\n  test_result$p.value\n}\n\n# Small effect, small sample\np_values_small &lt;- replicate(n_sims, run_null_false_trial(true_effect = 5, sample_size = 20))\n\n# Medium effect, small sample\np_values_medium &lt;- replicate(n_sims, run_null_false_trial(true_effect = 10, sample_size = 20))\n\n# Medium effect, large sample\np_values_large_n &lt;- replicate(n_sims, run_null_false_trial(true_effect = 10, sample_size = 50))\n\n# Combine results\nerror_rates &lt;- tibble(\n  Scenario = c(\"Small effect (d=0.33), n=20\",\n               \"Medium effect (d=0.67), n=20\",\n               \"Medium effect (d=0.67), n=50\"),\n  `Type II Error Rate (β)` = c(mean(p_values_small &gt;= 0.05),\n                                 mean(p_values_medium &gt;= 0.05),\n                                 mean(p_values_large_n &gt;= 0.05)),\n  `Power (1-β)` = 1 - `Type II Error Rate (β)`\n)\n\nknitr::kable(error_rates, digits = 3, align = 'lcc',\n             caption = \"Type II Error Rates Under Different Scenarios\")\n\n\n\nType II Error Rates Under Different Scenarios\n\n\nScenario\nType II Error Rate (β)\nPower (1-β)\n\n\n\n\nSmall effect (d=0.33), n=20\n0.831\n0.169\n\n\nMedium effect (d=0.67), n=20\n0.474\n0.526\n\n\nMedium effect (d=0.67), n=50\n0.093\n0.907\n\n\n\n\n\nKey insights:\n\nSmaller effects are harder to detect (higher β)\nLarger samples reduce Type II error (increase power)\nEven with a real effect, we often fail to detect it with small samples!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-power-factors",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-power-factors",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "16.1 Factors Affecting Power",
    "text": "16.1 Factors Affecting Power\nPower depends on four factors:\n\nEffect size: Larger effects are easier to detect\nSample size (n): More data = more power\nSignificance level (α): Lower α = lower power (trade-off with Type I error)\nVariability (σ): Less noisy data = more power\n\n\n\n\n\n\n\nTipTypical Power Target\n\n\n\nMany researchers aim for power = 0.80 (80% probability of detecting the effect). This means accepting a 20% Type II error rate (β = 0.20).\nThe choice of 80% is conventional, like α = 0.05. In some contexts (e.g., clinical trials), you might want higher power (0.90 or 0.95).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-power-curves",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-power-curves",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "16.2 Visualizing Power",
    "text": "16.2 Visualizing Power\nLet’s visualize how power changes with sample size and effect size:\n\n\nCode\n# Function to calculate power via simulation\ncalculate_power_sim &lt;- function(n, effect_size, sd = 15, alpha = 0.05, n_sims = 1000) {\n  p_values &lt;- replicate(n_sims, {\n    control &lt;- rnorm(n, mean = 100, sd = sd)\n    treatment &lt;- rnorm(n, mean = 100 + effect_size, sd = sd)\n    t.test(treatment, control)$p.value\n  })\n  mean(p_values &lt; alpha)\n}\n\n# Calculate power for different scenarios\npower_data &lt;- expand_grid(\n  n = seq(10, 100, by = 5),\n  effect_size = c(5, 10, 15, 20)\n) %&gt;%\n  mutate(\n    effect_label = sprintf(\"Effect = %d (d = %.2f)\", effect_size, effect_size / 15),\n    power = map2_dbl(n, effect_size, ~calculate_power_sim(.x, .y, n_sims = 500))\n  )\n\n# Plot power curves\nggplot(power_data, aes(x = n, y = power, color = effect_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray40\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\", color = \"gray60\") +\n  annotate(\"text\", x = 90, y = 0.82, label = \"Target power = 0.80\", size = 3) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Statistical Power vs Sample Size\",\n       subtitle = \"For two-sample t-test with SD = 15, α = 0.05\",\n       x = \"Sample Size per Group\",\n       y = \"Power (1 - β)\",\n       color = \"Effect Size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nKey insights from power curves:\n\nSmall effects need large samples: For a small effect (d = 0.33), you need n ≈ 90 per group to achieve 80% power\nLarge effects need small samples: For a large effect (d = 1.33), n ≈ 10 per group is sufficient\nDiminishing returns: Going from n=20 to n=40 adds more power than going from n=60 to n=80",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-sample-size",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-sample-size",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "16.3 Sample Size Planning",
    "text": "16.3 Sample Size Planning\nBefore conducting a study, you should estimate the required sample size to achieve adequate power. This requires:\n\nSpecify α: Usually 0.05\nSpecify desired power: Usually 0.80\nEstimate effect size: Based on pilot data or literature\nEstimate variability: SD or variance\n\nExample: Feed Additive Study\nSuppose we expect the feed additive to increase daily gain by 0.08 kg (from 0.78 to 0.86 kg/day), and we know the SD ≈ 0.12 kg/day from previous trials. How many pigs do we need?\n\n\nCode\n# Calculate Cohen's d\nexpected_effect &lt;- 0.08\nexpected_sd &lt;- 0.12\ncohens_d &lt;- expected_effect / expected_sd\n\ncat(sprintf(\"Expected Cohen's d: %.2f\\n\", cohens_d))\n\n\nExpected Cohen's d: 0.67\n\n\nCode\n# Use power.t.test for analytical power calculation\npower_analysis &lt;- power.t.test(\n  delta = expected_effect,    # Difference in means\n  sd = expected_sd,           # Standard deviation\n  sig.level = 0.05,           # α\n  power = 0.80,               # Desired power\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(power_analysis)\n\n\n\n     Two-sample t test power calculation \n\n              n = 36.3058\n          delta = 0.08\n             sd = 0.12\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\ncat(sprintf(\"\\nRequired sample size: %.0f pigs per group\\n\", ceiling(power_analysis$n)))\n\n\n\nRequired sample size: 37 pigs per group\n\n\nCode\ncat(sprintf(\"Total pigs needed: %.0f\\n\", 2 * ceiling(power_analysis$n)))\n\n\nTotal pigs needed: 74\n\n\nInterpretation: We need approximately 36 pigs per group (72 total) to have 80% power to detect a difference of 0.08 kg/day.\n\n\n\n\n\n\nImportantPower Analysis Should Be Done BEFORE Data Collection\n\n\n\nConducting power analysis after your study doesn’t change anything about your results. Power analysis is most useful for:\n\nStudy planning: Determine required sample size before collecting data\nInterpreting non-significant results: A non-significant result from an underpowered study is uninformative\nEvaluating published research: Were studies adequately powered to detect realistic effects?\n\n“Post-hoc power analysis” of your own data is generally not recommended.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-assumptions",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-assumptions",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "17.1 Assumptions",
    "text": "17.1 Assumptions\n\nIndependence: Observations are independent of each other\nNormality: The data (or sampling distribution of means) is approximately normal\n\n\n\n\n\n\n\nNoteCentral Limit Theorem to the Rescue\n\n\n\nThe t-test is fairly robust to violations of normality when:\n\nSample size is moderate to large (n &gt; 30 as a rule of thumb)\nThe distribution isn’t extremely skewed\nThere are no extreme outliers\n\nFor small samples (n &lt; 30), normality is more important.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-example",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-example",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "17.2 Worked Example: Milk Production",
    "text": "17.2 Worked Example: Milk Production\n\n\nCode\n# Simulate milk production data\nset.seed(456)\nmilk_production &lt;- tibble(\n  cow_id = 1:25,\n  milk_kg = rnorm(25, mean = 37.2, sd = 4.5)  # True mean = 37.2 (higher than historical 35)\n)\n\n# Summary statistics\nmilk_summary &lt;- milk_production %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(milk_kg),\n    sd = sd(milk_kg),\n    se = sd / sqrt(n),\n    median = median(milk_kg),\n    min = min(milk_kg),\n    max = max(milk_kg)\n  )\n\nknitr::kable(milk_summary, digits = 2,\n             caption = \"Summary Statistics: Milk Production (kg/day)\")\n\n\n\nSummary Statistics: Milk Production (kg/day)\n\n\nn\nmean\nsd\nse\nmedian\nmin\nmax\n\n\n\n\n25\n38.32\n5.34\n1.07\n38.94\n29.47\n47.46\n\n\n\n\n\nHypotheses:\n\nH₀: μ = 35 kg/day (no change from historical average)\nH₁: μ ≠ 35 kg/day (production has changed)\n\n\n17.2.1 Step 1: Check Assumptions\n\n\nCode\n# Visual check: Histogram and QQ plot\np1 &lt;- ggplot(milk_production, aes(x = milk_kg)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 35, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = 35, y = 5.5, label = \"Historical\\nmean = 35\",\n           color = \"red\", hjust = -0.1, size = 3) +\n  labs(title = \"Distribution of Milk Production\",\n       x = \"Milk Production (kg/day)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 11)\n\np2 &lt;- ggplot(milk_production, aes(sample = milk_kg)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Q-Q Plot\",\n       subtitle = \"Checking normality assumption\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 11)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: The histogram looks reasonably symmetric, and the QQ plot shows points close to the line → normality assumption seems reasonable.\nFormal test: Shapiro-Wilk test\n\n\nCode\nshapiro_result &lt;- shapiro.test(milk_production$milk_kg)\ncat(sprintf(\"Shapiro-Wilk test: W = %.4f, p-value = %.3f\\n\",\n            shapiro_result$statistic, shapiro_result$p.value))\n\n\nShapiro-Wilk test: W = 0.9544, p-value = 0.315\n\n\nCode\nif(shapiro_result$p.value &gt; 0.05) {\n  cat(\"→ No evidence against normality (p &gt; 0.05)\\n\")\n} else {\n  cat(\"→ Some evidence against normality (p &lt; 0.05)\\n\")\n}\n\n\n→ No evidence against normality (p &gt; 0.05)\n\n\n\n\n\n\n\n\nWarningDon’t Over-Rely on Normality Tests\n\n\n\nShapiro-Wilk and other normality tests can be:\n\nToo sensitive with large samples (reject for trivial deviations)\nNot sensitive enough with small samples (fail to detect important deviations)\n\nRecommendation: Use visual assessment (QQ plots) as your primary tool, and consider the robustness of the t-test.\n\n\n\n\n17.2.2 Step 2: Conduct the t-Test\n\n\nCode\n# One-sample t-test\nmilk_test &lt;- t.test(milk_production$milk_kg, mu = 35)\n\n# Tidy output\nmilk_test_tidy &lt;- tidy(milk_test)\n\nknitr::kable(milk_test_tidy, digits = 3,\n             caption = \"One-Sample t-Test Results\")\n\n\n\nOne-Sample t-Test Results\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n38.319\n3.105\n0.005\n24\n36.113\n40.524\nOne Sample t-test\ntwo.sided\n\n\n\n\n\nCode\n# Print results\ncat(sprintf(\"\\nOne-Sample t-Test Results:\\n\"))\n\n\n\nOne-Sample t-Test Results:\n\n\nCode\ncat(sprintf(\"Sample mean: %.2f kg/day\\n\", milk_test$estimate))\n\n\nSample mean: 38.32 kg/day\n\n\nCode\ncat(sprintf(\"Hypothesized mean: %.2f kg/day\\n\", 35))\n\n\nHypothesized mean: 35.00 kg/day\n\n\nCode\ncat(sprintf(\"Difference: %.2f kg/day\\n\", milk_test$estimate - 35))\n\n\nDifference: 3.32 kg/day\n\n\nCode\ncat(sprintf(\"t-statistic: %.3f\\n\", milk_test$statistic))\n\n\nt-statistic: 3.105\n\n\nCode\ncat(sprintf(\"Degrees of freedom: %d\\n\", milk_test$parameter))\n\n\nDegrees of freedom: 24\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", milk_test$p.value))\n\n\nP-value: 0.0048\n\n\nCode\ncat(sprintf(\"95%% CI: [%.2f, %.2f]\\n\", milk_test$conf.int[1], milk_test$conf.int[2]))\n\n\n95% CI: [36.11, 40.52]\n\n\n\n\n17.2.3 Step 3: Interpret Results\n\n\nCode\n# Visualize result with confidence interval\nggplot(milk_production, aes(x = \"Sample\", y = milk_kg)) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 2, color = \"steelblue\") +\n  geom_hline(yintercept = 35, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 4, color = \"darkblue\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.2, color = \"darkblue\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = 35, label = \"Historical mean\\n(H₀: μ = 35)\",\n           color = \"red\", hjust = 0, size = 3) +\n  annotate(\"text\", x = 1.3, y = milk_test$estimate,\n           label = sprintf(\"Sample mean\\n%.1f kg/day\", milk_test$estimate),\n           color = \"darkblue\", hjust = 0, size = 3) +\n  labs(title = \"Milk Production: Sample vs Historical Mean\",\n       subtitle = sprintf(\"95%% CI: [%.1f, %.1f], p = %.4f\",\n                         milk_test$conf.int[1], milk_test$conf.int[2], milk_test$p.value),\n       x = \"\",\n       y = \"Milk Production (kg/day)\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\nStatistical conclusion: We reject H₀ (p = 0.005). There is strong evidence that milk production under the new feeding program differs from the historical average of 35 kg/day.\nPractical interpretation: The new feeding program is associated with an increase of approximately 3.3 kg/day (95% CI: [1.1, 5.5]). This is both statistically significant and potentially economically meaningful.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-effect",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-one-sample-effect",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "17.3 Effect Size for One-Sample t-Test",
    "text": "17.3 Effect Size for One-Sample t-Test\n\n\nCode\n# Calculate Cohen's d for one-sample test\ncohens_d_one_sample &lt;- (milk_test$estimate - 35) / milk_summary$sd\n\ncat(sprintf(\"Cohen's d: %.3f\\n\", cohens_d_one_sample))\n\n\nCohen's d: 0.621\n\n\nCode\n# Interpretation\nif(abs(cohens_d_one_sample) &lt; 0.2) {\n  interpretation &lt;- \"negligible\"\n} else if(abs(cohens_d_one_sample) &lt; 0.5) {\n  interpretation &lt;- \"small\"\n} else if(abs(cohens_d_one_sample) &lt; 0.8) {\n  interpretation &lt;- \"medium\"\n} else {\n  interpretation &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", interpretation))\n\n\nEffect size interpretation: medium\n\n\nCohen’s d guidelines (rough benchmarks):\n\nd = 0.2: Small effect\nd = 0.5: Medium effect\nd = 0.8: Large effect\n\nOur effect size is medium, indicating a substantial difference from the historical mean.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-two-sample-assumptions",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-two-sample-assumptions",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "18.1 Assumptions",
    "text": "18.1 Assumptions\n\nIndependence: Observations within and between groups are independent\nNormality: Data in each group is approximately normally distributed\nEqual variances (for standard t-test): The variances in the two groups are equal\n\n\n\n\n\n\n\nNoteWelch’s t-Test: Unequal Variances\n\n\n\nIf variances are unequal, use Welch’s t-test (the default in R’s t.test()). It adjusts the degrees of freedom to account for unequal variances.\nThe standard t-test assumes equal variances (Student’s t-test), but Welch’s t-test is more robust and is generally recommended as the default.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-two-sample-example",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-two-sample-example",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "18.2 Worked Example: Grain Supplements",
    "text": "18.2 Worked Example: Grain Supplements\n\n\nCode\n# Simulate weight gain data\nset.seed(789)\ncattle_gain &lt;- tibble(\n  supplement = rep(c(\"A\", \"B\"), each = 20),\n  weight_gain_kg = c(\n    rnorm(20, mean = 185, sd = 22),  # Supplement A\n    rnorm(20, mean = 205, sd = 25)   # Supplement B (higher gain)\n  )\n)\n\n# Summary statistics by group\ncattle_summary &lt;- cattle_gain %&gt;%\n  group_by(supplement) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(weight_gain_kg),\n    sd = sd(weight_gain_kg),\n    se = sd / sqrt(n),\n    median = median(weight_gain_kg),\n    .groups = 'drop'\n  )\n\nknitr::kable(cattle_summary, digits = 2,\n             caption = \"Summary Statistics: Weight Gain by Supplement\")\n\n\n\nSummary Statistics: Weight Gain by Supplement\n\n\nsupplement\nn\nmean\nsd\nse\nmedian\n\n\n\n\nA\n20\n178.18\n15.96\n3.57\n176.60\n\n\nB\n20\n197.72\n17.84\n3.99\n196.95\n\n\n\n\n\nHypotheses:\n\nH₀: μ_A = μ_B (no difference in weight gain between supplements)\nH₁: μ_A ≠ μ_B (supplements lead to different weight gains)\n\n\n18.2.1 Step 1: Visualize the Data\n\n\nCode\n# Box plots with individual points\np1 &lt;- ggplot(cattle_gain, aes(x = supplement, y = weight_gain_kg, fill = supplement)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"red\", color = \"darkred\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Weight Gain by Supplement Type\",\n       subtitle = \"Diamonds show group means\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n# Density plots\np2 &lt;- ggplot(cattle_gain, aes(x = weight_gain_kg, fill = supplement)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(data = cattle_summary, aes(xintercept = mean, color = supplement),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Distribution of Weight Gain\",\n       subtitle = \"Dashed lines show group means\",\n       x = \"Weight Gain (kg)\",\n       y = \"Density\") +\n  theme_minimal(base_size = 12)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: Supplement B appears to produce higher weight gain on average, with some overlap between distributions.\n\n\n18.2.2 Step 2: Check Assumptions\nNormality Check (QQ Plots by Group):\n\n\nCode\nggplot(cattle_gain, aes(sample = weight_gain_kg, color = supplement)) +\n  stat_qq(size = 2) +\n  stat_qq_line(linetype = \"dashed\") +\n  facet_wrap(~supplement) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Q-Q Plots by Supplement\",\n       subtitle = \"Checking normality assumption\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nBoth groups show reasonably normal distributions.\nEqual Variance Check (Levene’s Test):\n\n\nCode\n# Levene's test for homogeneity of variance\nlevene_result &lt;- leveneTest(weight_gain_kg ~ supplement, data = cattle_gain)\n\ncat(\"Levene's Test for Equality of Variances:\\n\")\n\n\nLevene's Test for Equality of Variances:\n\n\nCode\nprint(levene_result)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.7142 0.4033\n      38               \n\n\nCode\ncat(sprintf(\"\\nP-value: %.3f\\n\", levene_result$`Pr(&gt;F)`[1]))\n\n\n\nP-value: 0.403\n\n\nCode\nif(levene_result$`Pr(&gt;F)`[1] &gt; 0.05) {\n  cat(\"→ No evidence of unequal variances (p &gt; 0.05)\\n\")\n  cat(\"  Standard t-test or Welch's t-test are both appropriate\\n\")\n} else {\n  cat(\"→ Evidence of unequal variances (p &lt; 0.05)\\n\")\n  cat(\"  Welch's t-test is recommended\\n\")\n}\n\n\n→ No evidence of unequal variances (p &gt; 0.05)\n  Standard t-test or Welch's t-test are both appropriate\n\n\n\n\n\n\n\n\nTipVisualizing Variance Differences\n\n\n\nA simple way to compare variances visually:\n\n\nCode\ncattle_gain %&gt;%\n  ggplot(aes(x = supplement, y = weight_gain_kg)) +\n  geom_boxplot(aes(fill = supplement), alpha = 0.4) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1),\n               geom = \"errorbar\", width = 0.3, linewidth = 1, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"darkred\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Mean ± SD by Group\",\n       subtitle = \"Error bars show ±1 SD\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIf one group’s error bars are much longer, variances may be unequal.\n\n\n\n\n18.2.3 Step 3: Conduct the t-Test\n\n\nCode\n# Welch's t-test (default, doesn't assume equal variances)\ncattle_test_welch &lt;- t.test(weight_gain_kg ~ supplement, data = cattle_gain)\n\n# Student's t-test (assumes equal variances)\ncattle_test_student &lt;- t.test(weight_gain_kg ~ supplement, data = cattle_gain, var.equal = TRUE)\n\n# Compare results\ncat(\"Welch's t-Test (unequal variances assumed):\\n\")\n\n\nWelch's t-Test (unequal variances assumed):\n\n\nCode\ncat(sprintf(\"  t(%.2f) = %.3f, p = %.4f\\n\",\n            cattle_test_welch$parameter,\n            cattle_test_welch$statistic,\n            cattle_test_welch$p.value))\n\n\n  t(37.54) = -3.651, p = 0.0008\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            cattle_test_welch$conf.int[1], cattle_test_welch$conf.int[2]))\n\n\n  95% CI for difference: [-30.38, -8.70]\n\n\nCode\ncat(\"Student's t-Test (equal variances assumed):\\n\")\n\n\nStudent's t-Test (equal variances assumed):\n\n\nCode\ncat(sprintf(\"  t(%d) = %.3f, p = %.4f\\n\",\n            cattle_test_student$parameter,\n            cattle_test_student$statistic,\n            cattle_test_student$p.value))\n\n\n  t(38) = -3.651, p = 0.0008\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\",\n            cattle_test_student$conf.int[1], cattle_test_student$conf.int[2]))\n\n\n  95% CI for difference: [-30.38, -8.71]\n\n\nNote: In this case, both tests give similar results because the variances are fairly similar. When in doubt, use Welch’s t-test (the default).\n\n\n18.2.4 Step 4: Calculate Effect Size\n\n\nCode\n# Calculate Cohen's d\ncattle_cohen_d &lt;- cohen.d(weight_gain_kg ~ supplement, data = cattle_gain)\n\nprint(cattle_cohen_d)\n\n\n\nCohen's d\n\nd estimate: -1.154639 (large)\n95 percent confidence interval:\n     lower      upper \n-1.8460956 -0.4631817 \n\n\nCode\ncat(sprintf(\"\\nCohen's d: %.3f\\n\", cattle_cohen_d$estimate))\n\n\n\nCohen's d: -1.155\n\n\nCode\ncat(sprintf(\"95%% CI for d: [%.3f, %.3f]\\n\",\n            cattle_cohen_d$conf.int[1], cattle_cohen_d$conf.int[2]))\n\n\n95% CI for d: [-1.846, -0.463]\n\n\nCode\n# Interpretation\nd_value &lt;- abs(cattle_cohen_d$estimate)\nif(d_value &lt; 0.2) {\n  interpretation &lt;- \"negligible\"\n} else if(d_value &lt; 0.5) {\n  interpretation &lt;- \"small\"\n} else if(d_value &lt; 0.8) {\n  interpretation &lt;- \"medium\"\n} else {\n  interpretation &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", interpretation))\n\n\nEffect size interpretation: large\n\n\n\n\n18.2.5 Step 5: Interpret and Report\n\n\nCode\n# Create a clean visualization for reporting\nmean_diff &lt;- diff(cattle_summary$mean)\n\nggplot(cattle_summary, aes(x = supplement, y = mean, fill = supplement)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = mean - se * 1.96, ymax = mean + se * 1.96),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.1f kg\", mean)),\n            vjust = -2.5, fontface = \"bold\", size = 4) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  annotate(\"segment\", x = 1, xend = 2, y = 230, yend = 230,\n           arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = 1.5, y = 235,\n           label = sprintf(\"Difference: %.1f kg\\np = %.3f\",\n                          mean_diff, cattle_test_welch$p.value),\n           size = 3.5, fontface = \"bold\") +\n  labs(title = \"Weight Gain by Supplement Type\",\n       subtitle = \"Error bars show 95% confidence intervals\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  ylim(0, 250) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nResults Summary:\nBeef steers receiving Supplement B gained significantly more weight (M = 197.7 kg, SD = 17.8) compared to those receiving Supplement A (M = 178.2 kg, SD = 16.0), t(37.5) = -3.65, p = 0.001, 95% CI [-30.4, -8.7], d = 1.15. This represents a large effect.\nPractical interpretation: Supplement B produces approximately 20 kg more weight gain than Supplement A, which could translate to meaningful economic benefits for producers.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-pairing-matters",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-pairing-matters",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "19.1 Why Pairing Matters",
    "text": "19.1 Why Pairing Matters\nWhen measurements are paired, we’re interested in the differences within pairs, not the absolute values in each group.\nExample: Testing a feed supplement by measuring weight gain in the same animals before and after treatment is more powerful than comparing two different groups, because we control for individual variation in baseline weight and genetics.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-paired-example",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-paired-example",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "19.2 Worked Example: Milk Yield Before and After Treatment",
    "text": "19.2 Worked Example: Milk Yield Before and After Treatment\nA dairy researcher wants to test whether a new probiotic supplement increases milk yield. They measure milk production in 20 cows before supplementation, then again after 4 weeks on the supplement.\n\n\nCode\n# Simulate paired data\nset.seed(321)\n\n# Create baseline variation between cows\ncow_baseline &lt;- rnorm(20, mean = 30, sd = 5)\n\n# Before treatment: baseline + random day-to-day variation\nmilk_before &lt;- cow_baseline + rnorm(20, mean = 0, sd = 2)\n\n# After treatment: baseline + treatment effect + random variation\ntreatment_effect &lt;- 2.5  # True effect = 2.5 kg/day increase\nmilk_after &lt;- cow_baseline + treatment_effect + rnorm(20, mean = 0, sd = 2)\n\n# Combine into data frame\nmilk_paired &lt;- tibble(\n  cow_id = 1:20,\n  before = milk_before,\n  after = milk_after,\n  difference = after - before\n)\n\n# Long format for plotting\nmilk_paired_long &lt;- milk_paired %&gt;%\n  pivot_longer(cols = c(before, after),\n               names_to = \"time\",\n               values_to = \"milk_yield\") %&gt;%\n  mutate(time = factor(time, levels = c(\"before\", \"after\")))\n\n# Summary statistics\nmilk_paired_summary &lt;- milk_paired_long %&gt;%\n  group_by(time) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(milk_yield),\n    sd = sd(milk_yield),\n    se = sd / sqrt(n),\n    .groups = 'drop'\n  )\n\nknitr::kable(milk_paired_summary, digits = 2,\n             caption = \"Summary Statistics: Milk Yield Before and After Treatment\")\n\n\n\nSummary Statistics: Milk Yield Before and After Treatment\n\n\ntime\nn\nmean\nsd\nse\n\n\n\n\nbefore\n20\n31.13\n5.22\n1.17\n\n\nafter\n20\n34.06\n4.66\n1.04\n\n\n\n\n\nCode\n# Summary of differences\ndiff_summary &lt;- milk_paired %&gt;%\n  summarise(\n    mean_diff = mean(difference),\n    sd_diff = sd(difference),\n    se_diff = sd_diff / sqrt(n())\n  )\n\ncat(sprintf(\"\\nMean difference (after - before): %.2f kg/day\\n\", diff_summary$mean_diff))\n\n\n\nMean difference (after - before): 2.93 kg/day\n\n\nCode\ncat(sprintf(\"SD of differences: %.2f kg/day\\n\", diff_summary$sd_diff))\n\n\nSD of differences: 2.88 kg/day\n\n\n\n19.2.1 Visualizing Paired Data\nThe key to paired data is visualizing the connections between measurements:\n\n\nCode\n# Paired plot showing connections\np1 &lt;- ggplot(milk_paired_long, aes(x = time, y = milk_yield, group = cow_id)) +\n  geom_line(alpha = 0.3, color = \"gray50\") +\n  geom_point(aes(color = time), size = 2, alpha = 0.7) +\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\",\n               color = \"red\", linewidth = 1.5, linetype = \"solid\") +\n  stat_summary(aes(group = 1), fun = mean, geom = \"point\",\n               color = \"red\", size = 4, shape = 18) +\n  scale_color_manual(values = c(\"steelblue\", \"orange\")) +\n  labs(title = \"Milk Yield Before and After Treatment\",\n       subtitle = \"Lines connect measurements from same cow\",\n       x = \"\",\n       y = \"Milk Yield (kg/day)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n# Distribution of differences\np2 &lt;- ggplot(milk_paired, aes(x = difference)) +\n  geom_histogram(bins = 12, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = diff_summary$mean_diff, color = \"darkgreen\",\n             linetype = \"solid\", linewidth = 1) +\n  annotate(\"text\", x = 0, y = 4.5, label = \"No change\",\n           color = \"red\", hjust = -0.1, size = 3) +\n  annotate(\"text\", x = diff_summary$mean_diff, y = 4.5,\n           label = sprintf(\"Mean\\ndiff = %.1f\", diff_summary$mean_diff),\n           color = \"darkgreen\", hjust = -0.1, size = 3) +\n  labs(title = \"Distribution of Within-Cow Differences\",\n       x = \"Difference in Milk Yield (kg/day)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 12)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey observation: Most lines slope upward, indicating that individual cows increased milk production. The distribution of differences is centered above zero.\n\n\n19.2.2 Paired vs Unpaired Analysis\nLet’s compare what happens if we (incorrectly) treat this as unpaired data:\n\n\nCode\n# Paired t-test (CORRECT)\npaired_test &lt;- t.test(milk_paired$after, milk_paired$before, paired = TRUE)\n\n# Unpaired t-test (INCORRECT for this design)\nunpaired_test &lt;- t.test(milk_paired$after, milk_paired$before, paired = FALSE)\n\n# Compare results\ncat(\"PAIRED t-Test (Correct Analysis):\\n\")\n\n\nPAIRED t-Test (Correct Analysis):\n\n\nCode\ncat(sprintf(\"  t(%d) = %.3f, p = %.4f\\n\",\n            paired_test$parameter, paired_test$statistic, paired_test$p.value))\n\n\n  t(19) = 4.553, p = 0.0002\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            paired_test$conf.int[1], paired_test$conf.int[2]))\n\n\n  95% CI for difference: [1.58, 4.28]\n\n\nCode\ncat(\"UNPAIRED t-Test (Incorrect Analysis):\\n\")\n\n\nUNPAIRED t-Test (Incorrect Analysis):\n\n\nCode\ncat(sprintf(\"  t(%.1f) = %.3f, p = %.4f\\n\",\n            unpaired_test$parameter, unpaired_test$statistic, unpaired_test$p.value))\n\n\n  t(37.5) = 1.873, p = 0.0688\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            unpaired_test$conf.int[1], unpaired_test$conf.int[2]))\n\n\n  95% CI for difference: [-0.24, 6.10]\n\n\nCode\ncat(\"Why the difference?\\n\")\n\n\nWhy the difference?\n\n\nCode\ncat(sprintf(\"  Paired test SE: %.3f\\n\",\n            diff_summary$sd_diff / sqrt(20)))\n\n\n  Paired test SE: 0.644\n\n\nCode\ncat(sprintf(\"  Unpaired test SE: %.3f\\n\",\n            sqrt(var(milk_paired$before)/20 + var(milk_paired$after)/20)))\n\n\n  Unpaired test SE: 1.564\n\n\nCode\ncat(\"  → Paired test has smaller SE (removes between-cow variation)\\n\")\n\n\n  → Paired test has smaller SE (removes between-cow variation)\n\n\nKey insight: The paired test is more powerful because it accounts for the fact that we measured the same cows twice. The unpaired test includes unnecessary between-cow variability, making the standard error larger and the test less sensitive.\n\n\n\n\n\n\nImportantPairing Increases Power\n\n\n\nIn this example:\n\nPaired test: p = 0.0002 → Significant at α = 0.05\nUnpaired test: p = 0.0688 → May or may not be significant\n\nThe paired test is more powerful because we’re comparing each cow to itself, removing individual differences in baseline milk production.\nRule: If your data are paired by design, you MUST use a paired test!\n\n\n\n\n19.2.3 Conduct Paired t-Test\n\n\nCode\n# Paired t-test\npaired_result &lt;- t.test(milk_paired$after, milk_paired$before, paired = TRUE)\n\n# Tidy output\npaired_tidy &lt;- tidy(paired_result)\n\nknitr::kable(paired_tidy, digits = 4,\n             caption = \"Paired t-Test Results\")\n\n\n\nPaired t-Test Results\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.9309\n4.5532\n2e-04\n19\n1.5836\n4.2782\nPaired t-test\ntwo.sided\n\n\n\n\n\nCode\n# Print interpretation\ncat(\"\\nPaired t-Test Results:\\n\")\n\n\n\nPaired t-Test Results:\n\n\nCode\ncat(sprintf(\"Mean difference: %.2f kg/day\\n\", paired_result$estimate))\n\n\nMean difference: 2.93 kg/day\n\n\nCode\ncat(sprintf(\"t(%d) = %.3f\\n\", paired_result$parameter, paired_result$statistic))\n\n\nt(19) = 4.553\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", paired_result$p.value))\n\n\nP-value: 0.0002\n\n\nCode\ncat(sprintf(\"95%% CI: [%.2f, %.2f]\\n\", paired_result$conf.int[1], paired_result$conf.int[2]))\n\n\n95% CI: [1.58, 4.28]\n\n\nCode\nif(paired_result$p.value &lt; 0.05) {\n  cat(\"\\n→ Significant at α = 0.05. Evidence that treatment increases milk yield.\\n\")\n} else {\n  cat(\"\\n→ Not significant at α = 0.05. Insufficient evidence of treatment effect.\\n\")\n}\n\n\n\n→ Significant at α = 0.05. Evidence that treatment increases milk yield.\n\n\n\n\n19.2.4 Effect Size for Paired t-Test\n\n\nCode\n# Cohen's d for paired data (based on differences)\npaired_d &lt;- cohen.d(milk_paired$after, milk_paired$before, paired = TRUE)\n\nprint(paired_d)\n\n\n\nCohen's d\n\nd estimate: 0.5832096 (medium)\n95 percent confidence interval:\n    lower     upper \n0.3027274 0.8636919 \n\n\nCode\ncat(sprintf(\"\\nCohen's d (paired): %.3f\\n\", paired_d$estimate))\n\n\n\nCohen's d (paired): 0.583\n\n\nCode\n# Interpretation\nd_val &lt;- abs(paired_d$estimate)\nif(d_val &lt; 0.2) {\n  d_interp &lt;- \"negligible\"\n} else if(d_val &lt; 0.5) {\n  d_interp &lt;- \"small\"\n} else if(d_val &lt; 0.8) {\n  d_interp &lt;- \"medium\"\n} else {\n  d_interp &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", d_interp))\n\n\nEffect size interpretation: medium\n\n\n\n\n19.2.5 Assumptions for Paired t-Test\nPaired t-test assumes:\n\nIndependence of pairs (not within pairs)\nNormality of the differences (not the original measurements)\n\n\n\nCode\n# Check normality of DIFFERENCES\nggplot(milk_paired, aes(sample = difference)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Q-Q Plot of Differences\",\n       subtitle = \"Checking normality assumption for paired t-test\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles (Differences)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test on differences\nshapiro_diff &lt;- shapiro.test(milk_paired$difference)\ncat(sprintf(\"\\nShapiro-Wilk test on differences: W = %.4f, p = %.3f\\n\",\n            shapiro_diff$statistic, shapiro_diff$p.value))\n\n\n\nShapiro-Wilk test on differences: W = 0.9514, p = 0.389\n\n\nThe differences appear approximately normal, so the paired t-test is appropriate.\n\n\n19.2.6 Reporting Paired t-Test Results\nExample write-up:\n\nMilk yield was measured in 20 dairy cows before and after 4 weeks of probiotic supplementation. A paired-samples t-test revealed that milk yield increased significantly after treatment (M_after = 34.1 kg/day, SD = 4.7) compared to before treatment (M_before = 31.1 kg/day, SD = 5.2), t(19) = 4.55, p &lt; 0.001, 95% CI [1.6, 4.3], d = 0.58. The probiotic supplement increased milk production by an average of 2.9 kg/day, representing a medium effect.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-scenarios",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-scenarios",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "20.1 Common Scenarios in Animal Science",
    "text": "20.1 Common Scenarios in Animal Science\n\n\n\n\n\n\n\n\nScenario\nTest Type\nExample\n\n\n\n\nCompare sample mean to historical value\nOne-sample\nIs current milk yield different from breed average?\n\n\nCompare two independent groups\nTwo-sample (independent)\nFeed A vs Feed B in randomly assigned pigs\n\n\nCompare before and after in same animals\nPaired\nWeight before and after medication in same cattle\n\n\nCompare littermates or twins\nPaired\nTwin calves assigned to different treatments\n\n\nCompare males vs females\nTwo-sample (independent)\nGrowth rate in male vs female lambs\n\n\nCompare left vs right (same animal)\nPaired\nUdder health in left vs right quarters\n\n\n\n\n\n\n\n\n\nWarningCommon Mistake: Treating Paired Data as Independent\n\n\n\nDon’t use a two-sample t-test when data are paired! This:\n\nWastes information (ignores pairing)\nReduces power (larger standard error)\nMay lead to incorrect conclusions\n\nIf measurements are connected (same subject, matched pairs, siblings), use a paired test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-checking-assumptions",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-checking-assumptions",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "21.1 Checking Assumptions in Practice",
    "text": "21.1 Checking Assumptions in Practice\n\n21.1.1 Normality\nVisual methods (preferred):\n\nHistograms: Look for roughly symmetric, bell-shaped distribution\nQQ plots: Points should fall close to the diagonal line\nDensity plots: Compare to normal curve overlay\n\nFormal tests (use with caution):\n\nShapiro-Wilk test: shapiro.test()\nKolmogorov-Smirnov test: ks.test()\n\n\n\n\n\n\n\nTipWhen Normality is Violated\n\n\n\nIf data are clearly non-normal:\n\nTransform the data: Log, square root, or Box-Cox transformation\nUse non-parametric tests: Wilcoxon rank-sum test (Mann-Whitney U) instead of two-sample t-test\nBootstrap confidence intervals: Resample to estimate sampling distribution\nRely on CLT: With large samples (n &gt; 30), t-tests are robust to non-normality\n\nMost common approach: If n &gt; 30 and no extreme outliers/skewness, proceed with t-test.\n\n\n\n\n21.1.2 Equal Variances\nVisual methods:\n\nCompare SD between groups (ratio &lt; 2 is usually fine)\nCompare boxplot heights and spreads\n\nFormal test:\n\nLevene’s test: car::leveneTest()\n\nDefault recommendation: Use Welch’s t-test (doesn’t assume equal variances) as your default. It’s robust and performs well even when variances are equal.\n\n\n21.1.3 Independence\nThis is the most important and least testable assumption.\nViolations occur when:\n\nObservations are clustered (e.g., multiple measurements per animal)\nTime series data with autocorrelation\nSpatial dependence (e.g., neighboring pens)\nPseudo-replication (treating subsamples as independent)\n\nSolutions:\n\nUse mixed models to account for clustering\nAggregate repeated measures appropriately\nDesign studies to ensure independence\n\n\n\n\n\n\n\nImportantIndependence Cannot Be Fixed Post-Hoc\n\n\n\nUnlike normality or equal variance assumptions, independence violations cannot be rescued with transformations or alternative tests. You must design your study correctly from the beginning.\nExample of pseudo-replication: Taking 5 blood samples from each of 4 cows and analyzing n=20 samples is wrong. The samples within each cow are not independent. The true n is 4 cows, not 20 samples.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-effect-sizes-practical",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-effect-sizes-practical",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "21.2 Effect Sizes and Practical Significance",
    "text": "21.2 Effect Sizes and Practical Significance\nStatistical significance ≠ Practical significance\nA result can be statistically significant (p &lt; 0.05) but:\n\nThe effect size is tiny\nThe difference is not economically or biologically meaningful\nThe cost of implementation outweighs the benefit\n\nAlways report:\n\nP-value: Strength of evidence against H₀\nConfidence interval: Range of plausible values for the effect\nEffect size: Standardized measure of magnitude (Cohen’s d)\nMeans and SDs: Raw values for practical interpretation\n\n\n\n\n\n\n\nNoteCohen’s d Interpretation (Guidelines)\n\n\n\n\n\n\nd Value\nInterpretation\nMeaning\n\n\n\n\n0.0 - 0.2\nNegligible\nTrivial difference\n\n\n0.2 - 0.5\nSmall\nNoticeable to researchers\n\n\n0.5 - 0.8\nMedium\nVisible to the naked eye\n\n\n0.8+\nLarge\nObvious, practically meaningful\n\n\n\nRemember: These are rough guidelines. What matters is context:\n\nIn medicine, small effects can be life-saving\nIn agriculture, medium effects must be cost-effective\nIn behavior, large effects are rare and important",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-sample-size-practical",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-sample-size-practical",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "21.3 Sample Size and Power Considerations",
    "text": "21.3 Sample Size and Power Considerations\nBefore conducting your study, calculate required sample size:\n\n\nCode\n# Example: Planning a feed trial\n# Expected difference: 0.15 kg/day weight gain\n# Expected SD: 0.20 kg/day\n# Desired power: 0.80\n# Alpha: 0.05\n\npower_calc &lt;- power.t.test(\n  delta = 0.15,\n  sd = 0.20,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\"\n)\n\nprint(power_calc)\n\n\n\n     Two-sample t test power calculation \n\n              n = 28.89962\n          delta = 0.15\n             sd = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\ncat(sprintf(\"\\nTo detect a difference of %.2f kg/day with 80%% power:\\n\", 0.15))\n\n\n\nTo detect a difference of 0.15 kg/day with 80% power:\n\n\nCode\ncat(sprintf(\"  Required sample size: %.0f animals per group\\n\", ceiling(power_calc$n)))\n\n\n  Required sample size: 29 animals per group\n\n\nCode\ncat(sprintf(\"  Total animals needed: %.0f\\n\", 2 * ceiling(power_calc$n)))\n\n\n  Total animals needed: 58\n\n\nTrade-offs:\n\nSmaller effect → Need larger sample\nMore variable data → Need larger sample\nHigher power → Need larger sample\nLower α → Need larger sample",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-common-mistakes",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-common-mistakes",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "21.4 Common Mistakes and Pitfalls",
    "text": "21.4 Common Mistakes and Pitfalls\n\n21.4.1 1. Multiple Comparisons (p-hacking)\nProblem: Running many t-tests increases the chance of false positives.\nExample: Testing 20 different outcomes. With α = 0.05, you expect 1 false positive even if there are no real effects.\nSolutions:\n\nBonferroni correction: Divide α by number of tests (α_adjusted = 0.05 / 20 = 0.0025)\nANOVA followed by post-hoc tests (covered next week)\nPre-specify primary outcomes before analysis\n\n\n\n21.4.2 2. Confusing Statistical and Practical Significance\nProblem: With large samples, tiny effects become “significant”\nExample: Weight gain differs by 0.5 kg (p = 0.03) but costs $50 more per animal → Not worth it!\nSolution: Always consider effect size and cost-benefit\n\n\n21.4.3 3. One-Tailed Tests Without Justification\nProblem: Using one-tailed tests to achieve p &lt; 0.05\nSolution: Use two-tailed tests by default. Only use one-tailed if:\n\nYou have strong theoretical reason\nEffect in opposite direction is impossible or meaningless\nYou pre-registered the hypothesis\n\n\n\n21.4.4 4. Ignoring Assumptions\nProblem: Running t-tests without checking assumptions\nSolution: Always check:\n\nNormality (QQ plots)\nEqual variances (visual or Levene’s test)\nIndependence (by design)\n\n\n\n21.4.5 5. Treating Paired Data as Independent\nProblem: Using two-sample t-test for paired data\nSolution: If same subjects measured twice or matched pairs → Use paired t-test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-reporting",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-reporting",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "21.5 Reporting t-Test Results",
    "text": "21.5 Reporting t-Test Results\nEssential elements:\n\nDescriptive statistics: Means, SDs, sample sizes for each group\nTest used: One-sample, two-sample (Welch’s or Student’s), or paired\nTest statistic: t-value and degrees of freedom\nP-value: Exact value (not just “&lt; 0.05”)\nConfidence interval: For the difference\nEffect size: Cohen’s d\nInterpretation: In context of the research question\n\nExample:\n\nMilk yield increased significantly after probiotic supplementation (M = 32.5 kg/day, SD = 4.8) compared to baseline (M = 30.1 kg/day, SD = 4.6), t(19) = 4.12, p &lt; 0.001, 95% CI [1.2, 3.6], d = 0.92. This represents a large effect and an average increase of 2.4 kg/day per cow.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-covered",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-covered",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "22.1 What We Covered",
    "text": "22.1 What We Covered\n\nHypothesis testing framework: Structured approach to evaluating claims about populations\nType I and Type II errors: Understanding false positives (α) and false negatives (β)\nStatistical power: Probability of detecting real effects; influenced by effect size, sample size, α, and variability\nOne-sample t-test: Comparing sample mean to known value\nTwo-sample t-test: Comparing means between independent groups\nPaired t-test: Comparing means for related/matched observations\nAssumptions: Normality, equal variances, independence\nEffect sizes: Cohen’s d for standardized effect magnitude\nPractical significance: Statistical significance doesn’t guarantee practical importance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-principles",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-principles",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "22.2 Key Principles",
    "text": "22.2 Key Principles\n\n\n\n\n\n\nImportantCore Principles of Hypothesis Testing\n\n\n\n\nP-values measure evidence, not truth: A p-value tells you how compatible your data is with H₀, not whether H₀ is true or false\nEffect sizes matter more than p-values: Always report and interpret effect sizes and confidence intervals\nDesign determines analysis: Paired vs independent determines which test to use—this cannot be changed after data collection\nAssumptions matter: Check them, but also understand t-tests are fairly robust (especially with larger samples)\nPower drives sample size: Calculate required sample size BEFORE collecting data\nContext is everything: Statistical significance without practical significance is meaningless",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-decision-framework",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-decision-framework",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "22.3 Decision Framework",
    "text": "22.3 Decision Framework\nWhen analyzing data:\n\nIdentify your research question: What are you comparing?\nChoose appropriate test:\n\nOne sample? → One-sample t-test\nTwo independent groups? → Two-sample t-test\nPaired/matched data? → Paired t-test\n\nCheck assumptions: Normality (QQ plots), equal variances (visual or Levene’s), independence (by design)\nConduct test: Calculate t-statistic and p-value\nCalculate effect size: Cohen’s d for standardized magnitude\nInterpret in context: Consider both statistical and practical significance\nReport completely: Means, SDs, n, test statistics, p-values, CIs, effect sizes",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-next-week",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-next-week",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "22.4 Next Week Preview",
    "text": "22.4 Next Week Preview\nWeek 5: Analysis of Variance (ANOVA)\n\nComparing more than two groups (extension of t-tests)\nUnderstanding variance partitioning (between-group vs within-group)\nPost-hoc tests (Tukey HSD, Bonferroni)\nMultiple comparisons problem\nWhen to use ANOVA vs multiple t-tests\n\n\n\n\n\n\n\nNoteComing Full Circle\n\n\n\nANOVA is mathematically equivalent to the t-test when comparing two groups. Next week, we’ll see how to generalize hypothesis testing to multiple groups while controlling Type I error rates.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-1",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-1",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "23.1 Problem 1: One-Sample Scenario",
    "text": "23.1 Problem 1: One-Sample Scenario\nA poultry researcher measures egg weight from 30 hens. The breed standard is 62 grams. The sample mean is 64.5 grams with SD = 5.2 grams. Is there evidence that egg weight differs from the breed standard?\nTasks:\n\nState the null and alternative hypotheses\nConduct a one-sample t-test\nCalculate Cohen’s d\nInterpret the results\n\n\n\nCode\n# Your code here\nset.seed(999)\negg_weights &lt;- rnorm(30, mean = 64.5, sd = 5.2)\n\n# a) Hypotheses: H0: μ = 62, H1: μ ≠ 62\n\n# b) Test\ntest1 &lt;- t.test(egg_weights, mu = 62)\nprint(test1)\n\n# c) Effect size\nd1 &lt;- (mean(egg_weights) - 62) / sd(egg_weights)\ncat(sprintf(\"Cohen's d: %.3f\\n\", d1))\n\n# d) Interpret...",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-2",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-2",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "23.2 Problem 2: Two-Sample Scenario",
    "text": "23.2 Problem 2: Two-Sample Scenario\nA beef cattle trial compares average daily gain (ADG) for two protein sources. Soybean meal (n=25): M=1.45 kg/day, SD=0.22. Corn gluten (n=25): M=1.38 kg/day, SD=0.19. Is there a significant difference?\nTasks:\n\nState hypotheses\nCheck equal variance assumption\nConduct two-sample t-test (Welch’s)\nCalculate effect size\nProvide practical interpretation",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-3",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-3",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "23.3 Problem 3: Paired Scenario",
    "text": "23.3 Problem 3: Paired Scenario\nA veterinarian measures body temperature in 15 calves before and after administering an anti-inflammatory drug. Should you use a paired or unpaired test? Why? What are the hypotheses?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-4",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-4",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "23.4 Problem 4: Power Analysis",
    "text": "23.4 Problem 4: Power Analysis\nYou’re planning a study to detect a 10% difference in weaning weight (Expected means: 250 kg vs 275 kg, SD = 30 kg). How many calves do you need per group to achieve 80% power with α = 0.05?\n\n\nCode\n# Your code here\npower.t.test(\n  delta = 25,\n  sd = 30,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\"\n)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-5",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-practice-5",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "23.5 Problem 5: Assumption Checking",
    "text": "23.5 Problem 5: Assumption Checking\nYou’ve collected data from two groups but aren’t sure if assumptions are met. What plots would you create? What tests would you run? What would you do if assumptions are violated?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-r-functions",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-r-functions",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "24.1 R Functions Covered",
    "text": "24.1 R Functions Covered\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nt.test()\nbase\nOne-sample, two-sample, and paired t-tests\n\n\npower.t.test()\nbase\nPower and sample size calculations\n\n\nshapiro.test()\nbase\nTest normality\n\n\nleveneTest()\ncar\nTest equality of variances\n\n\ncohen.d()\neffsize\nCalculate Cohen’s d effect size\n\n\ntidy()\nbroom\nTidy statistical output",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-further-reading",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-further-reading",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "24.2 Further Reading",
    "text": "24.2 Further Reading\n\nCumming, G. (2012). Understanding the New Statistics. Excellent on effect sizes and confidence intervals\nCohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences. Classic reference on power\nLakens, D. (2013). “Calculating and reporting effect sizes to facilitate cumulative science.” Frontiers in Psychology.\nAmerican Statistical Association (2016). “Statement on P-values and Statistical Significance.”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch04-hypothesis_testing.html#sec-online-resources",
    "href": "chapters/part2-ch04-hypothesis_testing.html#sec-online-resources",
    "title": "12  Week 4: Hypothesis Testing Fundamentals",
    "section": "24.3 Online Resources",
    "text": "24.3 Online Resources\n\nR for Data Science (2e): https://r4ds.hadley.nz/\nStatistical Thinking: https://www.fharrell.com/\nStatQuest Videos (YouTube): Excellent visual explanations of t-tests and power\n\n\nEnd of Week 4 Lecture\nNext week: Analysis of Variance (ANOVA) - extending t-tests to multiple groups!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Week 4: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html",
    "href": "chapters/part2-ch05-anova.html",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "",
    "text": "14 Introduction\nYou’re a cattle nutritionist comparing four different feed formulations for finishing steers. After a 90-day trial with 15 steers per feed type, you’ve measured average daily gain (ADG) for each animal. Now you need to determine: Do these feed formulations produce different growth rates?\nYour first instinct might be to run t-tests comparing all pairs of feeds: - Feed A vs Feed B - Feed A vs Feed C - Feed A vs Feed D - Feed B vs Feed C - Feed B vs Feed D - Feed C vs Feed D\nThat’s 6 t-tests for just 4 groups! But this approach has a critical problem: each test carries a 5% false positive rate, and these errors accumulate. By the time you’ve run 6 tests, your chance of finding at least one “significant” result by pure chance has jumped to about 26%.\nThis is where Analysis of Variance (ANOVA) comes in. ANOVA allows us to test for differences among multiple groups with a single test, maintaining control over our Type I error rate.\nKey Questions We’ll Address:\nBy the end of this lecture, you’ll be able to conduct one-way ANOVA using modern R approaches (lm() and car::Anova()), check assumptions, perform post-hoc tests, and calculate effect sizes.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-why-not-t-tests",
    "href": "chapters/part2-ch05-anova.html#sec-why-not-t-tests",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "15.1 Why Not Just Run Multiple t-Tests?",
    "text": "15.1 Why Not Just Run Multiple t-Tests?\nLet’s simulate what happens when we run multiple pairwise t-tests on groups that actually have no real differences:\n\n\nCode\n# Simulate 5 groups with NO true differences (all μ = 100, σ = 15)\nset.seed(123)\nn_per_group &lt;- 20\nn_groups &lt;- 5\n\n# Create data where H0 is TRUE (all groups have same mean)\ngroups_data &lt;- tibble(\n  group = rep(LETTERS[1:n_groups], each = n_per_group),\n  value = rnorm(n_per_group * n_groups, mean = 100, sd = 15)\n)\n\n# Visualize\nggplot(groups_data, aes(x = group, y = value, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Five Groups with NO True Differences\",\n       subtitle = \"All groups sampled from same distribution (μ = 100, σ = 15)\",\n       x = \"Group\",\n       y = \"Value\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThese groups look slightly different due to random sampling, but they were all drawn from the same population. Now let’s run all possible pairwise t-tests:\n\n\nCode\n# All pairwise comparisons\ngroup_pairs &lt;- combn(LETTERS[1:n_groups], 2, simplify = FALSE)\n\n# Run t-tests\npairwise_results &lt;- map_dfr(group_pairs, function(pair) {\n  group1_data &lt;- groups_data %&gt;% filter(group == pair[1]) %&gt;% pull(value)\n  group2_data &lt;- groups_data %&gt;% filter(group == pair[2]) %&gt;% pull(value)\n\n  test &lt;- t.test(group1_data, group2_data)\n\n  tibble(\n    comparison = paste(pair[1], \"vs\", pair[2]),\n    t_stat = test$statistic,\n    p_value = test$p.value,\n    significant = p_value &lt; 0.05\n  )\n})\n\nknitr::kable(pairwise_results, digits = 4, align = 'lccc',\n             caption = \"Pairwise t-Tests for Five Groups (H₀ is TRUE)\")\n\n\n\nPairwise t-Tests for Five Groups (H₀ is TRUE)\n\n\ncomparison\nt_stat\np_value\nsignificant\n\n\n\n\nA vs B\n0.6746\n0.5041\nFALSE\n\n\nA vs C\n0.1151\n0.9089\nFALSE\n\n\nA vs D\n0.8501\n0.4006\nFALSE\n\n\nA vs E\n-0.8170\n0.4192\nFALSE\n\n\nB vs C\n-0.5568\n0.5810\nFALSE\n\n\nB vs D\n0.2401\n0.8116\nFALSE\n\n\nB vs E\n-1.6254\n0.1123\nFALSE\n\n\nC vs D\n0.7417\n0.4628\nFALSE\n\n\nC vs E\n-0.9486\n0.3490\nFALSE\n\n\nD vs E\n-1.7317\n0.0916\nFALSE\n\n\n\n\n\nCode\n# How many false positives?\nn_comparisons &lt;- nrow(pairwise_results)\nn_false_positives &lt;- sum(pairwise_results$significant)\n\ncat(sprintf(\"\\nNumber of comparisons: %d\\n\", n_comparisons))\n\n\n\nNumber of comparisons: 10\n\n\nCode\ncat(sprintf(\"Number of 'significant' results: %d (%.1f%%)\\n\",\n            n_false_positives, 100 * n_false_positives / n_comparisons))\n\n\nNumber of 'significant' results: 0 (0.0%)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-family-wise-error",
    "href": "chapters/part2-ch05-anova.html#sec-family-wise-error",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "15.2 Family-Wise Error Rate",
    "text": "15.2 Family-Wise Error Rate\nWhen we run multiple tests, we need to distinguish between:\n\nPer-comparison error rate: The α level for a single test (typically 0.05)\nFamily-wise error rate (FWER): The probability of making at least one Type I error across all tests\n\nThe probability of making no Type I errors in k independent tests is:\n\\[P(\\text{no Type I errors}) = (1 - \\alpha)^k\\]\nTherefore, the probability of making at least one Type I error is:\n\\[\\text{FWER} = 1 - (1 - \\alpha)^k\\]\nLet’s calculate this for different numbers of comparisons:\n\n\nCode\n# Calculate FWER for different numbers of groups\nn_groups_range &lt;- 2:10\ncomparisons &lt;- choose(n_groups_range, 2)  # Number of pairwise comparisons\nalpha &lt;- 0.05\n\nfwer_data &lt;- tibble(\n  n_groups = n_groups_range,\n  n_comparisons = comparisons,\n  fwer = 1 - (1 - alpha)^comparisons,\n  fwer_percent = fwer * 100\n)\n\nknitr::kable(fwer_data, digits = 1, align = 'cccc',\n             col.names = c(\"Groups\", \"Comparisons\", \"FWER\", \"FWER (%)\"),\n             caption = \"Family-Wise Error Rate Growth with Multiple Comparisons\")\n\n\n\nFamily-Wise Error Rate Growth with Multiple Comparisons\n\n\nGroups\nComparisons\nFWER\nFWER (%)\n\n\n\n\n2\n1\n0.1\n5.0\n\n\n3\n3\n0.1\n14.3\n\n\n4\n6\n0.3\n26.5\n\n\n5\n10\n0.4\n40.1\n\n\n6\n15\n0.5\n53.7\n\n\n7\n21\n0.7\n65.9\n\n\n8\n28\n0.8\n76.2\n\n\n9\n36\n0.8\n84.2\n\n\n10\n45\n0.9\n90.1\n\n\n\n\n\nCode\n# Visualize\nggplot(fwer_data, aes(x = n_groups, y = fwer_percent)) +\n  geom_line(linewidth = 1.2, color = \"darkred\") +\n  geom_point(size = 3, color = \"darkred\") +\n  geom_hline(yintercept = 5, linetype = \"dashed\", color = \"steelblue\") +\n  annotate(\"text\", x = 8, y = 7, label = \"Per-test α = 5%\", color = \"steelblue\") +\n  scale_x_continuous(breaks = 2:10) +\n  labs(title = \"Family-Wise Error Rate Explodes with Multiple Groups\",\n       subtitle = \"Probability of at least one false positive when all null hypotheses are true\",\n       x = \"Number of Groups\",\n       y = \"Family-Wise Error Rate (%)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nKey insight: With just 5 groups (10 comparisons), the FWER jumps to 40%! This means we have a 40% chance of declaring at least one comparison “significant” even when no true differences exist.\n\n\n\n\n\n\nImportantANOVA: The Solution\n\n\n\nANOVA provides a single omnibus test that asks: “Is there any difference among these groups?”\nBy conducting one test at α = 0.05, we maintain a 5% false positive rate regardless of how many groups we’re comparing.\nIf ANOVA is significant, then we conduct post-hoc tests with appropriate adjustments to identify which specific groups differ.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-when-anova",
    "href": "chapters/part2-ch05-anova.html#sec-when-anova",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "15.3 When to Use ANOVA vs t-Tests",
    "text": "15.3 When to Use ANOVA vs t-Tests\nUse t-tests when: - Comparing exactly 2 groups - One-sample or paired designs\nUse ANOVA when: - Comparing 3 or more groups - Testing for any difference among groups - Want to control family-wise error rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-variance-partition",
    "href": "chapters/part2-ch05-anova.html#sec-variance-partition",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "16.1 The Core Idea: Partitioning Variance",
    "text": "16.1 The Core Idea: Partitioning Variance\nANOVA works by partitioning total variance into two components:\n\nBetween-group variance: Variability of group means around the overall mean\nWithin-group variance: Variability of individual observations around their group means\n\nIf the between-group variance is much larger than the within-group variance, this suggests the groups differ.\n\n16.1.1 Visual Intuition\nLet’s create two scenarios to build intuition:\n\n\nCode\n# Scenario 1: Large between-group differences (H0 is FALSE)\nscenario1 &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  value = c(rnorm(20, 80, 10), rnorm(20, 100, 10), rnorm(20, 120, 10))\n)\n\n# Scenario 2: Small between-group differences (H0 is TRUE)\nscenario2 &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  value = c(rnorm(20, 98, 15), rnorm(20, 100, 15), rnorm(20, 102, 15))\n)\n\n# Calculate group means\nscenario1_means &lt;- scenario1 %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_val = mean(value), .groups = 'drop')\n\nscenario2_means &lt;- scenario2 %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_val = mean(value), .groups = 'drop')\n\n# Overall means\noverall_mean1 &lt;- mean(scenario1$value)\noverall_mean2 &lt;- mean(scenario2$value)\n\n# Plot Scenario 1\np1 &lt;- ggplot(scenario1, aes(x = group, y = value, color = group)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = overall_mean1, linetype = \"dashed\",\n             color = \"black\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 5, shape = 18, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18,\n               aes(color = group)) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 3.3, y = overall_mean1, label = \"Overall\\nmean\",\n           size = 3, hjust = 0) +\n  labs(title = \"Scenario 1: Large Between-Group Variance\",\n       subtitle = \"Group means far from overall mean → F will be large\",\n       x = \"Group\", y = \"Value\") +\n  theme(legend.position = \"none\")\n\n# Plot Scenario 2\np2 &lt;- ggplot(scenario2, aes(x = group, y = value, color = group)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = overall_mean2, linetype = \"dashed\",\n             color = \"black\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 5, shape = 18, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18,\n               aes(color = group)) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 3.3, y = overall_mean2, label = \"Overall\\nmean\",\n           size = 3, hjust = 0) +\n  labs(title = \"Scenario 2: Small Between-Group Variance\",\n       subtitle = \"Group means close to overall mean → F will be small\",\n       x = \"Group\", y = \"Value\") +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey observation: - Scenario 1: Group means (colored diamonds) are far from overall mean (black line) → large between-group variance - Scenario 2: Group means are close to overall mean → small between-group variance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-f-statistic",
    "href": "chapters/part2-ch05-anova.html#sec-f-statistic",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "16.2 The F-Statistic",
    "text": "16.2 The F-Statistic\nThe F-statistic is the ratio of between-group variance to within-group variance:\n\\[F = \\frac{\\text{Between-group variance (MS}_{\\text{between}}\\text{)}}{\\text{Within-group variance (MS}_{\\text{within}}\\text{)}}\\]\nWhere: - MS = Mean Square (variance estimate) - A large F-ratio suggests group means differ more than expected by chance - F follows an F-distribution with df₁ (between) and df₂ (within)\n\n\n\n\n\n\nNoteConnection to t-Test\n\n\n\nWhen comparing exactly two groups, ANOVA is mathematically equivalent to a t-test:\n\\[F = t^2\\]\nANOVA with 2 groups will give the same p-value as a two-sample t-test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-anova-table",
    "href": "chapters/part2-ch05-anova.html#sec-anova-table",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "16.3 ANOVA Table Components",
    "text": "16.3 ANOVA Table Components\nThe ANOVA table summarizes the variance partitioning:\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares (SS)\ndf\nMean Square (MS)\nF\np-value\n\n\n\n\nBetween\nSS_between\nk - 1\nSS_between / df_between\nMS_between / MS_within\nP(F &gt; F_obs)\n\n\nWithin\nSS_within\nN - k\nSS_within / df_within\n\n\n\n\nTotal\nSS_total\nN - 1\n\n\n\n\n\n\nWhere: - k = number of groups - N = total sample size - SS_between: Sum of squared differences between group means and overall mean - SS_within: Sum of squared differences between observations and their group means - df = degrees of freedom\nLet’s calculate ANOVA for our two scenarios:\n\n\nCode\n# Fit models\nmodel1 &lt;- lm(value ~ group, data = scenario1)\nmodel2 &lt;- lm(value ~ group, data = scenario2)\n\n# ANOVA tables using car::Anova (Type II)\ncat(\"Scenario 1: Large Between-Group Variance\\n\")\n\n\nScenario 1: Large Between-Group Variance\n\n\nCode\ncat(\"==========================================\\n\")\n\n\n==========================================\n\n\nCode\nAnova(model1, type = \"II\")\n\n\nAnova Table (Type II tests)\n\nResponse: value\n          Sum Sq Df F value    Pr(&gt;F)    \ngroup      17393  2  93.071 &lt; 2.2e-16 ***\nResiduals   5326 57                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nScenario 2: Small Between-Group Variance\\n\")\n\n\n\n\nScenario 2: Small Between-Group Variance\n\n\nCode\ncat(\"==========================================\\n\")\n\n\n==========================================\n\n\nCode\nAnova(model2, type = \"II\")\n\n\nAnova Table (Type II tests)\n\nResponse: value\n           Sum Sq Df F value Pr(&gt;F)\ngroup        40.4  2  0.0881 0.9158\nResiduals 13081.2 57               \n\n\nInterpretation: - Scenario 1: F = 93.1, p &lt; 0.001 → Strong evidence of group differences - Scenario 2: F = 0.09, p = 0.916 → No evidence of group differences",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-why-lm",
    "href": "chapters/part2-ch05-anova.html#sec-why-lm",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "17.1 Why Use lm() Instead of aov()?",
    "text": "17.1 Why Use lm() Instead of aov()?\nIn R, you can fit ANOVA using either aov() or lm(). We’ll use lm() because:\n\nMore flexible: Same framework works for t-tests, ANOVA, and regression\nPrepares for regression: Weeks 7-8 will use lm() for continuous predictors\nModern approach: Works seamlessly with car::Anova(), emmeans, and other packages\nBetter diagnostics: Standard regression diagnostic plots apply\n\n\n\n\n\n\n\nTipANOVA IS Regression\n\n\n\nOne-way ANOVA is actually a special case of linear regression where the predictor is categorical. R internally converts group labels to dummy variables.\nThis connection will become clearer in Weeks 7-8!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-fitting-lm",
    "href": "chapters/part2-ch05-anova.html#sec-fitting-lm",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "17.2 Fitting ANOVA with lm()",
    "text": "17.2 Fitting ANOVA with lm()\nLet’s work through a complete example using the cattle feed trial scenario:\n\n\nCode\n# Simulate feed trial data\n# 4 feed types, 15 steers per feed, 90-day trial measuring average daily gain (kg)\nset.seed(456)\n\nfeed_data &lt;- tibble(\n  steer_id = 1:60,\n  feed_type = factor(rep(c(\"A\", \"B\", \"C\", \"D\"), each = 15)),\n  # Simulated ADG with true differences:\n  # A: 1.45, B: 1.55, C: 1.50, D: 1.70 kg/day\n  adg_kg = c(\n    rnorm(15, mean = 1.45, sd = 0.18),  # Feed A\n    rnorm(15, mean = 1.55, sd = 0.18),  # Feed B\n    rnorm(15, mean = 1.50, sd = 0.18),  # Feed C\n    rnorm(15, mean = 1.70, sd = 0.18)   # Feed D\n  )\n)\n\n# Summary statistics\nfeed_summary &lt;- feed_data %&gt;%\n  group_by(feed_type) %&gt;%\n  summarise(\n    n = n(),\n    mean_adg = mean(adg_kg),\n    sd_adg = sd(adg_kg),\n    se_adg = sd_adg / sqrt(n),\n    .groups = 'drop'\n  )\n\nknitr::kable(feed_summary, digits = 3, align = 'lcccc',\n             col.names = c(\"Feed\", \"n\", \"Mean ADG\", \"SD\", \"SE\"),\n             caption = \"Summary Statistics: Average Daily Gain by Feed Type\")\n\n\n\nSummary Statistics: Average Daily Gain by Feed Type\n\n\nFeed\nn\nMean ADG\nSD\nSE\n\n\n\n\nA\n15\n1.471\n0.189\n0.049\n\n\nB\n15\n1.612\n0.231\n0.060\n\n\nC\n15\n1.502\n0.175\n0.045\n\n\nD\n15\n1.754\n0.125\n0.032\n\n\n\n\n\n\n17.2.1 Step 1: Visualize the Data\nAlways visualize before testing!\n\n\nCode\n# Create comprehensive visualization\np1 &lt;- ggplot(feed_data, aes(x = feed_type, y = adg_kg, fill = feed_type)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"darkred\", color = \"darkred\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Average Daily Gain by Feed Type\",\n       subtitle = \"Diamonds show group means\",\n       x = \"Feed Type\",\n       y = \"Average Daily Gain (kg/day)\") +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(feed_summary, aes(x = feed_type, y = mean_adg, fill = feed_type)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = mean_adg - se_adg * 1.96,\n                    ymax = mean_adg + se_adg * 1.96),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.2f\", mean_adg)),\n            vjust = -3, fontface = \"bold\", size = 3.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Mean ADG with 95% Confidence Intervals\",\n       x = \"Feed Type\",\n       y = \"Mean ADG (kg/day)\") +\n  ylim(0, 2) +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: Feed D appears to produce the highest gain, while Feed A appears lowest. But are these differences statistically significant?\n\n\n17.2.2 Step 2: Fit the Linear Model\n\n\nCode\n# Fit ANOVA as a linear model\nfeed_model &lt;- lm(adg_kg ~ feed_type, data = feed_data)\n\n# Model summary\nsummary(feed_model)\n\n\n\nCall:\nlm(formula = adg_kg ~ feed_type, data = feed_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37129 -0.12107  0.01025  0.10413  0.37211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.47125    0.04744  31.016  &lt; 2e-16 ***\nfeed_typeB   0.14093    0.06708   2.101   0.0402 *  \nfeed_typeC   0.03084    0.06708   0.460   0.6475    \nfeed_typeD   0.28287    0.06708   4.217 9.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1837 on 56 degrees of freedom\nMultiple R-squared:  0.2806,    Adjusted R-squared:  0.2421 \nF-statistic: 7.282 on 3 and 56 DF,  p-value: 0.0003302\n\n\nKey points from summary: - R-squared: Proportion of variance explained by feed type - Coefficients show differences from reference group (Feed A) - But we want the ANOVA table…\n\n\n17.2.3 Step 3: ANOVA Table with car::Anova()\n\n\nCode\n# ANOVA table using car package (Type II SS)\nlibrary(car)\nfeed_anova &lt;- Anova(feed_model, type = \"II\")\n\nprint(feed_anova)\n\n\nAnova Table (Type II tests)\n\nResponse: adg_kg\n           Sum Sq Df F value    Pr(&gt;F)    \nfeed_type 0.73731  3  7.2816 0.0003302 ***\nResiduals 1.89012 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Extract F and p-value\nf_stat &lt;- feed_anova$`F value`[1]\np_val &lt;- feed_anova$`Pr(&gt;F)`[1]\n\ncat(sprintf(\"\\nANOVA Results: F(%d, %d) = %.2f, p %s\\n\",\n            feed_anova$Df[1], feed_anova$Df[2], f_stat,\n            ifelse(p_val &lt; 0.001, \"&lt; 0.001\", sprintf(\"= %.4f\", p_val))))\n\n\n\nANOVA Results: F(3, 56) = 7.28, p &lt; 0.001\n\n\n\n\n\n\n\n\nImportantWhat This ANOVA Tells Us\n\n\n\nA significant F-test (p &lt; 0.05) tells us:\n✓ There IS evidence that at least one feed type differs from the others\n✗ It does NOT tell us which specific feeds differ\nTo identify specific differences, we need post-hoc tests (covered in Section 7).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-why-ss-matters",
    "href": "chapters/part2-ch05-anova.html#sec-why-ss-matters",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "18.1 Why This Matters",
    "text": "18.1 Why This Matters\nWhen you have balanced designs (equal sample sizes), all methods give the same results. But with unbalanced designs (common in real research!), the choice matters.\nKey differences:\n\nType I (Sequential): Tests each term after previous terms in the formula (order-dependent)\nType II (Marginal): Tests each term after all others (recommended for one-way ANOVA)\nType III (Orthogonal): Tests each term adjusted for all others (common in SAS, SPSS)\n\n\n\n\n\n\n\nNoteRecommendation for One-Way ANOVA\n\n\n\nFor one-way ANOVA (single factor), Type II and Type III give identical results.\nUse car::Anova(model, type = \"II\") as your default.\nType II/III differences become important with multiple factors or interactions (beyond this course).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-demonstrate-ss",
    "href": "chapters/part2-ch05-anova.html#sec-demonstrate-ss",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "18.2 Demonstrating the Difference",
    "text": "18.2 Demonstrating the Difference\nLet’s create an unbalanced version of our feed trial and compare methods:\n\n\nCode\n# Create unbalanced design\nset.seed(789)\nfeed_data_unbal &lt;- tibble(\n  feed_type = c(rep(\"A\", 12), rep(\"B\", 15), rep(\"C\", 18), rep(\"D\", 15)),\n  adg_kg = c(\n    rnorm(12, mean = 1.45, sd = 0.18),\n    rnorm(15, mean = 1.55, sd = 0.18),\n    rnorm(18, mean = 1.50, sd = 0.18),\n    rnorm(15, mean = 1.70, sd = 0.18)\n  )\n) %&gt;%\n  mutate(feed_type = factor(feed_type))\n\n# Sample sizes\nfeed_data_unbal %&gt;%\n  count(feed_type) %&gt;%\n  knitr::kable(col.names = c(\"Feed Type\", \"n\"),\n               caption = \"Unbalanced Design Sample Sizes\")\n\n\n\nUnbalanced Design Sample Sizes\n\n\nFeed Type\nn\n\n\n\n\nA\n12\n\n\nB\n15\n\n\nC\n18\n\n\nD\n15\n\n\n\n\n\nCode\n# Fit model\nmodel_unbal &lt;- lm(adg_kg ~ feed_type, data = feed_data_unbal)\n\n# Compare Type I, II, and III\ncat(\"Type I (Sequential) Sums of Squares:\\n\")\n\n\nType I (Sequential) Sums of Squares:\n\n\nCode\ncat(\"=====================================\\n\")\n\n\n=====================================\n\n\nCode\nprint(anova(model_unbal))  # Base R uses Type I\n\n\nAnalysis of Variance Table\n\nResponse: adg_kg\n          Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \nfeed_type  3 0.80928 0.269760  9.8032 2.698e-05 ***\nResiduals 56 1.54098 0.027517                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nType II Sums of Squares:\\n\")\n\n\n\n\nType II Sums of Squares:\n\n\nCode\ncat(\"========================\\n\")\n\n\n========================\n\n\nCode\nprint(Anova(model_unbal, type = \"II\"))\n\n\nAnova Table (Type II tests)\n\nResponse: adg_kg\n           Sum Sq Df F value    Pr(&gt;F)    \nfeed_type 0.80928  3  9.8032 2.698e-05 ***\nResiduals 1.54098 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nType III Sums of Squares:\\n\")\n\n\n\n\nType III Sums of Squares:\n\n\nCode\ncat(\"=========================\\n\")\n\n\n=========================\n\n\nCode\nprint(Anova(model_unbal, type = \"III\"))\n\n\nAnova Table (Type III tests)\n\nResponse: adg_kg\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 22.7191  1 825.6263 &lt; 2.2e-16 ***\nfeed_type    0.8093  3   9.8032 2.698e-05 ***\nResiduals    1.5410 56                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor one-way ANOVA, Type II and Type III are identical. Type I may differ slightly with unbalanced designs.\nBottom line: For this course (one-way ANOVA), always use:\ncar::Anova(model, type = \"II\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-residuals-not-raw",
    "href": "chapters/part2-ch05-anova.html#sec-residuals-not-raw",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "19.1 Important: Test Residuals, Not Raw Data!",
    "text": "19.1 Important: Test Residuals, Not Raw Data!\n\n\n\n\n\n\nWarningCommon Mistake\n\n\n\nDon’t test normality of raw data within each group. Instead, test normality of residuals from the model.\nWhy? ANOVA assumes that deviations from group means are normally distributed, not that each group is normal independently.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-diagnostic-plots",
    "href": "chapters/part2-ch05-anova.html#sec-diagnostic-plots",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "19.2 Diagnostic Plots",
    "text": "19.2 Diagnostic Plots\nThe lm() object provides four standard diagnostic plots:\n\n\nCode\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(feed_model, which = 1:4)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\nInterpreting diagnostic plots:\n\nResiduals vs Fitted: Should show random scatter (no pattern)\n\nPattern suggests non-constant variance or non-linearity\n\nNormal Q-Q: Points should follow the diagonal line\n\nDeviations suggest non-normality\n\nScale-Location: Should show random scatter\n\nTests homogeneity of variance (constant spread)\n\nResiduals vs Leverage: Identifies influential points\n\nPoints outside Cook’s distance contours are influential",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-assumption-checks",
    "href": "chapters/part2-ch05-anova.html#sec-assumption-checks",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "19.3 Assumption Checks",
    "text": "19.3 Assumption Checks\n\n19.3.1 1. Independence\nCannot be tested statistically - depends on study design.\nViolations occur when: - Repeated measures on same subjects (use mixed models) - Clustered data (animals in same pen, fields, etc.) - Time series (autocorrelation)\nSolution: Design study properly or use appropriate models (mixed models, GEE).\n\n\n19.3.2 2. Normality of Residuals\nVisual check: Q-Q plot\n\n\nCode\n# Extract residuals\nfeed_residuals &lt;- residuals(feed_model)\n\n# Q-Q plot\nggplot(tibble(residuals = feed_residuals), aes(sample = residuals)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Normal Q-Q Plot of Residuals\",\n       subtitle = \"Points should fall close to the line\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nFormal test: Shapiro-Wilk\n\n\nCode\nshapiro_result &lt;- shapiro.test(feed_residuals)\n\ncat(sprintf(\"Shapiro-Wilk Test for Normality\\n\"))\n\n\nShapiro-Wilk Test for Normality\n\n\nCode\ncat(sprintf(\"W = %.4f, p-value = %.4f\\n\",\n            shapiro_result$statistic, shapiro_result$p.value))\n\n\nW = 0.9869, p-value = 0.7660\n\n\nCode\nif(shapiro_result$p.value &gt; 0.05) {\n  cat(\"→ No evidence against normality (p &gt; 0.05)\\n\")\n} else {\n  cat(\"→ Some evidence against normality (p &lt; 0.05)\\n\")\n  cat(\"  Consider: transformation, non-parametric test, or rely on robustness\\n\")\n}\n\n\n→ No evidence against normality (p &gt; 0.05)\n\n\n\n\n\n\n\n\nNoteANOVA is Robust to Normality Violations\n\n\n\nWith moderate to large sample sizes and no extreme skewness, ANOVA is fairly robust to violations of normality, especially with balanced designs.\nIf violated: - Try transformations (log, square root, Box-Cox) - Use Kruskal-Wallis test (non-parametric alternative) - Bootstrap confidence intervals\n\n\n\n\n19.3.3 3. Homogeneity of Variance\nVisual check: Boxplots\n\n\nCode\nggplot(feed_data, aes(x = feed_type, y = adg_kg)) +\n  geom_boxplot(aes(fill = feed_type), alpha = 0.5) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1),\n               geom = \"errorbar\", width = 0.3, linewidth = 1, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"darkred\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Checking Homogeneity of Variance\",\n       subtitle = \"Red bars show mean ± SD; similar heights suggest equal variances\",\n       x = \"Feed Type\",\n       y = \"Average Daily Gain (kg/day)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFormal test: Levene’s Test\n\n\nCode\nlevene_result &lt;- leveneTest(feed_model)\n\nprint(levene_result)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3   2.119 0.1081\n      56               \n\n\nCode\ncat(sprintf(\"\\nLevene's Test: F(%d, %d) = %.3f, p = %.4f\\n\",\n            levene_result$Df[1], levene_result$Df[2],\n            levene_result$`F value`[1], levene_result$`Pr(&gt;F)`[1]))\n\n\n\nLevene's Test: F(3, 56) = 2.119, p = 0.1081\n\n\nCode\nif(levene_result$`Pr(&gt;F)`[1] &gt; 0.05) {\n  cat(\"→ No evidence of unequal variances (p &gt; 0.05)\\n\")\n} else {\n  cat(\"→ Evidence of unequal variances (p &lt; 0.05)\\n\")\n  cat(\"  Consider: Welch's ANOVA (oneway.test) or transformation\\n\")\n}\n\n\n→ No evidence of unequal variances (p &gt; 0.05)\n\n\nRule of thumb: If the ratio of largest to smallest SD is &lt; 2, you’re usually fine.\n\n\nCode\nsd_ratio &lt;- max(feed_summary$sd_adg) / min(feed_summary$sd_adg)\ncat(sprintf(\"Ratio of largest to smallest SD: %.2f\\n\", sd_ratio))\n\n\nRatio of largest to smallest SD: 1.85\n\n\nCode\nif(sd_ratio &lt; 2) {\n  cat(\"→ Ratio &lt; 2, variances similar enough for standard ANOVA\\n\")\n} else {\n  cat(\"→ Ratio ≥ 2, consider Welch's ANOVA\\n\")\n}\n\n\n→ Ratio &lt; 2, variances similar enough for standard ANOVA",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-violations",
    "href": "chapters/part2-ch05-anova.html#sec-violations",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "19.4 What to Do If Assumptions Are Violated",
    "text": "19.4 What to Do If Assumptions Are Violated\n\n19.4.1 Unequal Variances → Welch’s ANOVA\n\n\nCode\n# Welch's one-way test (doesn't assume equal variances)\nwelch_result &lt;- oneway.test(adg_kg ~ feed_type, data = feed_data)\n\nprint(welch_result)\n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  adg_kg and feed_type\nF = 10.682, num df = 3.00, denom df = 30.35, p-value = 5.907e-05\n\n\nCode\ncat(sprintf(\"\\nWelch's ANOVA: F(%.2f, %.2f) = %.3f, p = %.4f\\n\",\n            welch_result$parameter[1], welch_result$parameter[2],\n            welch_result$statistic, welch_result$p.value))\n\n\n\nWelch's ANOVA: F(3.00, 30.35) = 10.682, p = 0.0001\n\n\n\n\n19.4.2 Non-Normality → Kruskal-Wallis Test\n\n\nCode\n# Kruskal-Wallis (non-parametric alternative to ANOVA)\nkruskal_result &lt;- kruskal.test(adg_kg ~ feed_type, data = feed_data)\n\nprint(kruskal_result)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  adg_kg by feed_type\nKruskal-Wallis chi-squared = 15.964, df = 3, p-value = 0.001153\n\n\nCode\ncat(sprintf(\"\\nKruskal-Wallis: χ²(%d) = %.3f, p = %.4f\\n\",\n            kruskal_result$parameter,\n            kruskal_result$statistic,\n            kruskal_result$p.value))\n\n\n\nKruskal-Wallis: χ²(3) = 15.964, p = 0.0012\n\n\nNote: Kruskal-Wallis tests for differences in distributions, not just means. If significant, use Dunn’s test for post-hoc comparisons.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-post-hoc-problem",
    "href": "chapters/part2-ch05-anova.html#sec-post-hoc-problem",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.1 The Multiple Comparisons Problem Returns",
    "text": "20.1 The Multiple Comparisons Problem Returns\nIf ANOVA is significant, you might be tempted to run all pairwise t-tests. But this brings back the multiple comparisons problem!\nSolution: Post-hoc tests that adjust p-values to control family-wise error rate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-post-hoc-methods",
    "href": "chapters/part2-ch05-anova.html#sec-post-hoc-methods",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.2 Common Post-Hoc Methods",
    "text": "20.2 Common Post-Hoc Methods\n\n\n\n\n\n\n\n\nMethod\nUse When\nControl Level\n\n\n\n\nTukey HSD\nAll pairwise comparisons\nBalanced or unbalanced\n\n\nBonferroni\nSmall number of comparisons\nConservative, any design\n\n\nDunnett’s\nComparing to a control group\nSpecific control comparison\n\n\nScheffe\nComplex contrasts\nMost conservative\n\n\nNo adjustment\nDON’T DO THIS\nInflates Type I error\n\n\n\n\n\n\n\n\n\nTipRecommended: Tukey HSD\n\n\n\nTukey’s Honestly Significant Difference is the most commonly used post-hoc test. It: - Controls family-wise error rate at α - Works well with balanced and unbalanced designs - Tests all pairwise comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-emmeans-package",
    "href": "chapters/part2-ch05-anova.html#sec-emmeans-package",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.3 Post-Hoc Tests Using emmeans",
    "text": "20.3 Post-Hoc Tests Using emmeans\nThe emmeans (estimated marginal means) package provides a modern, flexible framework for post-hoc tests:\n\n\nCode\nlibrary(emmeans)\n\n# Step 1: Compute estimated marginal means\nfeed_emm &lt;- emmeans(feed_model, ~ feed_type)\n\nprint(feed_emm)\n\n\n feed_type emmean     SE df lower.CL upper.CL\n A           1.47 0.0474 56     1.38     1.57\n B           1.61 0.0474 56     1.52     1.71\n C           1.50 0.0474 56     1.41     1.60\n D           1.75 0.0474 56     1.66     1.85\n\nConfidence level used: 0.95 \n\n\nCode\n# Step 2: Pairwise comparisons with Tukey adjustment\nfeed_pairs_tukey &lt;- pairs(feed_emm, adjust = \"tukey\")\n\nprint(feed_pairs_tukey)\n\n\n contrast estimate     SE df t.ratio p.value\n A - B     -0.1409 0.0671 56  -2.101  0.1654\n A - C     -0.0308 0.0671 56  -0.460  0.9674\n A - D     -0.2829 0.0671 56  -4.217  0.0005\n B - C      0.1101 0.0671 56   1.641  0.3644\n B - D     -0.1419 0.0671 56  -2.116  0.1606\n C - D     -0.2520 0.0671 56  -3.757  0.0023\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n# Clean summary\nfeed_pairs_tukey_df &lt;- as.data.frame(feed_pairs_tukey)\n\nknitr::kable(feed_pairs_tukey_df, digits = 4, align = 'lcccc',\n             caption = \"Tukey HSD Post-Hoc Comparisons\")\n\n\n\nTukey HSD Post-Hoc Comparisons\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nA - B\n-0.1409\n0.0671\n56\n-2.1008\n0.1654\n\n\nA - C\n-0.0308\n0.0671\n56\n-0.4597\n0.9674\n\n\nA - D\n-0.2829\n0.0671\n56\n-4.2167\n0.0005\n\n\nB - C\n0.1101\n0.0671\n56\n1.6411\n0.3644\n\n\nB - D\n-0.1419\n0.0671\n56\n-2.1159\n0.1606\n\n\nC - D\n-0.2520\n0.0671\n56\n-3.7569\n0.0023\n\n\n\n\n\nInterpretation: - estimate: Difference in means (Feed 1 - Feed 2) - SE: Standard error of the difference - t.ratio: Test statistic - p.value: Adjusted p-value (controls FWER)\nWhich comparisons are significant (p &lt; 0.05)?\n\n\nCode\nsig_pairs &lt;- feed_pairs_tukey_df %&gt;%\n  filter(p.value &lt; 0.05) %&gt;%\n  dplyr::select(contrast, estimate, p.value)\n\nif(nrow(sig_pairs) &gt; 0) {\n  cat(\"Significant pairwise differences:\\n\")\n  knitr::kable(sig_pairs, digits = 4,\n               caption = \"Significant Pairs (Tukey-adjusted p &lt; 0.05)\")\n} else {\n  cat(\"No significant pairwise differences after Tukey adjustment.\\n\")\n}\n\n\nSignificant pairwise differences:\n\n\n\nSignificant Pairs (Tukey-adjusted p &lt; 0.05)\n\n\ncontrast\nestimate\np.value\n\n\n\n\nA - D\n-0.2829\n0.0005\n\n\nC - D\n-0.2520\n0.0023",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-compare-adjustments",
    "href": "chapters/part2-ch05-anova.html#sec-compare-adjustments",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.4 Comparing Adjustment Methods",
    "text": "20.4 Comparing Adjustment Methods\nLet’s compare Tukey vs Bonferroni vs no adjustment:\n\n\nCode\n# Different adjustments\npairs_none &lt;- pairs(feed_emm, adjust = \"none\")\npairs_bonf &lt;- pairs(feed_emm, adjust = \"bonferroni\")\npairs_tukey &lt;- pairs(feed_emm, adjust = \"tukey\")\n\n# Extract p-values\ncomparison_df &lt;- tibble(\n  Comparison = as.character(feed_pairs_tukey_df$contrast),\n  `No Adjust` = summary(pairs_none)$p.value,\n  Bonferroni = summary(pairs_bonf)$p.value,\n  Tukey = summary(pairs_tukey)$p.value\n)\n\nknitr::kable(comparison_df, digits = 4, align = 'lccc',\n             caption = \"P-values Under Different Adjustment Methods\")\n\n\n\nP-values Under Different Adjustment Methods\n\n\nComparison\nNo Adjust\nBonferroni\nTukey\n\n\n\n\nA - B\n0.0402\n0.2410\n0.1654\n\n\nA - C\n0.6475\n1.0000\n0.9674\n\n\nA - D\n0.0001\n0.0005\n0.0005\n\n\nB - C\n0.1064\n0.6383\n0.3644\n\n\nB - D\n0.0388\n0.2329\n0.1606\n\n\nC - D\n0.0004\n0.0025\n0.0023\n\n\n\n\n\nObservation: - No adjustment gives smallest p-values (most “discoveries”) but inflates Type I error - Bonferroni is most conservative (largest p-values) - Tukey is in between and recommended for pairwise comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-visualize-post-hoc",
    "href": "chapters/part2-ch05-anova.html#sec-visualize-post-hoc",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.5 Visualizing Post-Hoc Results",
    "text": "20.5 Visualizing Post-Hoc Results\n\n20.5.1 Compact Letter Display\nA compact letter display shows which groups differ using letters:\n\n\nCode\n# Compact letter display\nfeed_cld &lt;- cld(feed_emm, Letters = letters, adjust = \"tukey\")\n\nprint(feed_cld)\n\n\n feed_type emmean     SE df lower.CL upper.CL .group\n A           1.47 0.0474 56     1.35     1.59  a    \n C           1.50 0.0474 56     1.38     1.62  a    \n B           1.61 0.0474 56     1.49     1.73  ab   \n D           1.75 0.0474 56     1.63     1.88   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCode\n# Visualize with letters\nfeed_cld_df &lt;- as.data.frame(feed_cld)\n\nggplot(feed_cld_df, aes(x = feed_type, y = emmean, fill = feed_type)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.2f\", emmean)),\n            vjust = -2.5, fontface = \"bold\", size = 4) +\n  geom_text(aes(label = .group), vjust = -4.5, size = 5, fontface = \"bold\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Post-Hoc Results: Tukey HSD\",\n       subtitle = \"Groups sharing a letter are not significantly different\",\n       x = \"Feed Type\",\n       y = \"Estimated Marginal Mean ADG (kg/day)\") +\n  ylim(0, 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nReading the plot: - Groups with different letters are significantly different - Groups with same letter are not significantly different\n\n\n20.5.2 Pairwise Comparison Plot\n\n\nCode\n# Pairwise comparison plot\nplot(feed_pairs_tukey) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Pairwise Comparisons with 95% Confidence Intervals\",\n       subtitle = \"Intervals not crossing zero indicate significant differences\",\n       x = \"Difference in Mean ADG (kg/day)\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-tukeyhsd-function",
    "href": "chapters/part2-ch05-anova.html#sec-tukeyhsd-function",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "20.6 Alternative: Using TukeyHSD()",
    "text": "20.6 Alternative: Using TukeyHSD()\nFor comparison, here’s the traditional approach using aov() and TukeyHSD():\n\n\nCode\n# Fit with aov() (alternative to lm)\nfeed_aov &lt;- aov(adg_kg ~ feed_type, data = feed_data)\n\n# Tukey HSD\ntukey_traditional &lt;- TukeyHSD(feed_aov)\n\nprint(tukey_traditional)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = adg_kg ~ feed_type, data = feed_data)\n\n$feed_type\n           diff         lwr        upr     p adj\nB-A  0.14092989 -0.03670144 0.31856122 0.1654298\nC-A  0.03084118 -0.14679015 0.20847251 0.9674384\nD-A  0.28287289  0.10524156 0.46050422 0.0005176\nC-B -0.11008871 -0.28772004 0.06754262 0.3644403\nD-B  0.14194300 -0.03568833 0.31957433 0.1606251\nD-C  0.25203172  0.07440039 0.42966305 0.0022679\n\n\nCode\n# Visualize\nplot(tukey_traditional, las = 1, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nNote: Both approaches (emmeans and TukeyHSD) give equivalent results. We recommend emmeans for its flexibility and integration with lm().",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-eta-squared",
    "href": "chapters/part2-ch05-anova.html#sec-eta-squared",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "21.1 Eta-Squared (η²)",
    "text": "21.1 Eta-Squared (η²)\nEta-squared is the proportion of total variance explained by the grouping variable:\n\\[\\eta^2 = \\frac{SS_{between}}{SS_{total}}\\]\n\n\nCode\nlibrary(effectsize)\n\n# Calculate eta-squared\neta_sq &lt;- eta_squared(feed_model)\n\nprint(eta_sq)\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nfeed_type | 0.28 | [0.10, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nCode\ncat(sprintf(\"\\nEta-squared: %.3f (%.1f%% of variance explained)\\n\",\n            eta_sq$Eta2[1], eta_sq$Eta2[1] * 100))\n\n\n\nEta-squared: 0.281 (28.1% of variance explained)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-omega-squared",
    "href": "chapters/part2-ch05-anova.html#sec-omega-squared",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "21.2 Omega-Squared (ω²)",
    "text": "21.2 Omega-Squared (ω²)\nOmega-squared is a less biased estimate of effect size (adjusts for sample size):\n\\[\\omega^2 = \\frac{SS_{between} - (k-1)MS_{within}}{SS_{total} + MS_{within}}\\]\n\n\nCode\n# Calculate omega-squared\nomega_sq &lt;- omega_squared(feed_model)\n\nprint(omega_sq)\n\n\n# Effect Size for ANOVA\n\nParameter | Omega2 |       95% CI\n---------------------------------\nfeed_type |   0.24 | [0.07, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nCode\ncat(sprintf(\"\\nOmega-squared: %.3f\\n\", omega_sq$Omega2[1]))\n\n\n\nOmega-squared: 0.239\n\n\nInterpretation: ω² is typically slightly smaller than η² and is preferred for reporting.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-effect-size-guidelines",
    "href": "chapters/part2-ch05-anova.html#sec-effect-size-guidelines",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "21.3 Effect Size Guidelines",
    "text": "21.3 Effect Size Guidelines\nCohen’s (1988) benchmarks for η² and ω²:\n\n\n\nEffect Size\nη² / ω²\nInterpretation\n\n\n\n\nSmall\n0.01\n1% of variance explained\n\n\nMedium\n0.06\n6% of variance explained\n\n\nLarge\n0.14\n14% of variance explained\n\n\n\n\n\nCode\nomega_val &lt;- omega_sq$Omega2[1]\n\nif(omega_val &lt; 0.01) {\n  effect_interp &lt;- \"negligible\"\n} else if(omega_val &lt; 0.06) {\n  effect_interp &lt;- \"small\"\n} else if(omega_val &lt; 0.14) {\n  effect_interp &lt;- \"medium\"\n} else {\n  effect_interp &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", effect_interp))\n\n\nEffect size interpretation: large\n\n\n\n\n\n\n\n\nImportantAlways Report Effect Sizes\n\n\n\nWhen reporting ANOVA results, include:\n\nF-statistic and p-value\nEffect size (η² or ω²)\nDescriptive statistics (means, SDs)\nPost-hoc results if significant\n\nExample: Feed type significantly affected ADG, F(3, 56) = 12.45, p &lt; 0.001, ω² = 0.35. Post-hoc tests revealed Feed D produced significantly higher gain than all other feeds.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-unbalanced",
    "href": "chapters/part2-ch05-anova.html#sec-unbalanced",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "22.1 Unbalanced Designs",
    "text": "22.1 Unbalanced Designs\nUnbalanced designs (unequal sample sizes across groups) are common in practice due to: - Attrition (animals get sick, die, removed) - Unequal recruitment - Practical constraints\nConsequences: - Type I, II, III SS can differ - Slightly reduced power - Post-hoc tests still work\nRecommendation: Use Type II SS with car::Anova(model, type = \"II\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-power-sample-size",
    "href": "chapters/part2-ch05-anova.html#sec-power-sample-size",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "22.2 Power and Sample Size",
    "text": "22.2 Power and Sample Size\n\n22.2.1 Power Analysis for ANOVA\nBefore conducting a study, calculate required sample size:\n\n\nCode\nlibrary(pwr)\n\n# Power analysis for one-way ANOVA\n# Effect size f (Cohen's f):\n# f = 0.10 (small), 0.25 (medium), 0.40 (large)\n\n# Scenario: Want to detect medium effect (f = 0.25)\n# 4 groups, α = 0.05, power = 0.80\n\npower_result &lt;- pwr.anova.test(\n  k = 4,              # Number of groups\n  f = 0.25,           # Effect size (medium)\n  sig.level = 0.05,   # Alpha\n  power = 0.80        # Desired power\n)\n\nprint(power_result)\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nCode\ncat(sprintf(\"\\nRequired sample size: %.0f per group\\n\", ceiling(power_result$n)))\n\n\n\nRequired sample size: 45 per group\n\n\nCode\ncat(sprintf(\"Total sample size needed: %.0f\\n\", 4 * ceiling(power_result$n)))\n\n\nTotal sample size needed: 180\n\n\nNote: Cohen’s f is related to η² by:\n\\[f = \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\\]\n\n\n22.2.2 Power Curves\n\n\nCode\n# Calculate power for different sample sizes and effect sizes\npower_data &lt;- expand_grid(\n  n_per_group = seq(10, 50, by = 5),\n  effect_size = c(0.10, 0.25, 0.40)\n) %&gt;%\n  mutate(\n    effect_label = case_when(\n      effect_size == 0.10 ~ \"Small (f = 0.10)\",\n      effect_size == 0.25 ~ \"Medium (f = 0.25)\",\n      effect_size == 0.40 ~ \"Large (f = 0.40)\"\n    ),\n    power = map2_dbl(n_per_group, effect_size, ~ {\n      pwr.anova.test(k = 4, n = .x, f = .y, sig.level = 0.05)$power\n    })\n  )\n\nggplot(power_data, aes(x = n_per_group, y = power, color = effect_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray40\") +\n  annotate(\"text\", x = 45, y = 0.82, label = \"Target: 80% power\", size = 3) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Statistical Power for One-Way ANOVA\",\n       subtitle = \"4 groups, α = 0.05\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-common-mistakes",
    "href": "chapters/part2-ch05-anova.html#sec-common-mistakes",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "22.3 Common Mistakes and Pitfalls",
    "text": "22.3 Common Mistakes and Pitfalls\n\n22.3.1 1. Running Multiple t-Tests Instead of ANOVA\nProblem: Inflates Type I error rate\nSolution: Use ANOVA first, then post-hoc tests if significant\n\n\n22.3.2 2. Testing Groups Within Raw Data\nProblem: Normality/variance tests on each group separately\nSolution: Check assumptions on residuals from the model\n\n\n22.3.3 3. Ignoring Post-Hoc Adjustments\nProblem: Using adjust = \"none\" in post-hoc tests\nSolution: Always use adjustment (Tukey, Bonferroni, etc.)\n\n\n22.3.4 4. Reporting Only P-values\nProblem: No effect sizes or descriptive statistics\nSolution: Report means, SDs, effect sizes, and confidence intervals\n\n\n22.3.5 5. Treating ANOVA as Proof of Causation\nProblem: Correlation vs causation confusion\nSolution: Only randomized experiments support causal claims",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-reporting-results",
    "href": "chapters/part2-ch05-anova.html#sec-reporting-results",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "22.4 Reporting ANOVA Results",
    "text": "22.4 Reporting ANOVA Results\n\n22.4.1 Example Write-Up\n\nA one-way ANOVA was conducted to compare average daily gain across four feed formulations (A, B, C, D) in finishing steers (n = 15 per group). Assumptions of normality and homogeneity of variance were met (Levene’s test: p = 0.68).\nFeed type significantly affected ADG, F(3, 56) = 12.45, p &lt; 0.001, ω² = 0.35, indicating a large effect. Mean ADG was 1.45 kg/day (SD = 0.18) for Feed A, 1.55 kg/day (SD = 0.17) for Feed B, 1.50 kg/day (SD = 0.19) for Feed C, and 1.70 kg/day (SD = 0.16) for Feed D.\nTukey HSD post-hoc tests revealed that Feed D produced significantly higher ADG than all other feeds (all p &lt; 0.01). No significant differences were found among Feeds A, B, and C (all p &gt; 0.20).\n\n\n\n22.4.2 APA-Style Table\n\n\nCode\n# Create publication-ready table\nanova_table &lt;- tibble(\n  Source = c(\"Feed Type\", \"Residual\"),\n  SS = c(feed_anova$`Sum Sq`[1], feed_anova$`Sum Sq`[2]),\n  df = c(feed_anova$Df[1], feed_anova$Df[2]),\n  MS = c(SS[1]/df[1], SS[2]/df[2]),\n  F = c(feed_anova$`F value`[1], NA),\n  p = c(feed_anova$`Pr(&gt;F)`[1], NA)\n)\n\nknitr::kable(anova_table, digits = c(0, 3, 0, 3, 2, 4),\n             col.names = c(\"Source\", \"SS\", \"df\", \"MS\", \"F\", \"p\"),\n             caption = \"ANOVA Table for Feed Type Effect on Average Daily Gain\",\n             align = 'lccccc')\n\n\n\nANOVA Table for Feed Type Effect on Average Daily Gain\n\n\nSource\nSS\ndf\nMS\nF\np\n\n\n\n\nFeed Type\n0.737\n3\n0.246\n7.28\n3e-04\n\n\nResidual\n1.890\n56\n0.034\nNA\nNA",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-covered",
    "href": "chapters/part2-ch05-anova.html#sec-covered",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.1 What We Covered",
    "text": "23.1 What We Covered\n\nMultiple comparisons problem: Running multiple t-tests inflates Type I error rate\nANOVA logic: Partitions variance into between-group and within-group components\nF-statistic: Ratio of between to within variance; large F suggests group differences\nLinear model approach: Using lm() and car::Anova() provides flexibility and connects to regression\nType II/III SS: Important for unbalanced designs; use Type II for one-way ANOVA\nAssumptions: Independence, normality of residuals, homogeneity of variance\nPost-hoc tests: Tukey HSD, Bonferroni, and others for identifying specific group differences\nEffect sizes: η² and ω² quantify proportion of variance explained\nPower analysis: Plan sample sizes before data collection",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-principles",
    "href": "chapters/part2-ch05-anova.html#sec-principles",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.2 Key Principles",
    "text": "23.2 Key Principles\n\n\n\n\n\n\nImportantCore Principles of ANOVA\n\n\n\n\nANOVA controls FWER: Single test for multiple groups maintains α = 0.05\nSignificant F means “at least one difference”: Doesn’t tell you which groups differ\nAlways use post-hoc tests: If ANOVA is significant, conduct adjusted pairwise comparisons\nCheck residuals, not groups: Assumptions apply to model residuals\nReport effect sizes: Statistical significance ≠ practical importance\nUse modern tools: lm() + car::Anova() + emmeans is the recommended workflow",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-decision-framework",
    "href": "chapters/part2-ch05-anova.html#sec-decision-framework",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.3 ANOVA Decision Framework",
    "text": "23.3 ANOVA Decision Framework\nStep-by-step workflow:\n\nVisualize data: Boxplots, violin plots, means with error bars\nFit model: model &lt;- lm(outcome ~ group, data = df)\nRun ANOVA: car::Anova(model, type = \"II\")\nCheck assumptions: Diagnostic plots, Levene’s test, Q-Q plot\nIf significant: Conduct post-hoc tests with emmeans\nCalculate effect size: omega_squared(model)\nReport completely: F-statistic, p-value, effect size, means, SDs, post-hoc results",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-r-functions",
    "href": "chapters/part2-ch05-anova.html#sec-r-functions",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.4 R Functions Summary",
    "text": "23.4 R Functions Summary\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nlm()\nbase\nFit linear model (ANOVA)\n\n\nAnova()\ncar\nANOVA table with Type II/III SS\n\n\nemmeans()\nemmeans\nEstimated marginal means\n\n\npairs()\nemmeans\nPost-hoc pairwise comparisons\n\n\ncld()\nemmeans\nCompact letter display\n\n\nleveneTest()\ncar\nTest homogeneity of variance\n\n\neta_squared()\neffectsize\nEffect size (η²)\n\n\nomega_squared()\neffectsize\nEffect size (ω²)\n\n\npwr.anova.test()\npwr\nPower analysis\n\n\noneway.test()\nbase\nWelch’s ANOVA\n\n\nkruskal.test()\nbase\nNon-parametric alternative",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-regression-preview",
    "href": "chapters/part2-ch05-anova.html#sec-regression-preview",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.5 Connection to Regression (Preview)",
    "text": "23.5 Connection to Regression (Preview)\n\n\n\n\n\n\nNoteANOVA is Regression!\n\n\n\nOne-way ANOVA is actually linear regression with a categorical predictor. R converts your factor into dummy variables behind the scenes.\nComing in Weeks 7-8: We’ll see how ANOVA and regression are unified under the general linear model framework. Everything you learned this week applies directly to regression!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-next-week",
    "href": "chapters/part2-ch05-anova.html#sec-next-week",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "23.6 Next Week Preview",
    "text": "23.6 Next Week Preview\nWeek 6: Categorical Data Analysis\n\nChi-square tests (goodness-of-fit, independence)\nFisher’s exact test\nRisk ratios and odds ratios\n2×2 contingency tables\nMoving from continuous to categorical outcomes",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-practice-1",
    "href": "chapters/part2-ch05-anova.html#sec-practice-1",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "24.1 Problem 1: Beef Breed Comparison",
    "text": "24.1 Problem 1: Beef Breed Comparison\nA rancher wants to compare weight gain across three beef breeds. Here’s the data:\n\n\nCode\nbreed_data &lt;- tibble(\n  breed = rep(c(\"Angus\", \"Hereford\", \"Simmental\"), each = 12),\n  weight_gain_kg = c(\n    rnorm(12, mean = 320, sd = 35),\n    rnorm(12, mean = 305, sd = 32),\n    rnorm(12, mean = 340, sd = 38)\n  )\n)\n\n\nTasks:\n\nVisualize the data with boxplots\nFit a linear model and run ANOVA\nCheck assumptions (diagnostic plots, Levene’s test)\nIf significant, conduct Tukey HSD post-hoc tests\nCalculate omega-squared effect size\nWrite a results paragraph",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-practice-2",
    "href": "chapters/part2-ch05-anova.html#sec-practice-2",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "24.2 Problem 2: Unbalanced Design",
    "text": "24.2 Problem 2: Unbalanced Design\nYou have data on milk yield from four dairy management systems, but with unequal sample sizes:\n\nSystem A: n = 15\nSystem B: n = 12\nSystem C: n = 18\nSystem D: n = 10\n\nTasks:\n\nExplain why this is an unbalanced design\nWhich type of sums of squares should you use?\nHow would you handle this in your analysis?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-practice-3",
    "href": "chapters/part2-ch05-anova.html#sec-practice-3",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "24.3 Problem 3: Power Analysis",
    "text": "24.3 Problem 3: Power Analysis\nYou’re planning a study to compare five feeding regimens. Based on pilot data, you expect a medium effect size (f = 0.25).\nTasks:\n\nCalculate required sample size per group for 80% power\nCalculate required sample size for 90% power\nIf you can only recruit 20 animals per group, what power will you have?\n\n\n\nCode\n# Your code here\nlibrary(pwr)\n\n# Part a\npwr.anova.test(k = 5, f = 0.25, sig.level = 0.05, power = 0.80)\n\n# Part b\npwr.anova.test(k = 5, f = 0.25, sig.level = 0.05, power = 0.90)\n\n# Part c\npwr.anova.test(k = 5, n = 20, f = 0.25, sig.level = 0.05)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-practice-4",
    "href": "chapters/part2-ch05-anova.html#sec-practice-4",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "24.4 Problem 4: Assumption Violations",
    "text": "24.4 Problem 4: Assumption Violations\nYour ANOVA shows: - Levene’s test: p = 0.02 (unequal variances) - Shapiro-Wilk on residuals: p = 0.35 (normality OK)\nQuestions:\n\nWhich assumption is violated?\nWhat are two ways to address this?\nWrite the R code for an alternative analysis",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-practice-5",
    "href": "chapters/part2-ch05-anova.html#sec-practice-5",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "24.5 Problem 5: Interpretation",
    "text": "24.5 Problem 5: Interpretation\nYou conduct ANOVA on 4 groups (n = 20 each) and get: - F(3, 76) = 2.15, p = 0.10 - Omega-squared = 0.04\nQuestions:\n\nIs the overall ANOVA significant?\nShould you conduct post-hoc tests? Why or why not?\nWhat is the effect size interpretation?\nIf you had gotten p = 0.03, would post-hoc tests be guaranteed to find significant pairs?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-reading",
    "href": "chapters/part2-ch05-anova.html#sec-reading",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "25.1 Recommended Reading",
    "text": "25.1 Recommended Reading\n\nMaxwell & Delaney (2004). Designing Experiments and Analyzing Data. Comprehensive ANOVA coverage\nGelman & Hill (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Modern perspective\nKeppel & Wickens (2004). Design and Analysis: A Researcher’s Handbook. Classic experimental design text",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-online",
    "href": "chapters/part2-ch05-anova.html#sec-online",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "25.2 Online Resources",
    "text": "25.2 Online Resources\n\nUCLA Statistical Consulting: https://stats.oarc.ucla.edu/r/\nR Graphics Cookbook: https://r-graphics.org/\nemmeans package vignette: Excellent guide to post-hoc tests",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch05-anova.html#sec-papers-ss",
    "href": "chapters/part2-ch05-anova.html#sec-papers-ss",
    "title": "13  Week 5: Analysis of Variance (ANOVA)",
    "section": "25.3 Papers on Type I/II/III SS",
    "text": "25.3 Papers on Type I/II/III SS\n\nLangsrud (2003). “ANOVA for unbalanced data: Use Type II instead of Type III sums of squares”\nShaw & Mitchell-Olds (1993). “ANOVA for unbalanced data: An overview”\n\n\nEnd of Week 5 Lecture\nNext week: Categorical Data Analysis - moving from continuous outcomes to counts and proportions!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Week 5: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html",
    "href": "chapters/part2-ch06-categorical_data.html",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "",
    "text": "15 Introduction\nUp to this point in the course, we’ve focused primarily on continuous outcomes: cattle weight gains, milk production, feed efficiency, and similar numeric measurements. But many important questions in animal science involve categorical outcomes:\nWhen our outcome variable is categorical rather than continuous, we need different statistical tools. This week, we’ll explore methods for analyzing categorical data, from simple chi-square tests to logistic regression models.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#a-motivating-example",
    "href": "chapters/part2-ch06-categorical_data.html#a-motivating-example",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "15.1 A Motivating Example",
    "text": "15.1 A Motivating Example\nImagine you’re investigating bovine respiratory disease (BRD) in feedlot cattle. You’ve implemented a new vaccine protocol and want to know:\n\nDid it work? Are disease rates different between vaccinated and unvaccinated cattle?\nHow much did it help? What’s the reduction in disease risk?\nWhat factors matter? Do age, weight, or previous health status affect disease risk?\n\nTraditional t-tests and ANOVA won’t work here—our outcome is binary (disease: Yes/No), not continuous. We need categorical data analysis methods.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#key-questions-well-address",
    "href": "chapters/part2-ch06-categorical_data.html#key-questions-well-address",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "15.2 Key Questions We’ll Address",
    "text": "15.2 Key Questions We’ll Address\nBy the end of this week, you’ll be able to answer:\n\nWhen should I use chi-square tests vs Fisher’s exact test?\nHow do I calculate and interpret odds ratios and risk ratios?\nWhat’s the difference between association and causation with categorical data?\nWhen do I need logistic regression instead of simpler tests?\nHow do I interpret logistic regression coefficients?\n\n\n\n\n\n\n\nNoteBuilding on Previous Weeks\n\n\n\nWe’ve already covered hypothesis testing (Week 4) and ANOVA (Week 5). The logic is the same for categorical data:\n\nState hypotheses (null: no association; alternative: association exists)\nCalculate a test statistic\nDetermine if results are more extreme than expected by chance\nConsider effect sizes and practical significance\n\nThe difference is in how we measure associations with categorical variables.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#types-of-categorical-variables",
    "href": "chapters/part2-ch06-categorical_data.html#types-of-categorical-variables",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "16.1 Types of Categorical Variables",
    "text": "16.1 Types of Categorical Variables\nCategorical variables come in different forms:\nNominal: Categories with no inherent order - Breed (Holstein, Jersey, Angus) - Coat color (Black, White, Brown) - Disease diagnosis (BRD, Pneumonia, Healthy)\nOrdinal: Categories with a natural order - Body condition score (1, 2, 3, 4, 5) - Lameness score (None, Mild, Moderate, Severe) - Treatment response (Poor, Fair, Good, Excellent)\nBinary: Special case with only two categories - Disease status (Yes/No) - Survival (Alive/Dead) - Treatment group (Control/Treated)\n\n\n\n\n\n\nImportantThe methods we cover this week work best for nominal and binary data\n\n\n\nFor ordinal data with many categories, specialized methods (ordinal logistic regression, Wilcoxon tests) may be more appropriate. However, chi-square tests can still be used as a first approximation.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#contingency-tables",
    "href": "chapters/part2-ch06-categorical_data.html#contingency-tables",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "16.2 Contingency Tables",
    "text": "16.2 Contingency Tables\nA contingency table (also called a cross-tabulation) displays the frequency distribution of two or more categorical variables.\nLet’s create a simple example:\n\n\nCode\n# Simulate vaccine trial data\nvaccine_data &lt;- tibble(\n  animal_id = 1:200,\n  vaccine = rep(c(\"Control\", \"Vaccinated\"), each = 100),\n  disease = c(\n    sample(c(\"Yes\", \"No\"), 100, replace = TRUE, prob = c(0.30, 0.70)),  # Control\n    sample(c(\"Yes\", \"No\"), 100, replace = TRUE, prob = c(0.15, 0.85))   # Vaccinated\n  )\n)\n\n# Create contingency table\ntable_result &lt;- table(vaccine_data$vaccine, vaccine_data$disease)\ntable_result\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\nThis is a 2×2 contingency table (2 rows × 2 columns). Let’s make it more informative:\n\n\nCode\n# Add row/column totals\naddmargins(table_result)\n\n\n            \n              No Yes Sum\n  Control     67  33 100\n  Vaccinated  88  12 100\n  Sum        155  45 200\n\n\nWe can also calculate proportions:\n\n\nCode\n# Proportions by row (vaccine status)\nprop.table(table_result, margin = 1) %&gt;%\n  round(3)\n\n\n            \n               No  Yes\n  Control    0.67 0.33\n  Vaccinated 0.88 0.12\n\n\nInterpretation: In this simulated data: - 31% of control animals got disease - 12% of vaccinated animals got disease - Difference: 31% - 12% = 19 percentage points\nBut is this difference statistically significant, or could it have occurred by chance?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#when-to-use-it",
    "href": "chapters/part2-ch06-categorical_data.html#when-to-use-it",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "17.1 When to Use It",
    "text": "17.1 When to Use It\nUse this test when you have: - One categorical variable - A hypothesis about the expected distribution - Want to test if your data fits that distribution",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#classic-example-mendelian-genetics",
    "href": "chapters/part2-ch06-categorical_data.html#classic-example-mendelian-genetics",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "17.2 Classic Example: Mendelian Genetics",
    "text": "17.2 Classic Example: Mendelian Genetics\nSuppose you’re breeding horses and expect a 3:1 ratio of black to chestnut coat color (simple dominant inheritance). You observe:\n\nBlack: 73\nChestnut: 27\nTotal: 100\n\nDoes this match the expected 3:1 ratio?\n\n\nCode\n# Observed counts\nobserved &lt;- c(Black = 73, Chestnut = 27)\n\n# Expected proportions (3:1 ratio)\nexpected_props &lt;- c(0.75, 0.25)\n\n# Chi-square goodness of fit test\nchisq_result &lt;- chisq.test(observed, p = expected_props)\nchisq_result\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.21333, df = 1, p-value = 0.6442\n\n\n\n\nCode\n# Tidy output\ntidy(chisq_result)\n\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                  \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                   \n1     0.213   0.644         1 Chi-squared test for given probabilities\n\n\nInterpretation: - p-value = 0.644 - We fail to reject the null hypothesis - The observed data is consistent with a 3:1 ratio",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#how-it-works",
    "href": "chapters/part2-ch06-categorical_data.html#how-it-works",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "17.3 How It Works",
    "text": "17.3 How It Works\nThe chi-square test compares observed (O) and expected (E) frequencies:\n\\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\]\nLet’s calculate it manually:\n\n\nCode\n# Expected counts (3:1 ratio with n=100)\nexpected &lt;- c(75, 25)\n\n# Calculate chi-square statistic\nchi_square_stat &lt;- sum((observed - expected)^2 / expected)\nchi_square_stat\n\n\n[1] 0.2133333\n\n\nCode\n# Compare to test result\nchisq_result$statistic\n\n\nX-squared \n0.2133333 \n\n\nThe p-value comes from the chi-square distribution with degrees of freedom = k - 1 (where k = number of categories).\n\n\nCode\n# Visualize chi-square distribution\ntibble(x = seq(0, 10, 0.01)) %&gt;%\n  mutate(density = dchisq(x, df = 1)) %&gt;%\n  ggplot(aes(x = x, y = density)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_vline(xintercept = chi_square_stat, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_area(data = . %&gt;% filter(x &gt;= chi_square_stat),\n            fill = \"red\", alpha = 0.3) +\n  labs(title = \"Chi-Square Distribution (df = 1)\",\n       subtitle = \"Red area = p-value\",\n       x = \"Chi-square statistic\",\n       y = \"Density\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#assumptions",
    "href": "chapters/part2-ch06-categorical_data.html#assumptions",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "17.4 Assumptions",
    "text": "17.4 Assumptions\n\n\n\n\n\n\nWarningChi-Square Test Assumptions\n\n\n\n\nIndependent observations: Each animal counted once\nExpected cell counts ≥ 5: Rule of thumb for valid p-values\nRandom sampling: Data should be representative\n\nIf expected counts &lt; 5, the chi-square approximation may be poor. Consider: - Combining categories - Using exact tests - Collecting more data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#more-complex-example-dihybrid-cross",
    "href": "chapters/part2-ch06-categorical_data.html#more-complex-example-dihybrid-cross",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "17.5 More Complex Example: Dihybrid Cross",
    "text": "17.5 More Complex Example: Dihybrid Cross\nIn a dihybrid cross (two genes), we expect a 9:3:3:1 ratio. Let’s test data from 160 offspring:\n\n\nCode\n# Observed counts\nobserved_dihybrid &lt;- c(95, 28, 26, 11)\nnames(observed_dihybrid) &lt;- c(\"Both_Dominant\", \"First_Dominant\",\n                               \"Second_Dominant\", \"Both_Recessive\")\n\n# Expected proportions (9:3:3:1 ratio)\nexpected_props_dihybrid &lt;- c(9, 3, 3, 1) / 16\n\n# Chi-square test\nchisq_dihybrid &lt;- chisq.test(observed_dihybrid, p = expected_props_dihybrid)\ntidy(chisq_dihybrid)\n\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                                  \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                   \n1      1.04   0.790         3 Chi-squared test for given probabilities\n\n\n\n\nCode\n# Visualize observed vs expected\ntibble(\n  Phenotype = factor(names(observed_dihybrid),\n                     levels = names(observed_dihybrid)),\n  Observed = observed_dihybrid,\n  Expected = expected_props_dihybrid * sum(observed_dihybrid)\n) %&gt;%\n  pivot_longer(cols = c(Observed, Expected),\n               names_to = \"Type\", values_to = \"Count\") %&gt;%\n  ggplot(aes(x = Phenotype, y = Count, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Observed\" = \"steelblue\",\n                                \"Expected\" = \"coral\")) +\n  labs(title = \"Dihybrid Cross: Observed vs Expected Counts\",\n       subtitle = paste0(\"χ² = \", round(chisq_dihybrid$statistic, 2),\n                        \", p = \", round(chisq_dihybrid$p.value, 3)),\n       x = \"Phenotype\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nInterpretation: The data fits the expected 9:3:3:1 ratio well (p = 0.79).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#the-null-hypothesis",
    "href": "chapters/part2-ch06-categorical_data.html#the-null-hypothesis",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.1 The Null Hypothesis",
    "text": "18.1 The Null Hypothesis\n\nH₀: The two variables are independent (no association)\nH₁: The two variables are associated",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#example-vaccine-efficacy",
    "href": "chapters/part2-ch06-categorical_data.html#example-vaccine-efficacy",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.2 Example: Vaccine Efficacy",
    "text": "18.2 Example: Vaccine Efficacy\nLet’s return to our vaccine trial. Are vaccine status and disease outcome independent?\n\n\nCode\n# Recreate table for clarity\nvaccine_table &lt;- table(vaccine_data$vaccine, vaccine_data$disease)\nvaccine_table\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\nCode\n# Chi-square test of independence\nchi_vaccine &lt;- chisq.test(vaccine_table)\nchi_vaccine\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  vaccine_table\nX-squared = 11.47, df = 1, p-value = 0.0007075\n\n\n\n\nCode\ntidy(chi_vaccine)\n\n\n# A tibble: 1 × 4\n  statistic  p.value parameter method                                           \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                            \n1      11.5 0.000707         1 Pearson's Chi-squared test with Yates' continuit…\n\n\nInterpretation: - χ² = 11.47 - p-value = 7^{-4} - We reject the null hypothesis - There is an association between vaccination and disease status",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#what-does-the-test-do",
    "href": "chapters/part2-ch06-categorical_data.html#what-does-the-test-do",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.3 What Does the Test Do?",
    "text": "18.3 What Does the Test Do?\nUnder independence, we can calculate expected counts for each cell:\n\\[\nE_{ij} = \\frac{(\\text{row } i \\text{ total}) \\times (\\text{column } j \\text{ total})}{\\text{grand total}}\n\\]\n\n\nCode\n# Expected counts under independence\nchi_vaccine$expected\n\n\n            \n               No  Yes\n  Control    77.5 22.5\n  Vaccinated 77.5 22.5\n\n\nThe chi-square statistic measures how different observed counts are from expected:\n\\[\n\\chi^2 = \\sum_{all\\ cells} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#degrees-of-freedom",
    "href": "chapters/part2-ch06-categorical_data.html#degrees-of-freedom",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.4 Degrees of Freedom",
    "text": "18.4 Degrees of Freedom\nFor a contingency table: \\[\ndf = (r - 1) \\times (c - 1)\n\\]\nwhere r = number of rows, c = number of columns.\nFor our 2×2 table: df = (2-1) × (2-1) = 1",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#visualizing-associations",
    "href": "chapters/part2-ch06-categorical_data.html#visualizing-associations",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.5 Visualizing Associations",
    "text": "18.5 Visualizing Associations\n\n\nCode\n# Create visualization\nvaccine_data %&gt;%\n  count(vaccine, disease) %&gt;%\n  group_by(vaccine) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  ggplot(aes(x = vaccine, y = prop, fill = disease)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = paste0(round(prop * 100, 1), \"%\")),\n            position = position_dodge(width = 0.9),\n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = c(\"Yes\" = \"coral\", \"No\" = \"steelblue\")) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 1)) +\n  labs(title = \"Disease Rates by Vaccine Status\",\n       subtitle = paste0(\"χ² = \", round(chi_vaccine$statistic, 2),\n                        \", p = \", round(chi_vaccine$p.value, 4)),\n       x = \"Vaccine Status\", y = \"Proportion\", fill = \"Disease\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#effect-size-cramérs-v",
    "href": "chapters/part2-ch06-categorical_data.html#effect-size-cramérs-v",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.6 Effect Size: Cramér’s V",
    "text": "18.6 Effect Size: Cramér’s V\nChi-square tells us if an association exists, but not how strong it is. For effect size, we use Cramér’s V:\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (k - 1)}}\n\\]\nwhere n = sample size, k = min(rows, columns)\n\n\nCode\n# Calculate Cramér's V\nn &lt;- sum(vaccine_table)\nk &lt;- min(nrow(vaccine_table), ncol(vaccine_table))\ncramers_v &lt;- sqrt(chi_vaccine$statistic / (n * (k - 1)))\ncramers_v\n\n\nX-squared \n0.2394737 \n\n\nCode\n# Interpretation guide\ncat(\"Cramér's V =\", round(cramers_v, 3), \"\\n\")\n\n\nCramér's V = 0.239 \n\n\nCode\ncat(\"Effect size interpretation (Cohen's guidelines for 2x2 tables):\\n\")\n\n\nEffect size interpretation (Cohen's guidelines for 2x2 tables):\n\n\nCode\ncat(\"  Small:  V = 0.10\\n\")\n\n\n  Small:  V = 0.10\n\n\nCode\ncat(\"  Medium: V = 0.30\\n\")\n\n\n  Medium: V = 0.30\n\n\nCode\ncat(\"  Large:  V = 0.50\\n\")\n\n\n  Large:  V = 0.50\n\n\nOur effect size (V = 0.239) suggests a small to medium association.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#larger-tables",
    "href": "chapters/part2-ch06-categorical_data.html#larger-tables",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "18.7 Larger Tables",
    "text": "18.7 Larger Tables\nChi-square tests work with larger contingency tables too. Example: Three housing systems × three health outcomes:\n\n\nCode\n# Simulate housing system study\nset.seed(123)\nhousing_data &lt;- tibble(\n  housing = sample(c(\"Confinement\", \"Pasture\", \"Mixed\"),\n                   300, replace = TRUE)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    health = case_when(\n      housing == \"Confinement\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                        prob = c(0.60, 0.30, 0.10)),\n      housing == \"Pasture\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                    prob = c(0.75, 0.20, 0.05)),\n      housing == \"Mixed\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                  prob = c(0.70, 0.25, 0.05))\n    )\n  ) %&gt;%\n  ungroup()\n\n# Contingency table\nhousing_table &lt;- table(housing_data$housing, housing_data$health)\nhousing_table\n\n\n             \n              Healthy Mild Severe\n  Confinement      66   35     11\n  Mixed            68   20      2\n  Pasture          75   22      1\n\n\nCode\n# Chi-square test\nchi_housing &lt;- chisq.test(housing_table)\ntidy(chi_housing)\n\n\n# A tibble: 1 × 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      15.5 0.00384         4 Pearson's Chi-squared test\n\n\n\n\nCode\n# Mosaic-style visualization\nhousing_data %&gt;%\n  count(housing, health) %&gt;%\n  ggplot(aes(x = housing, y = n, fill = health)) +\n  geom_col(position = \"fill\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(labels = percent_format()) +\n  labs(title = \"Health Status by Housing System\",\n       subtitle = paste0(\"χ² = \", round(chi_housing$statistic, 2),\n                        \", p = \", round(chi_housing$p.value, 4)),\n       x = \"Housing System\", y = \"Proportion\", fill = \"Health Status\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#when-to-use-fishers-exact-test",
    "href": "chapters/part2-ch06-categorical_data.html#when-to-use-fishers-exact-test",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "19.1 When to Use Fisher’s Exact Test",
    "text": "19.1 When to Use Fisher’s Exact Test\n\nAny expected cell count &lt; 5\nSmall sample sizes (n &lt; 20 or so)\nWhen you want an exact p-value (not an approximation)\n\n\n\n\n\n\n\nNoteFisher’s Exact Test vs Chi-Square\n\n\n\nFisher’s exact test calculates the exact probability of observing the data (and more extreme tables) under the null hypothesis of independence. Chi-square uses an approximation based on the chi-square distribution.\nFor large samples, results are nearly identical. For small samples, Fisher’s test is more reliable.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#example-rare-disease-outbreak",
    "href": "chapters/part2-ch06-categorical_data.html#example-rare-disease-outbreak",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "19.2 Example: Rare Disease Outbreak",
    "text": "19.2 Example: Rare Disease Outbreak\nSuppose we’re investigating a rare disease in a small herd. Only 15 animals total:\n\n\nCode\n# Small sample data\nrare_disease &lt;- matrix(c(7, 1,    # Exposed\n                         4, 3),   # Not exposed\n                       nrow = 2, byrow = TRUE,\n                       dimnames = list(Exposure = c(\"Exposed\", \"Not Exposed\"),\n                                      Disease = c(\"Yes\", \"No\")))\nrare_disease\n\n\n             Disease\nExposure      Yes No\n  Exposed       7  1\n  Not Exposed   4  3\n\n\nCheck expected counts:\n\n\nCode\nchisq.test(rare_disease)$expected\n\n\n             Disease\nExposure           Yes       No\n  Exposed     5.866667 2.133333\n  Not Exposed 5.133333 1.866667\n\n\nExpected count in one cell is 3.2 (close to the threshold). Let’s use Fisher’s exact test:\n\n\nCode\nfisher_result &lt;- fisher.test(rare_disease)\nfisher_result\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  rare_disease\np-value = 0.2821\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2680235 313.3252444\nsample estimates:\nodds ratio \n  4.676004 \n\n\n\n\nCode\ntidy(fisher_result)\n\n\n# A tibble: 1 × 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     4.68   0.282    0.268      313. Fisher's Exact Test for Count… two.sided  \n\n\nInterpretation: - Exact p-value = 0.2821 - We reject the null hypothesis at α = 0.05 - There is evidence of an association between exposure and disease\nCompare to chi-square (for demonstration):\n\n\nCode\nchi_rare &lt;- chisq.test(rare_disease)\n\ntibble(\n  Test = c(\"Chi-Square\", \"Fisher's Exact\"),\n  `P-value` = c(chi_rare$p.value, fisher_result$p.value)\n) %&gt;%\n  kable(digits = 4,\n        caption = \"Comparison of Tests on Small Sample Data\")\n\n\n\nComparison of Tests on Small Sample Data\n\n\nTest\nP-value\n\n\n\n\nChi-Square\n0.4586\n\n\nFisher’s Exact\n0.2821\n\n\n\n\n\nThe warning on chi-square confirms we should use Fisher’s test here.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#computational-note",
    "href": "chapters/part2-ch06-categorical_data.html#computational-note",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "19.3 Computational Note",
    "text": "19.3 Computational Note\nFisher’s exact test can be computationally intensive for large tables. Most software (including R) can handle 2×2 tables easily, but larger tables may take time or use simulation-based approximations.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#table-notation",
    "href": "chapters/part2-ch06-categorical_data.html#table-notation",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "20.1 2×2 Table Notation",
    "text": "20.1 2×2 Table Notation\nFor a standard 2×2 table:\n\n\n\n\nDisease Yes\nDisease No\nTotal\n\n\n\n\nExposed\na\nb\na+b\n\n\nNot Exposed\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\nn",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#risk-ratio-relative-risk",
    "href": "chapters/part2-ch06-categorical_data.html#risk-ratio-relative-risk",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "20.2 Risk Ratio (Relative Risk)",
    "text": "20.2 Risk Ratio (Relative Risk)\nThe risk ratio (RR) compares the risk of disease in exposed vs unexposed groups:\n\\[\nRR = \\frac{a/(a+b)}{c/(c+d)} = \\frac{\\text{Risk in exposed}}{\\text{Risk in unexposed}}\n\\]\nInterpretation: - RR = 1: No association - RR &gt; 1: Exposure increases risk - RR &lt; 1: Exposure decreases risk (protective)\n\n20.2.1 Example: Vaccine Efficacy\n\n\nCode\n# Our vaccine data in 2x2 format\nvaccine_table\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\n\n\nCode\n# Extract counts\na &lt;- vaccine_table[\"Vaccinated\", \"Yes\"]    # Vaccinated & diseased\nb &lt;- vaccine_table[\"Vaccinated\", \"No\"]     # Vaccinated & not diseased\nc &lt;- vaccine_table[\"Control\", \"Yes\"]       # Control & diseased\nd &lt;- vaccine_table[\"Control\", \"No\"]        # Control & not diseased\n\n# Calculate risks\nrisk_vaccinated &lt;- a / (a + b)\nrisk_control &lt;- c / (c + d)\n\n# Risk ratio\nrisk_ratio &lt;- risk_vaccinated / risk_control\nrisk_ratio\n\n\n[1] 0.3636364\n\n\nCode\ncat(\"Risk in vaccinated group:\", round(risk_vaccinated, 3), \"\\n\")\n\n\nRisk in vaccinated group: 0.12 \n\n\nCode\ncat(\"Risk in control group:\", round(risk_control, 3), \"\\n\")\n\n\nRisk in control group: 0.33 \n\n\nCode\ncat(\"Risk Ratio:\", round(risk_ratio, 3), \"\\n\")\n\n\nRisk Ratio: 0.364 \n\n\nInterpretation: The vaccinated group has 0.36 times the risk of disease compared to controls. Since RR &lt; 1, vaccination is protective.\n\n\n20.2.2 Vaccine Efficacy\nVaccine efficacy = 1 - RR = proportion of disease prevented by vaccination\n\n\nCode\nvaccine_efficacy &lt;- (1 - risk_ratio) * 100\ncat(\"Vaccine efficacy:\", round(vaccine_efficacy, 1), \"%\\n\")\n\n\nVaccine efficacy: 63.6 %\n\n\nThe vaccine prevents approximately 64% of disease cases.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#odds-ratio",
    "href": "chapters/part2-ch06-categorical_data.html#odds-ratio",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "20.3 Odds Ratio",
    "text": "20.3 Odds Ratio\nThe odds ratio (OR) compares the odds of disease in exposed vs unexposed:\n\\[\nOR = \\frac{a/b}{c/d} = \\frac{a \\times d}{b \\times c}\n\\]\nOdds are different from risk (probability): - Risk = a / (a+b) - Odds = a / b\n\n\nCode\n# Calculate odds\nodds_vaccinated &lt;- a / b\nodds_control &lt;- c / d\n\n# Odds ratio\nodds_ratio &lt;- odds_vaccinated / odds_control\n# OR: Simplified calculation\nodds_ratio_simple &lt;- (a * d) / (b * c)\n\ncat(\"Odds in vaccinated group:\", round(odds_vaccinated, 3), \"\\n\")\n\n\nOdds in vaccinated group: 0.136 \n\n\nCode\ncat(\"Odds in control group:\", round(odds_control, 3), \"\\n\")\n\n\nOdds in control group: 0.493 \n\n\nCode\ncat(\"Odds Ratio:\", round(odds_ratio, 3), \"\\n\")\n\n\nOdds Ratio: 0.277 \n\n\n\n\n\n\n\n\nImportantRisk Ratio vs Odds Ratio\n\n\n\n\nRisk Ratio: More intuitive, easier to interpret\nOdds Ratio: More commonly used in logistic regression and case-control studies\nWhen the outcome is rare (&lt; 10%), OR ≈ RR\nWhen the outcome is common, OR and RR can differ substantially\n\nIn our vaccine example (disease rates ~12-31%), RR = 0.36 and OR = 0.28 are somewhat different.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#confidence-intervals",
    "href": "chapters/part2-ch06-categorical_data.html#confidence-intervals",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "20.4 Confidence Intervals",
    "text": "20.4 Confidence Intervals\nPoint estimates are useful, but we also need uncertainty quantification. Let’s calculate 95% CIs:\n\n\nCode\n# Using prop.test for risk ratio CI\nprop_test &lt;- prop.test(c(a, c), c(a+b, c+d))\n\n# For OR, use logarithmic CI\nlog_or &lt;- log(odds_ratio)\nse_log_or &lt;- sqrt(1/a + 1/b + 1/c + 1/d)\nci_log_or &lt;- log_or + c(-1.96, 1.96) * se_log_or\nci_or &lt;- exp(ci_log_or)\n\n# Display results\ntibble(\n  Measure = c(\"Risk Ratio\", \"Odds Ratio\"),\n  Estimate = c(risk_ratio, odds_ratio),\n  `95% CI Lower` = c(prop_test$estimate[1] / prop_test$estimate[2] *\n                       exp(-1.96 * sqrt(1/(a+b) + 1/(c+d))),\n                     ci_or[1]),\n  `95% CI Upper` = c(prop_test$estimate[1] / prop_test$estimate[2] *\n                       exp(1.96 * sqrt(1/(a+b) + 1/(c+d))),\n                     ci_or[2])\n) %&gt;%\n  kable(digits = 3,\n        caption = \"Risk Ratio and Odds Ratio with 95% Confidence Intervals\")\n\n\n\nRisk Ratio and Odds Ratio with 95% Confidence Intervals\n\n\nMeasure\nEstimate\n95% CI Lower\n95% CI Upper\n\n\n\n\nRisk Ratio\n0.364\n0.276\n0.480\n\n\nOdds Ratio\n0.277\n0.133\n0.576\n\n\n\n\n\nSince both CIs exclude 1.0, we conclude there’s a statistically significant association.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#why-not-linear-regression",
    "href": "chapters/part2-ch06-categorical_data.html#why-not-linear-regression",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.1 Why Not Linear Regression?",
    "text": "21.1 Why Not Linear Regression?\nWith a binary outcome (0/1, Yes/No), linear regression has problems:\n\n\nCode\n# Create data with binary outcome\ndisease_weight &lt;- tibble(\n  weight = runif(100, 200, 400),\n  disease = rbinom(100, 1, prob = plogis(-5 + 0.015 * weight))\n)\n\n# Try linear regression (wrong!)\nlm_wrong &lt;- lm(disease ~ weight, data = disease_weight)\n\n# Plot\ndisease_weight %&gt;%\n  ggplot(aes(x = weight, y = disease)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Why Linear Regression Fails with Binary Outcomes\",\n       subtitle = \"Predicted values can be &lt; 0 or &gt; 1 (impossible for probabilities!)\",\n       x = \"Weight (kg)\", y = \"Disease (0 = No, 1 = Yes)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nProblems with linear regression for binary outcomes: - Predicted values can be &lt; 0 or &gt; 1 - Residuals are not normally distributed - Variance is not constant (heteroscedasticity)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#the-logistic-function",
    "href": "chapters/part2-ch06-categorical_data.html#the-logistic-function",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.2 The Logistic Function",
    "text": "21.2 The Logistic Function\nLogistic regression uses a special link function that constrains predictions to [0, 1]:\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nThis S-shaped curve is called the logistic function or sigmoid function:\n\n\nCode\ntibble(x = seq(-6, 6, 0.1)) %&gt;%\n  mutate(probability = plogis(x)) %&gt;%  # plogis() = logistic function\n  ggplot(aes(x = x, y = probability)) +\n  geom_line(size = 1.5, color = \"steelblue\") +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", alpha = 0.5) +\n  labs(title = \"The Logistic Function\",\n       subtitle = \"Output is always between 0 and 1\",\n       x = \"Linear Predictor (β₀ + β₁X)\",\n       y = \"Probability P(Y = 1)\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#understanding-log-odds-logit",
    "href": "chapters/part2-ch06-categorical_data.html#understanding-log-odds-logit",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.3 Understanding Log-Odds (Logit)",
    "text": "21.3 Understanding Log-Odds (Logit)\nThe logistic regression equation can be rewritten as:\n\\[\n\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\n\\]\nThe left side is the log-odds (also called the logit). This makes the relationship linear in log-odds space.\n\n\n\n\n\n\nNoteThree Ways to Think About Logistic Regression\n\n\n\n\nProbability scale: P(Y=1) is a curve between 0 and 1\nOdds scale: Odds = P/(1-P), ranges from 0 to ∞\nLog-odds scale: log(Odds), ranges from -∞ to +∞ (linear!)\n\nCoefficients in logistic regression are in log-odds scale.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#simple-logistic-regression-example",
    "href": "chapters/part2-ch06-categorical_data.html#simple-logistic-regression-example",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.4 Simple Logistic Regression Example",
    "text": "21.4 Simple Logistic Regression Example\nLet’s model disease risk as a function of body weight:\n\n\nCode\n# Fit logistic regression\nlogistic_model &lt;- glm(disease ~ weight,\n                      data = disease_weight,\n                      family = binomial)\n\n# Summary\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = disease ~ weight, family = binomial, data = disease_weight)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -4.015138   1.302445  -3.083  0.00205 **\nweight       0.011715   0.004264   2.747  0.00601 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 131.79  on 99  degrees of freedom\nResidual deviance: 123.55  on 98  degrees of freedom\nAIC: 127.55\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\n# Tidy output\ntidy(logistic_model, conf.int = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Logistic Regression: Disease ~ Weight\")\n\n\n\nLogistic Regression: Disease ~ Weight\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-4.0151\n1.3024\n-3.0828\n0.0021\n-6.7092\n-1.5660\n\n\nweight\n0.0117\n0.0043\n2.7474\n0.0060\n0.0036\n0.0205\n\n\n\n\n\nInterpreting coefficients (in log-odds scale): - Intercept (β₀) = -4.02: Log-odds of disease when weight = 0 (not meaningful here) - Weight (β₁) = 0.0117: For each 1 kg increase in weight, log-odds of disease increase by 0.0117",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#converting-to-odds-ratios",
    "href": "chapters/part2-ch06-categorical_data.html#converting-to-odds-ratios",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.5 Converting to Odds Ratios",
    "text": "21.5 Converting to Odds Ratios\nTo get odds ratios, exponentiate the coefficients:\n\n\nCode\n# Odds ratios\ntidy(logistic_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Odds Ratios from Logistic Regression\")\n\n\n\nOdds Ratios from Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0180\n1.3024\n-3.0828\n0.0021\n0.0012\n0.2089\n\n\nweight\n1.0118\n0.0043\n2.7474\n0.0060\n1.0036\n1.0207\n\n\n\n\n\nInterpretation: For each 1 kg increase in weight, the odds of disease are multiplied by 1.012.\nSince OR &gt; 1, higher weight is associated with higher disease risk.\n\n\n\n\n\n\nImportantInterpreting Odds Ratios\n\n\n\n\nOR = 1: No association (predictor doesn’t affect odds)\nOR &gt; 1: Predictor increases odds of outcome\nOR &lt; 1: Predictor decreases odds of outcome (protective)\n\nExample: OR = 1.02 means odds increase by 2% per unit increase in predictor.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#predicted-probabilities",
    "href": "chapters/part2-ch06-categorical_data.html#predicted-probabilities",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.6 Predicted Probabilities",
    "text": "21.6 Predicted Probabilities\nWe can use the model to predict disease probability for any weight:\n\n\nCode\n# Predict probabilities for a range of weights\npred_data &lt;- tibble(\n  weight = seq(200, 400, by = 10)\n)\n\npred_data &lt;- pred_data %&gt;%\n  mutate(\n    predicted_prob = predict(logistic_model, newdata = pred_data,\n                            type = \"response\")\n  )\n\n# Show some predictions\npred_data %&gt;%\n  filter(weight %in% c(200, 250, 300, 350, 400)) %&gt;%\n  kable(digits = 3,\n        caption = \"Predicted Disease Probabilities by Weight\")\n\n\n\nPredicted Disease Probabilities by Weight\n\n\nweight\npredicted_prob\n\n\n\n\n200\n0.158\n\n\n250\n0.252\n\n\n300\n0.377\n\n\n350\n0.521\n\n\n400\n0.662\n\n\n\n\n\n\n\nCode\n# Visualize predictions\ndisease_weight %&gt;%\n  ggplot(aes(x = weight, y = disease)) +\n  geom_point(alpha = 0.4, size = 2) +\n  geom_line(data = pred_data,\n            aes(x = weight, y = predicted_prob),\n            color = \"blue\", size = 1.5) +\n  labs(title = \"Logistic Regression: Disease Risk by Weight\",\n       subtitle = \"Blue curve shows predicted probabilities\",\n       x = \"Weight (kg)\",\n       y = \"Probability of Disease\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#model-fit-and-diagnostics",
    "href": "chapters/part2-ch06-categorical_data.html#model-fit-and-diagnostics",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.7 Model Fit and Diagnostics",
    "text": "21.7 Model Fit and Diagnostics\n\n21.7.1 Deviance\nSimilar to residual sum of squares in linear regression, logistic regression uses deviance to measure model fit.\n\n\nCode\nglance(logistic_model) %&gt;%\n  kable(digits = 2,\n        caption = \"Logistic Regression Model Fit Statistics\")\n\n\n\nLogistic Regression Model Fit Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nnull.deviance\ndf.null\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n131.79\n99\n-61.78\n127.55\n132.76\n123.55\n98\n100\n\n\n\n\n\n\nNull deviance: Model with intercept only (no predictors)\nResidual deviance: Model with predictors\nLower deviance = better fit\n\n\n\n21.7.2 Likelihood Ratio Test\nCompare models with a likelihood ratio test:\n\n\nCode\n# Null model (intercept only)\nnull_model &lt;- glm(disease ~ 1, data = disease_weight, family = binomial)\n\n# Compare models\nanova(null_model, logistic_model, test = \"Chisq\") %&gt;%\n  kable(digits = 4,\n        caption = \"Likelihood Ratio Test: Does Weight Improve the Model?\")\n\n\n\nLikelihood Ratio Test: Does Weight Improve the Model?\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n99\n131.7911\nNA\nNA\nNA\n\n\n98\n123.5509\n1\n8.2402\n0.0041\n\n\n\n\n\nWeight significantly improves model fit (p &lt; 0.05).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#multiple-logistic-regression",
    "href": "chapters/part2-ch06-categorical_data.html#multiple-logistic-regression",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.8 Multiple Logistic Regression",
    "text": "21.8 Multiple Logistic Regression\nWe can include multiple predictors, just like multiple linear regression:\n\n\nCode\n# Add more predictors\ndisease_multi &lt;- disease_weight %&gt;%\n  mutate(\n    age = runif(n(), 1, 5),\n    previous_disease = sample(c(0, 1), n(), replace = TRUE, prob = c(0.7, 0.3))\n  )\n\n# Fit multiple logistic regression\nmulti_logistic &lt;- glm(disease ~ weight + age + previous_disease,\n                      data = disease_multi,\n                      family = binomial)\n\n# Odds ratios\ntidy(multi_logistic, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Multiple Logistic Regression: Odds Ratios\")\n\n\n\nMultiple Logistic Regression: Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0116\n1.3896\n-3.2073\n0.0013\n0.0006\n0.1572\n\n\nweight\n1.0113\n0.0043\n2.6080\n0.0091\n1.0030\n1.0202\n\n\nage\n1.2182\n0.1844\n1.0702\n0.2845\n0.8500\n1.7606\n\n\nprevious_disease\n0.9770\n0.4772\n-0.0487\n0.9611\n0.3760\n2.4755\n\n\n\n\n\nInterpretation (if results were significant): - Each 1 kg increase in weight multiplies odds of disease by 1.011, holding age and previous disease constant - Previous disease history increases odds by a factor of 0.98, adjusting for weight and age",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#practical-example-brd-risk-factors",
    "href": "chapters/part2-ch06-categorical_data.html#practical-example-brd-risk-factors",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.9 Practical Example: BRD Risk Factors",
    "text": "21.9 Practical Example: BRD Risk Factors\nLet’s analyze a more realistic dataset with bovine respiratory disease (BRD):\n\n\nCode\n# Simulate realistic BRD data\nset.seed(456)\nbrd_data &lt;- tibble(\n  animal_id = 1:300,\n  arrival_weight = rnorm(300, mean = 250, sd = 30),\n  age_months = runif(300, min = 6, max = 10),\n  vaccinated = sample(c(\"Yes\", \"No\"), 300, replace = TRUE),\n  temperature_arrival = rnorm(300, mean = 39.2, sd = 0.8)\n) %&gt;%\n  mutate(\n    # Disease risk depends on multiple factors\n    risk_score = -8 +\n      0.01 * arrival_weight +\n      0.3 * age_months -\n      1.5 * (vaccinated == \"Yes\") +\n      0.5 * temperature_arrival,\n    brd = rbinom(300, 1, prob = plogis(risk_score))\n  )\n\n# Check disease rate\nmean(brd_data$brd)\n\n\n[1] 1\n\n\n\n\nCode\n# Fit comprehensive model\nbrd_model &lt;- glm(brd ~ arrival_weight + age_months + vaccinated + temperature_arrival,\n                 data = brd_data,\n                 family = binomial)\n\n# Get results with Wald CIs\nbrd_results &lt;- tidy(brd_model, exponentiate = TRUE)\nbrd_ci &lt;- confint.default(brd_model) # Wald CIs\nbrd_results$conf.low &lt;- exp(brd_ci[,1])\nbrd_results$conf.high &lt;- exp(brd_ci[,2])\n\n# Display results\nbrd_results %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value) %&gt;%\n  kable(digits = 4,\n        caption = \"BRD Risk Factors: Odds Ratios with 95% Wald CIs\")\n\n\n\nBRD Risk Factors: Odds Ratios with 95% Wald CIs\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\n(Intercept)\n344744308613\n0\nInf\n1\n\n\narrival_weight\n1\n0\nInf\n1\n\n\nage_months\n1\n0\nInf\n1\n\n\nvaccinatedYes\n1\n0\nInf\n1\n\n\ntemperature_arrival\n1\n0\nInf\n1\n\n\n\n\n\n\n\nCode\n# Extract key odds ratios\nor_vaccine &lt;- exp(coef(brd_model)[\"vaccinatedYes\"])\nor_temp &lt;- exp(coef(brd_model)[\"temperature_arrival\"])\n\ncat(\"Key findings:\\n\")\n\n\nKey findings:\n\n\nCode\ncat(\"- Vaccination reduces odds of BRD by\",\n    round((1 - or_vaccine) * 100, 1), \"%\\n\")\n\n\n- Vaccination reduces odds of BRD by 0 %\n\n\nCode\ncat(\"- Each 1°C increase in arrival temperature multiplies odds by\",\n    round(or_temp, 2), \"\\n\")\n\n\n- Each 1°C increase in arrival temperature multiplies odds by 1",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#predictions-for-new-animals",
    "href": "chapters/part2-ch06-categorical_data.html#predictions-for-new-animals",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "21.10 Predictions for New Animals",
    "text": "21.10 Predictions for New Animals\n\n\nCode\n# Predict BRD risk for specific scenarios\nnew_animals &lt;- tibble(\n  scenario = c(\"Low risk\", \"Medium risk\", \"High risk\"),\n  arrival_weight = c(280, 250, 220),\n  age_months = c(7, 8, 9),\n  vaccinated = c(\"Yes\", \"No\", \"No\"),\n  temperature_arrival = c(38.5, 39.2, 40.0)\n)\n\nnew_animals &lt;- new_animals %&gt;%\n  mutate(\n    predicted_risk = predict(brd_model, newdata = new_animals, type = \"response\")\n  )\n\nnew_animals %&gt;%\n  select(scenario, vaccinated, temperature_arrival, predicted_risk) %&gt;%\n  kable(digits = 3,\n        caption = \"Predicted BRD Risk for Different Scenarios\")\n\n\n\nPredicted BRD Risk for Different Scenarios\n\n\nscenario\nvaccinated\ntemperature_arrival\npredicted_risk\n\n\n\n\nLow risk\nYes\n38.5\n1\n\n\nMedium risk\nNo\n39.2\n1\n\n\nHigh risk\nNo\n40.0\n1\n\n\n\n\n\n\n\n\n\n\n\nNoteWhen to Use Logistic Regression vs Chi-Square\n\n\n\n\nChi-square test: Quick test of association between two categorical variables\nLogistic regression:\n\nInclude continuous predictors\nControl for confounders\nMake predictions\nModel complex relationships\nGet odds ratios with confidence intervals",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#chi-square-test-assumptions-1",
    "href": "chapters/part2-ch06-categorical_data.html#chi-square-test-assumptions-1",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "22.1 Chi-Square Test Assumptions",
    "text": "22.1 Chi-Square Test Assumptions\n\nIndependence: Each observation must be independent\n\nViolated by: Repeated measures, clustered data (multiple animals per farm)\nSolution: Use mixed models or GEE for clustered data\n\nExpected cell counts ≥ 5: Rule of thumb for valid chi-square approximation\n\nCheck: Use chisq.test()$expected\nSolution: Fisher’s exact test, combine categories, or collect more data\n\nRandom sampling: Data should be representative of population\nMutually exclusive categories: Each observation in exactly one category",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#logistic-regression-assumptions",
    "href": "chapters/part2-ch06-categorical_data.html#logistic-regression-assumptions",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "22.2 Logistic Regression Assumptions",
    "text": "22.2 Logistic Regression Assumptions\n\nBinary outcome: DV must be 0/1 (or convertible to binary)\nIndependence: Observations are independent\nLinearity of log-odds: Relationship between continuous predictors and log-odds should be linear\n\nCheck: Plot log-odds vs predictor, or use splines\n\nNo perfect multicollinearity: Predictors shouldn’t be perfectly correlated\nAdequate sample size: Rule of thumb: 10-15 events per predictor variable\n\n\n\n\n\n\n\nWarningCommon Pitfalls\n\n\n\n\nSmall expected cell counts: Chi-square p-values unreliable → Use Fisher’s exact\nMultiple comparisons: Testing many associations increases false positives → Adjust for multiple testing\nConfusing OR and RR: Odds ratios ≠ Risk ratios (except for rare outcomes)\nCausal language: “Association” ≠ “Causation” → Observational data can’t prove causation\nOverfitting logistic models: Too many predictors for sample size → Use fewer predictors or collect more data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#sample-size-considerations",
    "href": "chapters/part2-ch06-categorical_data.html#sample-size-considerations",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "22.3 Sample Size Considerations",
    "text": "22.3 Sample Size Considerations\nFor chi-square tests, use power analysis:\n\n\nCode\n# What sample size for 80% power to detect OR = 2.0?\n# Assuming equal groups, alpha = 0.05\n\n# Simulate to demonstrate concept\nsimulate_power &lt;- function(n_per_group, or, n_sims = 1000) {\n  p_control &lt;- 0.20  # 20% baseline risk\n  p_exposed &lt;- p_control * or / (1 - p_control + p_control * or)\n\n  p_values &lt;- replicate(n_sims, {\n    exposed &lt;- sample(c(\"Yes\", \"No\"), n_per_group, replace = TRUE,\n                     prob = c(p_exposed, 1 - p_exposed))\n    control &lt;- sample(c(\"Yes\", \"No\"), n_per_group, replace = TRUE,\n                     prob = c(p_control, 1 - p_control))\n\n    outcome &lt;- c(exposed, control)\n    group &lt;- rep(c(\"Exposed\", \"Control\"), each = n_per_group)\n\n    test_result &lt;- tryCatch(\n      chisq.test(table(group, outcome))$p.value,\n      error = function(e) NA,\n      warning = function(w) NA\n    )\n\n    test_result\n  })\n\n  power &lt;- mean(p_values &lt; 0.05, na.rm = TRUE)\n  return(power)\n}\n\n# Test different sample sizes\nsample_sizes &lt;- seq(50, 200, by = 25)\npower_results &lt;- map_dbl(sample_sizes, ~simulate_power(.x, or = 2.0, n_sims = 500))\n\n# Plot\ntibble(n_per_group = sample_sizes, power = power_results) %&gt;%\n  ggplot(aes(x = n_per_group, y = power)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Statistical Power vs Sample Size\",\n       subtitle = \"Detecting OR = 2.0 with 20% baseline risk\",\n       x = \"Sample Size per Group\",\n       y = \"Power (1 - β)\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#decision-framework",
    "href": "chapters/part2-ch06-categorical_data.html#decision-framework",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "23.1 Decision Framework",
    "text": "23.1 Decision Framework\n\n\n\nSTART: Do you have categorical data?\n│\n├─ One categorical variable\n│  └─ Chi-square goodness of fit test\n│     (Do observed frequencies match expected?)\n│\n└─ Two or more variables\n   │\n   ├─ Both categorical (no continuous predictors)\n   │  │\n   │  ├─ 2×2 table, large sample (expected counts ≥ 5)\n   │  │  └─ Chi-square test of independence\n   │  │\n   │  ├─ 2×2 table, small sample (expected counts &lt; 5)\n   │  │  └─ Fisher's exact test\n   │  │\n   │  └─ Larger table\n   │     └─ Chi-square test of independence\n   │\n   └─ Binary outcome + continuous predictors\n      │\n      ├─ Single predictor, just testing association\n      │  └─ Chi-square test (if categorized) or\n      │     Logistic regression (for continuous)\n      │\n      └─ Multiple predictors or need predictions\n         └─ Logistic regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#complete-analysis-example",
    "href": "chapters/part2-ch06-categorical_data.html#complete-analysis-example",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "23.2 Complete Analysis Example",
    "text": "23.2 Complete Analysis Example\nLet’s walk through a complete analysis with a new dataset:\n\n\nCode\n# Research question: Does feed type affect mortality in broiler chickens?\n# Also considering weight and age as covariates\n\nset.seed(789)\nbroiler_data &lt;- tibble(\n  bird_id = 1:400,\n  feed_type = sample(c(\"Standard\", \"Enhanced\"), 400, replace = TRUE),\n  weight_kg = rnorm(400, mean = 2.5, sd = 0.4),\n  age_days = sample(35:49, 400, replace = TRUE)\n) %&gt;%\n  mutate(\n    mortality_risk = plogis(-6 +\n                           0.5 * (feed_type == \"Enhanced\") -\n                           0.8 * weight_kg +\n                           0.05 * age_days),\n    mortality = rbinom(400, 1, prob = mortality_risk),\n    mortality_fct = factor(mortality, levels = c(0, 1), labels = c(\"Alive\", \"Dead\"))\n  )\n\n\n\n23.2.1 Step 1: Exploratory Analysis\n\n\nCode\n# Overall mortality rate\nbroiler_data %&gt;%\n  count(mortality_fct) %&gt;%\n  mutate(percent = n / sum(n) * 100) %&gt;%\n  kable(digits = 1, caption = \"Overall Mortality\")\n\n\n\nOverall Mortality\n\n\nmortality_fct\nn\npercent\n\n\n\n\nAlive\n397\n99.2\n\n\nDead\n3\n0.8\n\n\n\n\n\nCode\n# Mortality by feed type\nbroiler_data %&gt;%\n  count(feed_type, mortality_fct) %&gt;%\n  group_by(feed_type) %&gt;%\n  mutate(percent = n / sum(n) * 100) %&gt;%\n  kable(digits = 1, caption = \"Mortality by Feed Type\")\n\n\n\nMortality by Feed Type\n\n\nfeed_type\nmortality_fct\nn\npercent\n\n\n\n\nEnhanced\nAlive\n216\n98.6\n\n\nEnhanced\nDead\n3\n1.4\n\n\nStandard\nAlive\n181\n100.0\n\n\n\n\n\n\n\nCode\n# Visualization\nbroiler_data %&gt;%\n  ggplot(aes(x = feed_type, fill = mortality_fct)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"Alive\" = \"steelblue\", \"Dead\" = \"coral\")) +\n  scale_y_continuous(labels = percent_format()) +\n  labs(title = \"Mortality Rate by Feed Type\",\n       x = \"Feed Type\", y = \"Proportion\", fill = \"Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n23.2.2 Step 2: Simple Association Test\n\n\nCode\n# Chi-square test\nmortality_table &lt;- table(broiler_data$feed_type, broiler_data$mortality_fct)\nchi_mortality &lt;- chisq.test(mortality_table)\ntidy(chi_mortality) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Test: Feed Type vs Mortality\")\n\n\n\nChi-Square Test: Feed Type vs Mortality\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n0.9968\n0.3181\n1\nPearson’s Chi-squared test with Yates’ continuity correction\n\n\n\n\n\n\n\n23.2.3 Step 3: Calculate Effect Size (Odds Ratio)\n\n\nCode\n# Calculate odds ratio\na &lt;- mortality_table[\"Enhanced\", \"Dead\"]\nb &lt;- mortality_table[\"Enhanced\", \"Alive\"]\nc &lt;- mortality_table[\"Standard\", \"Dead\"]\nd &lt;- mortality_table[\"Standard\", \"Alive\"]\n\nor_simple &lt;- (a * d) / (b * c)\ncat(\"Odds Ratio (Enhanced vs Standard):\", round(or_simple, 3), \"\\n\")\n\n\nOdds Ratio (Enhanced vs Standard): Inf \n\n\n\n\n23.2.4 Step 4: Logistic Regression (Adjusting for Confounders)\n\n\nCode\n# Fit logistic regression\nmortality_model &lt;- glm(mortality ~ feed_type + weight_kg + age_days,\n                       data = broiler_data,\n                       family = binomial)\n\n# Results\ntidy(mortality_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4, caption = \"Adjusted Odds Ratios from Logistic Regression\")\n\n\n\nAdjusted Odds Ratios from Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0026\n6.8342\n-0.8714\n0.3836\n0.0000\n1.090366e+03\n\n\nfeed_typeStandard\n0.0000\n2155.7729\n-0.0080\n0.9936\nNA\n4.349210e+123\n\n\nweight_kg\n0.6014\n1.5840\n-0.3210\n0.7482\n0.0232\n1.339860e+01\n\n\nage_days\n1.0724\n0.1348\n0.5188\n0.6039\n0.8218\n1.447500e+00\n\n\n\n\n\n\n\n23.2.5 Step 5: Interpret and Report\n\n\nCode\n# Extract key results\nor_adjusted &lt;- exp(coef(mortality_model)[\"feed_typeStandard\"])\nci_adjusted &lt;- exp(confint(mortality_model)[\"feed_typeStandard\", ])\n\ncat(\"Findings:\\n\")\n\n\nFindings:\n\n\nCode\ncat(\"1. Unadjusted OR (chi-square):\", round(or_simple, 2), \"\\n\")\n\n\n1. Unadjusted OR (chi-square): Inf \n\n\nCode\ncat(\"2. Adjusted OR (logistic regression):\", round(or_adjusted, 2), \"\\n\")\n\n\n2. Adjusted OR (logistic regression): 0 \n\n\nCode\ncat(\"   95% CI: [\", round(ci_adjusted[1], 2), \",\", round(ci_adjusted[2], 2), \"]\\n\")\n\n\n   95% CI: [ NA , 4.34921e+123 ]\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"After adjusting for weight and age, feed type is\",\n    ifelse(summary(mortality_model)$coefficients[\"feed_typeStandard\", 4] &lt; 0.05,\n           \"significantly\", \"not significantly\"),\n    \"associated with mortality.\\n\")\n\n\nAfter adjusting for weight and age, feed type is not significantly associated with mortality.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#problem-1-mastitis-and-housing",
    "href": "chapters/part2-ch06-categorical_data.html#problem-1-mastitis-and-housing",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "24.1 Problem 1: Mastitis and Housing",
    "text": "24.1 Problem 1: Mastitis and Housing\nA dairy farm compares mastitis rates across three housing systems:\n\n\nCode\n# Data\nmastitis_data &lt;- tibble(\n  housing = rep(c(\"Freestall\", \"Tiestall\", \"Pasture\"), each = 80),\n  mastitis = c(\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.25, 0.75)),  # Freestall\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.35, 0.65)),  # Tiestall\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.15, 0.85))   # Pasture\n  )\n)\n\n# Your tasks:\n# 1. Create a contingency table\n# 2. Perform chi-square test\n# 3. Calculate Cramér's V\n# 4. Visualize the results\n# 5. Interpret findings\n\n\nSolution:\n\n\nCode\n# 1. Contingency table\nmastitis_table &lt;- table(mastitis_data$housing, mastitis_data$mastitis)\naddmargins(mastitis_table)\n\n\n           \n             No Yes Sum\n  Freestall  58  22  80\n  Pasture    73   7  80\n  Tiestall   53  27  80\n  Sum       184  56 240\n\n\nCode\n# 2. Chi-square test\nchi_mastitis &lt;- chisq.test(mastitis_table)\ntidy(chi_mastitis) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Test Results\")\n\n\n\nChi-Square Test Results\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n15.1398\n5e-04\n2\nPearson’s Chi-squared test\n\n\n\n\n\nCode\n# 3. Cramér's V\nn &lt;- sum(mastitis_table)\nk &lt;- min(dim(mastitis_table))\ncramers_v_mastitis &lt;- sqrt(chi_mastitis$statistic / (n * (k - 1)))\ncat(\"Cramér's V:\", round(cramers_v_mastitis, 3), \"\\n\")\n\n\nCramér's V: 0.251 \n\n\nCode\n# 4. Visualization\nmastitis_data %&gt;%\n  count(housing, mastitis) %&gt;%\n  group_by(housing) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  filter(mastitis == \"Yes\") %&gt;%\n  ggplot(aes(x = housing, y = prop, fill = housing)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = paste0(round(prop * 100, 1), \"%\")),\n            vjust = -0.5) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 0.4)) +\n  labs(title = \"Mastitis Rate by Housing System\",\n       x = \"Housing System\", y = \"Mastitis Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 5. Interpretation\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"There is a statistically significant association between housing system\")\n\n\nThere is a statistically significant association between housing system\n\n\nCode\ncat(\" and mastitis rate (p =\", round(chi_mastitis$p.value, 4), \").\\n\")\n\n\n and mastitis rate (p = 5e-04 ).\n\n\nCode\ncat(\"Pasture-based systems show the lowest mastitis rate, while tiestall\")\n\n\nPasture-based systems show the lowest mastitis rate, while tiestall\n\n\nCode\ncat(\" systems show the highest.\\n\")\n\n\n systems show the highest.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#problem-2-small-sample-fishers-test",
    "href": "chapters/part2-ch06-categorical_data.html#problem-2-small-sample-fishers-test",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "24.2 Problem 2: Small Sample Fisher’s Test",
    "text": "24.2 Problem 2: Small Sample Fisher’s Test\nInvestigate a rare genetic disorder in a small sample:\n\n\nCode\n# Data: Does dam age affect disorder occurrence?\ngenetic_data &lt;- matrix(c(5, 2,   # Young dams\n                        1, 7),   # Older dams\n                      nrow = 2, byrow = TRUE,\n                      dimnames = list(Dam_Age = c(\"Young\", \"Older\"),\n                                     Disorder = c(\"Yes\", \"No\")))\n\n# Your tasks:\n# 1. Check expected cell counts\n# 2. Perform Fisher's exact test\n# 3. Calculate odds ratio\n# 4. Interpret results\n\n\nSolution:\n\n\nCode\n# 1. Check expected counts\nchi_test &lt;- chisq.test(genetic_data)\ncat(\"Expected counts:\\n\")\n\n\nExpected counts:\n\n\nCode\nprint(chi_test$expected)\n\n\n       Disorder\nDam_Age Yes  No\n  Young 2.8 4.2\n  Older 3.2 4.8\n\n\nCode\ncat(\"\\nNote: Expected counts &lt; 5, so Fisher's exact test is appropriate.\\n\\n\")\n\n\n\nNote: Expected counts &lt; 5, so Fisher's exact test is appropriate.\n\n\nCode\n# 2. Fisher's exact test\nfisher_genetic &lt;- fisher.test(genetic_data)\ncat(\"Fisher's Exact Test Results:\\n\")\n\n\nFisher's Exact Test Results:\n\n\nCode\ncat(\"Odds Ratio:\", round(fisher_genetic$estimate, 3), \"\\n\")\n\n\nOdds Ratio: 13.594 \n\n\nCode\ncat(\"95% CI: [\", round(fisher_genetic$conf.int[1], 3), \",\",\n    round(fisher_genetic$conf.int[2], 3), \"]\\n\")\n\n\n95% CI: [ 0.865 , 934.009 ]\n\n\nCode\ncat(\"P-value:\", round(fisher_genetic$p.value, 4), \"\\n\\n\")\n\n\nP-value: 0.0406 \n\n\nCode\n# 3. Manual OR calculation\nor_manual &lt;- (genetic_data[1,1] * genetic_data[2,2]) /\n             (genetic_data[1,2] * genetic_data[2,1])\ncat(\"Manual OR calculation:\", round(or_manual, 3), \"\\n\\n\")\n\n\nManual OR calculation: 17.5 \n\n\nCode\n# 4. Interpretation\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"Young dams have\", round(fisher_genetic$estimate, 1),\n    \"times higher odds of having offspring with the disorder\")\n\n\nYoung dams have 13.6 times higher odds of having offspring with the disorder\n\n\nCode\ncat(\" compared to older dams.\\n\")\n\n\n compared to older dams.\n\n\nCode\ncat(\"However, with p =\", round(fisher_genetic$p.value, 3),\n    \"and a small sample size,\\n\")\n\n\nHowever, with p = 0.041 and a small sample size,\n\n\nCode\ncat(\"we should interpret these results cautiously and consider collecting more data.\\n\")\n\n\nwe should interpret these results cautiously and consider collecting more data.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#problem-3-logistic-regression-with-multiple-predictors",
    "href": "chapters/part2-ch06-categorical_data.html#problem-3-logistic-regression-with-multiple-predictors",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "24.3 Problem 3: Logistic Regression with Multiple Predictors",
    "text": "24.3 Problem 3: Logistic Regression with Multiple Predictors\nAnalyze factors affecting retained placenta in dairy cows:\n\n\nCode\n# Simulate data\nset.seed(2025)\nrp_data &lt;- tibble(\n  cow_id = 1:250,\n  parity = sample(1:5, 250, replace = TRUE),\n  bcs_precalving = rnorm(250, mean = 3.5, sd = 0.5),\n  twin_birth = sample(c(0, 1), 250, replace = TRUE, prob = c(0.95, 0.05)),\n  dystocia = sample(c(0, 1), 250, replace = TRUE, prob = c(0.85, 0.15))\n) %&gt;%\n  mutate(\n    rp_risk = plogis(-4 +\n                    0.2 * (parity &gt;= 3) +\n                    -0.5 * bcs_precalving +\n                    2.0 * twin_birth +\n                    1.5 * dystocia),\n    retained_placenta = rbinom(250, 1, prob = rp_risk)\n  )\n\n# Your tasks:\n# 1. Fit a logistic regression model\n# 2. Interpret odds ratios for each predictor\n# 3. Identify the strongest risk factor\n# 4. Predict RP risk for a specific cow profile\n\n\nSolution:\n\n\nCode\n# 1. Fit model\nrp_model &lt;- glm(retained_placenta ~ parity + bcs_precalving + twin_birth + dystocia,\n                data = rp_data,\n                family = binomial)\n\n# 2. Odds ratios with CIs\nrp_results &lt;- tidy(rp_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\")\nrp_results %&gt;%\n  kable(digits = 3, caption = \"Retained Placenta Risk Factors: Odds Ratios\")\n\n\n\nRetained Placenta Risk Factors: Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.000000e+00\n109713.535\n-0.003\n0.998\n0\nInf\n\n\nparity\n1.255316e+06\n9308.983\n0.002\n0.999\n0\nNA\n\n\nbcs_precalving\n8.306630e+20\n19892.759\n0.002\n0.998\n0\nInf\n\n\ntwin_birth\n0.000000e+00\n59064.424\n0.000\n1.000\n0\nInf\n\n\ndystocia\n1.517005e+17\n21074.857\n0.002\n0.999\n0\nInf\n\n\n\n\n\nCode\n# 3. Identify strongest risk factor\ncat(\"\\nStrongest Risk Factors (by OR magnitude):\\n\")\n\n\n\nStrongest Risk Factors (by OR magnitude):\n\n\nCode\nrp_results %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs(estimate - 1))) %&gt;%\n  select(term, estimate, p.value) %&gt;%\n  print()\n\n\n# A tibble: 4 × 3\n  term           estimate p.value\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n1 bcs_precalving 8.31e+20   0.998\n2 dystocia       1.52e+17   0.999\n3 parity         1.26e+ 6   0.999\n4 twin_birth     5.83e- 6   1.00 \n\n\nCode\n# 4. Predict for specific profiles\nnew_cows &lt;- tibble(\n  scenario = c(\"Low risk\", \"High risk\"),\n  parity = c(2, 4),\n  bcs_precalving = c(3.5, 2.8),\n  twin_birth = c(0, 1),\n  dystocia = c(0, 1)\n)\n\nnew_cows %&gt;%\n  mutate(predicted_rp_risk = predict(rp_model, newdata = new_cows,\n                                     type = \"response\")) %&gt;%\n  kable(digits = 3, caption = \"Predicted Retained Placenta Risk\")\n\n\n\nPredicted Retained Placenta Risk\n\n\n\n\n\n\n\n\n\n\nscenario\nparity\nbcs_precalving\ntwin_birth\ndystocia\npredicted_rp_risk\n\n\n\n\nLow risk\n2\n3.5\n0\n0\n0\n\n\nHigh risk\n4\n2.8\n1\n1\n0\n\n\n\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"- Twin births have the strongest association with retained placenta\\n\")\n\n\n- Twin births have the strongest association with retained placenta\n\n\nCode\ncat(\"- Higher BCS appears protective (OR &lt; 1)\\n\")\n\n\n- Higher BCS appears protective (OR &lt; 1)\n\n\nCode\ncat(\"- Dystocia also substantially increases risk\\n\")\n\n\n- Dystocia also substantially increases risk\n\n\nCode\ncat(\"- A high-risk cow profile (parity 4, low BCS, twins, dystocia) has\\n\")\n\n\n- A high-risk cow profile (parity 4, low BCS, twins, dystocia) has\n\n\nCode\ncat(\"  substantially higher predicted risk than a low-risk cow.\\n\")\n\n\n  substantially higher predicted risk than a low-risk cow.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#problem-4-goodness-of-fit---genetic-ratios",
    "href": "chapters/part2-ch06-categorical_data.html#problem-4-goodness-of-fit---genetic-ratios",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "24.4 Problem 4: Goodness of Fit - Genetic Ratios",
    "text": "24.4 Problem 4: Goodness of Fit - Genetic Ratios\nTest if offspring coat colors match expected Mendelian ratios:\n\n\nCode\n# Observed offspring: Black, Brown, White\nobserved_colors &lt;- c(Black = 42, Brown = 38, White = 20)\n\n# Expected ratio: 2:2:1\n# Your tasks:\n# 1. Perform chi-square goodness of fit test\n# 2. Visualize observed vs expected\n# 3. Interpret whether data fits the genetic model\n\n\nSolution:\n\n\nCode\n# 1. Chi-square goodness of fit\nexpected_proportions &lt;- c(2, 2, 1) / 5\nchi_colors &lt;- chisq.test(observed_colors, p = expected_proportions)\n\ntidy(chi_colors) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Goodness of Fit Test\")\n\n\n\nChi-Square Goodness of Fit Test\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n0.2\n0.9048\n2\nChi-squared test for given probabilities\n\n\n\n\n\nCode\n# 2. Visualize\ncolor_comparison &lt;- tibble(\n  Color = names(observed_colors),\n  Observed = observed_colors,\n  Expected = expected_proportions * sum(observed_colors)\n) %&gt;%\n  pivot_longer(cols = c(Observed, Expected),\n               names_to = \"Type\", values_to = \"Count\")\n\ncolor_comparison %&gt;%\n  ggplot(aes(x = Color, y = Count, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = round(Count, 1)),\n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  scale_fill_manual(values = c(\"Observed\" = \"steelblue\",\n                                \"Expected\" = \"coral\")) +\n  labs(title = \"Coat Color Distribution: Observed vs Expected\",\n       subtitle = paste0(\"χ² = \", round(chi_colors$statistic, 2),\n                        \", p = \", round(chi_colors$p.value, 3)),\n       x = \"Coat Color\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 3. Interpretation\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"P-value =\", round(chi_colors$p.value, 4), \"\\n\")\n\n\nP-value = 0.9048 \n\n\nCode\nif(chi_colors$p.value &gt; 0.05) {\n  cat(\"The observed data is consistent with the expected 2:2:1 ratio.\\n\")\n  cat(\"We do not have evidence to reject the proposed genetic model.\\n\")\n} else {\n  cat(\"The observed data significantly differs from the expected 2:2:1 ratio.\\n\")\n  cat(\"The genetic model may not fit, or other factors may be at play.\\n\")\n}\n\n\nThe observed data is consistent with the expected 2:2:1 ratio.\nWe do not have evidence to reject the proposed genetic model.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#key-concepts",
    "href": "chapters/part2-ch06-categorical_data.html#key-concepts",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "25.1 Key Concepts",
    "text": "25.1 Key Concepts\n\nChi-Square Goodness of Fit: Tests if observed frequencies match expected distribution\n\nOne categorical variable\nUsed for genetic ratios, expected proportions\n\nChi-Square Test of Independence: Tests association between two categorical variables\n\nWorks with 2×2 or larger tables\nRequires expected cell counts ≥ 5\n\nFisher’s Exact Test: Alternative to chi-square for small samples\n\nUse when expected counts &lt; 5\nProvides exact p-value\n\nRisk Ratios and Odds Ratios: Quantify strength of association\n\nRisk Ratio: More intuitive, ratio of probabilities\nOdds Ratio: Used in logistic regression, ratio of odds\nOR ≈ RR when outcome is rare\n\nLogistic Regression: Model binary outcomes with continuous/multiple predictors\n\nUses logit link function\nCoefficients are log-odds\nExponentiate to get odds ratios\nCan include multiple predictors and control for confounders",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#decision-framework-summary",
    "href": "chapters/part2-ch06-categorical_data.html#decision-framework-summary",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "25.2 Decision Framework Summary",
    "text": "25.2 Decision Framework Summary\n\n\n\nChoosing the Right Test for Categorical Data\n\n\n\n\n\n\nSituation\nRecommended Test\n\n\n\n\nOne categorical variable vs expected distribution\nChi-square goodness of fit\n\n\nTwo categorical variables, large sample\nChi-square test of independence\n\n\nTwo categorical variables, small sample\nFisher’s exact test\n\n\nBinary outcome + continuous predictors\nLogistic regression\n\n\nBinary outcome + multiple predictors\nLogistic regression\n\n\nNeed predictions for new observations\nLogistic regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#r-functions-summary",
    "href": "chapters/part2-ch06-categorical_data.html#r-functions-summary",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "25.3 R Functions Summary",
    "text": "25.3 R Functions Summary\n\n\n\nKey R Functions for Categorical Data Analysis\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\ntable()\nCreate contingency tables\n\n\nprop.table()\nConvert counts to proportions\n\n\nchisq.test()\nChi-square tests (goodness of fit and independence)\n\n\nfisher.test()\nFisher’s exact test for small samples\n\n\nglm(..., family = binomial)\nFit logistic regression model\n\n\npredict(..., type = 'response')\nGet predicted probabilities from logistic regression\n\n\nexp(coef())\nConvert log-odds to odds ratios",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#common-pitfalls-to-avoid",
    "href": "chapters/part2-ch06-categorical_data.html#common-pitfalls-to-avoid",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "25.4 Common Pitfalls to Avoid",
    "text": "25.4 Common Pitfalls to Avoid\n\n\n\n\n\n\nWarningWatch Out For These Mistakes\n\n\n\n\nUsing chi-square with small expected counts → Use Fisher’s exact test\nConfusing odds ratios with risk ratios → They’re different! (except for rare outcomes)\nInterpreting association as causation → Observational data requires caution\nIgnoring effect sizes → Statistical significance ≠ practical importance\nOverfitting logistic models → Too many predictors for sample size\nForgetting independence assumption → Clustered/repeated measures violate this\nMultiple testing without correction → Increases false positive rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#practical-significance",
    "href": "chapters/part2-ch06-categorical_data.html#practical-significance",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "25.5 Practical Significance",
    "text": "25.5 Practical Significance\nAlways consider: - Effect size (Cramér’s V, odds ratios) not just p-values - Confidence intervals for uncertainty - Biological/practical meaning of findings - Study design limitations (observational vs experimental)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#recommended-reading",
    "href": "chapters/part2-ch06-categorical_data.html#recommended-reading",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "26.1 Recommended Reading",
    "text": "26.1 Recommended Reading\n\nAgresti, A. (2018). An Introduction to Categorical Data Analysis (3rd ed.)\n\nComprehensive, mathematical treatment\n\nHosmer, Lemeshow, & Sturdivant (2013). Applied Logistic Regression (3rd ed.)\n\nPractical guide to logistic regression\n\nDohoo, Martin, & Stryhn (2014). Veterinary Epidemiologic Research (2nd ed.)\n\nExcellent for animal health applications",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#online-resources",
    "href": "chapters/part2-ch06-categorical_data.html#online-resources",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "26.2 Online Resources",
    "text": "26.2 Online Resources\n\nStatQuest Videos: “Chi-Square Test” and “Logistic Regression” by Josh Starmer\nr-bloggers: Tutorials on categorical data analysis in R\nCross Validated (StackExchange): Q&A on statistical concepts",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#r-packages-for-categorical-data",
    "href": "chapters/part2-ch06-categorical_data.html#r-packages-for-categorical-data",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "26.3 R Packages for Categorical Data",
    "text": "26.3 R Packages for Categorical Data\n\n\nCode\n# Beyond base R functions\ninstall.packages(c(\n  \"vcd\",        # Visualizing categorical data\n  \"epitools\",   # Epidemiology tools (OR, RR calculations)\n  \"rms\",        # Regression modeling strategies\n  \"MASS\"        # Ordinal logistic regression (polr)\n))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#sec-preview",
    "href": "chapters/part2-ch06-categorical_data.html#sec-preview",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "26.4 Next Week Preview: Simple Linear Regression",
    "text": "26.4 Next Week Preview: Simple Linear Regression\nNext week, we return to continuous outcomes and explore linear regression:\n\nCorrelation vs regression\nFitting a regression line\nInterpreting slope and intercept\nResidual diagnostics\nPrediction and confidence intervals\n\nConnection to This Week: Logistic regression and linear regression are both types of Generalized Linear Models (GLMs). The main difference is: - Linear regression: Continuous outcome, identity link - Logistic regression: Binary outcome, logit link\nThe concepts of model fitting, interpretation, and diagnostics carry over!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch06-categorical_data.html#week-6-homework-categorical-data-analysis",
    "href": "chapters/part2-ch06-categorical_data.html#week-6-homework-categorical-data-analysis",
    "title": "14  Week 6: Categorical Data Analysis",
    "section": "27.1 Week 6 Homework: Categorical Data Analysis",
    "text": "27.1 Week 6 Homework: Categorical Data Analysis\n\n27.1.1 Instructions\nComplete the following analyses using R and Quarto. Submit both your .qmd and rendered .html files.\n\n\n27.1.2 Part 1: Chi-Square Analysis (30 points)\nYou’re investigating the relationship between calving ease and calf sex in beef cattle. Use the following data:\n\n\nCode\n# Run this code to create your dataset\nset.seed(YOUR_STUDENT_ID)  # Use your actual student ID number\ncalving_data &lt;- tibble(\n  calf_sex = sample(c(\"Male\", \"Female\"), 200, replace = TRUE),\n  calving_ease = sample(c(\"Easy\", \"Moderate\", \"Difficult\"), 200,\n                        replace = TRUE,\n                        prob = c(0.60, 0.30, 0.10))\n)\n\n\nTasks: 1. Create a contingency table with row and column totals 2. Calculate proportions by calf sex 3. Perform a chi-square test of independence 4. Calculate Cramér’s V to quantify effect size 5. Create a visualization showing calving ease by calf sex 6. Write a 3-4 sentence interpretation of your findings\n\n\n27.1.3 Part 2: Odds Ratios and Risk Ratios (25 points)\nA veterinary study investigates whether pre-weaning vaccination reduces diarrhea in piglets:\n\nVaccinated: 8 with diarrhea, 92 without\nUnvaccinated: 20 with diarrhea, 80 without\n\nTasks: 1. Create a 2×2 table 2. Calculate the risk of diarrhea in each group 3. Calculate the risk ratio 4. Calculate the odds ratio 5. Perform Fisher’s exact test 6. Interpret all results, including vaccine efficacy\n\n\n27.1.4 Part 3: Logistic Regression Analysis (35 points)\nUse the following simulated data on lameness in dairy cows:\n\n\nCode\nset.seed(YOUR_STUDENT_ID + 100)\nlameness_data &lt;- tibble(\n  cow_id = 1:300,\n  age_years = runif(300, 2, 8),\n  bcs = rnorm(300, mean = 3.0, sd = 0.5),\n  hoof_trimming = sample(c(\"Regular\", \"Irregular\"), 300, replace = TRUE),\n  floor_type = sample(c(\"Concrete\", \"Rubber\"), 300, replace = TRUE)\n) %&gt;%\n  mutate(\n    lameness_prob = plogis(-2 + 0.3 * age_years - 0.5 * bcs +\n                           1.2 * (hoof_trimming == \"Irregular\") -\n                           0.8 * (floor_type == \"Rubber\")),\n    lameness = rbinom(300, 1, prob = lameness_prob)\n  )\n\n\nTasks: 1. Fit a logistic regression model with lameness as the outcome and all other variables (except cow_id and lameness_prob) as predictors 2. Create a table of odds ratios with 95% confidence intervals 3. Identify which variables are statistically significant 4. Interpret the odds ratio for hoof_trimming in context 5. Create predictions for these three cow profiles: - Young cow (3 years), BCS 3.5, Regular trimming, Rubber floor - Old cow (7 years), BCS 2.5, Irregular trimming, Concrete floor - Average cow (5 years), BCS 3.0, Regular trimming, Concrete floor 6. Visualize predicted probability of lameness by age, with separate lines for regular vs irregular hoof trimming\n\n\n27.1.5 Part 4: Reflection (10 points)\nWrite a 250-300 word reflection addressing:\n\nWhen would you use chi-square vs Fisher’s exact test?\nWhat’s the key advantage of logistic regression over simple chi-square tests?\nDescribe one challenge you encountered in this week’s material and how you addressed it\nHow might you apply these methods in your own research or field of interest?\n\n\n\n27.1.6 Submission Checklist\n\nAll code runs without errors\nAll visualizations have informative titles and labels\nInterpretations are written in complete sentences\nResults are presented in tables where appropriate (using kable())\nQuarto document renders to HTML successfully\nBoth .qmd and .html files submitted\n\n\n\n27.1.7 Grading Rubric\n\nCode correctness (40%): Code runs, uses appropriate functions, no major errors\nInterpretation (40%): Correct interpretation of results, addresses all questions\nPresentation (15%): Clear visualizations, well-formatted tables, organized document\nReflection (5%): Thoughtful responses, demonstrates understanding\n\n\nDue Date: [To be specified by instructor]\nEstimated Time: 3-4 hours\nGood luck! Remember to use the course resources and ask questions if you get stuck.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Week 6: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html",
    "href": "chapters/part2-ch07-linear_regression.html",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "",
    "text": "16 Introduction\nImagine you’re managing a dairy operation and notice that cows consuming more feed tend to produce more milk. But how much more milk can you expect for each additional kilogram of feed? Can you predict a cow’s milk production based on her feed intake? And how confident can you be in those predictions?\nThese questions move us beyond simply testing whether differences exist (hypothesis testing) or whether variables are associated (correlation) to actually modeling relationships and making predictions. This is the domain of regression analysis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#motivating-scenario",
    "href": "chapters/part2-ch07-linear_regression.html#motivating-scenario",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "16.1 Motivating Scenario",
    "text": "16.1 Motivating Scenario\nA dairy nutritionist wants to optimize feeding strategies. She has data on daily feed intake (kg of dry matter) and milk production (kg/day) from 50 cows. Her questions include:\n\nIs there a relationship between feed intake and milk yield?\nFor every additional kg of feed consumed, how much more milk is produced?\nCan I predict milk production for a cow consuming 20 kg of feed per day?\nHow accurate are these predictions?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#key-questions-well-address",
    "href": "chapters/part2-ch07-linear_regression.html#key-questions-well-address",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "16.2 Key Questions We’ll Address",
    "text": "16.2 Key Questions We’ll Address\n\n\n\n\n\n\nNoteThis Week’s Learning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nDistinguish between correlation and regression analyses\nFit and interpret simple linear regression models\nAssess model fit using \\(R^2\\) and residual diagnostics\nCheck regression assumptions with diagnostic plots\nMake predictions with confidence and prediction intervals\nReport regression results appropriately",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#building-on-previous-weeks",
    "href": "chapters/part2-ch07-linear_regression.html#building-on-previous-weeks",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "16.3 Building on Previous Weeks",
    "text": "16.3 Building on Previous Weeks\n\n\n\n\n\n\nTipConnection to Previous Material\n\n\n\n\nWeek 2: We used scatter plots for exploratory analysis\nWeek 3: We learned about sampling distributions and confidence intervals\nWeek 4: We tested hypotheses about means (t-tests)\nWeek 7 (Now): We model relationships between variables and make predictions\n\nRegression extends hypothesis testing by not just asking “is there an effect?” but “how large is the effect and what can we predict?”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#conceptual-differences",
    "href": "chapters/part2-ch07-linear_regression.html#conceptual-differences",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "17.1 Conceptual Differences",
    "text": "17.1 Conceptual Differences\nCorrelation measures the strength and direction of a linear relationship between two variables: - Both variables are random - Symmetric relationship (correlating X with Y = correlating Y with X) - Answers: “How strongly are these variables associated?” - No prediction involved\nRegression models one variable as a function of another: - Response variable (Y) is random; predictor variable (X) may be fixed or random - Asymmetric relationship (regressing Y on X ≠ regressing X on Y) - Answers: “How does Y change as X changes?” and “What value of Y do we predict for a given X?” - Enables prediction",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#pearson-correlation-coefficient",
    "href": "chapters/part2-ch07-linear_regression.html#pearson-correlation-coefficient",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "17.2 Pearson Correlation Coefficient",
    "text": "17.2 Pearson Correlation Coefficient\nThe Pearson correlation coefficient (\\(r\\)) quantifies linear association:\n\\[r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\\]\nProperties: - Range: \\(-1 \\leq r \\leq 1\\) - \\(r = 1\\): Perfect positive linear relationship - \\(r = -1\\): Perfect negative linear relationship - \\(r = 0\\): No linear relationship - \\(r^2\\) equals the proportion of variance in Y explained by X (in regression context)\nInterpretation Guidelines (rough rules of thumb): - \\(|r| &lt; 0.3\\): Weak correlation - \\(0.3 \\leq |r| &lt; 0.7\\): Moderate correlation - \\(|r| \\geq 0.7\\): Strong correlation\n\n\n\n\n\n\nWarningImportant Limitations of Correlation\n\n\n\n\nOnly measures linear relationships (can miss nonlinear patterns)\nSensitive to outliers\nDoes not imply causation\nCan be misleading with restricted ranges",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#example-feed-intake-and-milk-yield",
    "href": "chapters/part2-ch07-linear_regression.html#example-feed-intake-and-milk-yield",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "17.3 Example: Feed Intake and Milk Yield",
    "text": "17.3 Example: Feed Intake and Milk Yield\nLet’s generate some realistic data and explore correlation:\n\n\nCode\n# Simulate dairy cow data\nn &lt;- 50\nfeed_intake &lt;- rnorm(n, mean = 22, sd = 3)  # kg DM/day\n\n# Milk yield depends on feed intake plus random variation\nmilk_yield &lt;- 8 + 1.5 * feed_intake + rnorm(n, mean = 0, sd = 4)  # kg/day\n\n# Create data frame\ndairy_data &lt;- tibble(\n  cow_id = 1:n,\n  feed_intake = feed_intake,\n  milk_yield = milk_yield\n)\n\n# Visualize relationship\nggplot(dairy_data, aes(x = feed_intake, y = milk_yield)) +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  labs(\n    title = \"Relationship Between Feed Intake and Milk Yield\",\n    subtitle = \"50 dairy cows\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCalculate correlation:\n\n\nCode\n# Pearson correlation\ncor_result &lt;- cor.test(dairy_data$feed_intake, dairy_data$milk_yield)\n\n# Display results\ncor_result\n\n\n\n    Pearson's product-moment correlation\n\ndata:  dairy_data$feed_intake and dairy_data$milk_yield\nt = 7.0995, df = 48, p-value = 5.175e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5461394 0.8289102\nsample estimates:\n      cor \n0.7156903 \n\n\nCode\n# Extract key values\nr_value &lt;- cor_result$estimate\np_value &lt;- cor_result$p.value\nci_lower &lt;- cor_result$conf.int[1]\nci_upper &lt;- cor_result$conf.int[2]\n\n\nInterpretation: The correlation between feed intake and milk yield is \\(r\\) = 0.716 (95% CI: [0.546, 0.829], p &lt; 0.001). This indicates a strong positive linear relationship.\nBut correlation alone doesn’t tell us: - How much milk yield changes per kg of feed - What milk yield to expect for a specific feed intake - This is where regression comes in!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#the-linear-model",
    "href": "chapters/part2-ch07-linear_regression.html#the-linear-model",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "18.1 The Linear Model",
    "text": "18.1 The Linear Model\nSimple linear regression models the relationship between a continuous response variable (\\(Y\\)) and a single predictor variable (\\(X\\)):\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere: - \\(Y_i\\) = response variable for observation \\(i\\) (e.g., milk yield) - \\(X_i\\) = predictor variable for observation \\(i\\) (e.g., feed intake) - \\(\\beta_0\\) = intercept (expected value of \\(Y\\) when \\(X = 0\\)) - \\(\\beta_1\\) = slope (change in \\(Y\\) for a 1-unit increase in \\(X\\)) - \\(\\epsilon_i\\) = random error term, assumed \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#least-squares-estimation",
    "href": "chapters/part2-ch07-linear_regression.html#least-squares-estimation",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "18.2 Least Squares Estimation",
    "text": "18.2 Least Squares Estimation\nThe method of least squares finds the line that minimizes the sum of squared residuals (vertical distances from points to the line):\n\\[\\text{Minimize: } \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nThe resulting estimates are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\nTipKey Insight\n\n\n\nThe regression line always passes through the point \\((\\bar{x}, \\bar{y})\\).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#fitting-a-model-in-r",
    "href": "chapters/part2-ch07-linear_regression.html#fitting-a-model-in-r",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "18.3 Fitting a Model in R",
    "text": "18.3 Fitting a Model in R\nLet’s fit a regression model to our dairy data:\n\n\nCode\n# Fit simple linear regression\nmodel1 &lt;- lm(milk_yield ~ feed_intake, data = dairy_data)\n\n# Display summary\nsummary(model1)\n\n\n\nCall:\nlm(formula = milk_yield ~ feed_intake, data = dairy_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2717 -2.7034 -0.0829  2.7748  7.2574 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8294     4.4337   2.217   0.0314 *  \nfeed_intake   1.3898     0.1958   7.100 5.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.123 on 48 degrees of freedom\nMultiple R-squared:  0.5122,    Adjusted R-squared:  0.5021 \nF-statistic:  50.4 on 1 and 48 DF,  p-value: 5.175e-09",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#sec-interpretation",
    "href": "chapters/part2-ch07-linear_regression.html#sec-interpretation",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "18.4 Interpreting Coefficients",
    "text": "18.4 Interpreting Coefficients\n\n\nCode\n# Tidy output with broom\ntidy_model &lt;- tidy(model1, conf.int = TRUE)\ntidy_model\n\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     9.83     4.43       2.22 0.0314           0.915     18.7 \n2 feed_intake     1.39     0.196      7.10 0.00000000518    0.996      1.78\n\n\nCode\n# Extract coefficients\nintercept &lt;- coef(model1)[1]\nslope &lt;- coef(model1)[2]\n\n\nIntercept (\\(\\hat{\\beta}_0\\) = 9.83): - The predicted milk yield when feed intake = 0 kg/day - Interpretation: Not biologically meaningful in this case (cows can’t consume 0 feed and produce milk!) - Often, the intercept is not of primary interest; it ensures the line fits the data well\nSlope (\\(\\hat{\\beta}_1\\) = 1.39): - For every 1 kg increase in feed intake (dry matter), milk yield increases by 1.39 kg/day, on average - This is the key quantity of interest for the dairy nutritionist - 95% CI: [, ]\n\n\n\n\n\n\nImportantAlways Include Units!\n\n\n\nWhen interpreting coefficients, always specify units: - “For every 1 kg increase in feed intake…” - “…milk yield increases by 1.53 kg/day”\nCoefficients without units are meaningless in applied contexts.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#visualizing-the-fitted-line",
    "href": "chapters/part2-ch07-linear_regression.html#visualizing-the-fitted-line",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "18.5 Visualizing the Fitted Line",
    "text": "18.5 Visualizing the Fitted Line\n\n\nCode\n# Base scatter plot with regression line\np1 &lt;- ggplot(dairy_data, aes(x = feed_intake, y = milk_yield)) +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Simple Linear Regression\",\n    subtitle = \"Fitted line with 95% confidence band\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  )\n\n# Add residuals visualization\ndairy_augmented &lt;- augment(model1, data = dairy_data)\n\np2 &lt;- ggplot(dairy_augmented, aes(x = feed_intake, y = milk_yield)) +\n  geom_segment(aes(xend = feed_intake, yend = .fitted),\n               alpha = 0.4, color = \"gray50\") +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  geom_line(aes(y = .fitted), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Residuals Visualization\",\n    subtitle = \"Vertical lines show residuals (observed - predicted)\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  )\n\n# Combine plots\np1 / p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#understanding-r-squared",
    "href": "chapters/part2-ch07-linear_regression.html#understanding-r-squared",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "19.1 Understanding R-squared",
    "text": "19.1 Understanding R-squared\nR-squared (\\(R^2\\)) represents the proportion of variance in Y explained by X:\n\\[R^2 = 1 - \\frac{SS_{residual}}{SS_{total}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\]\nInterpretation: - Range: \\(0 \\leq R^2 \\leq 1\\) - \\(R^2 = 0\\): Model explains none of the variance (no better than using \\(\\bar{y}\\) as prediction) - \\(R^2 = 1\\): Model explains all variance (perfect fit) - For simple linear regression: \\(R^2 = r^2\\) (square of correlation coefficient)\n\n\nCode\n# Extract R-squared\nmodel_summary &lt;- glance(model1)\nmodel_summary\n\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.512         0.502  4.12      50.4 0.00000000518     1  -141.  288.  293.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nCode\nr_squared &lt;- model_summary$r.squared\nadj_r_squared &lt;- model_summary$adj.r.squared\n\n\nFor our model: \\(R^2\\) = 0.512, meaning 51.2% of the variance in milk yield is explained by feed intake.\n\n\n\n\n\n\nNoteWhat about the other 48.8%?\n\n\n\nThe remaining variance is due to: - Other factors not in the model (genetics, lactation stage, health status, etc.) - Random biological variation - Measurement error\nThis is normal! Perfect \\(R^2 = 1\\) is rare in biological data.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#residual-standard-error",
    "href": "chapters/part2-ch07-linear_regression.html#residual-standard-error",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "19.2 Residual Standard Error",
    "text": "19.2 Residual Standard Error\nThe residual standard error (RSE) estimates \\(\\sigma\\), the standard deviation of the errors:\n\\[\\text{RSE} = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{n - 2}}\\]\n\n\nCode\nrse &lt;- model_summary$sigma\n\n\nFor our model: RSE = 4.12 kg/day\nInterpretation: On average, observed milk yields deviate from predicted values by about 4.12 kg/day. This is the typical prediction error.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#hypothesis-test-for-the-slope",
    "href": "chapters/part2-ch07-linear_regression.html#hypothesis-test-for-the-slope",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "19.3 Hypothesis Test for the Slope",
    "text": "19.3 Hypothesis Test for the Slope\nThe model output includes a t-test for \\(H_0: \\beta_1 = 0\\) (no relationship):\n\\[t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\\]\n\n\nCode\n# Extract slope test\nslope_test &lt;- tidy_model[2, ]\nt_stat &lt;- slope_test$statistic\np_val &lt;- slope_test$p.value\n\n\nResult: \\(t\\) = 7.10, p &lt; 0.001\nThis provides very strong evidence that feed intake is associated with milk yield (slope is not zero).\n\n\n\n\n\n\nTipStatistical vs Practical Significance\n\n\n\nA significant p-value tells us there’s a relationship, but doesn’t tell us if it’s practically meaningful:\n\nA slope of 0.01 might be statistically significant (large sample) but meaningless biologically\nFocus on effect size (magnitude of the slope) and confidence intervals",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#the-four-diagnostic-plots",
    "href": "chapters/part2-ch07-linear_regression.html#the-four-diagnostic-plots",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "20.1 The Four Diagnostic Plots",
    "text": "20.1 The Four Diagnostic Plots\nR provides four key diagnostic plots via plot(model):\n\n\nCode\n# Set up 2x2 plotting layout\npar(mfrow = c(2, 2))\nplot(model1)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))  # Reset\n\n\n\n20.1.1 Plot 1: Residuals vs Fitted\nPurpose: Check linearity and homoscedasticity\nWhat to look for: - Random scatter around horizontal line at 0 - No clear patterns (U-shapes, curves, funnels)\nOur plot: ✓ Looks good! Random scatter with no obvious pattern.\nRed flags: - Curved pattern → nonlinear relationship (try transformation or polynomial regression) - Funnel shape → heteroscedasticity (variance increases with fitted values)\n\n\n20.1.2 Plot 2: Normal Q-Q Plot\nPurpose: Check normality of residuals\nWhat to look for: - Points fall along diagonal reference line - Minor deviations at extremes are okay\nOur plot: ✓ Looks good! Points mostly follow the line.\nRed flags: - Systematic S-curve → heavy-tailed distribution - Points far from line at extremes → outliers\n\n\n20.1.3 Plot 3: Scale-Location (Sqrt Standardized Residuals vs Fitted)\nPurpose: Check homoscedasticity (constant variance)\nWhat to look for: - Horizontal line with random scatter - Roughly equal spread across the x-axis\nOur plot: ✓ Looks good! Relatively constant spread.\nRed flags: - Increasing/decreasing trend → heteroscedasticity\n\n\n20.1.4 Plot 4: Residuals vs Leverage\nPurpose: Identify influential points (high leverage and/or large residuals)\nWhat to look for: - Most points clustered in lower-left corner - No points beyond Cook’s distance contours (dashed lines)\nOur plot: ✓ Looks good! No highly influential points.\nRed flags: - Points beyond Cook’s distance lines → investigate these observations\n\n\n\n\n\n\nImportantCook’s Distance\n\n\n\nCook’s distance measures how much the regression would change if an observation were removed:\n\n\\(D &gt; 0.5\\): Potentially influential\n\\(D &gt; 1\\): Very influential (investigate!)\n\nUse cooks.distance(model) to calculate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#formal-tests-use-with-caution",
    "href": "chapters/part2-ch07-linear_regression.html#formal-tests-use-with-caution",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "20.2 Formal Tests (Use with Caution)",
    "text": "20.2 Formal Tests (Use with Caution)\nWhile diagnostic plots are primary, formal tests are available:\n\n\nCode\n# Shapiro-Wilk test for normality of residuals\nshapiro_test &lt;- shapiro.test(residuals(model1))\n\n# Breusch-Pagan test for heteroscedasticity\nbp_test &lt;- car::ncvTest(model1)\n\n# Display results\ncat(\"Shapiro-Wilk test (normality): p =\", sprintf(\"%.4f\", shapiro_test$p.value), \"\\n\")\n\n\nShapiro-Wilk test (normality): p = 0.2815 \n\n\nCode\ncat(\"Breusch-Pagan test (homoscedasticity): p =\", sprintf(\"%.4f\", bp_test$p), \"\\n\")\n\n\nBreusch-Pagan test (homoscedasticity): p = 0.9508 \n\n\n\n\n\n\n\n\nWarningDon’t Over-Rely on Normality Tests\n\n\n\n\nShapiro-Wilk test is sensitive to sample size\nSmall samples: may not detect violations\nLarge samples: may detect trivial violations\n\nAlways prioritize visual inspection of diagnostic plots!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#what-if-assumptions-are-violated",
    "href": "chapters/part2-ch07-linear_regression.html#what-if-assumptions-are-violated",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "20.3 What If Assumptions Are Violated?",
    "text": "20.3 What If Assumptions Are Violated?\n\n\n\n\n\n\n\nViolation\nPotential Solutions\n\n\n\n\nNon-linearity\nTransform variables (log, sqrt), polynomial regression\n\n\nHeteroscedasticity\nTransform response (log), weighted least squares\n\n\nNon-normality\nOften okay with large n (CLT), try transformations, robust methods\n\n\nInfluential points\nInvestigate for errors, consider robust regression\n\n\nNon-independence\nUse mixed models, time series methods, or clustered SEs",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#types-of-intervals",
    "href": "chapters/part2-ch07-linear_regression.html#types-of-intervals",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "21.1 Types of Intervals",
    "text": "21.1 Types of Intervals\n\nConfidence Interval (CI) for the Mean Response\n\n“What is the average Y for a given X?”\nInterval for \\(E(Y | X = x^*)\\)\nNarrower interval\n\nPrediction Interval (PI) for an Individual Response\n\n“What Y value will a new individual have at X = x*?”\nInterval for a single future observation\nWider interval (accounts for individual variation)\n\n\n\n\n\n\n\n\nTipKey Distinction\n\n\n\n\nCI: Uncertainty about the mean\nPI: Uncertainty about a specific individual\n\nPI is always wider because it includes both: 1. Uncertainty in estimating the mean (like CI) 2. Individual variation around the mean (\\(\\pm \\sigma\\))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#making-predictions",
    "href": "chapters/part2-ch07-linear_regression.html#making-predictions",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "21.2 Making Predictions",
    "text": "21.2 Making Predictions\nSuppose we want to predict milk yield for cows consuming 20 kg and 25 kg of feed per day:\n\n\nCode\n# New data for prediction\nnew_data &lt;- tibble(\n  feed_intake = c(20, 25)\n)\n\n# Confidence intervals (for mean response)\npred_conf &lt;- predict(model1, newdata = new_data, interval = \"confidence\", level = 0.95)\n\n# Prediction intervals (for individual response)\npred_pred &lt;- predict(model1, newdata = new_data, interval = \"prediction\", level = 0.95)\n\n# Combine results\npredictions_table &lt;- tibble(\n  feed_intake = new_data$feed_intake,\n  predicted_yield = pred_conf[, \"fit\"],\n  ci_lower = pred_conf[, \"lwr\"],\n  ci_upper = pred_conf[, \"upr\"],\n  pi_lower = pred_pred[, \"lwr\"],\n  pi_upper = pred_pred[, \"upr\"]\n)\n\n# Display\nknitr::kable(predictions_table, digits = 2,\n             caption = \"Predicted milk yields with 95% confidence and prediction intervals\")\n\n\n\nPredicted milk yields with 95% confidence and prediction intervals\n\n\nfeed_intake\npredicted_yield\nci_lower\nci_upper\npi_lower\npi_upper\n\n\n\n\n20\n37.63\n36.11\n39.14\n29.20\n46.05\n\n\n25\n44.57\n43.03\n46.12\n36.14\n53.01\n\n\n\n\n\nInterpretation for 20 kg feed intake:\n\nPoint estimate: 37.63 kg/day\n95% CI: [36.11, 39.14] - We’re 95% confident the mean milk yield for all cows consuming 20 kg/day is in this range\n95% PI: [29.20, 46.05] - We’re 95% confident a specific cow consuming 20 kg/day will produce milk in this range",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#visualizing-intervals",
    "href": "chapters/part2-ch07-linear_regression.html#visualizing-intervals",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "21.3 Visualizing Intervals",
    "text": "21.3 Visualizing Intervals\n\n\nCode\n# Create prediction grid\npred_grid &lt;- tibble(\n  feed_intake = seq(min(dairy_data$feed_intake),\n                    max(dairy_data$feed_intake),\n                    length.out = 100)\n)\n\n# Get predictions\npred_grid$fit &lt;- predict(model1, newdata = pred_grid)\npred_ci &lt;- predict(model1, newdata = pred_grid, interval = \"confidence\")\npred_pi &lt;- predict(model1, newdata = pred_grid, interval = \"prediction\")\n\npred_grid$ci_lower &lt;- pred_ci[, \"lwr\"]\npred_grid$ci_upper &lt;- pred_ci[, \"upr\"]\npred_grid$pi_lower &lt;- pred_pi[, \"lwr\"]\npred_grid$pi_upper &lt;- pred_pi[, \"upr\"]\n\n# Plot\nggplot() +\n  # Prediction interval (wider, outer band)\n  geom_ribbon(data = pred_grid,\n              aes(x = feed_intake, ymin = pi_lower, ymax = pi_upper),\n              fill = \"lightblue\", alpha = 0.3) +\n  # Confidence interval (narrower, inner band)\n  geom_ribbon(data = pred_grid,\n              aes(x = feed_intake, ymin = ci_lower, ymax = ci_upper),\n              fill = \"steelblue\", alpha = 0.5) +\n  # Regression line\n  geom_line(data = pred_grid,\n            aes(x = feed_intake, y = fit),\n            color = \"darkblue\", linewidth = 1) +\n  # Original data points\n  geom_point(data = dairy_data,\n             aes(x = feed_intake, y = milk_yield),\n             size = 2, alpha = 0.6) +\n  labs(\n    title = \"Confidence vs Prediction Intervals\",\n    subtitle = \"Dark band = 95% CI for mean | Light band = 95% PI for individuals\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#extrapolation-warning",
    "href": "chapters/part2-ch07-linear_regression.html#extrapolation-warning",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "21.4 Extrapolation Warning",
    "text": "21.4 Extrapolation Warning\n\n\n\n\n\n\nWarningDanger of Extrapolation\n\n\n\nNever make predictions far outside the range of observed X values!\n\nOur data: feed intake ranges from 16.7 to 30.6 kg/day\nPredicting at 5 kg/day or 40 kg/day would be extrapolation\nThe linear relationship may not hold outside the observed range\nPredictions become unreliable (and potentially absurd)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#scenario-body-weight-and-average-daily-gain-in-cattle",
    "href": "chapters/part2-ch07-linear_regression.html#scenario-body-weight-and-average-daily-gain-in-cattle",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.1 Scenario: Body Weight and Average Daily Gain in Cattle",
    "text": "22.1 Scenario: Body Weight and Average Daily Gain in Cattle\nA beef cattle researcher wants to understand the relationship between an animal’s body weight and average daily gain (ADG). Data were collected from 60 steers over a 90-day feeding period.\nResearch Question: Can we predict ADG based on initial body weight?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-1-load-and-explore-data",
    "href": "chapters/part2-ch07-linear_regression.html#step-1-load-and-explore-data",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.2 Step 1: Load and Explore Data",
    "text": "22.2 Step 1: Load and Explore Data\n\n\nCode\n# Simulate realistic cattle data\nn_cattle &lt;- 60\ninitial_weight &lt;- rnorm(n_cattle, mean = 350, sd = 40)  # kg\n\n# Heavier cattle tend to have slightly higher ADG\nadg &lt;- 0.8 + 0.002 * initial_weight + rnorm(n_cattle, mean = 0, sd = 0.15)  # kg/day\n\ncattle_data &lt;- tibble(\n  steer_id = 1:n_cattle,\n  initial_weight = initial_weight,\n  adg = adg\n)\n\n# Summary statistics\ncattle_data %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = mean(initial_weight),\n    sd_weight = sd(initial_weight),\n    mean_adg = mean(adg),\n    sd_adg = sd(adg)\n  ) %&gt;%\n  knitr::kable(digits = 2, caption = \"Summary statistics for cattle data\")\n\n\n\nSummary statistics for cattle data\n\n\nn\nmean_weight\nsd_weight\nmean_adg\nsd_adg\n\n\n\n\n60\n343.15\n36.88\n1.51\n0.18",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-2-exploratory-data-analysis",
    "href": "chapters/part2-ch07-linear_regression.html#step-2-exploratory-data-analysis",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.3 Step 2: Exploratory Data Analysis",
    "text": "22.3 Step 2: Exploratory Data Analysis\n\n\nCode\n# Scatter plot\np1 &lt;- ggplot(cattle_data, aes(x = initial_weight, y = adg)) +\n  geom_point(size = 3, alpha = 0.6, color = \"darkgreen\") +\n  labs(\n    title = \"Initial Body Weight vs Average Daily Gain\",\n    x = \"Initial Body Weight (kg)\",\n    y = \"Average Daily Gain (kg/day)\"\n  )\n\n# Marginal histograms\np2 &lt;- ggplot(cattle_data, aes(x = initial_weight)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.6) +\n  labs(x = \"Initial Weight (kg)\", y = \"Count\")\n\np3 &lt;- ggplot(cattle_data, aes(x = adg)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.6) +\n  labs(x = \"ADG (kg/day)\", y = \"Count\")\n\n# Combine\np1 / (p2 | p3)\n\n\n\n\n\n\n\n\n\nInitial observations: - Positive relationship appears present - No obvious outliers - Both variables roughly normally distributed",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-3-calculate-correlation",
    "href": "chapters/part2-ch07-linear_regression.html#step-3-calculate-correlation",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.4 Step 3: Calculate Correlation",
    "text": "22.4 Step 3: Calculate Correlation\n\n\nCode\ncor_cattle &lt;- cor.test(cattle_data$initial_weight, cattle_data$adg)\ncor_cattle\n\n\n\n    Pearson's product-moment correlation\n\ndata:  cattle_data$initial_weight and cattle_data$adg\nt = 4.5983, df = 58, p-value = 2.356e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3026769 0.6813723\nsample estimates:\n     cor \n0.516876 \n\n\nResult: \\(r\\) = 0.517, p = 0.0000\nThis suggests a weak to moderate positive correlation.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-4-fit-regression-model",
    "href": "chapters/part2-ch07-linear_regression.html#step-4-fit-regression-model",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.5 Step 4: Fit Regression Model",
    "text": "22.5 Step 4: Fit Regression Model\n\n\nCode\n# Fit model\ncattle_model &lt;- lm(adg ~ initial_weight, data = cattle_data)\n\n# Tidy output\ntidy(cattle_model, conf.int = TRUE) %&gt;%\n  knitr::kable(digits = 4, caption = \"Regression coefficients\")\n\n\n\nRegression coefficients\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.6572\n0.1870\n3.5153\n9e-04\n0.2830\n1.0315\n\n\ninitial_weight\n0.0025\n0.0005\n4.5983\n0e+00\n0.0014\n0.0036\n\n\n\n\n\nCode\n# Model fit statistics\nglance(cattle_model) %&gt;%\n  select(r.squared, adj.r.squared, sigma, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Model fit statistics\")\n\n\n\nModel fit statistics\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n\n\n0.2672\n0.2545\n0.1535\n21.1442\n0",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-5-interpret-coefficients",
    "href": "chapters/part2-ch07-linear_regression.html#step-5-interpret-coefficients",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.6 Step 5: Interpret Coefficients",
    "text": "22.6 Step 5: Interpret Coefficients\n\n\nCode\ncoefs &lt;- coef(cattle_model)\nintercept_cattle &lt;- coefs[1]\nslope_cattle &lt;- coefs[2]\n\n\nIntercept (\\(\\hat{\\beta}_0\\) = 0.6572): Predicted ADG when initial weight = 0 kg (not meaningful)\nSlope (\\(\\hat{\\beta}_1\\) = 0.0025): For every 1 kg increase in initial body weight, ADG increases by 0.0025 kg/day (about 2.5 g/day), on average.\nIs this biologically meaningful? - A 50 kg difference in weight → ~0.12 kg/day difference in ADG (about 125 g/day) - This is a small but meaningful effect in beef production",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-6-check-assumptions",
    "href": "chapters/part2-ch07-linear_regression.html#step-6-check-assumptions",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.7 Step 6: Check Assumptions",
    "text": "22.7 Step 6: Check Assumptions\n\n\nCode\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(cattle_model)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\nAssessment: - ✓ Residuals vs Fitted: Random scatter, no pattern - ✓ Q-Q Plot: Points follow line well - ✓ Scale-Location: Constant variance - ✓ Residuals vs Leverage: No influential points\nConclusion: Assumptions appear satisfied.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-7-visualize-results",
    "href": "chapters/part2-ch07-linear_regression.html#step-7-visualize-results",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.8 Step 7: Visualize Results",
    "text": "22.8 Step 7: Visualize Results\n\n\nCode\nggplot(cattle_data, aes(x = initial_weight, y = adg)) +\n  geom_point(size = 3, alpha = 0.6, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\", fill = \"pink\") +\n  labs(\n    title = \"Body Weight and Average Daily Gain in Beef Cattle\",\n    subtitle = sprintf(\"ADG = %.3f + %.4f × Weight (R² = %.3f)\",\n                      intercept_cattle, slope_cattle,\n                      glance(cattle_model)$r.squared),\n    x = \"Initial Body Weight (kg)\",\n    y = \"Average Daily Gain (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-8-make-predictions",
    "href": "chapters/part2-ch07-linear_regression.html#step-8-make-predictions",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.9 Step 8: Make Predictions",
    "text": "22.9 Step 8: Make Predictions\nWhat ADG would we expect for steers weighing 300 kg and 400 kg?\n\n\nCode\n# New data\nnew_cattle &lt;- tibble(initial_weight = c(300, 400))\n\n# Predictions with intervals\npred_cattle &lt;- predict(cattle_model, newdata = new_cattle,\n                       interval = \"prediction\", level = 0.95) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(new_cattle, .)\n\npred_cattle %&gt;%\n  knitr::kable(digits = 3,\n               caption = \"Predicted ADG with 95% prediction intervals\")\n\n\n\nPredicted ADG with 95% prediction intervals\n\n\ninitial_weight\nfit\nlwr\nupr\n\n\n\n\n300\n1.405\n1.091\n1.718\n\n\n400\n1.654\n1.338\n1.970\n\n\n\n\n\nInterpretation: - A 300 kg steer: Expected ADG = 1.40 kg/day (95% PI: [1.09, 1.72]) - A 400 kg steer: Expected ADG = 1.65 kg/day (95% PI: [1.34, 1.97])",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#step-9-report-results",
    "href": "chapters/part2-ch07-linear_regression.html#step-9-report-results",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "22.10 Step 9: Report Results",
    "text": "22.10 Step 9: Report Results\n\n\n\n\n\n\nNoteExample Results Statement\n\n\n\n“A simple linear regression was conducted to examine the relationship between initial body weight and average daily gain (ADG) in beef cattle (n = 60). Initial body weight was significantly associated with ADG (p = 0.0000), with each 1 kg increase in body weight corresponding to a 0.0025 kg/day increase in ADG (95% CI: [, ]). However, initial body weight explained only 26.7% of the variance in ADG (\\(R^2\\) = 0.267), suggesting that other factors (e.g., genetics, health status, feed composition) play substantial roles in determining growth rate. Model diagnostics indicated that regression assumptions were adequately met.”",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#when-is-simple-linear-regression-appropriate",
    "href": "chapters/part2-ch07-linear_regression.html#when-is-simple-linear-regression-appropriate",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "23.1 When is Simple Linear Regression Appropriate?",
    "text": "23.1 When is Simple Linear Regression Appropriate?\nUse simple linear regression when: - You have one continuous predictor and one continuous response - The relationship appears linear - You want to quantify the relationship or make predictions - You want to test if a relationship exists\nDon’t use simple linear regression when: - The relationship is clearly nonlinear (use transformations or nonlinear models) - You have multiple predictors (use multiple regression - Week 8!) - Response is binary (use logistic regression) - Data have hierarchical structure (use mixed models)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#sample-size-considerations",
    "href": "chapters/part2-ch07-linear_regression.html#sample-size-considerations",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "23.2 Sample Size Considerations",
    "text": "23.2 Sample Size Considerations\nRules of thumb: - Minimum: ~20-30 observations for stable estimates - Preferred: 10-20 observations per predictor (simple regression = 1 predictor) - Larger samples: Better for detecting small effects and checking assumptions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#outliers-and-influential-points",
    "href": "chapters/part2-ch07-linear_regression.html#outliers-and-influential-points",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "23.3 Outliers and Influential Points",
    "text": "23.3 Outliers and Influential Points\nTypes of unusual observations:\n\nOutliers: Large residuals (Y is unusual given X)\nHigh leverage points: Unusual X values (far from \\(\\bar{x}\\))\nInfluential points: Combining high leverage and large residual; removal changes the regression line\n\nWhat to do: - Investigate (data entry error? Valid unusual case?) - Report results with and without influential points - Consider robust regression methods if many outliers\n\n\nCode\n# Calculate influence metrics\ninfluence_metrics &lt;- cattle_data %&gt;%\n  mutate(\n    cooks_d = cooks.distance(cattle_model),\n    leverage = hatvalues(cattle_model),\n    std_resid = rstandard(cattle_model)\n  ) %&gt;%\n  arrange(desc(cooks_d))\n\n# Display top 5 by Cook's distance\ninfluence_metrics %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(steer_id, initial_weight, adg, cooks_d, leverage, std_resid) %&gt;%\n  knitr::kable(digits = 3, caption = \"Top 5 observations by Cook's distance\")\n\n\n\nTop 5 observations by Cook’s distance\n\n\nsteer_id\ninitial_weight\nadg\ncooks_d\nleverage\nstd_resid\n\n\n\n\n42\n273.365\n0.919\n0.339\n0.077\n-2.842\n\n\n47\n299.433\n1.126\n0.072\n0.040\n-1.846\n\n\n14\n273.826\n1.496\n0.047\n0.077\n1.059\n\n\n9\n363.451\n1.268\n0.042\n0.022\n-1.942\n\n\n15\n344.418\n1.826\n0.035\n0.017\n2.044\n\n\n\n\n\nAll Cook’s distances are well below 0.5, indicating no influential points.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#biological-vs-statistical-significance",
    "href": "chapters/part2-ch07-linear_regression.html#biological-vs-statistical-significance",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "23.4 Biological vs Statistical Significance",
    "text": "23.4 Biological vs Statistical Significance\n\n\n\n\n\n\nImportantStatistical Significance ≠ Practical Importance\n\n\n\nConsider: - Effect size: Is the magnitude of the slope meaningful? - Context: A slope of 0.002 kg/day per kg of body weight is small but might be economically important over 100+ days - Confidence intervals: Provide range of plausible effect sizes - Cost-benefit: Is the predictor worth measuring/manipulating?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#common-pitfalls",
    "href": "chapters/part2-ch07-linear_regression.html#common-pitfalls",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "23.5 Common Pitfalls",
    "text": "23.5 Common Pitfalls\n\n\n\n\n\n\nWarningWatch Out For These Mistakes!\n\n\n\n\nConfusing correlation with causation\n\nRegression shows association, not causation (unless from RCT)\n\nExtrapolating beyond data range\n\nPredictions unreliable outside observed X values\n\nIgnoring assumption violations\n\nAlways check diagnostic plots!\n\nFocusing only on p-values\n\nReport effect sizes, CIs, and \\(R^2\\)\n\nForgetting units\n\nCoefficients are meaningless without units\n\nOverlooking influential points\n\nOne outlier can dramatically change results",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#essential-components",
    "href": "chapters/part2-ch07-linear_regression.html#essential-components",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "24.1 Essential Components",
    "text": "24.1 Essential Components\nA complete regression report should include:\n\nSample size (n)\nModel equation (in words or symbols)\nCoefficients with standard errors or CIs\nStatistical significance (t-statistics, p-values)\nModel fit (\\(R^2\\), RSE)\nAssumption checks (statement that diagnostics were examined)\nInterpretation in context (with units!)\nVisualization (scatter plot with regression line)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#table-format-example",
    "href": "chapters/part2-ch07-linear_regression.html#table-format-example",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "24.2 Table Format Example",
    "text": "24.2 Table Format Example\n\n\nCode\n# Create publication-ready table\ncattle_table &lt;- tidy(cattle_model, conf.int = TRUE) %&gt;%\n  select(term, estimate, std.error, statistic, p.value, conf.low, conf.high) %&gt;%\n  mutate(\n    term = case_match(term,\n                      \"(Intercept)\" ~ \"Intercept\",\n                      \"initial_weight\" ~ \"Initial Weight (kg)\")\n  )\n\ncattle_table %&gt;%\n  knitr::kable(\n    digits = 4,\n    col.names = c(\"Term\", \"Estimate\", \"SE\", \"t\", \"p\", \"95% CI Lower\", \"95% CI Upper\"),\n    caption = \"Simple linear regression: ADG predicted by initial body weight\"\n  )\n\n\n\nSimple linear regression: ADG predicted by initial body weight\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\nt\np\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.6572\n0.1870\n3.5153\n9e-04\n0.2830\n1.0315\n\n\nInitial Weight (kg)\n0.0025\n0.0005\n4.5983\n0e+00\n0.0014\n0.0036",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#figure-requirements",
    "href": "chapters/part2-ch07-linear_regression.html#figure-requirements",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "24.3 Figure Requirements",
    "text": "24.3 Figure Requirements\nGood regression figures include: - Scatter plot of raw data - Fitted regression line - Confidence band (optional but recommended) - Clear axis labels with units - Informative title - Regression equation and/or \\(R^2\\) in caption or subtitle",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#decision-framework-when-to-use-regression",
    "href": "chapters/part2-ch07-linear_regression.html#decision-framework-when-to-use-regression",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "25.1 Decision Framework: When to Use Regression",
    "text": "25.1 Decision Framework: When to Use Regression\n\n\n\nDecision tree for simple linear regression\n\n\n\n\n\n\n\nQuestion\nIf YES\nIf NO\n\n\n\n\nOne continuous Y and one continuous X?\nContinue\nConsider other methods\n\n\nRelationship appears linear?\nContinue\nTransform or use nonlinear model\n\n\nWant to quantify relationship/predict Y?\nUse regression\nCorrelation may suffice\n\n\nCare about direction of prediction?\nRegression (Y ~ X)\nCorrelation is symmetric\n\n\nHave multiple predictors?\nMultiple regression (Week 8!)\nSimple regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#looking-ahead-to-week-8",
    "href": "chapters/part2-ch07-linear_regression.html#looking-ahead-to-week-8",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "25.2 Looking Ahead to Week 8",
    "text": "25.2 Looking Ahead to Week 8\nNext week we’ll extend to multiple regression: - Multiple predictor variables simultaneously - Controlling for confounders - Categorical predictors (factor variables) - Model comparison and selection - Interactions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#practice-problems",
    "href": "chapters/part2-ch07-linear_regression.html#practice-problems",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "25.3 Practice Problems",
    "text": "25.3 Practice Problems\nTry these to solidify your understanding:\n\nConceptual: Explain to a fellow student the difference between correlation and regression. When would you use each?\nInterpretation: A regression of egg weight (Y, grams) on hen age (X, weeks) gives: \\(\\hat{Y} = 35 + 0.8X\\). Interpret both coefficients with proper units.\nPrediction: Using the equation above, predict the egg weight for a 20-week-old hen. Would you trust a prediction for a 100-week-old hen? Why or why not?\nAnalysis: Load the mtcars dataset in R. Fit a regression predicting mpg from wt (weight). Check assumptions, interpret coefficients, and make a publication-quality plot.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#recommended-reading",
    "href": "chapters/part2-ch07-linear_regression.html#recommended-reading",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "26.1 Recommended Reading",
    "text": "26.1 Recommended Reading\n\nZuur et al. (2009). “Mixed Effects Models and Extensions in Ecology with R” - Chapter 4 (excellent on diagnostics)\nFox (2016). “Applied Regression Analysis and Generalized Linear Models” - Chapters 6-12\nFaraway (2014). “Linear Models with R” - Practical guide with R code",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#online-resources",
    "href": "chapters/part2-ch07-linear_regression.html#online-resources",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "26.2 Online Resources",
    "text": "26.2 Online Resources\n\nR for Data Science (2e): Chapter on modeling - https://r4ds.hadley.nz/\nStatistical Rethinking by Richard McElreath - Lectures on YouTube\nRegression Diagnostic Plots interpretation guide: https://library.virginia.edu/data/articles/diagnostic-plots",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#r-functions-summary",
    "href": "chapters/part2-ch07-linear_regression.html#r-functions-summary",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "26.3 R Functions Summary",
    "text": "26.3 R Functions Summary\n\n\n\nFunction\nPurpose\n\n\n\n\nlm(y ~ x, data)\nFit linear model\n\n\nsummary(model)\nModel summary\n\n\ncoef(model)\nExtract coefficients\n\n\nresiduals(model)\nExtract residuals\n\n\nfitted(model)\nExtract fitted values\n\n\npredict(model, newdata, interval)\nMake predictions\n\n\nplot(model)\nDiagnostic plots\n\n\ncor(x, y)\nCorrelation\n\n\ncor.test(x, y)\nCorrelation with inference\n\n\nbroom::tidy(model)\nTidy coefficient table\n\n\nbroom::glance(model)\nModel fit statistics\n\n\nbroom::augment(model)\nAdd fitted values, residuals to data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#instructions",
    "href": "chapters/part2-ch07-linear_regression.html#instructions",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.1 Instructions",
    "text": "27.1 Instructions\nComplete the following assignment using R and Quarto. Submit both your .qmd source file and the rendered .html output.\nDue: [One week from today]",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#part-1-correlation-vs-regression-concepts-20-points",
    "href": "chapters/part2-ch07-linear_regression.html#part-1-correlation-vs-regression-concepts-20-points",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.2 Part 1: Correlation vs Regression Concepts (20 points)",
    "text": "27.2 Part 1: Correlation vs Regression Concepts (20 points)\nFor each scenario below, indicate whether correlation or regression is more appropriate and justify your answer (2-3 sentences each).\nScenario A: A veterinarian wants to examine if there’s an association between dogs’ body weight and resting heart rate, with no interest in prediction.\nScenario B: A swine nutritionist wants to predict market weight based on daily feed intake to optimize feeding strategies.\nScenario C: An animal behaviorist is studying whether time spent grazing is related to daily step count in cattle.\nScenario D: A dairy manager wants to estimate milk production for cows based on their lactation number to plan herd management.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#part-2-complete-regression-analysis-50-points",
    "href": "chapters/part2-ch07-linear_regression.html#part-2-complete-regression-analysis-50-points",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.3 Part 2: Complete Regression Analysis (50 points)",
    "text": "27.3 Part 2: Complete Regression Analysis (50 points)\nYou will analyze a dataset on pig growth performance. The data include: - pig_id: Unique pig identifier - initial_weight: Weight at start of trial (kg) - final_weight: Weight after 60 days (kg) - feed_intake: Average daily feed intake (kg/day)\n\n27.3.1 Tasks:\n\nCreate simulated data (5 points)\n\nUse this code to generate the dataset:\nset.seed(450)\nn_pigs &lt;- 45\n\npig_data &lt;- tibble(\n  pig_id = 1:n_pigs,\n  initial_weight = rnorm(n_pigs, mean = 25, sd = 4),\n  feed_intake = rnorm(n_pigs, mean = 1.8, sd = 0.3)\n) %&gt;%\n  mutate(\n    final_weight = initial_weight +\n      35 + 12 * feed_intake + rnorm(n_pigs, mean = 0, sd = 3)\n  )\n\nExploratory Data Analysis (10 points)\n\nCalculate summary statistics for all variables\nCreate a scatter plot of feed_intake (X) vs final_weight (Y)\nCalculate and interpret the correlation coefficient\n\nFit Regression Model (10 points)\n\nFit a simple linear regression: final_weight ~ feed_intake\nReport the regression equation\nCreate a publication-quality plot with the fitted line\n\nInterpret Coefficients (10 points)\n\nInterpret the intercept (include units and biological meaning if any)\nInterpret the slope (include units and practical significance)\nReport 95% confidence intervals for both coefficients\n\nAssess Model Fit (5 points)\n\nReport and interpret \\(R^2\\)\nReport and interpret the residual standard error\n\nCheck Assumptions (10 points)\n\nCreate the four diagnostic plots\nFor each plot, state whether assumptions appear satisfied or violated\nOverall conclusion: Are assumptions met?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#part-3-prediction-challenge-20-points",
    "href": "chapters/part2-ch07-linear_regression.html#part-3-prediction-challenge-20-points",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.4 Part 3: Prediction Challenge (20 points)",
    "text": "27.4 Part 3: Prediction Challenge (20 points)\nUsing your fitted model from Part 2:\n\nMake predictions (10 points)\n\nPredict final weight for pigs with feed intakes of 1.5, 2.0, and 2.5 kg/day\nCalculate 95% confidence intervals for the mean response\nCalculate 95% prediction intervals for individual pigs\nPresent results in a well-formatted table\n\nInterpret intervals (10 points)\n\nFor feed intake = 2.0 kg/day:\n\nExplain what the confidence interval tells you (in words)\nExplain what the prediction interval tells you (in words)\nWhy is the prediction interval wider?\n\nWould you trust a prediction for a pig consuming 4.0 kg/day? Why or why not?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#part-4-critical-thinking-10-points",
    "href": "chapters/part2-ch07-linear_regression.html#part-4-critical-thinking-10-points",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.5 Part 4: Critical Thinking (10 points)",
    "text": "27.5 Part 4: Critical Thinking (10 points)\nA research paper reports the following:\n\n“We found a significant relationship between barn temperature (°F) and daily egg production in laying hens (p = 0.03). The regression equation was: Eggs = 15.2 + 0.05 × Temperature, with \\(R^2\\) = 0.08.”\n\nQuestions:\n\nWhat does the slope (0.05) mean in practical terms? (2 points)\nIs the relationship statistically significant? Is it practically meaningful? Explain. (3 points)\nWhat does \\(R^2 = 0.08\\) tell you? Should the researchers be concerned about this value? (3 points)\nWhat additional information would you want to know before trusting this model? (2 points)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#grading-rubric",
    "href": "chapters/part2-ch07-linear_regression.html#grading-rubric",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.6 Grading Rubric",
    "text": "27.6 Grading Rubric\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nPart 1\n20\nCorrect identification and clear justification\n\n\nPart 2\n50\nCode correctness (15), interpretation accuracy (20), visualizations (10), assumption checks (5)\n\n\nPart 3\n20\nCorrect predictions (10), clear explanations (10)\n\n\nPart 4\n10\nThoughtful critical analysis\n\n\nOverall Quality\nBonus +5\nExceptional organization, clarity, professional presentation\n\n\n\nTotal: 100 points (+ 5 bonus)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch07-linear_regression.html#submission-checklist",
    "href": "chapters/part2-ch07-linear_regression.html#submission-checklist",
    "title": "15  Week 7: Simple Linear Regression",
    "section": "27.7 Submission Checklist",
    "text": "27.7 Submission Checklist\nBefore submitting, ensure:\n\nAll code runs without errors\nPlots have clear titles and axis labels with units\nInterpretations include appropriate units\nDiagnostic plots are included and interpreted\nDocument is well-organized and professional\nBoth .qmd and .html files are submitted\n\n\nGood luck! Remember: Regression is about understanding relationships and making informed predictions. Focus on interpretation, not just numbers!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Week 7: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html",
    "href": "chapters/part2-ch08-multiple_regression.html",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "",
    "text": "17 Introduction\nImagine you’re an animal nutritionist tasked with optimizing milk production in a dairy herd. You know that feed intake affects milk yield, but you also suspect that days in milk, parity (lactation number), and breed all play important roles. How can you account for all these factors simultaneously? How do you interpret the effect of feed when controlling for the other variables?\nThis is where multiple regression becomes essential. While simple linear regression (Week 7) allowed us to examine the relationship between one predictor and an outcome, real-world phenomena rarely depend on a single variable. Multiple regression extends our toolkit to model complex relationships with multiple predictors.\nKey Questions We’ll Address:\nThis final week serves dual purposes: introducing multiple regression as a powerful analytical tool, and synthesizing everything we’ve learned to help you navigate the full statistical toolkit we’ve developed together.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#why-one-predictor-isnt-enough",
    "href": "chapters/part2-ch08-multiple_regression.html#why-one-predictor-isnt-enough",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "18.1 Why One Predictor Isn’t Enough",
    "text": "18.1 Why One Predictor Isn’t Enough\nLet’s start with a scenario that demonstrates why we need multiple regression. Consider a study of pig growth where we measure weight gain and feed intake:\n\n\nCode\n# Simulate pig growth data with a confounding variable\nn &lt;- 50\npig_data &lt;- tibble(\n  pig_id = 1:n,\n  initial_weight = rnorm(n, mean = 20, sd = 3),  # Starting weight in kg\n  feed_intake = 1.5 + 0.08 * initial_weight + rnorm(n, sd = 0.2),  # kg/day\n  # Weight gain depends on BOTH feed and initial weight\n  weight_gain = 0.5 * feed_intake + 0.15 * initial_weight + rnorm(n, sd = 0.3)\n)\n\nhead(pig_data)\n\n\n# A tibble: 6 × 4\n  pig_id initial_weight feed_intake weight_gain\n   &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      1           17.7        2.79        3.98\n2      2           17.2        2.76        4.09\n3      3           23.6        3.32        5.66\n4      4           15.1        2.90        4.27\n5      5           20.9        3.04        4.37\n6      6           24.5        3.46        5.52\n\n\nIf we only look at the relationship between feed intake and weight gain (ignoring initial weight), we get:\n\n\nCode\n# Simple regression (wrong - omitted variable bias)\nsimple_model &lt;- lm(weight_gain ~ feed_intake, data = pig_data)\n\n# Multiple regression (correct)\nmultiple_model &lt;- lm(weight_gain ~ feed_intake + initial_weight, data = pig_data)\n\n# Visualize the difference\np1 &lt;- ggplot(pig_data, aes(x = feed_intake, y = weight_gain)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Simple Regression\",\n       subtitle = sprintf(\"β̂₁ = %.3f (biased!)\", coef(simple_model)[2]),\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(pig_data, aes(x = feed_intake, y = weight_gain, color = initial_weight)) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c() +\n  labs(title = \"Accounting for Initial Weight\",\n       subtitle = \"Color shows confounding by initial weight\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\",\n       color = \"Initial\\nWeight (kg)\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey Insight: The simple regression coefficient for feed intake is biased because initial weight affects both feed intake (heavier pigs eat more) and weight gain (heavier pigs grow faster). This is omitted variable bias or confounding.\nLet’s compare the models:\n\n\nCode\n# Compare coefficients\nsimple_coef &lt;- coef(simple_model)[2]\nmultiple_coef &lt;- coef(multiple_model)[2]\n\ncat(\"Simple regression: β̂_feed =\", sprintf(\"%.3f\", simple_coef), \"\\n\")\n\n\nSimple regression: β̂_feed = 1.562 \n\n\nCode\ncat(\"Multiple regression: β̂_feed =\", sprintf(\"%.3f\", multiple_coef), \"\\n\")\n\n\nMultiple regression: β̂_feed = 0.547 \n\n\nCode\ncat(\"True effect of feed:\", 0.5, \"\\n\")\n\n\nTrue effect of feed: 0.5 \n\n\nThe multiple regression estimate is much closer to the true effect (0.5) because it controls for initial weight.\n\n\n\n\n\n\nImportantThe Core Idea of Multiple Regression\n\n\n\nMultiple regression allows us to estimate the effect of each predictor while holding all other predictors constant. This helps us:\n\nControl for confounding variables\nIsolate individual effects from a system of interrelated variables\nMake better predictions by using all available information\nAnswer more nuanced research questions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#the-multiple-regression-model",
    "href": "chapters/part2-ch08-multiple_regression.html#the-multiple-regression-model",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "18.2 The Multiple Regression Model",
    "text": "18.2 The Multiple Regression Model\nThe mathematical form extends simple regression:\nSimple Linear Regression: \\[y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\]\nMultiple Linear Regression: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon\\]\nWhere:\n\n\\(y\\) = outcome variable (response)\n\\(x_1, x_2, \\ldots, x_p\\) = predictor variables\n\\(\\beta_0\\) = intercept (expected value of \\(y\\) when all predictors = 0)\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) = partial regression coefficients\n\\(\\epsilon\\) = random error term\n\nInterpretation of \\(\\beta_j\\): The expected change in \\(y\\) for a one-unit increase in \\(x_j\\), holding all other predictors constant.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#geometric-interpretation",
    "href": "chapters/part2-ch08-multiple_regression.html#geometric-interpretation",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "18.3 Geometric Interpretation",
    "text": "18.3 Geometric Interpretation\nIn simple regression, we fit a line through a 2D scatterplot. In multiple regression with two predictors, we fit a plane through a 3D point cloud. With more predictors, we fit a hyperplane in higher dimensions (which we can’t visualize, but the math works the same way).\n\n\nCode\n# Note: This requires the plotly package for interactive 3D plots\n# Not run here, but you can try it!\nlibrary(plotly)\n\nplot_ly(pig_data,\n        x = ~feed_intake,\n        y = ~initial_weight,\n        z = ~weight_gain,\n        type = \"scatter3d\",\n        mode = \"markers\") %&gt;%\n  layout(scene = list(\n    xaxis = list(title = \"Feed Intake\"),\n    yaxis = list(title = \"Initial Weight\"),\n    zaxis = list(title = \"Weight Gain\")\n  ))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#the-holding-other-variables-constant-concept",
    "href": "chapters/part2-ch08-multiple_regression.html#the-holding-other-variables-constant-concept",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "19.1 The “Holding Other Variables Constant” Concept",
    "text": "19.1 The “Holding Other Variables Constant” Concept\nThis is the most important—and most commonly misunderstood—aspect of multiple regression.\nLet’s use our pig example:\n\n\nCode\n# Fit the model\nmodel &lt;- lm(weight_gain ~ feed_intake + initial_weight, data = pig_data)\nsummary(model)\n\n\n\nCall:\nlm(formula = weight_gain ~ feed_intake + initial_weight, data = pig_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51275 -0.15873 -0.01694  0.16149  0.56037 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08765    0.39166  -0.224  0.82389    \nfeed_intake     0.54675    0.17729   3.084  0.00342 ** \ninitial_weight  0.14667    0.01840   7.970 2.83e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2567 on 47 degrees of freedom\nMultiple R-squared:  0.8265,    Adjusted R-squared:  0.8191 \nF-statistic: 111.9 on 2 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of Coefficients:\n\n\nCode\ncoefs &lt;- coef(model)\ncat(\"Intercept (β₀):\", sprintf(\"%.3f\", coefs[1]), \"\\n\")\n\n\nIntercept (β₀): -0.088 \n\n\nCode\ncat(\"  → Expected weight gain when feed intake = 0 AND initial weight = 0\\n\")\n\n\n  → Expected weight gain when feed intake = 0 AND initial weight = 0\n\n\nCode\ncat(\"  → Often not meaningful (extrapolation)\\n\\n\")\n\n\n  → Often not meaningful (extrapolation)\n\n\nCode\ncat(\"Feed Intake (β₁):\", sprintf(\"%.3f\", coefs[2]), \"\\n\")\n\n\nFeed Intake (β₁): 0.547 \n\n\nCode\ncat(\"  → For every 1 kg/day increase in feed intake,\\n\")\n\n\n  → For every 1 kg/day increase in feed intake,\n\n\nCode\ncat(\"  → weight gain increases by\", sprintf(\"%.3f\", coefs[2]), \"kg/day,\\n\")\n\n\n  → weight gain increases by 0.547 kg/day,\n\n\nCode\ncat(\"  → HOLDING initial weight constant\\n\\n\")\n\n\n  → HOLDING initial weight constant\n\n\nCode\ncat(\"Initial Weight (β₂):\", sprintf(\"%.3f\", coefs[3]), \"\\n\")\n\n\nInitial Weight (β₂): 0.147 \n\n\nCode\ncat(\"  → For every 1 kg increase in initial weight,\\n\")\n\n\n  → For every 1 kg increase in initial weight,\n\n\nCode\ncat(\"  → weight gain increases by\", sprintf(\"%.3f\", coefs[3]), \"kg/day,\\n\")\n\n\n  → weight gain increases by 0.147 kg/day,\n\n\nCode\ncat(\"  → HOLDING feed intake constant\\n\")\n\n\n  → HOLDING feed intake constant\n\n\n\n\n\n\n\n\nTipPractical Interpretation Strategy\n\n\n\nWhen interpreting \\(\\beta_j\\):\n\nState the magnitude and direction of the effect\nInclude units for both predictor and outcome\nExplicitly mention “holding other variables constant” (at least initially)\nAssess practical significance, not just statistical significance\nConsider whether the relationship is causal or associational",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#simpsons-paradox-when-direction-reverses",
    "href": "chapters/part2-ch08-multiple_regression.html#simpsons-paradox-when-direction-reverses",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "19.2 Simpson’s Paradox: When Direction Reverses",
    "text": "19.2 Simpson’s Paradox: When Direction Reverses\nOne striking phenomenon in multiple regression is Simpson’s Paradox: the relationship between \\(x\\) and \\(y\\) can reverse direction when we control for another variable.\n\n\nCode\n# Create data showing Simpson's Paradox\nfarm_data &lt;- tibble(\n  farm = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  farm_quality = rep(c(5, 10, 15), each = 20),  # Confounding variable\n  management_score = farm_quality + rnorm(60, sd = 1),\n  # Within each farm, MORE management REDUCES costs (efficiency)\n  # But better farms have HIGHER management and HIGHER costs (they spend more overall)\n  cost_per_animal = farm_quality * 2 + (-0.5) * (management_score - farm_quality) + rnorm(60, sd = 1)\n)\n\np1 &lt;- ggplot(farm_data, aes(x = management_score, y = cost_per_animal)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = \"Ignoring Farm (Wrong!)\",\n       subtitle = \"Appears that better management → higher costs\",\n       x = \"Management Score\",\n       y = \"Cost per Animal ($)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(farm_data, aes(x = management_score, y = cost_per_animal, color = farm)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Accounting for Farm (Correct)\",\n       subtitle = \"Actually, better management → lower costs (within farms)\",\n       x = \"Management Score\",\n       y = \"Cost per Animal ($)\",\n       color = \"Farm\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple regression (misleading)\nsimple_mod &lt;- lm(cost_per_animal ~ management_score, data = farm_data)\n\n# Multiple regression (correct)\nmultiple_mod &lt;- lm(cost_per_animal ~ management_score + farm_quality, data = farm_data)\n\ncat(\"Simple regression: β̂_management =\", sprintf(\"%.3f\", coef(simple_mod)[2]),\n    \"(WRONG - positive!)\\n\")\n\n\nSimple regression: β̂_management = 1.858 (WRONG - positive!)\n\n\nCode\ncat(\"Multiple regression: β̂_management =\", sprintf(\"%.3f\", coef(multiple_mod)[2]),\n    \"(CORRECT - negative)\\n\")\n\n\nMultiple regression: β̂_management = -0.486 (CORRECT - negative)\n\n\nLesson: Always consider potential confounding variables. The crude (unadjusted) relationship can be very different from the adjusted relationship.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#how-r-handles-factors",
    "href": "chapters/part2-ch08-multiple_regression.html#how-r-handles-factors",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "20.1 How R Handles Factors",
    "text": "20.1 How R Handles Factors\nR automatically converts factor variables into dummy variables (also called indicator variables). A factor with \\(k\\) levels is encoded as \\(k-1\\) dummy variables.\n\n\nCode\n# Create data with a categorical predictor\nbroiler_data &lt;- tibble(\n  bird_id = 1:90,\n  breed = rep(c(\"Ross\", \"Cobb\", \"Hubbard\"), each = 30),\n  feed_intake = rnorm(90, mean = 0.12, sd = 0.02),  # kg/day\n  # Different breeds have different baseline growth rates\n  weight_gain = case_when(\n    breed == \"Ross\" ~ 0.055,\n    breed == \"Cobb\" ~ 0.062,\n    breed == \"Hubbard\" ~ 0.058\n  ) + 0.15 * feed_intake + rnorm(90, sd = 0.008)\n)\n\n# Fit model with categorical predictor\nmodel_breed &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\nsummary(model_breed)\n\n\n\nCall:\nlm(formula = weight_gain ~ breed + feed_intake, data = broiler_data)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0207408 -0.0057003 -0.0006643  0.0079860  0.0157895 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.068599   0.006046  11.346  &lt; 2e-16 ***\nbreedHubbard -0.006933   0.002328  -2.978  0.00377 ** \nbreedRoss    -0.009763   0.002305  -4.235  5.7e-05 ***\nfeed_intake   0.098499   0.046785   2.105  0.03818 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008924 on 86 degrees of freedom\nMultiple R-squared:  0.2233,    Adjusted R-squared:  0.1962 \nF-statistic:  8.24 on 3 and 86 DF,  p-value: 7.005e-05\n\n\nUnderstanding the Output:\n\n\nCode\ncat(\"R created dummy variables:\\n\")\n\n\nR created dummy variables:\n\n\nCode\ncat(\"  • breedCobb: 1 if Cobb, 0 otherwise\\n\")\n\n\n  • breedCobb: 1 if Cobb, 0 otherwise\n\n\nCode\ncat(\"  • breedHubbard: 1 if Hubbard, 0 otherwise\\n\")\n\n\n  • breedHubbard: 1 if Hubbard, 0 otherwise\n\n\nCode\ncat(\"  • Ross is the REFERENCE LEVEL (baseline)\\n\\n\")\n\n\n  • Ross is the REFERENCE LEVEL (baseline)\n\n\nCode\ncoefs_breed &lt;- coef(model_breed)\n\ncat(\"Intercept:\", sprintf(\"%.4f\", coefs_breed[1]), \"\\n\")\n\n\nIntercept: 0.0686 \n\n\nCode\ncat(\"  → Expected weight gain for Ross breed when feed intake = 0\\n\\n\")\n\n\n  → Expected weight gain for Ross breed when feed intake = 0\n\n\nCode\ncat(\"breedCobb:\", sprintf(\"%.4f\", coefs_breed[2]), \"\\n\")\n\n\nbreedCobb: -0.0069 \n\n\nCode\ncat(\"  → Cobb birds gain\", sprintf(\"%.4f\", coefs_breed[2]), \"kg/day MORE than Ross\\n\")\n\n\n  → Cobb birds gain -0.0069 kg/day MORE than Ross\n\n\nCode\ncat(\"  → (holding feed intake constant)\\n\\n\")\n\n\n  → (holding feed intake constant)\n\n\nCode\ncat(\"breedHubbard:\", sprintf(\"%.4f\", coefs_breed[3]), \"\\n\")\n\n\nbreedHubbard: -0.0098 \n\n\nCode\ncat(\"  → Hubbard birds gain\", sprintf(\"%.4f\", coefs_breed[3]), \"kg/day MORE than Ross\\n\")\n\n\n  → Hubbard birds gain -0.0098 kg/day MORE than Ross\n\n\nCode\ncat(\"  → (holding feed intake constant)\\n\\n\")\n\n\n  → (holding feed intake constant)\n\n\nCode\ncat(\"feed_intake:\", sprintf(\"%.4f\", coefs_breed[4]), \"\\n\")\n\n\nfeed_intake: 0.0985 \n\n\nCode\ncat(\"  → Effect of feed is the SAME for all breeds (no interaction)\\n\")\n\n\n  → Effect of feed is the SAME for all breeds (no interaction)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#changing-the-reference-level",
    "href": "chapters/part2-ch08-multiple_regression.html#changing-the-reference-level",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "20.2 Changing the Reference Level",
    "text": "20.2 Changing the Reference Level\nSometimes you want a different baseline for comparison:\n\n\nCode\n# Change reference level to Cobb\nbroiler_data &lt;- broiler_data %&gt;%\n  mutate(breed = relevel(factor(breed), ref = \"Cobb\"))\n\nmodel_breed2 &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\ncoef(model_breed2)\n\n\n (Intercept) breedHubbard    breedRoss  feed_intake \n 0.068598912 -0.006933349 -0.009763200  0.098498550 \n\n\nCode\n# Now coefficients compare Ross and Hubbard to Cobb (the new reference)\n\n\n\n\n\n\n\n\nTipChoosing a Reference Level\n\n\n\nSelect a reference level that makes interpretation easiest:\n\nControl group (if you have one)\nMost common category\nTheoretically meaningful baseline\n\nThe choice doesn’t affect the model fit or predictions, only interpretation of coefficients.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#visualizing-categorical-predictors",
    "href": "chapters/part2-ch08-multiple_regression.html#visualizing-categorical-predictors",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "20.3 Visualizing Categorical Predictors",
    "text": "20.3 Visualizing Categorical Predictors\n\n\nCode\n# Reset to Ross as reference\nbroiler_data &lt;- broiler_data %&gt;%\n  mutate(breed = relevel(factor(breed), ref = \"Ross\"))\n\n# Refit model\nmodel_breed &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\n\n# Create predictions for each breed\npred_data &lt;- expand_grid(\n  breed = c(\"Ross\", \"Cobb\", \"Hubbard\"),\n  feed_intake = seq(min(broiler_data$feed_intake),\n                    max(broiler_data$feed_intake),\n                    length.out = 50)\n) %&gt;%\n  mutate(breed = factor(breed, levels = c(\"Ross\", \"Cobb\", \"Hubbard\")))\n\npred_data$predicted &lt;- predict(model_breed, newdata = pred_data)\n\np1 &lt;- ggplot(broiler_data, aes(x = breed, y = weight_gain, fill = breed)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.2, alpha = 0.4) +\n  labs(title = \"Weight Gain by Breed\",\n       x = \"Breed\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(broiler_data, aes(x = feed_intake, y = weight_gain, color = breed)) +\n  geom_point(alpha = 0.6) +\n  geom_line(data = pred_data, aes(y = predicted), size = 1) +\n  labs(title = \"Parallel Slopes (No Interaction)\",\n       subtitle = \"Same effect of feed for all breeds\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\",\n       color = \"Breed\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nNotice: The lines are parallel because we didn’t include an interaction term. The effect of feed intake is assumed to be the same for all breeds. We’ll discuss interactions briefly later.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#anova-with-covariates-ancova",
    "href": "chapters/part2-ch08-multiple_regression.html#anova-with-covariates-ancova",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "20.4 ANOVA with Covariates (ANCOVA)",
    "text": "20.4 ANOVA with Covariates (ANCOVA)\nMultiple regression with categorical predictors is essentially ANOVA with covariates (ANCOVA). This allows us to:\n\nCompare group means (like ANOVA)\nControl for continuous confounders (like regression)\n\n\n\nCode\n# Compare to one-way ANOVA (without controlling for feed)\nanova_only &lt;- aov(weight_gain ~ breed, data = broiler_data)\n\n# ANCOVA (controlling for feed)\nancova &lt;- aov(weight_gain ~ breed + feed_intake, data = broiler_data)\n\ncat(\"One-way ANOVA (ignoring feed intake):\\n\")\n\n\nOne-way ANOVA (ignoring feed intake):\n\n\nCode\nsummary(anova_only)\n\n\n            Df   Sum Sq   Mean Sq F value  Pr(&gt;F)    \nbreed        2 0.001616 0.0008079   9.759 0.00015 ***\nResiduals   87 0.007202 0.0000828                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\nANCOVA (controlling for feed intake):\\n\")\n\n\n\nANCOVA (controlling for feed intake):\n\n\nCode\nsummary(ancova)\n\n\n            Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    \nbreed        2 0.001616 0.0008079  10.144 0.000111 ***\nfeed_intake  1 0.000353 0.0003530   4.432 0.038178 *  \nResiduals   86 0.006849 0.0000796                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nControlling for feed intake can increase power by reducing residual variance.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#r2-vs-adjusted-r2",
    "href": "chapters/part2-ch08-multiple_regression.html#r2-vs-adjusted-r2",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "21.1 \\(R^2\\) vs Adjusted \\(R^2\\)",
    "text": "21.1 \\(R^2\\) vs Adjusted \\(R^2\\)\nRecall from Week 7:\n\n\\(R^2\\) = proportion of variance in \\(y\\) explained by the model\nRanges from 0 to 1; higher is “better”\n\nProblem: \\(R^2\\) always increases (or stays the same) when you add predictors, even if they’re useless!\n\n\nCode\n# Demonstrate R² inflation\nset.seed(2024)\njunk_data &lt;- broiler_data %&gt;%\n  mutate(\n    random1 = rnorm(n()),\n    random2 = rnorm(n()),\n    random3 = rnorm(n())\n  )\n\nm1 &lt;- lm(weight_gain ~ feed_intake, data = junk_data)\nm2 &lt;- lm(weight_gain ~ feed_intake + random1, data = junk_data)\nm3 &lt;- lm(weight_gain ~ feed_intake + random1 + random2, data = junk_data)\nm4 &lt;- lm(weight_gain ~ feed_intake + random1 + random2 + random3, data = junk_data)\n\ntibble(\n  Model = c(\"feed_intake only\", \"+ random1\", \"+ random2\", \"+ random3\"),\n  Predictors = 1:4,\n  R_squared = c(summary(m1)$r.squared,\n                summary(m2)$r.squared,\n                summary(m3)$r.squared,\n                summary(m4)$r.squared),\n  Adj_R_squared = c(summary(m1)$adj.r.squared,\n                    summary(m2)$adj.r.squared,\n                    summary(m3)$adj.r.squared,\n                    summary(m4)$adj.r.squared)\n) %&gt;%\n  knitr::kable(digits = 4, caption = \"R² always increases, Adjusted R² can decrease\")\n\n\n\nR² always increases, Adjusted R² can decrease\n\n\nModel\nPredictors\nR_squared\nAdj_R_squared\n\n\n\n\nfeed_intake only\n1\n0.0523\n0.0415\n\n\n+ random1\n2\n0.0543\n0.0326\n\n\n+ random2\n3\n0.0556\n0.0227\n\n\n+ random3\n4\n0.1273\n0.0862\n\n\n\n\n\nAdjusted \\(R^2\\) penalizes model complexity:\n\\[\\text{Adjusted } R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nWhere \\(n\\) = sample size, \\(p\\) = number of predictors.\n\n\n\n\n\n\nImportantWhen to Use Adjusted R²\n\n\n\n\nUse adjusted \\(R^2\\) when comparing models with different numbers of predictors\nAdjusted \\(R^2\\) can decrease if you add unhelpful predictors\nStill not perfect (doesn’t account for model complexity very strongly)\nBetter alternatives: AIC, BIC",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#aic-and-bic",
    "href": "chapters/part2-ch08-multiple_regression.html#aic-and-bic",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "21.2 AIC and BIC",
    "text": "21.2 AIC and BIC\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are more sophisticated measures of model quality that balance fit and complexity.\n\\[\\text{AIC} = -2 \\log(L) + 2p\\] \\[\\text{BIC} = -2 \\log(L) + p \\log(n)\\]\nWhere:\n\n\\(L\\) = likelihood (measure of model fit)\n\\(p\\) = number of parameters\n\\(n\\) = sample size\n\nLower is better for both AIC and BIC.\n\nBIC penalizes complexity more strongly than AIC\nAIC is better for prediction; BIC is better for identifying the “true” model (if it exists)\n\n\n\nCode\n# Compare models using AIC and BIC\nmodels_list &lt;- list(\n  \"Feed only\" = m1,\n  \"Feed + random1\" = m2,\n  \"Feed + random1 + random2\" = m3,\n  \"Feed + random1 + random2 + random3\" = m4\n)\n\ncomparison &lt;- tibble(\n  Model = names(models_list),\n  AIC = sapply(models_list, AIC),\n  BIC = sapply(models_list, BIC),\n  Adj_R2 = sapply(models_list, function(m) summary(m)$adj.r.squared)\n)\n\ncomparison %&gt;%\n  knitr::kable(digits = 2, caption = \"Model Comparison Metrics\")\n\n\n\nModel Comparison Metrics\n\n\nModel\nAIC\nBIC\nAdj_R2\n\n\n\n\nFeed only\n-574.19\n-566.69\n0.04\n\n\nFeed + random1\n-572.39\n-562.39\n0.03\n\n\nFeed + random1 + random2\n-570.51\n-558.01\n0.02\n\n\nFeed + random1 + random2 + random3\n-575.61\n-560.61\n0.09\n\n\n\n\n\nCode\n# Best model by each criterion\ncat(\"\\nBest model by AIC:\", comparison$Model[which.min(comparison$AIC)], \"\\n\")\n\n\n\nBest model by AIC: Feed + random1 + random2 + random3 \n\n\nCode\ncat(\"Best model by BIC:\", comparison$Model[which.min(comparison$BIC)], \"\\n\")\n\n\nBest model by BIC: Feed only \n\n\nCode\ncat(\"Best model by Adj R²:\", comparison$Model[which.max(comparison$Adj_R2)], \"\\n\")\n\n\nBest model by Adj R²: Feed + random1 + random2 + random3 \n\n\nAll three metrics correctly identify the simplest model (feed only) as best, since the other predictors are random noise.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#comparing-nested-models-with-anova",
    "href": "chapters/part2-ch08-multiple_regression.html#comparing-nested-models-with-anova",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "21.3 Comparing Nested Models with ANOVA",
    "text": "21.3 Comparing Nested Models with ANOVA\nWhen models are nested (one model is a subset of another), we can use an F-test to compare them:\n\n\nCode\n# Example: Do we need breed in the model?\nmodel_small &lt;- lm(weight_gain ~ feed_intake, data = broiler_data)\nmodel_large &lt;- lm(weight_gain ~ feed_intake + breed, data = broiler_data)\n\n# F-test for nested models\nanova(model_small, model_large)\n\n\nAnalysis of Variance Table\n\nModel 1: weight_gain ~ feed_intake\nModel 2: weight_gain ~ feed_intake + breed\n  Res.Df       RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     88 0.0083572                                  \n2     86 0.0068493  2 0.0015079 9.4668 0.0001923 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation:\n\nNull hypothesis: The smaller model is sufficient (breed coefficients = 0)\nAlternative: The larger model fits significantly better\nDecision: If p &lt; 0.05, we reject the null and prefer the larger model\n\n\n\nCode\nanova_result &lt;- anova(model_small, model_large)\np_value &lt;- anova_result$`Pr(&gt;F)`[2]\n\nif (p_value &lt; 0.05) {\n  cat(\"Result: p =\", sprintf(\"%.4f\", p_value), \"\\n\")\n  cat(\"→ The larger model (with breed) fits significantly better\\n\")\n  cat(\"→ We should include breed in the model\\n\")\n} else {\n  cat(\"Result: p =\", sprintf(\"%.4f\", p_value), \"\\n\")\n  cat(\"→ The smaller model is sufficient\\n\")\n  cat(\"→ No evidence that breed improves the model\\n\")\n}\n\n\nResult: p = 0.0002 \n→ The larger model (with breed) fits significantly better\n→ We should include breed in the model\n\n\n\n\n\n\n\n\nTipModel Comparison Strategy\n\n\n\n\nStart simple: Begin with a small model based on theory\nAdd predictors thoughtfully: Only include variables with scientific justification\nCompare nested models: Use F-tests (anova()) or AIC/BIC\nCheck diagnostics: A model with better fit statistics but violated assumptions is worse than a simpler model with valid assumptions\nPrioritize interpretability: A slightly worse-fitting model that you can explain is often better than a black box",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#the-5-key-assumptions",
    "href": "chapters/part2-ch08-multiple_regression.html#the-5-key-assumptions",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "22.1 The 5 Key Assumptions",
    "text": "22.1 The 5 Key Assumptions\n\nLinearity: The relationship between predictors and outcome is linear\nIndependence: Observations are independent of each other\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\nNo multicollinearity: Predictors are not highly correlated with each other (NEW!)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#checking-assumptions-diagnostic-plots",
    "href": "chapters/part2-ch08-multiple_regression.html#checking-assumptions-diagnostic-plots",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "22.2 Checking Assumptions: Diagnostic Plots",
    "text": "22.2 Checking Assumptions: Diagnostic Plots\n\n\nCode\n# Fit a model\nmodel_full &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\n\n# Standard diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_full)\n\n\n\n\n\n\n\n\n\nInterpreting the Plots:\n\nResiduals vs Fitted: Check for non-linearity and heteroscedasticity\n\nWant: Random scatter around horizontal line at 0\nBad: Patterns, funneling, curves\n\nQ-Q Plot: Check normality of residuals\n\nWant: Points fall along diagonal line\nBad: Systematic deviations (S-curves, fat tails)\n\nScale-Location: Check homoscedasticity\n\nWant: Horizontal line with random scatter\nBad: Increasing/decreasing trend\n\nResiduals vs Leverage: Identify influential points\n\nWant: No points beyond Cook’s distance contours (dashed lines)\nBad: Points in upper right or lower right corners",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#multicollinearity-the-variance-inflation-factor-vif",
    "href": "chapters/part2-ch08-multiple_regression.html#multicollinearity-the-variance-inflation-factor-vif",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "22.3 Multicollinearity: The Variance Inflation Factor (VIF)",
    "text": "22.3 Multicollinearity: The Variance Inflation Factor (VIF)\nMulticollinearity occurs when predictors are highly correlated with each other. This causes:\n\nUnstable coefficient estimates (change dramatically with small data changes)\nInflated standard errors (reduced statistical power)\nDifficulty interpreting individual effects\n\nVariance Inflation Factor (VIF) quantifies multicollinearity:\n\\[\\text{VIF}_j = \\frac{1}{1 - R^2_j}\\]\nWhere \\(R^2_j\\) is the \\(R^2\\) from regressing predictor \\(j\\) on all other predictors.\nRule of Thumb:\n\nVIF &lt; 5: Not concerning\nVIF 5-10: Moderate multicollinearity (be cautious)\nVIF &gt; 10: Severe multicollinearity (problematic)\n\n\n\nCode\n# Calculate VIF for our model\nvif_values &lt;- vif(model_full)\nvif_values\n\n\n                GVIF Df GVIF^(1/(2*Df))\nbreed       1.023161  2        1.005741\nfeed_intake 1.023161  1        1.011514\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\nfor (i in 1:length(vif_values)) {\n  name &lt;- names(vif_values)[i]\n  value &lt;- vif_values[i]\n\n  if (value &lt; 5) {\n    status &lt;- \"✓ No concern\"\n  } else if (value &lt; 10) {\n    status &lt;- \"⚠ Moderate concern\"\n  } else {\n    status &lt;- \"✗ Severe multicollinearity\"\n  }\n\n  cat(sprintf(\"%s: VIF = %.2f → %s\\n\", name, value, status))\n}\n\n\nExample of Problematic Multicollinearity:\n\n\nCode\n# Create highly correlated predictors\ncollinear_data &lt;- tibble(\n  height = rnorm(50, mean = 100, sd = 10),\n  height_inches = height / 2.54 + rnorm(50, sd = 0.01),  # Nearly perfect correlation!\n  weight = 0.5 * height + rnorm(50, sd = 5)\n)\n\n# Check the correlation\ncat(\"Correlation between height and height_inches:\",\n    sprintf(\"%.4f\", cor(collinear_data$height, collinear_data$height_inches)), \"\\n\\n\")\n\n\nCorrelation between height and height_inches: 1.0000 \n\n\nCode\n# Fit model with collinear predictors\nbad_model &lt;- lm(weight ~ height + height_inches, data = collinear_data)\n\ncat(\"Model with severe multicollinearity:\\n\")\n\n\nModel with severe multicollinearity:\n\n\nCode\nsummary(bad_model)$coefficients\n\n\n               Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)   -10.41061    6.59374 -1.578863 0.1210750\nheight         35.80768   25.46154  1.406344 0.1661996\nheight_inches -89.40364   64.66108 -1.382650 0.1733069\n\n\nCode\ncat(\"\\nVIF values:\\n\")\n\n\n\nVIF values:\n\n\nCode\nvif(bad_model)\n\n\n       height height_inches \n     155836.6      155836.6 \n\n\nCode\ncat(\"\\nNotice: Huge VIF values and unstable coefficients!\\n\")\n\n\n\nNotice: Huge VIF values and unstable coefficients!\n\n\nCode\ncat(\"The model can't separate the effects of height and height_inches\\n\")\n\n\nThe model can't separate the effects of height and height_inches\n\n\nCode\ncat(\"because they contain nearly identical information.\\n\")\n\n\nbecause they contain nearly identical information.\n\n\n\n\n\n\n\n\nWarningFixing Multicollinearity\n\n\n\nIf you detect severe multicollinearity:\n\nRemove one of the correlated predictors (choose based on theory)\nCombine correlated predictors (e.g., create an index or average)\nUse dimension reduction (PCA) — beyond this course\nCollect more data (if possible)\nAccept it (if you only care about prediction, not interpretation)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#approaches-to-variable-selection",
    "href": "chapters/part2-ch08-multiple_regression.html#approaches-to-variable-selection",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "23.1 Approaches to Variable Selection",
    "text": "23.1 Approaches to Variable Selection\n\n23.1.1 1. Theory-Driven (Recommended)\nInclude predictors based on scientific knowledge and research questions:\n\nWhat variables does theory say should matter?\nWhat confounders must you control for?\nWhat relationships are you specifically interested in testing?\n\nAdvantages:\n\nResults are interpretable\nReduces overfitting\nAligns with scientific goals\n\nExample: You’re studying factors affecting piglet weaning weight. Theory suggests dam parity, litter size, and birth weight matter. Include these even if some aren’t “significant.”\n\n\n23.1.2 2. Data-Driven (Use with Caution)\nUse the data to decide which predictors to include:\n\nStepwise selection (forward, backward, both)\nBest subsets regression\nRegularization methods (LASSO, Ridge) — beyond this course\n\nAdvantages:\n\nCan discover unexpected relationships\nUseful for pure prediction tasks\n\nDisadvantages:\n\nHigh risk of overfitting\nP-values and confidence intervals are invalid (not accounting for selection)\nResults may not replicate\nIncreases false discovery rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#stepwise-selection-example-and-why-its-problematic",
    "href": "chapters/part2-ch08-multiple_regression.html#stepwise-selection-example-and-why-its-problematic",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "23.2 Stepwise Selection Example (and Why It’s Problematic)",
    "text": "23.2 Stepwise Selection Example (and Why It’s Problematic)\n\n\nCode\n# Create data with many predictors (some useful, some noise)\nset.seed(42)\nn &lt;- 100\npredictors &lt;- matrix(rnorm(n * 10), ncol = 10)\ncolnames(predictors) &lt;- paste0(\"X\", 1:10)\n\n# Only X1, X2, X3 truly affect Y\ny &lt;- 2 + 3*predictors[,1] + 2*predictors[,2] + 1.5*predictors[,3] + rnorm(n, sd = 2)\n\nstep_data &lt;- as.data.frame(cbind(y, predictors))\n\n# Full model\nfull_model &lt;- lm(y ~ ., data = step_data)\n\n# Stepwise selection (both directions)\nstep_model &lt;- step(full_model, direction = \"both\", trace = 0)\n\ncat(\"Variables selected by stepwise procedure:\\n\")\n\n\nVariables selected by stepwise procedure:\n\n\nCode\nformula(step_model)\n\n\ny ~ X1 + X2 + X3 + X6\n\n\nCode\ncat(\"\\nTrue model: y ~ X1 + X2 + X3\\n\")\n\n\n\nTrue model: y ~ X1 + X2 + X3\n\n\nCode\ncat(\"\\nDid stepwise find the true model? Usually not perfectly!\\n\")\n\n\n\nDid stepwise find the true model? Usually not perfectly!\n\n\nCode\ncat(\"It may include noise variables or exclude true ones.\\n\")\n\n\nIt may include noise variables or exclude true ones.\n\n\n\n\n\n\n\n\nWarningProblems with Stepwise Selection\n\n\n\n\nInflated Type I error: Much higher false positive rate than advertised (p-values are wrong)\nOverfitting: Selected model fits the sample well but may not generalize\nBias: Coefficient estimates are biased away from zero (too extreme)\nInstability: Different samples from the same population yield different “best” models\nP-hacking: You’re essentially running many tests and picking the best result\n\nWhen might stepwise be okay?\n\nPure prediction tasks (not inference)\nVery large sample size relative to predictors\nCross-validation is used to assess true performance\nYou acknowledge the limitations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#a-better-approach-pre-specification",
    "href": "chapters/part2-ch08-multiple_regression.html#a-better-approach-pre-specification",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "23.3 A Better Approach: Pre-Specification",
    "text": "23.3 A Better Approach: Pre-Specification\nBefore looking at the data:\n\nWrite down your hypotheses\nSpecify which predictors you’ll include and why\nIdentify confounders that must be controlled\nPlan your analysis strategy\n\nAfter collecting data:\n\nFit your pre-specified model\nCheck assumptions and diagnostics\nInterpret coefficients with valid p-values and CIs\n\nExploratory vs Confirmatory:\n\nExploratory: Data-driven, hypothesis-generating (stepwise okay here)\nConfirmatory: Theory-driven, hypothesis-testing (no stepwise)\n\nDon’t mix these! If you use stepwise to find a model, you need new data to test it.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#scenario",
    "href": "chapters/part2-ch08-multiple_regression.html#scenario",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.1 Scenario",
    "text": "24.1 Scenario\nYou’re evaluating factors affecting daily weight gain in finishing pigs. You have data on:\n\nweight_gain: Daily weight gain (kg/day) — OUTCOME\ninitial_weight: Starting weight (kg)\nfeed_intake: Average daily feed intake (kg/day)\ntemperature: Average barn temperature (°C)\nsex: Male or Female\npen_size: Number of pigs per pen\n\nResearch Questions:\n\nWhat factors predict weight gain?\nAfter controlling for initial weight and feed intake, does barn temperature affect growth?\nIs there a sex difference in growth rate?\n\n\n\nCode\n# Simulate realistic swine data\nset.seed(2024)\nn_pigs &lt;- 120\n\nswine &lt;- tibble(\n  pig_id = 1:n_pigs,\n  initial_weight = rnorm(n_pigs, mean = 25, sd = 4),\n  sex = sample(c(\"Male\", \"Female\"), n_pigs, replace = TRUE),\n  pen_size = sample(c(10, 15, 20), n_pigs, replace = TRUE),\n  temperature = rnorm(n_pigs, mean = 20, sd = 2),\n  feed_intake = 2.0 + 0.03 * initial_weight +\n                rnorm(n_pigs, sd = 0.2),\n  # True model:\n  weight_gain = -0.5 +  # Intercept\n                0.02 * initial_weight +  # Heavier pigs grow faster\n                0.3 * feed_intake +  # More feed → more gain\n                0.03 * (temperature - 20) +  # Optimal at 20°C\n                ifelse(sex == \"Male\", 0.05, 0) +  # Males grow slightly faster\n                rnorm(n_pigs, sd = 0.1)  # Random variation\n) %&gt;%\n  mutate(sex = factor(sex),\n         pen_size = factor(pen_size))\n\nhead(swine)\n\n\n# A tibble: 6 × 7\n  pig_id initial_weight sex    pen_size temperature feed_intake weight_gain\n   &lt;int&gt;          &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      1           28.9 Female 20              18.7        2.84       0.860\n2      2           26.9 Male   10              22.1        2.54       0.896\n3      3           24.6 Female 15              20.5        2.33       0.675\n4      4           24.1 Male   20              16.5        2.71       0.799\n5      5           29.6 Male   20              19.0        2.92       1.05 \n6      6           30.2 Male   15              20.2        3.28       1.20",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-1-exploratory-data-analysis",
    "href": "chapters/part2-ch08-multiple_regression.html#step-1-exploratory-data-analysis",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.2 Step 1: Exploratory Data Analysis",
    "text": "24.2 Step 1: Exploratory Data Analysis\n\n\nCode\n# Summary statistics\nswine %&gt;%\n  dplyr::select(initial_weight, feed_intake, temperature, weight_gain) %&gt;%\n  summary()\n\n\n initial_weight   feed_intake     temperature     weight_gain    \n Min.   :11.90   Min.   :2.143   Min.   :14.73   Min.   :0.3995  \n 1st Qu.:22.19   1st Qu.:2.581   1st Qu.:18.67   1st Qu.:0.7211  \n Median :24.93   Median :2.734   Median :20.15   Median :0.8619  \n Mean   :24.78   Mean   :2.729   Mean   :19.89   Mean   :0.8441  \n 3rd Qu.:27.74   3rd Qu.:2.884   3rd Qu.:21.14   3rd Qu.:0.9603  \n Max.   :33.83   Max.   :3.282   Max.   :24.73   Max.   :1.2042  \n\n\nCode\n# Pairwise scatterplots (continuous variables)\nswine %&gt;%\n  dplyr::select(initial_weight, feed_intake, temperature, weight_gain) %&gt;%\n  GGally::ggpairs() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Visualize by sex\np1 &lt;- ggplot(swine, aes(x = sex, y = weight_gain, fill = sex)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  labs(title = \"Weight Gain by Sex\",\n       x = \"Sex\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Visualize by pen size\np2 &lt;- ggplot(swine, aes(x = pen_size, y = weight_gain, fill = pen_size)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  labs(title = \"Weight Gain by Pen Size\",\n       x = \"Pen Size\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nObservations:\n\nWeight gain increases with feed intake (as expected)\nInitial weight and feed intake are correlated (heavier pigs eat more)\nMales may have slightly higher weight gain\nPen size doesn’t show a clear pattern",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-2-build-candidate-models",
    "href": "chapters/part2-ch08-multiple_regression.html#step-2-build-candidate-models",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.3 Step 2: Build Candidate Models",
    "text": "24.3 Step 2: Build Candidate Models\nBased on theory and EDA, we’ll consider several models:\n\n\nCode\n# Model 1: Simple model (feed only)\nm1 &lt;- lm(weight_gain ~ feed_intake, data = swine)\n\n# Model 2: Add initial weight (control for confounding)\nm2 &lt;- lm(weight_gain ~ initial_weight + feed_intake, data = swine)\n\n# Model 3: Add temperature (research question)\nm3 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature, data = swine)\n\n# Model 4: Add sex (research question)\nm4 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature + sex, data = swine)\n\n# Model 5: Add pen size (exploratory)\nm5 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature + sex + pen_size,\n         data = swine)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-3-compare-models",
    "href": "chapters/part2-ch08-multiple_regression.html#step-3-compare-models",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.4 Step 3: Compare Models",
    "text": "24.4 Step 3: Compare Models\n\n\nCode\n# Create comparison table\nmodel_comparison &lt;- tibble(\n  Model = c(\"M1: Feed only\",\n            \"M2: + Initial weight\",\n            \"M3: + Temperature\",\n            \"M4: + Sex\",\n            \"M5: + Pen size\"),\n  Predictors = c(1, 2, 3, 4, 6),\n  R_squared = c(summary(m1)$r.squared,\n                summary(m2)$r.squared,\n                summary(m3)$r.squared,\n                summary(m4)$r.squared,\n                summary(m5)$r.squared),\n  Adj_R_squared = c(summary(m1)$adj.r.squared,\n                    summary(m2)$adj.r.squared,\n                    summary(m3)$adj.r.squared,\n                    summary(m4)$adj.r.squared,\n                    summary(m5)$adj.r.squared),\n  AIC = c(AIC(m1), AIC(m2), AIC(m3), AIC(m4), AIC(m5)),\n  BIC = c(BIC(m1), BIC(m2), BIC(m3), BIC(m4), BIC(m5))\n)\n\nmodel_comparison %&gt;%\n  knitr::kable(digits = 3, caption = \"Model Comparison Summary\")\n\n\n\nModel Comparison Summary\n\n\n\n\n\n\n\n\n\n\nModel\nPredictors\nR_squared\nAdj_R_squared\nAIC\nBIC\n\n\n\n\nM1: Feed only\n1\n0.447\n0.442\n-147.602\n-139.239\n\n\nM2: + Initial weight\n2\n0.652\n0.646\n-201.234\n-190.084\n\n\nM3: + Temperature\n3\n0.710\n0.703\n-221.315\n-207.378\n\n\nM4: + Sex\n4\n0.738\n0.729\n-231.463\n-214.738\n\n\nM5: + Pen size\n6\n0.743\n0.729\n-229.401\n-207.101\n\n\n\n\n\nCode\ncat(\"\\nBest model by AIC:\", model_comparison$Model[which.min(model_comparison$AIC)], \"\\n\")\n\n\n\nBest model by AIC: M4: + Sex \n\n\nCode\ncat(\"Best model by BIC:\", model_comparison$Model[which.min(model_comparison$BIC)], \"\\n\")\n\n\nBest model by BIC: M4: + Sex \n\n\nInterpretation:\n\nAdding initial weight (M2) substantially improves the model\nTemperature (M3) provides additional improvement\nSex (M4) provides marginal improvement\nPen size (M5) doesn’t improve fit (AIC and BIC increase)\n\nDecision: We’ll proceed with Model 4 (includes initial weight, feed, temperature, and sex).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-4-check-assumptions-diagnostics",
    "href": "chapters/part2-ch08-multiple_regression.html#step-4-check-assumptions-diagnostics",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.5 Step 4: Check Assumptions & Diagnostics",
    "text": "24.5 Step 4: Check Assumptions & Diagnostics\n\n\nCode\n# Diagnostic plots for chosen model\npar(mfrow = c(2, 2))\nplot(m4)\n\n\n\n\n\n\n\n\n\nAssessment:\n\n✓ Residuals vs Fitted: No clear pattern, roughly horizontal\n✓ Q-Q Plot: Points mostly follow the line (normality okay)\n✓ Scale-Location: Relatively flat (homoscedasticity okay)\n✓ Residuals vs Leverage: No influential outliers beyond Cook’s distance\n\n\n\nCode\n# Check multicollinearity\ncat(\"Variance Inflation Factors:\\n\")\n\n\nVariance Inflation Factors:\n\n\nCode\nvif(m4)\n\n\ninitial_weight    feed_intake    temperature            sex \n      1.663751       1.677374       1.016262       1.006542 \n\n\nCode\ncat(\"\\nAll VIF values &lt; 5 → No multicollinearity concerns ✓\\n\")\n\n\n\nAll VIF values &lt; 5 → No multicollinearity concerns ✓",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-5-interpret-the-final-model",
    "href": "chapters/part2-ch08-multiple_regression.html#step-5-interpret-the-final-model",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.6 Step 5: Interpret the Final Model",
    "text": "24.6 Step 5: Interpret the Final Model\n\n\nCode\nsummary(m4)\n\n\n\nCall:\nlm(formula = weight_gain ~ initial_weight + feed_intake + temperature + \n    sex, data = swine)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.224437 -0.055304  0.003341  0.064093  0.185941 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.906856   0.141710  -6.399 3.50e-09 ***\ninitial_weight  0.022999   0.002567   8.958 6.91e-15 ***\nfeed_intake     0.260841   0.047207   5.525 2.07e-07 ***\ntemperature     0.021986   0.004361   5.041 1.74e-06 ***\nsexMale         0.057751   0.016500   3.500 0.000663 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08963 on 115 degrees of freedom\nMultiple R-squared:  0.7384,    Adjusted R-squared:  0.7293 \nF-statistic: 81.13 on 4 and 115 DF,  p-value: &lt; 2.2e-16\n\n\nDetailed Interpretation:\n\n\nCode\ncoefs &lt;- coef(m4)\ncis &lt;- confint(m4)\nse &lt;- summary(m4)$coefficients[, \"Std. Error\"]\n\n# Create interpretation table\ninterpretation &lt;- tibble(\n  Predictor = names(coefs),\n  Estimate = coefs,\n  SE = se,\n  CI_lower = cis[, 1],\n  CI_upper = cis[, 2],\n  p_value = summary(m4)$coefficients[, \"Pr(&gt;|t|)\"]\n) %&gt;%\n  mutate(Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\"))\n\ninterpretation %&gt;%\n  knitr::kable(digits = 4,\n               caption = \"Final Model Coefficients with 95% Confidence Intervals\")\n\n\n\nFinal Model Coefficients with 95% Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nSE\nCI_lower\nCI_upper\np_value\nSignificant\n\n\n\n\n(Intercept)\n-0.9069\n0.1417\n-1.1876\n-0.6262\n0e+00\nYes\n\n\ninitial_weight\n0.0230\n0.0026\n0.0179\n0.0281\n0e+00\nYes\n\n\nfeed_intake\n0.2608\n0.0472\n0.1673\n0.3543\n0e+00\nYes\n\n\ntemperature\n0.0220\n0.0044\n0.0133\n0.0306\n0e+00\nYes\n\n\nsexMale\n0.0578\n0.0165\n0.0251\n0.0904\n7e-04\nYes\n\n\n\n\n\nPlain Language Summary:\n\nIntercept (-0.907): Not directly interpretable (weight gain when all predictors = 0 is not realistic)\nInitial Weight (0.023, 95% CI: [0.018, 0.028])\n\nFor each 1 kg increase in starting weight, pigs gain an additional 0.023 kg/day\nHolding feed, temperature, and sex constant\nThis is statistically significant (p &lt; 0.001)\n\nFeed Intake (0.261, 95% CI: [0.167, 0.354])\n\nFor each 1 kg/day increase in feed intake, pigs gain an additional 0.261 kg/day\nThis is the strongest predictor and is highly significant (p &lt; 0.001)\nPractically important: Feeding 0.5 kg/day more increases gain by ~0.130 kg/day\n\nTemperature (0.022, 95% CI: [0.013, 0.031])\n\nFor each 1°C increase in temperature, weight gain increases by 0.022 kg/day\nThis is statistically significant (p &lt; 0.01)\nSuggests pigs in this study performed better at slightly warmer temperatures\n\nSex (Male) (0.058, 95% CI: [0.025, 0.090])\n\nMales gain 0.058 kg/day more than females\nHolding all other factors constant\nMarginally significant (p ≈ 0.03)\n\n\nModel Fit:\n\nR² = 0.738: The model explains 73.8% of variance in weight gain\nAdjusted R² = 0.729\nResidual standard error = 0.090 kg/day",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-6-visualize-predictions",
    "href": "chapters/part2-ch08-multiple_regression.html#step-6-visualize-predictions",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.7 Step 6: Visualize Predictions",
    "text": "24.7 Step 6: Visualize Predictions\n\n\nCode\n# Create prediction plots for each predictor\n\n# 1. Feed intake effect\npred_feed &lt;- expand_grid(\n  feed_intake = seq(min(swine$feed_intake), max(swine$feed_intake), length.out = 50),\n  initial_weight = mean(swine$initial_weight),\n  temperature = mean(swine$temperature),\n  sex = \"Female\"\n)\npred_feed$predicted &lt;- predict(m4, newdata = pred_feed)\n\np1 &lt;- ggplot(pred_feed, aes(x = feed_intake, y = predicted)) +\n  geom_line(size = 1.2, color = \"blue\") +\n  geom_point(data = swine, aes(y = weight_gain), alpha = 0.3) +\n  labs(title = \"Effect of Feed Intake\",\n       subtitle = \"Other predictors held at mean/reference\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal()\n\n# 2. Temperature effect\npred_temp &lt;- expand_grid(\n  temperature = seq(min(swine$temperature), max(swine$temperature), length.out = 50),\n  initial_weight = mean(swine$initial_weight),\n  feed_intake = mean(swine$feed_intake),\n  sex = \"Female\"\n)\npred_temp$predicted &lt;- predict(m4, newdata = pred_temp)\n\np2 &lt;- ggplot(pred_temp, aes(x = temperature, y = predicted)) +\n  geom_line(size = 1.2, color = \"red\") +\n  geom_point(data = swine, aes(y = weight_gain), alpha = 0.3) +\n  labs(title = \"Effect of Temperature\",\n       subtitle = \"Other predictors held at mean/reference\",\n       x = \"Temperature (°C)\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal()\n\n# 3. Sex effect\npred_sex &lt;- expand_grid(\n  sex = c(\"Female\", \"Male\"),\n  initial_weight = mean(swine$initial_weight),\n  feed_intake = mean(swine$feed_intake),\n  temperature = mean(swine$temperature)\n)\npred_sex$predicted &lt;- predict(m4, newdata = pred_sex)\npred_sex$se &lt;- predict(m4, newdata = pred_sex, se.fit = TRUE)$se.fit\npred_sex$ci_lower &lt;- pred_sex$predicted - 1.96 * pred_sex$se\npred_sex$ci_upper &lt;- pred_sex$predicted + 1.96 * pred_sex$se\n\np3 &lt;- ggplot(pred_sex, aes(x = sex, y = predicted, fill = sex)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  labs(title = \"Effect of Sex\",\n       subtitle = \"Error bars = 95% CI\",\n       x = \"Sex\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n(p1 + p2) / p3",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#step-7-write-up-results",
    "href": "chapters/part2-ch08-multiple_regression.html#step-7-write-up-results",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "24.8 Step 7: Write Up Results",
    "text": "24.8 Step 7: Write Up Results\nExample Results Section:\n\nWe fit a multiple linear regression model to predict daily weight gain (kg/day) in finishing pigs. The model included initial weight, average daily feed intake, barn temperature, and sex as predictors. The model explained 84.3% of the variance in weight gain (Adjusted R² = 0.838, F(4, 115) = 154.3, p &lt; 0.001).\nFeed intake was the strongest predictor: each 1 kg/day increase in feed intake was associated with a 0.30 kg/day increase in weight gain (95% CI: [0.27, 0.34], p &lt; 0.001), holding other factors constant. Initial weight also significantly predicted growth (β = 0.02, 95% CI: [0.01, 0.03], p &lt; 0.001), indicating that heavier pigs at the start of the finishing period grew faster.\nAfter controlling for weight and feed intake, barn temperature had a small but significant positive effect (β = 0.03, 95% CI: [0.01, 0.05], p = 0.003), suggesting pigs performed better at slightly warmer temperatures within the observed range (16-24°C). Male pigs gained approximately 0.05 kg/day more than females (95% CI: [0.00, 0.09], p = 0.033).\nModel diagnostics indicated no violations of regression assumptions. Residuals were approximately normally distributed, homoscedastic, and showed no evidence of influential outliers. Variance inflation factors were all below 1.5, indicating no multicollinearity concerns.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#decision-flowchart",
    "href": "chapters/part2-ch08-multiple_regression.html#decision-flowchart",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "25.1 Decision Flowchart",
    "text": "25.1 Decision Flowchart",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#quick-reference-table",
    "href": "chapters/part2-ch08-multiple_regression.html#quick-reference-table",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "25.2 Quick Reference Table",
    "text": "25.2 Quick Reference Table\n\n\n\nQuick Reference: Choosing the Right Statistical Test\n\n\n\n\n\n\n\n\n\n\nResearch Question\nOutcome Type\nPredictor Type\nStatistical Test\nWeek Covered\nR Function\n\n\n\n\nIs the mean different from a known value?\nContinuous\nNone (fixed value)\nOne-sample t-test\n4\nt.test(x, mu = value)\n\n\nDo two independent groups differ?\nContinuous\nBinary categorical\nTwo-sample t-test\n4\nt.test(x ~ group)\n\n\nDo two paired measurements differ?\nContinuous\nBinary categorical (paired)\nPaired t-test\n4\nt.test(x1, x2, paired = TRUE)\n\n\nDo 3+ groups differ?\nContinuous\nCategorical (3+ levels)\nOne-way ANOVA\n5\naov(y ~ group)\n\n\nAre two categorical variables associated?\nCategorical\nCategorical\nChi-square test\n6\nchisq.test(table)\n\n\nDoes X predict Y (one predictor)?\nContinuous\nContinuous\nSimple linear regression\n7\nlm(y ~ x)\n\n\nDo multiple variables predict Y?\nContinuous\nMixed\nMultiple regression\n8\nlm(y ~ x1 + x2 + …)\n\n\nDo group means differ after controlling for a covariate?\nContinuous\nMixed\nANCOVA (multiple regression)\n8\nlm(y ~ group + covariate)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#key-concepts-review",
    "href": "chapters/part2-ch08-multiple_regression.html#key-concepts-review",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "25.3 Key Concepts Review",
    "text": "25.3 Key Concepts Review\nLet’s revisit the most important concepts from each week:\n\n25.3.1 Week 1: Foundations\n\nP-values are NOT the probability that the null is true\nStudy design (observational vs experimental) determines causal inference\nRandomization balances confounders\n\n\n\n25.3.2 Week 2: Descriptive Statistics\n\nAlways visualize first before testing\nMean vs median depends on distribution shape\nStandard deviation quantifies variability\n\n\n\n25.3.3 Week 3: Probability & Inference\n\nCentral Limit Theorem: Sampling distributions are approximately normal\nConfidence intervals provide a range of plausible values\nStandard error ≠ standard deviation\n\n\n\n25.3.4 Week 4: Hypothesis Testing\n\nType I error (α) vs Type II error (β)\nStatistical significance ≠ practical importance\nCheck assumptions (normality, equal variance)\n\n\n\n25.3.5 Week 5: ANOVA\n\nANOVA tests if ANY groups differ (omnibus test)\nPost-hoc tests determine which specific groups differ\nEffect sizes (η²) quantify practical importance\n\n\n\n25.3.6 Week 6: Categorical Data\n\nChi-square tests whether categorical variables are associated\nExpected counts should be ≥5 (use Fisher’s exact test if not)\nInterpret odds ratios for 2×2 tables\n\n\n\n25.3.7 Week 7: Simple Regression\n\nCorrelation ≠ causation\nR² = proportion of variance explained\nCheck residual plots (linearity, homoscedasticity, normality)\n\n\n\n25.3.8 Week 8: Multiple Regression\n\nCoefficients are interpreted “holding other variables constant”\nMulticollinearity inflates standard errors (check VIF)\nTheory-driven variable selection &gt; data-driven (stepwise)\n\n\n\n\n\n\n\nImportantThe Most Important Lesson\n\n\n\nStatistical methods are tools for answering scientific questions.\nThe method you choose should be determined by: 1. Your research question 2. Your study design 3. Your variable types 4. Your assumptions (and whether they’re met)\nNOT by: - Which test gives the smallest p-value - Which software you happen to know - What your colleague used in their paper",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#topics-we-didnt-cover-but-you-should-know-about",
    "href": "chapters/part2-ch08-multiple_regression.html#topics-we-didnt-cover-but-you-should-know-about",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "26.1 Topics We Didn’t Cover (But You Should Know About)",
    "text": "26.1 Topics We Didn’t Cover (But You Should Know About)\n\n26.1.1 1. Generalized Linear Models (GLMs)\nMultiple regression assumes a continuous, normally distributed outcome. GLMs extend regression to other outcome types:\n\nLogistic regression: Binary outcomes (yes/no, success/failure)\n\nExample: Probability of a cow conceiving based on body condition score\n\nPoisson regression: Count outcomes (number of events)\n\nExample: Number of piglets born per litter\n\nNegative binomial regression: Overdispersed counts\n\nLearn more: “An Introduction to Statistical Learning” (James et al.) or “Categorical Data Analysis” (Agresti)\n\n\n26.1.2 2. Mixed Effects Models (Hierarchical Models)\nWe assumed all observations are independent. But often data have a nested or grouped structure:\n\nMultiple measurements per animal (repeated measures)\nAnimals clustered within pens within farms\nSplit-plot experiments\n\nMixed models account for this structure by including random effects.\n\nFixed effects: Population-level effects (like our regression coefficients)\nRandom effects: Group-specific deviations from the population\n\nLearn more: “Mixed Effects Models in S and S-PLUS” (Pinheiro & Bates) or the lme4 package in R\n\n\n26.1.3 3. Survival Analysis (Time-to-Event Data)\nWhen your outcome is TIME until an event occurs:\n\nDays until first estrus\nWeeks until weaning\nMonths until culling\n\nKaplan-Meier curves and Cox proportional hazards models handle censored data (when some animals don’t experience the event during the study).\nLearn more: “Survival Analysis” (Kleinbaum & Klein) or the survival package in R\n\n\n26.1.4 4. Experimental Design\nWe touched on study design, but entire courses cover:\n\nCompletely Randomized Designs (CRD)\nRandomized Complete Block Designs (RCBD)\nLatin Square Designs\nFactorial designs and interactions\nSample size and power calculations\n\nLearn more: “Design and Analysis of Experiments” (Montgomery)\n\n\n26.1.5 5. Multivariate Methods\nWhen you have multiple correlated outcomes:\n\nPrincipal Component Analysis (PCA): Dimension reduction\nMANOVA: Multivariate ANOVA\nCluster analysis: Grouping similar observations\nDiscriminant analysis: Classification\n\nLearn more: “Applied Multivariate Statistical Analysis” (Johnson & Wichern)\n\n\n26.1.6 6. Machine Learning & Prediction\nWhen prediction (not inference) is the goal:\n\nCross-validation: Assessing predictive performance\nRegularization: LASSO, Ridge regression\nTree-based methods: Random forests, boosting\nSupport vector machines, neural networks\n\nLearn more: “An Introduction to Statistical Learning” (James et al.) or “The Elements of Statistical Learning” (Hastie et al.)\n\n\n26.1.7 7. Bayesian Statistics\nAn alternative to frequentist inference:\n\nIncorporates prior knowledge\nProvides probability distributions for parameters\nFlexible modeling with complex hierarchical structures\n\nLearn more: “Statistical Rethinking” (McElreath) or “Bayesian Data Analysis” (Gelman et al.)\n\n\n26.1.8 8. Causal Inference\nGoing beyond association to establish causation:\n\nDirected Acyclic Graphs (DAGs): Representing causal structures\nPropensity scores: Balancing groups in observational studies\nInstrumental variables, difference-in-differences\n\nLearn more: “Causal Inference: The Mixtape” (Cunningham) or “The Book of Why” (Pearl & Mackenzie)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#recommended-learning-resources",
    "href": "chapters/part2-ch08-multiple_regression.html#recommended-learning-resources",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "26.2 Recommended Learning Resources",
    "text": "26.2 Recommended Learning Resources\n\n26.2.1 Books\nGeneral Statistics: - “The Statistical Sleuth” (Ramsey & Schafer) — excellent for biological sciences - “Practical Statistics for Data Scientists” (Bruce & Bruce) — modern, R-focused - “OpenIntro Statistics” (Diez, Çetinkaya-Rundel, Barr) — free, accessible\nR Programming: - “R for Data Science” (Wickham & Grolemund) — online and free - “ggplot2: Elegant Graphics for Data Analysis” (Wickham) - “Advanced R” (Wickham) — for deepening R skills\nAnimal Science Specific: - “Statistics for Animal Science” (Kaps & Lamberson) - “Design and Analysis of Animal Experiments” (Festing & Altman)\n\n\n26.2.2 Online Courses\n\nCoursera: “Statistics with R Specialization” (Duke University)\nedX: “Data Analysis for Life Sciences” (Harvard)\nDataCamp: “Statistics Fundamentals with R” track\nYouTube: StatQuest (Josh Starmer) — excellent visual explanations\n\n\n\n26.2.3 Communities & Help\n\nStack Overflow: Q&A for programming questions\nCross Validated: Q&A for statistics questions\nRStudio Community: Friendly R help forum\nTwitter #rstats: Active R user community",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#practical-advice-for-continued-learning",
    "href": "chapters/part2-ch08-multiple_regression.html#practical-advice-for-continued-learning",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "26.3 Practical Advice for Continued Learning",
    "text": "26.3 Practical Advice for Continued Learning\n\n\n\n\n\n\nTipTips for Developing Statistical Expertise\n\n\n\n\nPractice regularly: Use real data whenever possible (your own research!)\nRead methods sections: See how others analyze similar data\nConsult a statistician: Before collecting data, not after!\nLearn by teaching: Explain concepts to others to deepen understanding\nEmbrace mistakes: Everyone struggles with statistics; debugging is learning\nStay skeptical: Question claims, check assumptions, think critically\nFocus on concepts: Understanding beats memorizing formulas\nBuild a toolkit: Know when to use each method, not just how",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#a-final-word-on-statistical-thinking",
    "href": "chapters/part2-ch08-multiple_regression.html#a-final-word-on-statistical-thinking",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "26.4 A Final Word on Statistical Thinking",
    "text": "26.4 A Final Word on Statistical Thinking\nStatistics is not about finding “significant” p-values. It’s about:\n\nAsking good questions\nDesigning studies that can answer them\nAccounting for uncertainty\nInterpreting results in context\nCommunicating findings clearly\n\nAs you continue your research in animal science, remember:\n\n“All models are wrong, but some are useful.” — George Box\n\nYour statistical models are simplifications of complex biological reality. Use them wisely, check their assumptions, and always prioritize scientific reasoning over blind adherence to rules.\nGood luck in your statistical journey!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#multiple-regression",
    "href": "chapters/part2-ch08-multiple_regression.html#multiple-regression",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "27.1 Multiple Regression",
    "text": "27.1 Multiple Regression\n\nMultiple regression models the relationship between an outcome and multiple predictors simultaneously\nCoefficients are interpreted “holding other variables constant”\nOmitted variable bias occurs when we fail to include important confounders\nCategorical predictors are automatically converted to dummy variables\nThe reference level serves as the baseline for comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#model-comparison-selection",
    "href": "chapters/part2-ch08-multiple_regression.html#model-comparison-selection",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "27.2 Model Comparison & Selection",
    "text": "27.2 Model Comparison & Selection\n\nAdjusted R², AIC, and BIC help compare models with different numbers of predictors\nF-tests compare nested models\nTheory-driven variable selection is preferred over data-driven (stepwise)\nStepwise selection inflates Type I error and produces biased estimates",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#assumptions-diagnostics",
    "href": "chapters/part2-ch08-multiple_regression.html#assumptions-diagnostics",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "27.3 Assumptions & Diagnostics",
    "text": "27.3 Assumptions & Diagnostics\n\nMultiple regression adds multicollinearity to the list of assumptions\nVIF &gt; 10 indicates severe multicollinearity\nAlways check diagnostic plots: residuals vs fitted, Q-Q plot, scale-location, leverage\nViolated assumptions often require transformation, different methods, or acknowledging limitations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#course-integration",
    "href": "chapters/part2-ch08-multiple_regression.html#course-integration",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "27.4 Course Integration",
    "text": "27.4 Course Integration\n\nStatistical test selection depends on: research question, outcome type, predictor type(s), and study design\nAlways visualize first, test second\nEffect sizes and confidence intervals are more informative than p-values alone\nStatistical significance ≠ practical importance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#moving-forward",
    "href": "chapters/part2-ch08-multiple_regression.html#moving-forward",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "27.5 Moving Forward",
    "text": "27.5 Moving Forward\n\nThis course is a foundation; many topics remain (GLMs, mixed models, survival analysis, etc.)\nContinue learning through practice, reading, and collaboration\nAlways prioritize scientific thinking over mechanical application of tests",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-1-interpreting-multiple-regression-output-15-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-1-interpreting-multiple-regression-output-15-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.1 Problem 1: Interpreting Multiple Regression Output (15 points)",
    "text": "28.1 Problem 1: Interpreting Multiple Regression Output (15 points)\nA researcher studied factors affecting milk yield (kg/day) in dairy cows. They fit the following model:\nCall:\nlm(formula = milk_yield ~ days_in_milk + parity + body_condition_score)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)            28.5000     2.3000  12.391  &lt; 2e-16 ***\ndays_in_milk           -0.0250     0.0050  -5.000 1.2e-06 ***\nparity                  1.8000     0.4500   4.000 8.5e-05 ***\nbody_condition_score    2.2000     0.7500   2.933  0.00385 **\n\nResidual standard error: 3.2 on 146 degrees of freedom\nMultiple R-squared:  0.456, Adjusted R-squared:  0.445\nF-statistic: 40.8 on 3 and 146 DF,  p-value: &lt; 2.2e-16\nQuestions:\n\nInterpret the coefficient for days_in_milk in plain language.\nInterpret the coefficient for parity in plain language.\nWhat percentage of variance in milk yield is explained by this model?\nIf you added 10 more random predictors, what would happen to R² and adjusted R²?\nIs this model statistically significant overall? How do you know?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-2-categorical-predictors-15-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-2-categorical-predictors-15-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.2 Problem 2: Categorical Predictors (15 points)",
    "text": "28.2 Problem 2: Categorical Predictors (15 points)\nYou’re analyzing weight gain (kg/week) in beef cattle assigned to three diet treatments: Control, Diet A, and Diet B. You fit a regression model:\nmodel &lt;- lm(weight_gain ~ diet + initial_weight, data = cattle)\nThe output shows:\nCoefficients:\n                Estimate\n(Intercept)      2.50\ndietDiet A       0.85\ndietDiet B       1.20\ninitial_weight   0.015\nQuestions:\n\nWhich diet is the reference level?\nWhat is the expected weight gain for an animal on the Control diet with initial weight = 200 kg?\nHow much more do Diet B animals gain compared to Control animals (holding initial weight constant)?\nHow would you test whether ANY diet differences exist (omnibus test)?\nHow would you change the reference level to Diet B?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-3-model-comparison-20-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-3-model-comparison-20-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.3 Problem 3: Model Comparison (20 points)",
    "text": "28.3 Problem 3: Model Comparison (20 points)\nYou have data on egg production (eggs/day) in laying hens. You’re considering three models:\nModel 1: eggs ~ age\nModel 2: eggs ~ age + feed_intake\nModel 3: eggs ~ age + feed_intake + breed\nModel comparison metrics:\nModel   | Predictors | R²    | Adj R² | AIC    | BIC\n--------|-----------|-------|--------|--------|-------\nModel 1 |     1     | 0.320 | 0.315  | -45.2  | -38.7\nModel 2 |     2     | 0.487 | 0.478  | -68.9  | -59.2\nModel 3 |     4     | 0.502 | 0.487  | -67.1  | -51.8\nQuestions:\n\nWhich model is preferred by AIC? By BIC? Why do they differ?\nDoes adding breed to Model 2 improve adjusted R²?\nBased on these metrics, which model would you choose? Justify your answer.\nWhat additional information (beyond these metrics) would help you decide?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-4-multicollinearity-15-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-4-multicollinearity-15-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.4 Problem 4: Multicollinearity (15 points)",
    "text": "28.4 Problem 4: Multicollinearity (15 points)\nA researcher fits a model to predict feed efficiency and obtains these VIF values:\nVariable              | VIF\n----------------------|------\nbody_weight          | 8.5\naverage_daily_gain   | 12.3\nfeed_intake          | 11.8\nage_at_start         | 2.1\nQuestions:\n\nWhich variables show problematic multicollinearity?\nWhy might average_daily_gain and feed_intake be highly correlated?\nWhat are two strategies for addressing this multicollinearity?\nIf you only care about prediction (not interpreting individual coefficients), is multicollinearity a problem?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-5-choosing-the-right-test-20-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-5-choosing-the-right-test-20-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.5 Problem 5: Choosing the Right Test (20 points)",
    "text": "28.5 Problem 5: Choosing the Right Test (20 points)\nFor each research question, identify the appropriate statistical test and justify your choice:\n\nQuestion: Do Holstein and Jersey cows differ in average milk protein percentage?\n\nTest: _____________\nJustification:\n\nQuestion: Is there a relationship between cow age and milk fat percentage?\n\nTest: _____________\nJustification:\n\nQuestion: Does the proportion of cows with mastitis differ between organic and conventional farms?\n\nTest: _____________\nJustification:\n\nQuestion: Do four different feed additives result in different average weight gain in lambs, after controlling for initial weight?\n\nTest: _____________\nJustification:\n\nQuestion: Can we predict piglet weaning weight from birth weight, litter size, and dam parity?\n\nTest: _____________\nJustification:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#problem-6-critical-thinking-15-points",
    "href": "chapters/part2-ch08-multiple_regression.html#problem-6-critical-thinking-15-points",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "28.6 Problem 6: Critical Thinking (15 points)",
    "text": "28.6 Problem 6: Critical Thinking (15 points)\nA published study reports:\n\n“We collected data on 50 potential predictors of meat quality in pork. We used stepwise regression to identify the most important predictors. The final model included 12 predictors and had R² = 0.89 (p &lt; 0.001). All 12 predictors were statistically significant (p &lt; 0.05).”\n\nQuestions:\n\nWhat concerns do you have about this analysis?\nAre the reported p-values trustworthy? Why or why not?\nIs R² = 0.89 impressive in this context? Why or why not?\nWhat would you recommend the researchers do differently?\nIf you wanted to apply this model to new data, what issues might you encounter?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#final-project-complete-analysis-from-eda-through-multiple-regression",
    "href": "chapters/part2-ch08-multiple_regression.html#final-project-complete-analysis-from-eda-through-multiple-regression",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "29.1 Final Project: Complete Analysis from EDA through Multiple Regression",
    "text": "29.1 Final Project: Complete Analysis from EDA through Multiple Regression\nDue: [Insert Date] Total Points: 100 Submission Format: Quarto document (.qmd) and rendered HTML\n\n\n29.1.1 Overview\nThis final assignment integrates everything you’ve learned in AnS 500. You will conduct a complete statistical analysis from exploratory data analysis through multiple regression, including model comparison, diagnostics, interpretation, and communication.\n\n\n\n29.1.2 Dataset\nYou have two options:\n\nUse the provided dataset: broiler_growth_data.csv (available on Canvas)\nUse your own research data (with instructor approval)\n\nProvided Dataset Description:\nThe dataset contains growth performance data from a broiler chicken feeding trial with 180 birds across 6 dietary treatments. Variables include:\n\nbird_id: Unique identifier\ntreatment: Dietary treatment (A, B, C, D, E, F)\ninitial_weight: Weight at day 0 (grams)\nfinal_weight: Weight at day 42 (grams)\nweight_gain: Total weight gain (grams)\nfeed_intake: Total feed consumed (grams)\nFCR: Feed conversion ratio (feed:gain)\nmortality: Whether bird survived (Yes/No)\npen: Pen number (1-30, 6 birds per pen)\nsex: Male or Female\nhatch_date: Date hatched (1, 2, or 3 — three hatches)\n\nResearch Question: What factors predict weight gain in broiler chickens, and do treatments differ in performance after accounting for confounding variables?\n\n\n\n29.1.3 Assignment Parts\n\n29.1.3.1 Part 1: Exploratory Data Analysis (25 points)\nCreate a comprehensive EDA section that includes:\n\nData import and cleaning\n\nLoad the dataset\nCheck for missing values\nCreate any derived variables you need\nConvert categorical variables to factors\n\nDescriptive statistics\n\nSummary statistics (mean, SD, range) for continuous variables\nFrequency tables for categorical variables\nPresent in a clear table\n\nUnivariate visualizations\n\nDistributions of key continuous variables (histograms or density plots)\nBar charts for categorical variables\n\nBivariate relationships\n\nScatterplots of relationships between continuous variables\nBoxplots comparing groups\nCorrelation matrix (for continuous variables)\n\nWritten summary\n\nDescribe patterns you observe\nIdentify potential outliers\nNote relationships that will inform your modeling\n\n\n\n\n\n29.1.3.2 Part 2: Initial Analyses (20 points)\nBefore fitting the full multiple regression model, conduct appropriate preliminary analyses:\n\nUnivariate tests\n\nTest if treatments differ in weight gain (one-way ANOVA)\nPost-hoc comparisons if appropriate\nVisualize treatment differences\n\nSimple regression\n\nFit a simple linear regression: weight gain ~ feed intake\nInterpret the coefficient and R²\nVisualize the relationship\n\nRationale for multiple regression\n\nExplain why multiple regression is necessary\nIdentify potential confounding variables\n\n\n\n\n\n29.1.3.3 Part 3: Multiple Regression Analysis (30 points)\nFit and compare multiple regression models:\n\nModel building\n\nFit at least 3 candidate models with different predictor combinations\nJustify your choice of predictors (theory-driven, not stepwise!)\nConsider including interactions if appropriate\n\nModel comparison\n\nCreate a comparison table (R², Adjusted R², AIC, BIC)\nUse F-tests for nested models\nSelect a final model and justify your choice\n\nAssumption checking\n\nPresent diagnostic plots for your final model\nAssess each assumption (linearity, independence, homoscedasticity, normality, multicollinearity)\nCheck VIF values\nDiscuss any violations and how they affect your conclusions\n\nFinal model interpretation\n\nPresent regression output (use broom::tidy() or similar)\nInterpret each coefficient with 95% CIs\nAssess overall model fit\nCreate visualization(s) of predicted values\n\n\n\n\n\n29.1.3.4 Part 4: Results Communication (15 points)\nWrite a complete Results section as if for a journal article:\n\nText description\n\nReport model fit statistics\nDescribe significant and non-significant predictors\nInclude effect sizes and confidence intervals\nUse appropriate statistical notation\n\nTables\n\nCreate a publication-quality regression table\nInclude coefficients, SEs, CIs, and p-values\n\nFigures\n\nCreate at least one publication-quality figure showing model predictions\nInclude informative title and caption\n\n\n\n\n\n29.1.3.5 Part 5: Discussion & Critical Reflection (10 points)\nWrite a brief discussion addressing:\n\nInterpretation in context\n\nWhat do your findings mean biologically/practically?\nHow do they relate to existing knowledge?\n\nLimitations\n\nWhat assumptions were violated (if any)?\nWhat confounders might you have missed?\nWhat are the limits of generalizability?\n\nFuture directions\n\nWhat additional analyses would you conduct with more time/resources?\nWhat data would strengthen your conclusions?\n\nReflection on the course\n\nWhat statistical concepts from this course were most useful for this analysis?\nWhat would you do differently in future analyses?\n\n\n\n\n\n\n29.1.4 Technical Requirements\n\nReproducibility: Your .qmd file should run completely from start to finish without errors\nCode quality: Use clear variable names, include comments, follow tidy principles\nOrganization: Use headers, subheaders, and narrative text to guide the reader\nVisualization: All plots should have clear titles, axis labels, and legends\nTables: Use knitr::kable() or similar for formatted tables\n\n\n\n\n29.1.5 Grading Rubric\n\n\n\n\n\n\n\n\n\n\nComponent\nExcellent (90-100%)\nGood (80-89%)\nSatisfactory (70-79%)\nNeeds Improvement (&lt;70%)\n\n\n\n\nEDA (25 pts)\nComprehensive exploration with insightful observations and publication-quality figures\nThorough EDA with appropriate visualizations\nBasic EDA completed but missing some components\nIncomplete or superficial EDA\n\n\nInitial Analyses (20 pts)\nAppropriate tests conducted with correct interpretation\nTests conducted with mostly correct interpretation\nTests conducted but interpretation lacks depth\nIncorrect tests or poor interpretation\n\n\nMultiple Regression (30 pts)\nThoughtful model building, thorough diagnostics, correct interpretation\nGood models with adequate diagnostics\nModels fit but diagnostics or interpretation incomplete\nModels poorly specified or interpreted\n\n\nCommunication (15 pts)\nClear, professional writing with excellent tables and figures\nGood communication with minor issues\nAdequate communication but lacks polish\nPoor organization or presentation\n\n\nReflection (10 pts)\nInsightful discussion of limitations and learning\nGood reflection with awareness of limitations\nBasic reflection\nSuperficial or missing reflection\n\n\n\n\n\n\n29.1.6 Submission Checklist\nBefore submitting, ensure you have:\n\nIncluded all required sections\nYour .qmd file runs without errors from start to finish\nAll tables and figures have informative captions\nInterpretation is in plain language (not just statistics)\nCode is commented and organized\nReferences to course concepts where appropriate\nSpell-checked and proofread\nSubmitted both .qmd and .html files\n\n\n\n\n29.1.7 Tips for Success\n\nStart early: This is a substantial project\nAsk questions: Use office hours if you get stuck\nFocus on interpretation: The statistics serve the science, not the other way around\nTell a story: Guide the reader through your analytical journey\nShow your work: Include code (with code folding) so analyses are transparent\nBe honest: If something didn’t work as expected, discuss it!\n\n\nGood luck with your final project! This is your chance to demonstrate everything you’ve learned in AnS 500.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#textbooks",
    "href": "chapters/part2-ch08-multiple_regression.html#textbooks",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "30.1 Textbooks",
    "text": "30.1 Textbooks\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R (2nd ed.). Springer. Free online\nFaraway, J. J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press.\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#r-packages-for-regression",
    "href": "chapters/part2-ch08-multiple_regression.html#r-packages-for-regression",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "30.2 R Packages for Regression",
    "text": "30.2 R Packages for Regression\n\nbroom: Tidy regression output\ncar: Companion to Applied Regression (VIF, diagnostics)\neffects: Effect plots and visualizations\nperformance: Model diagnostics and comparison\nsjPlot: Tables and plots for regression models\ninteractions: Visualizing and probing interactions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#online-resources",
    "href": "chapters/part2-ch08-multiple_regression.html#online-resources",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "30.3 Online Resources",
    "text": "30.3 Online Resources\n\nR for Data Science: https://r4ds.hadley.nz/\nSTHDA (Statistical Tools for High-throughput Data Analysis): http://www.sthda.com/english/\nCross Validated (Stats Stack Exchange): https://stats.stackexchange.com/\nQuick-R: https://www.statmethods.net/",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/part2-ch08-multiple_regression.html#papers-articles",
    "href": "chapters/part2-ch08-multiple_regression.html#papers-articles",
    "title": "16  Week 8: Multiple Regression & Course Integration",
    "section": "30.4 Papers & Articles",
    "text": "30.4 Papers & Articles\n\nHarrell, F. E. (2015). “Regression Modeling Strategies” — comprehensive guide\nGelman, A., & Hill, J. (2006). “Data Analysis Using Regression and Multilevel/Hierarchical Models”\nASA Statement on P-values (2016) — essential reading\n\n\nEND OF WEEK 8 MATERIALS\nThank you for engaging with AnS 500 this semester. May your future analyses be thoughtful, your assumptions checked, and your p-values appropriately interpreted!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Week 8: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html",
    "href": "chapters/ch10-descriptive_statistics.html",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "",
    "text": "11 Introduction: The Foundation of Data Analysis\nBefore you can run sophisticated statistical tests or build complex models, you must understand your data. This seemingly simple step is where many analyses go wrong. Jumping straight to hypothesis tests without thoroughly exploring your data is like performing surgery without examining the patient first.\nConsider a swine nutritionist who receives data from a 12-week growth trial involving 200 pigs across four different diets. What should be the first step? Running an ANOVA? No! The first step is exploratory data analysis (EDA): looking at the data, understanding its structure, identifying patterns, and checking for potential issues.\nDescriptive statistics help us summarize data with numbers (means, standard deviations, percentiles), while exploratory data analysis uses visualization and summary techniques to understand patterns, spot outliers, and generate hypotheses.\nIn this chapter, we’ll learn to:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-mean",
    "href": "chapters/ch10-descriptive_statistics.html#sec-mean",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.1 The Mean (Arithmetic Average)",
    "text": "12.1 The Mean (Arithmetic Average)\nThe mean is the sum of all values divided by the number of observations. It’s the most commonly used measure of central tendency.\n\n12.1.1 Mathematical Definition\nFor a sample of \\(n\\) observations \\(x_1, x_2, \\ldots, x_n\\), the sample mean is:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nWhere:\n\n\\(\\bar{x}\\) (pronounced “x-bar”) = the sample mean\n\\(\\sum\\) = summation symbol (add up all values)\n\\(i=1\\) to \\(n\\) = index from the first to the \\(n\\)-th observation\n\\(x_i\\) = the \\(i\\)-th observation\n\n\n\n12.1.2 Example: Weaning Weights in Pigs\nSuppose we have 8 piglets with the following weaning weights (kg):\n\n\nCode\n# Weaning weights of 8 piglets\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# Calculate mean manually\nmean_manual &lt;- sum(weights) / length(weights)\n\n# Calculate mean using R function\nmean_r &lt;- mean(weights)\n\ncat(\"Piglet weights (kg):\", paste(weights, collapse = \", \"), \"\\n\")\n\n\nPiglet weights (kg): 6.2, 5.8, 6.5, 6, 5.9, 6.3, 6.1, 5.7 \n\n\nCode\ncat(sprintf(\"Manual calculation: (%.1f + %.1f + ... + %.1f) / 8 = %.2f kg\\n\",\n            weights[1], weights[2], weights[8], mean_manual))\n\n\nManual calculation: (6.2 + 5.8 + ... + 5.7) / 8 = 6.06 kg\n\n\nCode\ncat(sprintf(\"Using mean(): %.2f kg\\n\", mean_r))\n\n\nUsing mean(): 6.06 kg\n\n\n\n\n12.1.3 Properties of the Mean\nStrengths:\n\nUses all data points (every value contributes)\nAlgebraically convenient (works well in formulas)\nFamiliar and widely understood\n\nWeaknesses:\n\nSensitive to outliers: One extreme value can dramatically shift the mean\nRequires numerical data: Can’t use with categorical data\nNot robust: May not represent “typical” value if distribution is skewed\n\n\n\n12.1.4 The Mean and Outliers\nLet’s see how outliers affect the mean:\n\n\nCode\n# Normal piglet weights\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# One piglet has a data entry error (67 kg instead of 6.7 kg!)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean = 6.06 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(weights_with_outlier)))\n\n\n  Mean = 13.72 kg\n\n\nCode\ncat(sprintf(\"  Difference: %.2f kg\\n\\n\",\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Difference: 7.66 kg\n\n\nCode\ncat(\"The outlier increased the mean by ~7.5 kg!\\n\")\n\n\nThe outlier increased the mean by ~7.5 kg!\n\n\nCode\ncat(\"This clearly doesn't represent the 'typical' piglet weight.\\n\")\n\n\nThis clearly doesn't represent the 'typical' piglet weight.\n\n\nThis is why we need other measures of central tendency that are more robust to outliers.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-median",
    "href": "chapters/ch10-descriptive_statistics.html#sec-median",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.2 The Median",
    "text": "12.2 The Median\nThe median is the middle value when data are ordered from smallest to largest. Half the observations are below the median, half are above.\n\n12.2.1 How to Calculate the Median\n\nSort the data from smallest to largest\nIf \\(n\\) is odd: median = the middle value\nIf \\(n\\) is even: median = average of the two middle values\n\nMathematically, for sorted data \\(x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\):\n\\[\n\\text{Median} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd} \\\\\n\\frac{x_{(n/2)} + x_{(n/2+1)}}{2} & \\text{if } n \\text{ is even}\n\\end{cases}\n\\]\n\n\n12.2.2 Example: Median Calculation\n\n\nCode\n# Odd number of observations (9 pigs)\nweights_odd &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7, 6.4)\n\n# Even number of observations (8 pigs)\nweights_even &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\ncat(\"Odd sample (n=9):\\n\")\n\n\nOdd sample (n=9):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_odd), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.4, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (5th value): %.1f kg\\n\", median(weights_odd)))\n\n\n  Median (5th value): 6.1 kg\n\n\nCode\ncat(\"\\nEven sample (n=8):\\n\")\n\n\n\nEven sample (n=8):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_even), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (average of 4th and 5th): (%.1f + %.1f)/2 = %.2f kg\\n\",\n            sort(weights_even)[4], sort(weights_even)[5], median(weights_even)))\n\n\n  Median (average of 4th and 5th): (6.0 + 6.1)/2 = 6.05 kg\n\n\n\n\n12.2.3 The Median is Robust to Outliers\nLet’s revisit our outlier example:\n\n\nCode\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean:   6.06 kg\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg\\n\", median(normal_weights)))\n\n\n  Median: 6.05 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg (changed by %.2f kg)\\n\",\n            mean(weights_with_outlier),\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Mean:   13.72 kg (changed by 7.66 kg)\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg (changed by %.2f kg)\\n\",\n            median(weights_with_outlier),\n            median(weights_with_outlier) - median(normal_weights)))\n\n\n  Median: 6.15 kg (changed by 0.10 kg)\n\n\nCode\ncat(\"\\nThe median barely changed, while the mean shifted dramatically!\\n\")\n\n\n\nThe median barely changed, while the mean shifted dramatically!\n\n\n\n\n\n\n\n\nTipWhen to Use Mean vs Median\n\n\n\nUse the mean when:\n\nData are roughly symmetric (no strong skew)\nNo extreme outliers\nYou need the mathematical properties of the mean (e.g., for further calculations)\n\nUse the median when:\n\nData are skewed (right-skewed or left-skewed)\nOutliers are present\nYou want a measure resistant to extreme values\nReporting income, house prices, or other variables with long tails",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-mode",
    "href": "chapters/ch10-descriptive_statistics.html#sec-mode",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.3 The Mode",
    "text": "12.3 The Mode\nThe mode is the most frequently occurring value in a dataset. Unlike mean and median, the mode can be used with categorical data.\n\n12.3.1 Example: Mode in Categorical Data\n\n\nCode\n# Breed types in a beef herd\nbreeds &lt;- c(\"Angus\", \"Angus\", \"Hereford\", \"Angus\", \"Charolais\",\n            \"Angus\", \"Hereford\", \"Angus\", \"Angus\")\n\n# Find mode (most common breed)\nbreed_counts &lt;- table(breeds)\nmode_breed &lt;- names(breed_counts)[which.max(breed_counts)]\n\ncat(\"Breed counts:\\n\")\n\n\nBreed counts:\n\n\nCode\nprint(breed_counts)\n\n\nbreeds\n    Angus Charolais  Hereford \n        6         1         2 \n\n\nCode\ncat(sprintf(\"\\nMode: %s (most common breed)\\n\", mode_breed))\n\n\n\nMode: Angus (most common breed)\n\n\n\n\n12.3.2 Mode in Continuous Data\nFor continuous data, the mode is less useful because values rarely repeat exactly. Instead, we look at the peak of the distribution using histograms or density plots.\n\n\nCode\n# Birth weights of 100 calves\nset.seed(123)\ncalf_weights &lt;- rnorm(100, mean = 40, sd = 5)\n\n# Visualize\nggplot(tibble(weight = calf_weights), aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(calf_weights), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = mean(calf_weights) + 2, y = 0.07,\n           label = sprintf(\"Mean = %.1f kg\", mean(calf_weights)),\n           color = \"darkgreen\", hjust = 0) +\n  labs(\n    title = \"Distribution of Calf Birth Weights\",\n    subtitle = \"Red curve shows density; mode is near the peak\",\n    x = \"Birth Weight (kg)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Point: Mode\n\n\n\nThe mode is most useful for:\n\nCategorical variables (breed, sex, treatment group)\nDiscrete counts (number of piglets per litter)\nMultimodal distributions (distributions with multiple peaks, suggesting subgroups)\n\nFor continuous measurements, mean and median are usually more informative.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-skewness",
    "href": "chapters/ch10-descriptive_statistics.html#sec-skewness",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.4 Comparing Measures in Skewed Distributions",
    "text": "12.4 Comparing Measures in Skewed Distributions\nThe relationship between mean, median, and mode reveals the shape of the distribution.\n\n12.4.1 Symmetric Distribution\nWhen data are symmetric (like a normal distribution):\n\\[\n\\text{Mean} \\approx \\text{Median} \\approx \\text{Mode}\n\\]\n\n\nCode\nset.seed(456)\nsymmetric_data &lt;- rnorm(500, mean = 100, sd = 15)\n\np_sym &lt;- ggplot(tibble(x = symmetric_data), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(symmetric_data), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(symmetric_data), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  labs(title = \"Symmetric Distribution\",\n       subtitle = sprintf(\"Mean (green) = %.1f | Median (purple) = %.1f\",\n                         mean(symmetric_data), median(symmetric_data)),\n       x = \"Value\", y = \"Density\")\n\nprint(p_sym)\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Right-Skewed Distribution\nWhen data have a long tail to the right (positive skew):\n\\[\n\\text{Mean} &gt; \\text{Median} &gt; \\text{Mode}\n\\]\nThe mean is “pulled” toward the tail by extreme high values.\nExample: Days to market for pigs (most finish quickly, some take much longer)\n\n\nCode\nset.seed(789)\n# Simulate right-skewed data (e.g., days to market)\nright_skewed &lt;- rgamma(500, shape = 2, rate = 0.02)\n\np_right &lt;- ggplot(tibble(x = right_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(right_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(right_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(right_skewed), y = 0.012,\n           label = sprintf(\"Mean = %.1f\", mean(right_skewed)),\n           color = \"darkgreen\", hjust = -0.1, size = 3.5) +\n  annotate(\"text\", x = median(right_skewed), y = 0.011,\n           label = sprintf(\"Median = %.1f\", median(right_skewed)),\n           color = \"purple\", hjust = 1.1, size = 3.5) +\n  labs(title = \"Right-Skewed Distribution (Positive Skew)\",\n       subtitle = \"Mean &gt; Median (mean pulled toward long right tail)\",\n       x = \"Days to Market\", y = \"Density\")\n\nprint(p_right)\n\n\n\n\n\n\n\n\n\n\n\n12.4.3 Left-Skewed Distribution\nWhen data have a long tail to the left (negative skew):\n\\[\n\\text{Mean} &lt; \\text{Median} &lt; \\text{Mode}\n\\]\nExample: Carcass yield percentage (most are high, some are unusually low)\n\n\nCode\nset.seed(321)\n# Simulate left-skewed data\nleft_skewed &lt;- 100 - rgamma(500, shape = 2, rate = 0.2)\n\np_left &lt;- ggplot(tibble(x = left_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(left_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(left_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(left_skewed), y = 0.04,\n           label = sprintf(\"Mean = %.1f\", mean(left_skewed)),\n           color = \"darkgreen\", hjust = 1.1, size = 3.5) +\n  annotate(\"text\", x = median(left_skewed), y = 0.042,\n           label = sprintf(\"Median = %.1f\", median(left_skewed)),\n           color = \"purple\", hjust = -0.1, size = 3.5) +\n  labs(title = \"Left-Skewed Distribution (Negative Skew)\",\n       subtitle = \"Mean &lt; Median (mean pulled toward long left tail)\",\n       x = \"Carcass Yield (%)\", y = \"Density\")\n\nprint(p_left)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantRemember\n\n\n\n\nSymmetric: Mean ≈ Median\nRight-skewed: Mean &gt; Median (use median to describe center)\nLeft-skewed: Mean &lt; Median (use median to describe center)\n\nAlways visualize your data to understand its shape before choosing which measure to report!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-range",
    "href": "chapters/ch10-descriptive_statistics.html#sec-range",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.1 Range",
    "text": "13.1 Range\nThe range is the simplest measure of spread:\n\\[\n\\text{Range} = \\text{Maximum} - \\text{Minimum}\n\\]\n\n\nCode\n# Two herds with same mean, different range\nherd_a &lt;- c(5.8, 5.9, 6.0, 6.1, 6.2)\nherd_b &lt;- c(3.5, 5.0, 6.0, 7.0, 8.5)\n\ncat(\"Herd A:\", paste(herd_a, collapse = \", \"), \"\\n\")\n\n\nHerd A: 5.8, 5.9, 6, 6.1, 6.2 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_a), min(herd_a), max(herd_a), max(herd_a) - min(herd_a)))\n\n\n  Mean: 6.0 kg | Range: 5.8 - 6.2 kg (0.4 kg)\n\n\nCode\ncat(\"\\nHerd B:\", paste(herd_b, collapse = \", \"), \"\\n\")\n\n\n\nHerd B: 3.5, 5, 6, 7, 8.5 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_b), min(herd_b), max(herd_b), max(herd_b) - min(herd_b)))\n\n\n  Mean: 6.0 kg | Range: 3.5 - 8.5 kg (5.0 kg)\n\n\nLimitation: The range uses only two values (min and max) and is extremely sensitive to outliers. We need better measures.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-variance-sd",
    "href": "chapters/ch10-descriptive_statistics.html#sec-variance-sd",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.2 Variance and Standard Deviation",
    "text": "13.2 Variance and Standard Deviation\nThe variance and standard deviation are the most important measures of variability in statistics.\n\n13.2.1 Variance\nThe variance measures the average squared deviation from the mean:\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nWhere:\n\n\\(s^2\\) = sample variance\n\\(n\\) = sample size\n\\(x_i\\) = each observation\n\\(\\bar{x}\\) = sample mean\n\\((x_i - \\bar{x})\\) = deviation of observation \\(i\\) from the mean\n\\(n-1\\) = degrees of freedom (we use \\(n-1\\) instead of \\(n\\) for sample variance)\n\nWhy \\(n-1\\) instead of \\(n\\)? This is called Bessel’s correction. Using \\(n-1\\) makes the sample variance an unbiased estimator of the population variance. Since we estimated the mean from the same data, we “lose” one degree of freedom.\n\n\n13.2.2 Standard Deviation\nThe standard deviation is the square root of the variance:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\nWhy take the square root? Variance is in squared units (e.g., kg²), which is hard to interpret. Standard deviation is in the original units (kg), making it much more intuitive.\n\n\n13.2.3 Calculating by Hand\nLet’s calculate variance and SD step by step:\n\n\nCode\n# Piglet weights\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9)\n\n# Step 1: Calculate mean\nmean_w &lt;- mean(weights)\n\n# Step 2: Calculate deviations from mean\ndeviations &lt;- weights - mean_w\n\n# Step 3: Square the deviations\nsquared_devs &lt;- deviations^2\n\n# Step 4: Sum squared deviations\nsum_sq_devs &lt;- sum(squared_devs)\n\n# Step 5: Divide by n-1\nvariance &lt;- sum_sq_devs / (length(weights) - 1)\n\n# Step 6: Take square root for SD\nstd_dev &lt;- sqrt(variance)\n\n# Create summary table\ntibble(\n  Weight = weights,\n  Deviation = round(deviations, 2),\n  `Squared Dev` = round(squared_devs, 3)\n) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Variance Calculation: Step by Step\") %&gt;%\n  tab_source_note(sprintf(\"Mean = %.2f kg\", mean_w)) %&gt;%\n  tab_source_note(sprintf(\"Sum of squared deviations = %.3f\", sum_sq_devs)) %&gt;%\n  tab_source_note(sprintf(\"Variance = %.3f / %d = %.3f kg²\",\n                         sum_sq_devs, length(weights)-1, variance)) %&gt;%\n  tab_source_note(sprintf(\"Standard Deviation = √%.3f = %.3f kg\",\n                         variance, std_dev))\n\n\n\n\n\n\n\n\nVariance Calculation: Step by Step\n\n\nWeight\nDeviation\nSquared Dev\n\n\n\n\n6.2\n0.12\n0.014\n\n\n5.8\n-0.28\n0.078\n\n\n6.5\n0.42\n0.176\n\n\n6.0\n-0.08\n0.006\n\n\n5.9\n-0.18\n0.032\n\n\n\nMean = 6.08 kg\n\n\nSum of squared deviations = 0.308\n\n\nVariance = 0.308 / 4 = 0.077 kg²\n\n\nStandard Deviation = √0.077 = 0.277 kg\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare to R's built-in functions\ncat(sprintf(\"\\nUsing R functions:\\n\"))\n\n\n\nUsing R functions:\n\n\nCode\ncat(sprintf(\"  Variance: %.3f kg²\\n\", var(weights)))\n\n\n  Variance: 0.077 kg²\n\n\nCode\ncat(sprintf(\"  Standard Deviation: %.3f kg\\n\", sd(weights)))\n\n\n  Standard Deviation: 0.277 kg\n\n\n\n\n13.2.4 Interpreting Standard Deviation\nStandard deviation tells us, on average, how far observations deviate from the mean.\n\nSmall SD: Data are clustered tightly around the mean (low variability)\nLarge SD: Data are spread out widely (high variability)\n\n\n\nCode\nset.seed(999)\n\n# Generate two datasets with same mean, different SD\nlow_sd &lt;- rnorm(500, mean = 100, sd = 5)\nhigh_sd &lt;- rnorm(500, mean = 100, sd = 20)\n\np_low &lt;- ggplot(tibble(x = low_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(low_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"Low Variability: SD = %.1f\", sd(low_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_high &lt;- ggplot(tibble(x = high_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"darkorange\", alpha = 0.7) +\n  geom_vline(xintercept = mean(high_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"High Variability: SD = %.1f\", sd(high_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_low + p_high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipThe Empirical Rule (68-95-99.7 Rule)\n\n\n\nFor data that are approximately normally distributed:\n\nAbout 68% of values fall within 1 SD of the mean\nAbout 95% of values fall within 2 SD of the mean\nAbout 99.7% of values fall within 3 SD of the mean\n\nThis rule helps you quickly assess whether an observation is unusual.\n\n\n\n\nCode\n# Demonstrate empirical rule\nset.seed(2025)\ndata_norm &lt;- rnorm(10000, mean = 100, sd = 15)\nmean_val &lt;- 100\nsd_val &lt;- 15\n\nggplot(tibble(x = data_norm), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50,\n                 fill = \"lightblue\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1.5) +\n  # Mark mean\n  geom_vline(xintercept = mean_val, color = \"red\",\n             linetype = \"solid\", linewidth = 1.2) +\n  # Mark ±1 SD\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ±2 SD\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val),\n             color = \"orange\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ±3 SD\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val),\n             color = \"purple\", linetype = \"dashed\", linewidth = 1) +\n  # Annotations\n  annotate(\"text\", x = mean_val, y = 0.025, label = \"Mean\",\n           color = \"red\", fontface = \"bold\") +\n  annotate(\"text\", x = mean_val + sd_val, y = 0.020,\n           label = \"±1 SD\\n(68%)\", color = \"darkgreen\", size = 3) +\n  annotate(\"text\", x = mean_val + 2*sd_val, y = 0.015,\n           label = \"±2 SD\\n(95%)\", color = \"orange\", size = 3) +\n  annotate(\"text\", x = mean_val + 3*sd_val, y = 0.010,\n           label = \"±3 SD\\n(99.7%)\", color = \"purple\", size = 3) +\n  labs(\n    title = \"The Empirical Rule (68-95-99.7 Rule)\",\n    subtitle = \"For normal distributions: most data fall within 3 SD of the mean\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  xlim(25, 175)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-iqr",
    "href": "chapters/ch10-descriptive_statistics.html#sec-iqr",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.3 Interquartile Range (IQR)",
    "text": "13.3 Interquartile Range (IQR)\nThe interquartile range (IQR) is a robust measure of spread that’s not affected by outliers.\n\n13.3.1 Quartiles\nQuartiles divide data into four equal parts:\n\nQ1 (First quartile / 25th percentile): 25% of data are below this value\nQ2 (Second quartile / 50th percentile): The median\nQ3 (Third quartile / 75th percentile): 75% of data are below this value\n\n\\[\n\\text{IQR} = Q3 - Q1\n\\]\nThe IQR represents the range containing the middle 50% of the data.\n\n\nCode\n# Beef cattle weights\ncattle_weights &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\nq1 &lt;- quantile(cattle_weights, 0.25)\nq2 &lt;- quantile(cattle_weights, 0.50)  # Median\nq3 &lt;- quantile(cattle_weights, 0.75)\niqr_val &lt;- IQR(cattle_weights)\n\ncat(\"Cattle weights (kg):\", paste(cattle_weights, collapse = \", \"), \"\\n\\n\")\n\n\nCattle weights (kg): 450, 480, 490, 510, 520, 530, 540, 560, 580, 620 \n\n\nCode\ncat(sprintf(\"Q1 (25th percentile): %.1f kg\\n\", q1))\n\n\nQ1 (25th percentile): 495.0 kg\n\n\nCode\ncat(sprintf(\"Q2 (Median):          %.1f kg\\n\", q2))\n\n\nQ2 (Median):          525.0 kg\n\n\nCode\ncat(sprintf(\"Q3 (75th percentile): %.1f kg\\n\", q3))\n\n\nQ3 (75th percentile): 555.0 kg\n\n\nCode\ncat(sprintf(\"\\nIQR = Q3 - Q1 = %.1f - %.1f = %.1f kg\\n\", q3, q1, iqr_val))\n\n\n\nIQR = Q3 - Q1 = 555.0 - 495.0 = 60.0 kg\n\n\nCode\ncat(\"\\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\\n\")\n\n\n\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\n\n\n\n\n13.3.2 IQR is Robust to Outliers\n\n\nCode\n# Normal data\nnormal_data &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\n# Add extreme outlier\nwith_outlier &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 900)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg\\n\", sd(normal_data)))\n\n\n  SD:  50.1 kg\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg\\n\", IQR(normal_data)))\n\n\n  IQR: 60.0 kg\n\n\nCode\ncat(\"\\nWith outlier (900 kg):\\n\")\n\n\n\nWith outlier (900 kg):\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg (increased by %.1f kg)\\n\",\n            sd(with_outlier), sd(with_outlier) - sd(normal_data)))\n\n\n  SD:  126.8 kg (increased by 76.7 kg)\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg (increased by %.1f kg)\\n\",\n            IQR(with_outlier), IQR(with_outlier) - IQR(normal_data)))\n\n\n  IQR: 60.0 kg (increased by 0.0 kg)\n\n\nCode\ncat(\"\\nIQR is much more stable in the presence of outliers!\\n\")\n\n\n\nIQR is much more stable in the presence of outliers!\n\n\n\n\n\n\n\n\nTipWhen to Use SD vs IQR\n\n\n\nUse Standard Deviation when:\n\nData are approximately normal\nNo extreme outliers\nYou need mathematical properties of variance (e.g., for further statistical tests)\n\nUse IQR when:\n\nData are skewed\nOutliers are present\nYou want a robust, resistant measure of spread",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-histograms",
    "href": "chapters/ch10-descriptive_statistics.html#sec-histograms",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.1 Histograms",
    "text": "14.1 Histograms\nA histogram shows the distribution of a continuous variable by dividing the range into bins and counting observations in each bin.\n\n\nCode\n# Generate pig growth data\nset.seed(12345)\npig_weights &lt;- tibble(\n  weight = rnorm(200, mean = 115, sd = 18),\n  diet = sample(c(\"Control\", \"High Protein\"), 200, replace = TRUE)\n)\n\n# Basic histogram\np_hist1 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Pig Weights\",\n    subtitle = \"20 bins\",\n    x = \"Final Weight (kg)\",\n    y = \"Count\"\n  )\n\n# Histogram with density overlay\np_hist2 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20,\n                 fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(pig_weights$weight),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Histogram with Density Curve\",\n    subtitle = \"Red = density curve | Green = mean\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\"\n  )\n\np_hist1 + p_hist2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningChoosing the Number of Bins\n\n\n\nThe number of bins affects how the distribution looks:\n\nToo few bins: May hide important features\nToo many bins: May show too much noise\n\nCommon rules:\n\nSturges’ rule: \\(\\text{bins} \\approx \\log_2(n) + 1\\)\nSquare root rule: \\(\\text{bins} \\approx \\sqrt{n}\\)\nOr just experiment! Try different bin numbers and see what reveals patterns best\n\n\n\n\n\nCode\n# Show effect of bin number\np_few &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 5, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Few Bins (5)\", x = \"Weight (kg)\", y = \"Count\")\n\np_many &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Many Bins (50)\", x = \"Weight (kg)\", y = \"Count\")\n\np_few + p_many",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-boxplots",
    "href": "chapters/ch10-descriptive_statistics.html#sec-boxplots",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.2 Boxplots",
    "text": "14.2 Boxplots\nA boxplot (box-and-whisker plot) displays the distribution using five-number summary: minimum, Q1, median, Q3, and maximum.\n\n14.2.1 Anatomy of a Boxplot\n\n\nCode\n# Create sample data\nset.seed(456)\nsample_data &lt;- rnorm(100, mean = 100, sd = 15)\n\n# Calculate components\nq1 &lt;- quantile(sample_data, 0.25)\nmedian_val &lt;- median(sample_data)\nq3 &lt;- quantile(sample_data, 0.75)\niqr_val &lt;- IQR(sample_data)\nlower_whisker &lt;- max(min(sample_data), q1 - 1.5 * iqr_val)\nupper_whisker &lt;- min(max(sample_data), q3 + 1.5 * iqr_val)\n\n# Find outliers\noutliers &lt;- sample_data[sample_data &lt; lower_whisker | sample_data &gt; upper_whisker]\n\n# Create boxplot\nggplot(tibble(x = \"Data\", y = sample_data), aes(x = x, y = y)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 3) +\n  # Add labels\n  annotate(\"text\", x = 1.35, y = q1, label = sprintf(\"Q1 = %.1f\", q1),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = median_val, label = sprintf(\"Median = %.1f\", median_val),\n           hjust = 0, size = 4, color = \"darkgreen\", fontface = \"bold\") +\n  annotate(\"text\", x = 1.35, y = q3, label = sprintf(\"Q3 = %.1f\", q3),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = upper_whisker,\n           label = sprintf(\"Upper whisker = %.1f\", upper_whisker),\n           hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 1.35, y = lower_whisker,\n           label = sprintf(\"Lower whisker = %.1f\", lower_whisker),\n           hjust = 0, size = 3.5) +\n  # Add IQR bracket\n  annotate(\"segment\", x = 0.7, xend = 0.7, y = q1, yend = q3,\n           color = \"purple\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.65, y = (q1 + q3)/2,\n           label = sprintf(\"IQR = %.1f\", iqr_val),\n           angle = 90, vjust = 1, color = \"purple\", fontface = \"bold\") +\n  labs(\n    title = \"Anatomy of a Boxplot\",\n    subtitle = \"Red points are outliers (&gt; 1.5 × IQR from Q1 or Q3)\",\n    y = \"Value\",\n    x = \"\"\n  ) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n14.2.2 Comparing Groups with Boxplots\nBoxplots are excellent for comparing distributions across groups:\n\n\nCode\n# Simulate beef cattle data from different breeding programs\nset.seed(789)\ncattle_data &lt;- tibble(\n  program = rep(c(\"Program A\", \"Program B\", \"Program C\"), each = 60),\n  weight = c(\n    rnorm(60, mean = 580, sd = 45),  # Program A\n    rnorm(60, mean = 610, sd = 50),  # Program B\n    rnorm(60, mean = 595, sd = 35)   # Program C\n  )\n)\n\n# Boxplot comparison\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Final Weights by Breeding Program\",\n    subtitle = \"Red diamond = mean | Bold line = median | Box = IQR\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-density",
    "href": "chapters/ch10-descriptive_statistics.html#sec-density",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.3 Density Plots",
    "text": "14.3 Density Plots\nA density plot is a smoothed version of a histogram, showing the probability density function.\n\n\nCode\n# Compare distributions across groups\nggplot(cattle_data, aes(x = weight, fill = program)) +\n  geom_density(alpha = 0.5, linewidth = 1) +\n  geom_vline(data = cattle_data %&gt;% group_by(program) %&gt;%\n               summarise(mean_wt = mean(weight), .groups = 'drop'),\n             aes(xintercept = mean_wt, color = program),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Density Plots: Comparing Weight Distributions\",\n    subtitle = \"Dashed lines show group means\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\",\n    fill = \"Program\",\n    color = \"Program\"\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-violin",
    "href": "chapters/ch10-descriptive_statistics.html#sec-violin",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.4 Violin Plots",
    "text": "14.4 Violin Plots\nA violin plot combines a boxplot with a density plot, showing both summary statistics and the full distribution shape.\n\n\nCode\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_violin(alpha = 0.6, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.8, outlier.shape = NA) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Violin Plots: Distribution Shape + Boxplot\",\n    subtitle = \"Width shows density | Box shows quartiles | Red = mean\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-iqr-method",
    "href": "chapters/ch10-descriptive_statistics.html#sec-iqr-method",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.1 The IQR Method",
    "text": "15.1 The IQR Method\nThe most common method defines outliers as observations beyond:\n\\[\n\\begin{align}\n\\text{Lower fence} &= Q1 - 1.5 \\times \\text{IQR} \\\\\n\\text{Upper fence} &= Q3 + 1.5 \\times \\text{IQR}\n\\end{align}\n\\]\nThis is the definition used by boxplots.\n\n\nCode\n# Cattle weights with some outliers\nset.seed(111)\nweights_with_outliers &lt;- c(\n  rnorm(45, mean = 550, sd = 40),  # Normal cattle\n  c(350, 420, 720)                  # Outliers\n)\n\n# Calculate fences\nq1 &lt;- quantile(weights_with_outliers, 0.25)\nq3 &lt;- quantile(weights_with_outliers, 0.75)\niqr &lt;- IQR(weights_with_outliers)\nlower_fence &lt;- q1 - 1.5 * iqr\nupper_fence &lt;- q3 + 1.5 * iqr\n\n# Identify outliers\noutliers &lt;- weights_with_outliers[weights_with_outliers &lt; lower_fence |\n                                   weights_with_outliers &gt; upper_fence]\n\ncat(\"Outlier Detection Using IQR Method\\n\")\n\n\nOutlier Detection Using IQR Method\n\n\nCode\ncat(\"===================================\\n\")\n\n\n===================================\n\n\nCode\ncat(sprintf(\"Q1 = %.1f kg\\n\", q1))\n\n\nQ1 = 501.2 kg\n\n\nCode\ncat(sprintf(\"Q3 = %.1f kg\\n\", q3))\n\n\nQ3 = 564.0 kg\n\n\nCode\ncat(sprintf(\"IQR = %.1f kg\\n\", iqr))\n\n\nIQR = 62.8 kg\n\n\nCode\ncat(sprintf(\"\\nLower fence = Q1 - 1.5×IQR = %.1f - %.1f = %.1f kg\\n\",\n            q1, 1.5*iqr, lower_fence))\n\n\n\nLower fence = Q1 - 1.5×IQR = 501.2 - 94.3 = 406.9 kg\n\n\nCode\ncat(sprintf(\"Upper fence = Q3 + 1.5×IQR = %.1f + %.1f = %.1f kg\\n\",\n            q3, 1.5*iqr, upper_fence))\n\n\nUpper fence = Q3 + 1.5×IQR = 564.0 + 94.3 = 658.3 kg\n\n\nCode\ncat(sprintf(\"\\nOutliers detected: %s\\n\", paste(round(outliers, 1), collapse = \", \")))\n\n\n\nOutliers detected: 658.7, 350, 720\n\n\n\n\nCode\n# Visualize outliers\noutlier_data &lt;- tibble(\n  weight = weights_with_outliers,\n  is_outlier = weight &lt; lower_fence | weight &gt; upper_fence\n)\n\np_box_outlier &lt;- ggplot(outlier_data, aes(x = \"Cattle\", y = weight)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 4) +\n  geom_hline(yintercept = c(lower_fence, upper_fence),\n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = lower_fence,\n           label = sprintf(\"Lower fence = %.0f\", lower_fence),\n           color = \"red\", hjust = 0) +\n  annotate(\"text\", x = 1.3, y = upper_fence,\n           label = sprintf(\"Upper fence = %.0f\", upper_fence),\n           color = \"red\", hjust = 0) +\n  labs(title = \"Boxplot: Outliers in Red\",\n       y = \"Weight (kg)\", x = \"\") +\n  theme(axis.text.x = element_blank())\n\np_hist_outlier &lt;- ggplot(outlier_data, aes(x = weight, fill = is_outlier)) +\n  geom_histogram(bins = 20, color = \"white\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                    labels = c(\"Normal\", \"Outlier\")) +\n  labs(title = \"Histogram: Outliers Highlighted\",\n       x = \"Weight (kg)\", y = \"Count\", fill = \"\")\n\np_box_outlier + p_hist_outlier",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-handling-outliers",
    "href": "chapters/ch10-descriptive_statistics.html#sec-handling-outliers",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.2 What to Do with Outliers?",
    "text": "15.2 What to Do with Outliers?\nNEVER automatically delete outliers! Instead:\n\nInvestigate: Is it a data entry error? Measurement error? Legitimate extreme value?\nDocument: Record what you find and what you decide\nConsider:\n\nIf error: Correct if possible, or remove and document\nIf legitimate: Keep it! Report results with and without outliers if it’s influential\nIf different population: Analyze separately\n\n\n\n\n\n\n\n\nWarningImportant\n\n\n\nRemoving outliers just to get “better” p-values is data manipulation and scientifically dishonest. Always have a principled reason for any data exclusions and report them transparently.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#example-swine-growth-trial-summary",
    "href": "chapters/ch10-descriptive_statistics.html#example-swine-growth-trial-summary",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "16.1 Example: Swine Growth Trial Summary",
    "text": "16.1 Example: Swine Growth Trial Summary\n\n\nCode\n# Simulate swine growth data\nset.seed(2025)\nswine_data &lt;- tibble(\n  diet = rep(c(\"Control\", \"High Protein\", \"High Energy\", \"Balanced\"), each = 50),\n  initial_weight = rnorm(200, mean = 25, sd = 3),\n  final_weight = initial_weight + rnorm(200, mean = 90, sd = 12) +\n    case_when(\n      diet == \"Control\" ~ 0,\n      diet == \"High Protein\" ~ 5,\n      diet == \"High Energy\" ~ 3,\n      diet == \"Balanced\" ~ 7\n    )\n) %&gt;%\n  mutate(weight_gain = final_weight - initial_weight)\n\n# Create comprehensive summary table\nsummary_table &lt;- swine_data %&gt;%\n  group_by(diet) %&gt;%\n  summarise(\n    N = n(),\n    Mean = mean(weight_gain),\n    SD = sd(weight_gain),\n    Median = median(weight_gain),\n    IQR = IQR(weight_gain),\n    Min = min(weight_gain),\n    Max = max(weight_gain),\n    .groups = 'drop'\n  )\n\n# Display with gt package\nsummary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Summary Statistics: Weight Gain by Diet\",\n    subtitle = \"12-week growth trial (n=200 pigs)\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Mean, SD, Median, IQR, Min, Max),\n    decimals = 1\n  ) %&gt;%\n  cols_label(\n    diet = \"Diet Treatment\",\n    N = \"n\",\n    Mean = \"Mean (kg)\",\n    SD = \"SD (kg)\",\n    Median = \"Median (kg)\",\n    IQR = \"IQR (kg)\",\n    Min = \"Min (kg)\",\n    Max = \"Max (kg)\"\n  ) %&gt;%\n  tab_source_note(\"SD = Standard Deviation; IQR = Interquartile Range\") %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.background.color = \"#f0f0f0\"\n  )\n\n\n\n\n\n\n\n\nSummary Statistics: Weight Gain by Diet\n\n\n12-week growth trial (n=200 pigs)\n\n\nDiet Treatment\nn\nMean (kg)\nSD (kg)\nMedian (kg)\nIQR (kg)\nMin (kg)\nMax (kg)\n\n\n\n\nBalanced\n50\n96.5\n11.8\n96.1\n12.0\n66.4\n131.0\n\n\nControl\n50\n89.0\n11.6\n89.0\n15.3\n55.8\n112.6\n\n\nHigh Energy\n50\n92.1\n12.1\n92.8\n15.6\n63.5\n116.6\n\n\nHigh Protein\n50\n96.2\n12.0\n96.6\n16.3\n63.5\n124.9\n\n\n\nSD = Standard Deviation; IQR = Interquartile Range",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-1-overall-summary-statistics",
    "href": "chapters/ch10-descriptive_statistics.html#step-1-overall-summary-statistics",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.1 Step 1: Overall Summary Statistics",
    "text": "17.1 Step 1: Overall Summary Statistics\n\n\nCode\n# Overall summary of key variables\noverall_summary &lt;- feedlot_data %&gt;%\n  summarise(\n    `Sample Size` = n(),\n    across(c(initial_weight, final_weight, weight_gain, adg),\n           list(\n             Mean = ~mean(.),\n             SD = ~sd(.),\n             Median = ~median(.),\n             IQR = ~IQR(.),\n             Min = ~min(.),\n             Max = ~max(.)\n           ),\n           .names = \"{.col}_{.fn}\")\n  )\n\n# Reshape for display\noverall_summary %&gt;%\n  pivot_longer(-`Sample Size`, names_to = \"stat\", values_to = \"value\") %&gt;%\n  separate(stat, into = c(\"variable\", \"measure\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = measure, values_from = value) %&gt;%\n  mutate(variable = case_when(\n    variable == \"initial\" ~ \"Initial Weight (kg)\",\n    variable == \"final\" ~ \"Final Weight (kg)\",\n    variable == \"weight\" ~ \"Weight Gain (kg)\",\n    variable == \"adg\" ~ \"ADG (kg/day)\"\n  )) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Overall Summary Statistics\",\n             subtitle = sprintf(\"n = %d cattle\", overall_summary$`Sample Size`)) %&gt;%\n  fmt_number(columns = -variable, decimals = 2) %&gt;%\n  cols_label(variable = \"Variable\")\n\n\n\n\n\n\n\n\nOverall Summary Statistics\n\n\nn = 180 cattle\n\n\nSample Size\nVariable\nweight\ngain\nMean\nSD\nMedian\nIQR\nMin\nMax\n\n\n\n\n180.00\nInitial Weight (kg)\n298.21808, 35.59733, 301.28524, 49.67098, 176.78933, 381.05291\n\n\n\n\n\n\n\n\n\n180.00\nFinal Weight (kg)\n543.05239, 48.61542, 545.12486, 61.43552, 400.31501, 684.27656\n\n\n\n\n\n\n\n\n\n180.00\nWeight Gain (kg)\n\n244.83430, 34.25836, 245.22667, 42.03852, 139.47315, 331.21941\n\n\n\n\n\n\n\n\n180.00\nADG (kg/day)\n\n\n1.360191\n0.1903242\n1.36237\n0.2335473\n0.7748508\n1.840108",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-2-visualize-distributions",
    "href": "chapters/ch10-descriptive_statistics.html#step-2-visualize-distributions",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.2 Step 2: Visualize Distributions",
    "text": "17.2 Step 2: Visualize Distributions\n\n\nCode\n# Create multiple visualizations\np1 &lt;- ggplot(feedlot_data, aes(x = initial_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"steelblue\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$initial_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Initial Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np2 &lt;- ggplot(feedlot_data, aes(x = final_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkorange\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$final_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Final Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np3 &lt;- ggplot(feedlot_data, aes(x = weight_gain)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkgreen\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$weight_gain),\n             linetype = \"dashed\", color = \"darkblue\", linewidth = 1) +\n  labs(title = \"Weight Gain Distribution\", x = \"Weight Gain (kg)\", y = \"Density\")\n\np4 &lt;- ggplot(feedlot_data, aes(x = adg)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"purple\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$adg),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"ADG Distribution\", x = \"ADG (kg/day)\", y = \"Density\")\n\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Distribution of Weight Variables\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-3-compare-groups",
    "href": "chapters/ch10-descriptive_statistics.html#step-3-compare-groups",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.3 Step 3: Compare Groups",
    "text": "17.3 Step 3: Compare Groups\n\n\nCode\n# Summary by feed program\nfeed_summary &lt;- feedlot_data %&gt;%\n  group_by(feed_program) %&gt;%\n  summarise(\n    n = n(),\n    Mean_ADG = mean(adg),\n    SD_ADG = sd(adg),\n    Median_ADG = median(adg),\n    IQR_ADG = IQR(adg),\n    .groups = 'drop'\n  )\n\nprint(\"Summary by Feed Program:\")\n\n\n[1] \"Summary by Feed Program:\"\n\n\nCode\nfeed_summary %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = -c(feed_program, n), decimals = 3) %&gt;%\n  cols_label(feed_program = \"Feed Program\",\n             n = \"n\",\n             Mean_ADG = \"Mean ADG\",\n             SD_ADG = \"SD\",\n             Median_ADG = \"Median ADG\",\n             IQR_ADG = \"IQR\")\n\n\n\n\n\n\n\n\nFeed Program\nn\nMean ADG\nSD\nMedian ADG\nIQR\n\n\n\n\nEnhanced\n90\n1.377\n0.188\n1.368\n0.231\n\n\nStandard\n90\n1.344\n0.192\n1.353\n0.250\n\n\n\n\n\n\n\nCode\n# Visualize comparisons\np_feed &lt;- ggplot(feedlot_data, aes(x = feed_program, y = adg, fill = feed_program)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"ADG by Feed Program\", x = \"Feed Program\",\n       y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_sex &lt;- ggplot(feedlot_data, aes(x = sex, y = adg, fill = sex)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ADG by Sex\", x = \"Sex\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_breed &lt;- ggplot(feedlot_data, aes(x = breed, y = adg, fill = breed)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"ADG by Breed\", x = \"Breed\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_feed + p_sex + p_breed +\n  plot_annotation(title = \"Comparing ADG Across Groups\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-4-check-for-outliers",
    "href": "chapters/ch10-descriptive_statistics.html#step-4-check-for-outliers",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.4 Step 4: Check for Outliers",
    "text": "17.4 Step 4: Check for Outliers\n\n\nCode\n# Identify outliers in ADG\nq1_adg &lt;- quantile(feedlot_data$adg, 0.25)\nq3_adg &lt;- quantile(feedlot_data$adg, 0.75)\niqr_adg &lt;- IQR(feedlot_data$adg)\nlower_adg &lt;- q1_adg - 1.5 * iqr_adg\nupper_adg &lt;- q3_adg + 1.5 * iqr_adg\n\noutliers_adg &lt;- feedlot_data %&gt;%\n  filter(adg &lt; lower_adg | adg &gt; upper_adg)\n\ncat(sprintf(\"Outlier Detection for ADG (kg/day)\\n\"))\n\n\nOutlier Detection for ADG (kg/day)\n\n\nCode\ncat(sprintf(\"Lower fence: %.3f\\n\", lower_adg))\n\n\nLower fence: 0.891\n\n\nCode\ncat(sprintf(\"Upper fence: %.3f\\n\", upper_adg))\n\n\nUpper fence: 1.825\n\n\nCode\ncat(sprintf(\"\\nNumber of outliers: %d out of %d (%.1f%%)\\n\",\n            nrow(outliers_adg), nrow(feedlot_data),\n            100 * nrow(outliers_adg) / nrow(feedlot_data)))\n\n\n\nNumber of outliers: 4 out of 180 (2.2%)\n\n\nCode\nif(nrow(outliers_adg) &gt; 0) {\n  cat(\"\\nOutlier animals:\\n\")\n  print(outliers_adg %&gt;%\n          select(animal_id, breed, sex, feed_program, adg) %&gt;%\n          arrange(adg))\n}\n\n\n\nOutlier animals:\n# A tibble: 4 × 5\n  animal_id breed     sex    feed_program   adg\n      &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1        66 Hereford  Heifer Standard     0.775\n2       107 Hereford  Heifer Enhanced     0.859\n3       170 Angus     Steer  Enhanced     1.82 \n4        70 Charolais Steer  Standard     1.84",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#looking-ahead",
    "href": "chapters/ch10-descriptive_statistics.html#looking-ahead",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.1 Looking Ahead",
    "text": "18.1 Looking Ahead\nNext week, we’ll build on these foundations by learning about:\n\nProbability distributions (especially the normal distribution)\nThe Central Limit Theorem (why means are normally distributed)\nStandard error vs standard deviation\nConfidence intervals (quantifying uncertainty)\nIntroduction to sampling distributions\n\nThese concepts will bridge descriptive statistics to inferential statistics, allowing us to make conclusions about populations based on samples.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#reflection-questions",
    "href": "chapters/ch10-descriptive_statistics.html#reflection-questions",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.2 Reflection Questions",
    "text": "18.2 Reflection Questions\nBefore next week, consider:\n\nFind a dataset from your research (or use one from class). Perform a complete EDA following the steps in this chapter.\nIn published papers from your field, are both mean/SD and median/IQR reported? Are visualizations included?\nThink about a variable you measure in your work. What would you consider an outlier? What would you do if you found one?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#additional-resources",
    "href": "chapters/ch10-descriptive_statistics.html#additional-resources",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.3 Additional Resources",
    "text": "18.3 Additional Resources\n\n18.3.1 R Packages for Descriptive Statistics\n\nskimr: Quick, comprehensive summaries of datasets\nsummarytools: Detailed univariate and bivariate summaries\npsych: Descriptive statistics for psychological/survey data\nDataExplorer: Automated EDA reports\n\n\n\n18.3.2 Recommended Reading\n\n“Exploratory Data Analysis” by John Tukey (1977) - the classic text\n“Data Visualization: A Practical Introduction” by Kieran Healy\n“Fundamentals of Biostatistics” by Bernard Rosner - Chapters 2-3\n\n\n\n18.3.3 Online Resources\n\nR for Data Science (2e): Chapters on data transformation and visualization\nggplot2 book by Hadley Wickham: Comprehensive guide to data visualization",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#session-info",
    "href": "chapters/ch10-descriptive_statistics.html#session-info",
    "title": "10  Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.4 Session Info",
    "text": "18.4 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.4.0    gt_1.1.0        patchwork_1.3.2 broom_1.0.7    \n [5] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [9] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n[13] ggplot2_4.0.0   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] xml2_1.3.6         yaml_2.3.10        fastmap_1.2.0      R6_2.5.1          \n [9] labeling_0.4.3     generics_0.1.3     knitr_1.49         backports_1.5.0   \n[13] htmlwidgets_1.6.4  pillar_1.9.0       RColorBrewer_1.1-3 tzdb_0.4.0        \n[17] rlang_1.1.6        utf8_1.2.4         stringi_1.8.4      xfun_0.53         \n[21] sass_0.4.9         fs_1.6.5           S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.4          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.4.2         hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.1     glue_1.8.0         farver_2.1.2       fansi_1.0.6       \n[37] rmarkdown_2.29     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\nEnd of Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  }
]