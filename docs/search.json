[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Data Science & Statistics",
    "section": "",
    "text": "Welcome\nWelcome to Introduction to Data Science & Statistics (AnS 500) for Fall 2025!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "Introduction to Data Science & Statistics",
    "section": "Course Overview",
    "text": "Course Overview\nThis comprehensive 16-week course is divided into two parts, providing a complete foundation in data science and statistical methods for animal and agricultural sciences. All examples and exercises use R and Quarto for reproducible analysis.\nDuration: 16 weeks (8 weeks per part) Format: 2 hours lecture per week + homework assignments Tools: R + Quarto exclusively Prerequisites: None (we start from the basics!)",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#part-1-introduction-to-data-science-weeks-1-8",
    "href": "index.html#part-1-introduction-to-data-science-weeks-1-8",
    "title": "Introduction to Data Science & Statistics",
    "section": "Part 1: Introduction to Data Science (Weeks 1-8)",
    "text": "Part 1: Introduction to Data Science (Weeks 1-8)\nThe first half introduces you to modern data science practices using R, RStudio, and the tidyverse ecosystem. You‚Äôll learn to import, clean, transform, visualize, and communicate data effectively.\n\nWhat You‚Äôll Learn in Part 1\n\nFoundations: Data science workflow, best practices, project organization\nR & RStudio: Getting started, reading data (CSV, Excel)\nData Manipulation: dplyr for transforming and summarizing data\nVisualization: ggplot2 for creating publication-quality plots\nAdvanced Topics: Reshaping data, joining datasets, iteration with purrr\n\n\n\nPart 1 Chapters\n\nFoundations of Data Science - What is data science? Best practices, R/RStudio/Quarto intro, Git/GitHub\nR, RStudio & Reading Data - Interface, projects, packages, importing CSV/Excel\nData Types & Strings - Types, string manipulation, regex, intro to dplyr\nData Manipulation with dplyr - mutate, arrange, group_by, summarise, handling NAs\nIntroduction to ggplot2 - Grammar of Graphics, essential plot types, themes\nAdvanced ggplot2 - Faceting, statistical layers, combining plots\nReshaping & Joining - Tidy data, pivoting, joins, functional programming\nSpecial Formats & Wrap-up - SAS/SPSS/Stata, Excel, janitor, complete workflow",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#part-2-introduction-to-statistics-weeks-9-16",
    "href": "index.html#part-2-introduction-to-statistics-weeks-9-16",
    "title": "Introduction to Data Science & Statistics",
    "section": "Part 2: Introduction to Statistics (Weeks 9-16)",
    "text": "Part 2: Introduction to Statistics (Weeks 9-16)\nThe second half builds on your data science skills to conduct statistical analyses. You‚Äôll learn frequentist inference, hypothesis testing, and regression modeling.\n\nWhat You‚Äôll Learn in Part 2\n\nStatistical Foundations: P-values, study design, experimental vs observational\nDescriptive Statistics: Measures of central tendency and variability, EDA\nProbability & Inference: Normal distribution, Central Limit Theorem, confidence intervals\nHypothesis Testing: t-tests, ANOVA, power, Type I/II errors\nRegression: Simple and multiple linear regression, model diagnostics\n\n\n\nPart 2 Chapters\n\nStatistical Foundations - Frequentist vs Bayesian, p-values, RCTs, confounding\nDescriptive Statistics - Mean, median, variance, SD, visualization, outliers\nProbability Distributions - Normal distribution, CLT, sampling distributions\nHypothesis Testing - t-tests, null/alternative hypotheses, power\nANOVA - One-way ANOVA, post-hoc tests, multiple comparisons\nCategorical Data - Chi-square, Fisher‚Äôs exact, odds ratios\nSimple Linear Regression - Correlation, least squares, diagnostics\nMultiple Regression - Multiple predictors, model comparison, variable selection",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#r-packages-used",
    "href": "index.html#r-packages-used",
    "title": "Introduction to Data Science & Statistics",
    "section": "R Packages Used",
    "text": "R Packages Used\nThis course makes extensive use of R packages from the tidyverse ecosystem and beyond.\n\nPart 1 Packages (Data Science)\n# Core tidyverse\ninstall.packages(\"tidyverse\")\n\n# Additional Part 1 packages\ninstall.packages(c(\n  \"readxl\",      # Read Excel files\n  \"writexl\",     # Write Excel files\n  \"haven\",       # Read SAS/SPSS/Stata files\n  \"janitor\",     # Data cleaning\n  \"lubridate\",   # Date/time manipulation\n  \"cowplot\",     # Combining plots\n  \"patchwork\",   # Combining plots (alternative)\n  \"here\",        # Project-relative paths\n  \"glue\"         # String interpolation\n))\n\n\nPart 2 Packages (Statistics)\ninstall.packages(c(\n  \"broom\",       # Tidy statistical output\n  \"car\",         # Companion to Applied Regression\n  \"effsize\",     # Effect size calculations\n  \"ggpubr\",      # Publication-ready plots\n  \"rstatix\",     # Pipe-friendly statistical tests\n  \"gt\",          # Grammar of Tables\n  \"emmeans\"      # Estimated marginal means\n))",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-philosophy",
    "href": "index.html#course-philosophy",
    "title": "Introduction to Data Science & Statistics",
    "section": "Course Philosophy",
    "text": "Course Philosophy\nThis course emphasizes:\n\nTidyverse-first approach: Modern R practices from day one\nReproducible research: Every analysis in Quarto\nReal-world data: Animal and agricultural science datasets with real messiness\nBest practices early: Project organization, naming conventions, version control\nStatistical thinking: Understanding over button-pushing\nBuild incrementally: Each week builds on previous skills",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Introduction to Data Science & Statistics",
    "section": "Getting Started",
    "text": "Getting Started\nNavigate through the chapters using the sidebar on the left. Each chapter includes:\n\nLecture content with conceptual explanations\nR code examples you can run and modify\nVisualizations to build intuition\nPractice exercises and homework assignments\n\nLet‚Äôs begin your statistical journey!",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html",
    "href": "chapters/ch01-foundations.html",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "",
    "text": "1.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#learning-objectives",
    "href": "chapters/ch01-foundations.html#learning-objectives",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "",
    "text": "Define data science and understand its key components\nDescribe the data science workflow from data collection to communication\nExplain why statistics is essential for data science\nDistinguish between observational and experimental study designs\nCompare different programming languages for data science (R, Python, Julia)\nUnderstand when to use (and avoid) Excel for data analysis\nOrganize data science projects with proper folder structure and naming conventions\nApply best practices for data formatting, column naming, and documentation\nUnderstand database basics and the difference between long and wide data formats\nRecognize the role of version control (Git/GitHub) in reproducible research\nGet started with R, RStudio, and Quarto\nUnderstand career paths and skills needed for data science",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#what-is-data-science",
    "href": "chapters/ch01-foundations.html#what-is-data-science",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.2 What is Data Science?",
    "text": "1.2 What is Data Science?\n\n1.2.1 Definition and Scope\nData Science is the interdisciplinary field that uses scientific methods, processes, algorithms, and systems to extract knowledge and insights from structured and unstructured data.\nData science combines:\n\nComputer Science: Programming, algorithms, data structures\nStatistics/Mathematics: Probability, inference, modeling\nDomain Expertise: Subject matter knowledge (in our case, animal science!)\n\n\n\n\n\n\n\nNoteData Science vs Statistics vs Machine Learning\n\n\n\n\nStatistics: Focuses on inference and understanding relationships\nMachine Learning: Emphasizes prediction and automation\nData Science: Encompasses both, plus data engineering, visualization, and communication\n\n\n\n\n\n1.2.2 The Data Science Workflow\nEvery data science project follows a similar workflow:\n\n\n\n\n\nflowchart LR\n    A[Collect] --&gt; B[Clean]\n    B --&gt; C[Explore]\n    C --&gt; D[Analyze]\n    D --&gt; E[Communicate]\n    E -.-&gt; A\n\n\n\n\n\n\n\nCollect: Import or gather data from various sources\nClean: Handle missing values, fix errors, standardize formats\nExplore: Visualize and summarize to understand patterns\nAnalyze: Apply statistical methods or models\nCommunicate: Present findings clearly to stakeholders\n\nThis workflow is iterative‚Äîyou‚Äôll often cycle back to earlier steps as you learn more about your data.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#why-statistics-matters",
    "href": "chapters/ch01-foundations.html#why-statistics-matters",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.3 Why Statistics Matters",
    "text": "1.3 Why Statistics Matters\nWhile Part 1 focuses on data manipulation and visualization, statistics (Part 2 of this course) is essential because it allows us to:\n\nQuantify uncertainty: How confident are we in our findings?\nMake inferences: Can we generalize from a sample to a population?\nTest hypotheses: Is this treatment effect real or due to chance?\nBuild models: What factors predict our outcome of interest?\n\n\n\n\n\n\n\nImportantData Science Without Statistics is Dangerous\n\n\n\nBeautiful visualizations and complex models are useless if we don‚Äôt understand:\n\nWhether our sample is representative\nWhether differences are statistically significant\nWhether our assumptions are violated\nWhether our conclusions are justified\n\n\n\nWe‚Äôll preview some statistical concepts here, but Part 2 will dive deep into hypothesis testing, ANOVA, regression, and more.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#study-design-observational-vs-experimental",
    "href": "chapters/ch01-foundations.html#study-design-observational-vs-experimental",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.4 Study Design: Observational vs Experimental",
    "text": "1.4 Study Design: Observational vs Experimental\nUnderstanding study design is crucial for interpreting data correctly.\n\n1.4.1 Observational Studies\nIn observational studies, researchers observe without manipulating variables.\nCharacteristics:\n\nNo treatment assignment by researcher\nCannot control for all confounding variables\nCan show associations but not causation\n\nExamples:\n\nSurveying farmers about feeding practices and milk production\nTracking health outcomes in animals over time\nComparing herds that naturally differ in management\n\nAdvantages: Often cheaper, faster, and more ethical\nDisadvantages: Confounding variables make causal inference difficult\n\n\n1.4.2 Experimental Studies\nIn experimental studies, researchers actively manipulate variables (treatments).\nCharacteristics:\n\nResearcher assigns treatments\nRandomization balances confounders\nCan establish causation (under proper design)\n\nExamples:\n\nFeed trial: Randomly assign calves to different diets\nDrug efficacy: Randomly assign animals to treatment vs placebo\nBreeding trial: Randomly assign mating pairs\n\nAdvantages: Can establish cause-and-effect\nDisadvantages: Can be expensive, time-consuming, or unethical\n\n\n\n\n\n\nTipGold Standard: Randomized Controlled Trials (RCTs)\n\n\n\nThe strongest experimental design involves:\n\nRandomization: Random assignment to treatment/control\nControl group: Baseline for comparison\nReplication: Multiple subjects per treatment\nBlinding: When possible, subjects/observers don‚Äôt know treatment\n\nWe‚Äôll explore RCTs in detail in Part 2, Week 1.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#programming-languages-for-data-science",
    "href": "chapters/ch01-foundations.html#programming-languages-for-data-science",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.5 Programming Languages for Data Science",
    "text": "1.5 Programming Languages for Data Science\nData science can be done in many languages. Let‚Äôs compare the three most popular:\n\n1.5.1 R\nR is a programming language designed specifically for statistical computing and graphics.\nStrengths:\n\nExcellent for statistics and data visualization\nHuge ecosystem of packages (CRAN, Bioconductor)\nStrong in academia, especially biological/agricultural sciences\nFree and open-source\nRStudio IDE is fantastic\nQuarto for reproducible reports\n\nWeaknesses:\n\nSlower than compiled languages for some tasks\nInconsistent syntax (though tidyverse helps!)\nSteeper learning curve for general programming\n\nBest for: Statistical analysis, data visualization, academic research\n\n\n1.5.2 Python\nPython is a general-purpose programming language with strong data science libraries.\nStrengths:\n\nVersatile (web dev, automation, machine learning, data science)\nHuge community and job market\nExcellent for machine learning (scikit-learn, TensorFlow, PyTorch)\nPandas for data manipulation\nClean, readable syntax\n\nWeaknesses:\n\nData visualization not as elegant as R‚Äôs ggplot2\nStatistical methods less comprehensive than R\nMultiple competing tools for the same task\n\nBest for: Machine learning, automation, production systems, general programming\n\n\n1.5.3 Julia\nJulia is a newer language designed for high-performance numerical computing.\nStrengths:\n\nFast (C-like speed with Python-like syntax)\nDesigned for scientific computing\nEasy to write high-performance code\nMultiple dispatch system is powerful\n\nWeaknesses:\n\nSmaller ecosystem than R or Python\nLess mature tooling\nSmaller community\nFewer learning resources\n\nBest for: High-performance computing, complex simulations, numerical optimization\n\n\n1.5.4 Which Should You Learn?\nFor animal science and agricultural research, we recommend starting with R because:\n\nMost statistics textbooks and courses use R\nSuperior data visualization with ggplot2\nExcellent for reproducible research with Quarto\nStrong community in biological sciences\nEasier for non-programmers to learn\n\nBut: Python is valuable for machine learning and automation. Many data scientists know both!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#why-avoid-excel-for-analysis",
    "href": "chapters/ch01-foundations.html#why-avoid-excel-for-analysis",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.6 Why Avoid Excel for Analysis?",
    "text": "1.6 Why Avoid Excel for Analysis?\nMicrosoft Excel is ubiquitous and useful for simple tasks, but problematic for data analysis.\n\n1.6.1 Problems with Excel\n\nNot Reproducible:\n\nPoint-and-click operations aren‚Äôt documented\nCan‚Äôt easily share analysis steps\nHard to audit or replicate\n\nError-Prone:\n\nEasy to accidentally modify data\nNo validation of formulas\nCopy-paste errors are common\n\nLimited Capabilities:\n\nPoor for large datasets (1M row limit)\nLimited statistical functions\nVisualization options are restrictive\n\nAutomatic Conversions (notorious):\n\nConverts gene names to dates (SEPT1 ‚Üí Sep-1)\nChanges scientific notation unpredictably\nAlters leading zeros\n\nVersion Control Nightmare:\n\nBinary format, can‚Äôt use Git effectively\nMultiple versions proliferate (analysis_final_v3_FINAL.xlsx)\n\n\n\n\n\n\n\n\nWarningExcel Has Cost Researchers Dearly\n\n\n\n\nEconomics paper retracted due to Excel error (Reinhart-Rogoff, 2013)\nThousands of genomics papers have errors from Excel gene name conversions\nCOVID-19 cases lost in UK due to Excel row limit (2020)\n\n\n\n\n\n1.6.2 When Excel is Okay\nExcel is fine for:\n\nInitial data entry (but export to CSV immediately!)\nQuick visual inspection of small datasets\nSharing final results with non-technical collaborators\nSimple calculations that don‚Äôt need to be reproduced\n\n\n\n1.6.3 The Better Approach\nUse R + Quarto for analysis because:\n\nEvery step is documented in code\nAnalysis is fully reproducible\nEasy to update when data changes\nVersion control with Git\nProfessional, publication-quality output",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#project-organization-folder-structure",
    "href": "chapters/ch01-foundations.html#project-organization-folder-structure",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.7 Project Organization: Folder Structure",
    "text": "1.7 Project Organization: Folder Structure\nConsistent project organization makes collaboration easier and reduces errors.\n\n1.7.1 Recommended Folder Structure\nproject_name/\n‚îÇ\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Original, untouched data\n‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Cleaned data ready for analysis\n‚îÇ   ‚îî‚îÄ‚îÄ README.md         # Metadata documentation\n‚îÇ\n‚îú‚îÄ‚îÄ code/\n‚îÇ   ‚îú‚îÄ‚îÄ 01_import.R       # Data import and initial cleaning\n‚îÇ   ‚îú‚îÄ‚îÄ 02_clean.R        # Data cleaning\n‚îÇ   ‚îú‚îÄ‚îÄ 03_analyze.R      # Statistical analysis\n‚îÇ   ‚îî‚îÄ‚îÄ 04_visualize.R    # Create plots\n‚îÇ\n‚îú‚îÄ‚îÄ output/\n‚îÇ   ‚îú‚îÄ‚îÄ figures/          # Plots and visualizations\n‚îÇ   ‚îú‚îÄ‚îÄ tables/           # Summary tables\n‚îÇ   ‚îî‚îÄ‚îÄ reports/          # Rendered Quarto documents\n‚îÇ\n‚îú‚îÄ‚îÄ docs/                 # Documentation, notes\n‚îÇ\n‚îú‚îÄ‚îÄ project_name.Rproj    # RStudio project file\n‚îî‚îÄ‚îÄ README.md             # Project overview\n\n\n\n\n\n\nTipKey Principles\n\n\n\n\nNever modify raw data: Keep originals untouched in data/raw/\nNumber your scripts: Use prefixes like 01_, 02_ for execution order\nSeparate data from code: Don‚Äôt mix data files and scripts\nDocument everything: README files explain what‚Äôs what\nUse relative paths: R Projects make this easy",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#file-naming-conventions",
    "href": "chapters/ch01-foundations.html#file-naming-conventions",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.8 File Naming Conventions",
    "text": "1.8 File Naming Conventions\nGood file names are:\n\nMachine-readable: No spaces, special characters\nHuman-readable: Descriptive, not cryptic\nSortable: Use ISO dates (YYYY-MM-DD), leading zeros\n\n\n1.8.1 Examples\n\nBad NamesGood Names\n\n\nmy data.xlsx\nfinal.R\nfig1.png\nanalysis version 2 (old).R\nreport_10_1_23.docx\nProblems: spaces, not descriptive, ambiguous dates, ‚Äúfinal‚Äù lies\n\n\npig_weights_2024-11-14.csv\n01_import_data.R\n02_clean_data.R\nfigure_01_weight_by_treatment.png\nreport_feed_trial_2024-11-14.html\nBenefits: ISO dates, descriptive, machine-readable, sortable\n\n\n\n\n\n1.8.2 Naming Best Practices\n# Use underscores or hyphens (not spaces)\npig_growth_data.csv      # good\npig-growth-data.csv      # good\npig growth data.csv      # bad\n\n# Use ISO 8601 dates: YYYY-MM-DD\ndata_2024-11-14.csv      # good\ndata_11-14-24.csv        # ambiguous\ndata_Nov_14_2024.csv     # not sortable\n\n# Include important metadata\ncattle_weights_iowa_2024.csv       # good\ndata.csv                           # too vague\n\n# Use leading zeros for sorting\nfile_01.R, file_02.R, file_10.R    # good\nfile_1.R, file_2.R, file_10.R      # file_10.R sorts before file_2.R!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#data-best-practices",
    "href": "chapters/ch01-foundations.html#data-best-practices",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.9 Data Best Practices",
    "text": "1.9 Data Best Practices\n\n1.9.1 Column Naming\nGood column names are crucial for analysis.\nRules:\n\nNo spaces: Use snake_case or camelCase\nNo special characters: Avoid #, $, %, @, etc.\nStart with letters: Not numbers or symbols\nBe descriptive: body_weight_kg not bw\nUse consistent units: weight_kg, height_cm, temp_c\nLowercase preferred: Easier to type, less error-prone\n\n\nBad Column NamesGood Column Names\n\n\nAnimal #\nBody Weight (kg)\nDate-of-Birth\ntreatment$group\n2024_weight\n\n\nanimal_id\nbody_weight_kg\ndate_of_birth\ntreatment_group\nweight_2024_kg\n\n\n\n\n\n1.9.2 Data Format: CSV vs Excel\n\n\n\n\n\n\n\n\n\nFormat\nPros\nCons\nWhen to Use\n\n\n\n\nCSV\nPlain text, universal, version control friendly, fast\nNo formatting, one sheet only\nPreferred for analysis data\n\n\nExcel (.xlsx)\nMultiple sheets, formatting, formulas\nBinary format, Excel errors, version control issues\nData entry, sharing with non-technical users\n\n\n\n\n\n\n\n\n\nImportantGolden Rule\n\n\n\nAlways export to CSV for analysis, even if data was entered in Excel.\n# In R, read CSV files with:\nlibrary(readr)\ndata &lt;- read_csv(\"data/raw/animal_weights.csv\")\n\n\n\n\n1.9.3 Tidy Data Principles\nTidy data (Hadley Wickham) has a consistent structure that makes analysis easier:\n\nEach variable is a column\nEach observation is a row\nEach type of observational unit is a table\n\nExample:\n\nTidy FormatMessy Format (Wide)\n\n\n\n\n\nanimal_id\ndate\nweight_kg\ntreatment\n\n\n\n\nA01\n2024-01-01\n250\nControl\n\n\nA01\n2024-02-01\n275\nControl\n\n\nA02\n2024-01-01\n245\nTreatment\n\n\nA02\n2024-02-01\n285\nTreatment\n\n\n\n\n\n\n\n\nanimal_id\ntreatment\nweight_jan\nweight_feb\n\n\n\n\nA01\nControl\n250\n275\n\n\nA02\nTreatment\n245\n285\n\n\n\n\n\n\nThe tidy format is easier to filter, group, and plot. We‚Äôll learn to reshape data in Week 7.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#database-basics",
    "href": "chapters/ch01-foundations.html#database-basics",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.10 Database Basics",
    "text": "1.10 Database Basics\n\n1.10.1 Long vs Wide Format\nData can be organized in two ways:\nLong Format (Tidy):\n\nEach row is one observation\nMultiple rows per subject\nBetter for analysis and plotting\n\nWide Format:\n\nEach row is one subject\nMultiple columns for repeated measures\nMore compact, easier for humans to read\n\nWe‚Äôll use tidyr (Week 7) to convert between formats.\n\n\n1.10.2 Relational Databases\nIn complex projects, data often exists in multiple related tables.\nExample: Feed trial study\nTable 1: animals\n\n\n\nanimal_id\nbirth_date\nbreed\nfarm_id\n\n\n\n\nA001\n2023-05-10\nAngus\nF01\n\n\nA002\n2023-05-12\nHolstein\nF02\n\n\n\nTable 2: weights\n\n\n\nanimal_id\ndate\nweight_kg\n\n\n\n\nA001\n2024-01-01\n250\n\n\nA001\n2024-02-01\n275\n\n\nA002\n2024-01-01\n320\n\n\n\nTable 3: farms\n\n\n\nfarm_id\nfarm_name\nstate\n\n\n\n\nF01\nSmith Ranch\nIA\n\n\nF02\nJohnson Dairy\nWI\n\n\n\nThese tables are linked by keys (animal_id, farm_id). We‚Äôll learn to join them in Week 7.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#metadata-document-your-data",
    "href": "chapters/ch01-foundations.html#metadata-document-your-data",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.11 Metadata: Document Your Data",
    "text": "1.11 Metadata: Document Your Data\nMetadata is ‚Äúdata about data‚Äù‚Äîit explains what your data means.\n\n1.11.1 Create a Data Dictionary\nEvery dataset should have a README or data dictionary:\n# Pig Growth Trial Data Dictionary\n\n**File**: pig_weights_2024.csv\n**Date Created**: 2024-11-14\n**Author**: Dr. Jane Smith\n**Description**: Weekly weights for 100 pigs in feed trial\n\n## Variables\n\n- `pig_id`: Unique identifier (P001-P100)\n- `treatment`: Feed type (A, B, or Control)\n- `date`: Date of measurement (YYYY-MM-DD)\n- `weight_kg`: Body weight in kilograms\n- `pen`: Pen number (1-10, 10 pigs per pen)\n\n## Notes\n\n- Missing weights indicate pig was sick that week\n- Treatment A = high protein diet\n- Treatment B = standard diet with supplements\n- Control = standard commercial diet\nThis documentation is crucial for:\n\nCollaborators understanding your data\nFuture you remembering what you did\nJournal reviewers checking your work",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#version-control-git-and-github",
    "href": "chapters/ch01-foundations.html#version-control-git-and-github",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.12 Version Control: Git and GitHub",
    "text": "1.12 Version Control: Git and GitHub\n\n1.12.1 Why Version Control?\nWithout version control, you end up with:\nanalysis.R\nanalysis_final.R\nanalysis_final2.R\nanalysis_final_FINAL.R\nanalysis_final_FINAL_v2_USE_THIS_ONE.R\nVersion control tracks changes over time so you have:\nanalysis.R  (with full change history)\n\n\n1.12.2 Git\nGit is the most popular version control system.\nWhat Git does:\n\nTracks all changes to files over time\nAllows you to revert to previous versions\nShows who changed what and when\nEnables branching for experiments\nFacilitates collaboration\n\nBasic Git workflow:\ngit add file.R        # Stage changes\ngit commit -m \"Updated analysis\"  # Save snapshot\ngit push              # Upload to GitHub\n\n\n1.12.3 GitHub\nGitHub is a cloud platform for hosting Git repositories.\nBenefits:\n\nBackup your code online\nCollaborate with others\nShare code publicly or privately\nTrack issues and projects\nHost websites (GitHub Pages)\n\n\n\n\n\n\n\nNoteWe‚Äôll Use Git Lightly in This Course\n\n\n\nVersion control is important, but can be overwhelming for beginners. We‚Äôll introduce basic concepts, but won‚Äôt require mastery.\nAs you become more serious about data science, invest time in learning Git properly!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#introduction-to-r-rstudio-and-quarto",
    "href": "chapters/ch01-foundations.html#introduction-to-r-rstudio-and-quarto",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.13 Introduction to R, RStudio, and Quarto",
    "text": "1.13 Introduction to R, RStudio, and Quarto\n\n1.13.1 What is R?\nR is a free, open-source programming language designed for statistical computing and graphics.\nInstall R:\n\nGo to https://www.r-project.org/\nClick ‚Äúdownload R‚Äù\nChoose a CRAN mirror (any US mirror works)\nDownload for your operating system (Windows, Mac, Linux)\n\n\n\n1.13.2 What is RStudio?\nRStudio (now called Posit) is an Integrated Development Environment (IDE) for R‚Äîit makes R much easier to use.\nInstall RStudio:\n\nGo to https://posit.co/downloads/\nDownload RStudio Desktop (free version)\nInstall (requires R to be installed first)\n\n\n\n\n\n\n\nImportantInstall Order Matters!\n\n\n\n\nInstall R first\nThen install RStudio\n\n\n\n\n\n1.13.3 What is Quarto?\nQuarto is an open-source publishing system that lets you combine code, text, and output in a single document.\nWhy Quarto?\n\nWrite analysis and explanation together\nCode runs automatically when rendered\nOutputs to HTML, PDF, Word, slides, etc.\nReproducible reports with one click\nSuccessor to R Markdown with more features\n\nQuarto is included in RStudio, no separate installation needed!\n\n\n1.13.4 Your First R Session\nOpen RStudio and try some basic commands in the Console:\n\n# R as a calculator\n2 + 2\n\n[1] 4\n\n10 * 5\n\n[1] 50\n\nsqrt(16)\n\n[1] 4\n\n# Create variables\nx &lt;- 5\ny &lt;- 10\nx + y\n\n[1] 15\n\n# Create a vector\nweights &lt;- c(250, 275, 290, 310)\nweights\n\n[1] 250 275 290 310\n\n# Calculate mean\nmean(weights)\n\n[1] 281.25\n\n\nCongratulations, you‚Äôve written R code!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#what-does-it-take-to-be-a-data-scientist",
    "href": "chapters/ch01-foundations.html#what-does-it-take-to-be-a-data-scientist",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.14 What Does It Take to Be a Data Scientist?",
    "text": "1.14 What Does It Take to Be a Data Scientist?\n\n1.14.1 Core Skills\n\nProgramming: R or Python for data manipulation\nStatistics: Hypothesis testing, regression, experimental design\nVisualization: Communicating patterns clearly\nDomain Knowledge: Understanding the field (animal science!)\nCommunication: Explaining technical results to non-technical audiences\n\n\n\n1.14.2 Useful Additional Skills\n\nSQL: Querying databases\nMachine Learning: Predictive modeling\nCloud Computing: AWS, Google Cloud, Azure\nCommand Line: Bash scripting\nVersion Control: Git/GitHub\n\n\n\n1.14.3 Career Paths\nData science skills open many doors:\n\nResearch Scientist: University or industry R&D\nData Analyst: Descriptive analytics, reporting\nData Scientist: Predictive modeling, machine learning\nBioinformatician: Genomics and computational biology\nQuantitative Geneticist: Breeding programs, genomic selection\nStatistical Consultant: Supporting researchers\n\n\n\n\n\n\n\nTipYou Don‚Äôt Need to Master Everything!\n\n\n\nStart with the fundamentals (this course), then specialize based on your interests and career goals.\nEvery data scientist started as a beginner!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#summary",
    "href": "chapters/ch01-foundations.html#summary",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.15 Summary",
    "text": "1.15 Summary\nThis chapter introduced the foundations of data science:\n\nData science combines programming, statistics, and domain expertise\nThe data science workflow is iterative: collect ‚Üí clean ‚Üí explore ‚Üí analyze ‚Üí communicate\nStudy design matters: experimental studies can establish causation, observational studies show associations\nR is excellent for statistics and visualization; Python for machine learning; Julia for speed\nAvoid Excel for analysis; use R + Quarto for reproducibility\nOrganize projects with consistent folder structure and naming conventions\nData best practices: tidy format, good column names, CSV for analysis, metadata documentation\nVersion control (Git/GitHub) tracks changes and enables collaboration\nR, RStudio, and Quarto are our tools for reproducible analysis",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#homework-assignment",
    "href": "chapters/ch01-foundations.html#homework-assignment",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.16 Homework Assignment",
    "text": "1.16 Homework Assignment\n\n1.16.1 Assignment: Getting Started with Data Science\nDue: Before Week 2\n\n1.16.1.1 Part 1: Installation (30 points)\n\nInstall R from https://www.r-project.org/\nInstall RStudio from https://posit.co/downloads/\nInstall the tidyverse package in R:\ninstall.packages(\"tidyverse\")\nTake a screenshot of RStudio with the message showing tidyverse installed successfully\n\n\n\n1.16.1.2 Part 2: Project Setup (30 points)\n\nCreate a new RStudio Project called ‚Äúans500_firstname_lastname‚Äù\nWithin the project folder, create the following subfolders:\n\ndata/raw\ndata/processed\ncode\noutput/figures\n\nCreate a README.md file describing the project\nTake screenshots of your folder structure\n\n\n\n1.16.1.3 Part 3: First Quarto Document (40 points)\nCreate a Quarto document called week1_homework.qmd with:\n\nYAML header with title, author, and date\nIntroduction section explaining:\n\nWhat is data science?\nWhy are you taking this course?\nWhat do you hope to learn?\n\nR code chunk that:\n\nCreates a vector of animal weights (make up 10 values)\nCalculates the mean and standard deviation\nCreates a simple histogram\n\nReflection section (200-300 words) on:\n\nOne thing that surprised you about data science\nHow you currently manage data (Excel? Paper? Other?)\nWhat bad habits you might need to break\n\n\nRender to HTML and submit both .qmd and .html files.\n\n\n\n1.16.2 Recommended YAML\n---\ntitle: \"Week 1 Homework: Foundations of Data Science\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    code-fold: false\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n1.16.3 Grading Rubric\n\nInstallation (30%): All software installed, tidyverse working\nProject Setup (30%): Proper folder structure, README created\nQuarto Document (40%):\n\nCode runs without errors (10%)\nProper formatting and organization (10%)\nThoughtful reflection (20%)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch01-foundations.html#additional-resources",
    "href": "chapters/ch01-foundations.html#additional-resources",
    "title": "1¬† Foundations of Data Science & Best Practices",
    "section": "1.17 Additional Resources",
    "text": "1.17 Additional Resources\n\n1.17.1 Required Reading\n\nR for Data Science (2e) - Chapters 1-3: Introduction, Data Visualization, Workflow Basics\nQuarto Getting Started\n\n\n\n1.17.2 Optional Reading\n\nWickham, H. (2014). ‚ÄúTidy Data.‚Äù Journal of Statistical Software, 59(10). Link\nWilson, G. et al.¬†(2017). ‚ÄúGood enough practices in scientific computing.‚Äù PLOS Computational Biology. Link\n\n\n\n1.17.3 Videos\n\n‚ÄúWhat is Data Science?‚Äù by StatQuest\n‚ÄúIntroduction to R and RStudio‚Äù by RStudio\n\n\n\n1.17.4 Cheat Sheets\n\nRStudio IDE Cheat Sheet\nR Markdown (Quarto predecessor) Cheat Sheet\n\n\nNext Chapter: Getting Started with R, RStudio, and Reading Data",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Foundations of Data Science & Best Practices</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html",
    "href": "chapters/ch02-r_rstudio_reading_data.html",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "",
    "text": "2.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#learning-objectives",
    "href": "chapters/ch02-r_rstudio_reading_data.html#learning-objectives",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "",
    "text": "Navigate the RStudio interface and understand the purpose of each pane\nCreate and use R Projects for better project organization\nUnderstand working directories and file paths\nDistinguish between installing and loading R packages\nCreate and render Quarto documents with YAML headers, markdown, and code chunks\nRead CSV files using readr::read_csv()\nRead Excel files using readxl::read_excel()\nExplore data using functions like head(), tail(), glimpse(), str(), and summary()\nUnderstand the difference between data frames and tibbles\nUse R‚Äôs help system effectively",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#the-rstudio-interface",
    "href": "chapters/ch02-r_rstudio_reading_data.html#the-rstudio-interface",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.2 The RStudio Interface",
    "text": "2.2 The RStudio Interface\nWhen you open RStudio, you‚Äôll see a workspace divided into several panes. Understanding each pane‚Äôs purpose will help you work efficiently.\n\n2.2.1 The Four Main Panes\n\n\n\n\n\nflowchart TD\n    A[RStudio Interface] --&gt; B[Source Editor&lt;br/&gt;Top Left]\n    A --&gt; C[Console&lt;br/&gt;Bottom Left]\n    A --&gt; D[Environment/History&lt;br/&gt;Top Right]\n    A --&gt; E[Files/Plots/Help&lt;br/&gt;Bottom Right]\n\n    B --&gt; B1[Write and edit scripts]\n    B --&gt; B2[Create Quarto documents]\n\n    C --&gt; C1[Execute R commands]\n    C --&gt; C2[View output]\n\n    D --&gt; D1[View objects in memory]\n    D --&gt; D2[See command history]\n\n    E --&gt; E1[Browse files]\n    E --&gt; E2[View plots]\n    E --&gt; E3[Read documentation]\n\n\n\n\n\n\n\n2.2.1.1 1. Source Editor (Top Left)\nThis is where you write and edit your code.\n\nScripts (.R files): Plain R code\nQuarto documents (.qmd files): Mix code, text, and output\nMultiple tabs: Work on several files at once\n\nKeyboard shortcuts:\n\nCmd/Ctrl + Enter: Run current line or selection\nCmd/Ctrl + Shift + Enter: Run entire script\nCmd/Ctrl + S: Save file\n\n\n\n2.2.1.2 2. Console (Bottom Left)\nThis is where R actually runs and shows output.\n\nType commands directly for quick tests\nSee results from scripts\nView error messages and warnings\n\n\n\n\n\n\n\nNoteConsole vs Scripts\n\n\n\nConsole: Good for quick tests, exploring data Scripts: Good for saving your work, reproducibility\nAlways save important code in scripts or Quarto documents, not just in the console!\n\n\n\n\n2.2.1.3 3. Environment/History (Top Right)\nEnvironment tab:\n\nShows all objects currently in memory (datasets, variables, functions)\nClick on dataset names to view them\nShows object types and sizes\n\nHistory tab:\n\nShows previous commands\nSearch through past code\nReload old commands\n\n\n\n\n\n\n\nTipCleaning Your Environment\n\n\n\nTo start fresh:\n# Remove all objects from environment\nrm(list = ls())\nOr click the broom icon in the Environment pane.\n\n\n\n\n2.2.1.4 4. Files/Plots/Help (Bottom Right)\nFiles tab: Browse your project directory Plots tab: View visualizations Packages tab: Manage installed packages Help tab: Read function documentation Viewer tab: Preview HTML output\n\n\n\n2.2.2 Customizing RStudio\nMake RStudio work for you!\nTools ‚Üí Global Options (or Cmd/Ctrl + ,):\n\nAppearance: Choose themes (dark mode, font size, colors)\nCode: Enable syntax highlighting, code completion\nPane Layout: Rearrange panes to your preference\n\n\n\n\n\n\n\nTipRecommended Settings\n\n\n\n\nAppearance ‚Üí Choose a comfortable theme (I like ‚ÄúTomorrow Night Bright‚Äù for dark mode)\nCode ‚Üí Check ‚ÄúSoft-wrap R source files‚Äù (no horizontal scrolling)\nGeneral ‚Üí Uncheck ‚ÄúRestore .RData into workspace at startup‚Äù (for reproducibility)\nGeneral ‚Üí Uncheck ‚ÄúSave workspace to .RData on exit‚Äù",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#r-projects-and-working-directories",
    "href": "chapters/ch02-r_rstudio_reading_data.html#r-projects-and-working-directories",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.3 R Projects and Working Directories",
    "text": "2.3 R Projects and Working Directories\n\n2.3.1 What is a Working Directory?\nThe working directory is the folder where R looks for files and saves output.\nCheck your current working directory:\n\n# Where am I?\ngetwd()\n\n[1] \"/Users/austinputz/Claude/ans-5000/chapters\"\n\n\nProblem: If you use absolute paths, your code won‚Äôt work on other computers:\n# BAD: Absolute path (only works on my computer)\ndata &lt;- read.csv(\"/Users/jane/Documents/my_project/data/cattle.csv\")\nSolution: Use R Projects and relative paths!\n\n\n2.3.2 What is an R Project?\nAn R Project is a special file (.Rproj) that:\n\nSets the working directory to the project folder automatically\nKeeps all project files organized\nMakes code portable across computers\nTracks your workspace and settings\n\n\n\n2.3.3 Creating an R Project\nMethod 1: New Project\n\nFile ‚Üí New Project‚Ä¶\nChoose:\n\nNew Directory: Start from scratch\nExisting Directory: Use an existing folder\nVersion Control: Clone from Git/GitHub\n\nName your project (e.g., ans500_cattle_analysis)\nChoose where to save it\nClick Create Project\n\nMethod 2: From Existing Folder\nIf you already have a project folder:\n\nNavigate to the folder in RStudio‚Äôs Files pane\nMore ‚Üí Set As Working Directory\nFile ‚Üí New Project ‚Üí Existing Directory\n\n\n\n\n\n\n\nImportantAlways Use R Projects!\n\n\n\nBenefits:\n\nReproducibility: Code works on any computer\nOrganization: Everything in one place\nConvenience: No more setwd()!\nPortability: Easy to share with collaborators\n\n\n\n\n\n2.3.4 The here Package\nEven better than relative paths: use the here package!\n\n# Install once\ninstall.packages(\"here\")\n\n# Load the package\nlibrary(here)\n\n# Read data using here()\ncattle &lt;- read.csv(here(\"data\", \"raw\", \"cattle_weights.csv\"))\n\n# Save plots using here()\nggsave(here(\"output\", \"figures\", \"weight_plot.png\"))\n\nWhy here is awesome:\n\nWorks from any subdirectory\nWorks in Quarto documents\nWorks across operating systems (Mac, Windows, Linux)\nMakes paths explicit and readable",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#installing-and-loading-packages",
    "href": "chapters/ch02-r_rstudio_reading_data.html#installing-and-loading-packages",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.4 Installing and Loading Packages",
    "text": "2.4 Installing and Loading Packages\nR‚Äôs power comes from packages‚Äîcollections of functions written by the community.\n\n2.4.1 Packages vs Libraries\n\nAnalogyIn R\n\n\nPackage = App on your phone Library = Installing/opening the app\n\n\nPackage: A collection of functions, data, and documentation Library: The folder where packages are stored\n\n\n\n\n\n2.4.2 Installing Packages\nYou only need to install a package once (or when updating).\n\n# Install a single package\ninstall.packages(\"readr\")\n\n# Install multiple packages\ninstall.packages(c(\"readr\", \"dplyr\", \"ggplot2\"))\n\n# Or install the tidyverse (includes many packages)\ninstall.packages(\"tidyverse\")\n\n\n\n\n\n\n\nWarningInstallation Notes\n\n\n\n\nPut package names in quotes: install.packages(\"readr\")\nRequires internet connection\nMay take a few minutes for large packages\nChoose a CRAN mirror (any US mirror works)\n\n\n\n\n\n2.4.3 Loading Packages\nYou need to load a package every time you start R.\n\n# Load packages (no quotes!)\nlibrary(readr)\nlibrary(dplyr)\n\n# Or load everything at once\nlibrary(tidyverse)\n\n\n\n\n\n\n\nNoteInstall Once, Load Every Session\n\n\n\n\n# Do this ONCE:\ninstall.packages(\"tidyverse\")\n\n# Do this EVERY SESSION:\nlibrary(tidyverse)\n\nThink of it like installing an app once, but opening it every time you want to use it.\n\n\n\n\n2.4.4 Common Package Errors\nError: package not found\nlibrary(readr)\n# Error: there is no package called 'readr'\nSolution: Install the package first!\ninstall.packages(\"readr\")\nlibrary(readr)\nError: could not find function\nread_csv(\"data.csv\")\n# Error: could not find function \"read_csv\"\nSolution: Load the package!\nlibrary(readr)  # Now read_csv() is available\n\n\n2.4.5 Understanding CRAN\nCRAN (Comprehensive R Archive Network) is the official repository for R packages.\n\nOver 20,000 packages available\nAll packages are tested and documented\nSafe to install from CRAN\n\nOther sources:\n\nBioconductor: Bioinformatics packages\nGitHub: Development versions of packages\n\n\n\n2.4.6 Package Documentation\nEvery package has documentation:\n\n# View package overview\nhelp(package = \"readr\")\n\n# See all functions in a package\nlibrary(help = \"readr\")\n\n# Read vignettes (tutorials)\nvignette(package = \"readr\")\nvignette(\"readr\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#quarto-documents",
    "href": "chapters/ch02-r_rstudio_reading_data.html#quarto-documents",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.5 Quarto Documents",
    "text": "2.5 Quarto Documents\nQuarto is a publishing system that lets you combine code, text, and output in a single document.\n\n2.5.1 Why Use Quarto?\nTraditional workflow:\n\nAnalyze data in R\nCopy results to Word/Excel\nInsert plots manually\nWrite explanation\nIf data changes: Repeat all steps üò´\n\nQuarto workflow:\n\nWrite code and explanation together\nRender with one click\nIf data changes: Just re-render! ‚ú®\n\n\n\n\n\n\n\nImportantReproducible Research\n\n\n\nWith Quarto:\n\nCode and results are never out of sync\nAnyone can reproduce your analysis\nUpdating is automatic\nProfessional output (HTML, PDF, Word)\n\n\n\n\n\n2.5.2 Creating a Quarto Document\nFile ‚Üí New File ‚Üí Quarto Document\n\nEnter title and author\nChoose output format (HTML recommended to start)\nClick Create\n\nYou‚Äôll get a template with example content.\n\n\n2.5.3 Anatomy of a Quarto Document\nA Quarto document (.qmd) has three main components:\n\n\n\n\n\nflowchart LR\n    A[Quarto Document] --&gt; B[YAML Header]\n    A --&gt; C[Markdown Text]\n    A --&gt; D[Code Chunks]\n\n    B --&gt; B1[Metadata and settings]\n    C --&gt; C1[Formatted text]\n    D --&gt; D1[Executable R code]\n\n\n\n\n\n\n\n\n2.5.4 1. YAML Header\nAt the top of every Quarto document, between --- markers:\n---\ntitle: \"My First Analysis\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    code-fold: false\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\nCommon options:\n\ntitle, author, date: Document metadata\nformat: Output type (html, pdf, docx)\ntoc: true: Add table of contents\ncode-fold: true: Hide code by default (click to show)\ntheme: Visual appearance\nexecute: Control code execution\n\n\n\n\n\n\n\nTipYAML is Picky!\n\n\n\n\nIndentation matters (use 2 spaces)\nColons need a space after them\nUse today for automatic date\nIf you get an error, check your indentation!\n\n\n\n\n\n2.5.5 2. Markdown Text\nMarkdown is a simple way to format text:\n\nHeadersEmphasisListsLinksImages\n\n\n# Level 1 Header\n## Level 2 Header\n### Level 3 Header\n\n\n*italic* or _italic_\n**bold** or __bold__\n***bold italic***\n\n\nUnordered list:\n- Item 1\n- Item 2\n  - Sub-item\n\nNumbered list:\n1. First\n2. Second\n3. Third\n\n\n[Link text](https://example.com)\n[R for Data Science](https://r4ds.hadley.nz/)\n\n\n![Alt text](path/to/image.png)\n\n\n\n\n\n2.5.6 3. Code Chunks\nCode chunks contain R code that executes when you render:\n```{r}\n# R code goes here\nx &lt;- 1:10\nmean(x)\n```\nInsert code chunk:\n\nClick Insert ‚Üí Code Chunk ‚Üí R\nOr use keyboard shortcut: Cmd/Ctrl + Option + I (Mac) / Ctrl + Alt + I (Windows)\n\n\n2.5.6.1 Code Chunk Options\nControl how code chunks behave with options:\n```{r}\n#| label: load-packages\n#| echo: true\n#| eval: true\n#| warning: false\n#| message: false\n\nlibrary(tidyverse)\n```\nCommon options:\n\nlabel: Name the chunk (optional, but helpful)\necho: true/false: Show/hide code in output\neval: true/false: Run/don‚Äôt run code\nwarning: false: Hide warning messages\nmessage: false: Hide package loading messages\ninclude: false: Run code but show nothing in output\n\n\n\n\n\n\n\nNoteQuarto vs R Markdown\n\n\n\nIf you‚Äôve used R Markdown before:\n\nQuarto uses #| for chunk options (instead of {r option=value})\nYAML syntax is slightly different\nQuarto has more features and better outputs\nMost R Markdown code works in Quarto!\n\n\n\n\n\n\n2.5.7 Rendering Your Document\nRender = Execute all code and create output document\nThree ways to render:\n\nClick Render button (top of source pane)\nKeyboard shortcut: Cmd/Ctrl + Shift + K\nCommand line: quarto render document.qmd\n\nOutput:\n\nHTML: Opens in Viewer or browser\nPDF: Requires LaTeX (install TinyTeX)\nWord: Opens in Microsoft Word\n\n\n\n\n\n\n\nTipStart with HTML\n\n\n\nHTML output is easiest:\n\nNo additional software needed\nInteractive features (table of contents, code folding)\nEasy to share (single file)\nFast rendering\n\n\n\n\n\n2.5.8 Example Quarto Document\nHere‚Äôs a complete minimal example:\n---\ntitle: \"Cattle Weight Analysis\"\nauthor: \"Your Name\"\ndate: today\nformat: html\n---\n\n## Introduction\n\nThis analysis examines cattle weights from our feed trial.\n\n## Load Data\n\n```{r}\n#| message: false\n\nlibrary(readr)\ncattle &lt;- read_csv(\"data/raw/cattle_weights.csv\")\n```\n\n## Summary Statistics\n\nThe average initial weight was:\n\n```{r}\nmean(cattle$initial_weight_kg)\n```\n\n## Conclusion\n\nWe found interesting patterns in the data.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#reading-csv-files",
    "href": "chapters/ch02-r_rstudio_reading_data.html#reading-csv-files",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.6 Reading CSV Files",
    "text": "2.6 Reading CSV Files\nCSV (Comma-Separated Values) is the most common data format for analysis.\n\n2.6.1 Why CSV?\n\nPlain text: Open in any editor\nUniversal: Works with all software\nVersion control friendly: Git can track changes\nFast: Quick to read and write\nReliable: No Excel date conversion errors!\n\n\n\n2.6.2 The readr Package\nThe readr package (part of tidyverse) provides read_csv():\n\n# Load the package\nlibrary(readr)\n\n# Read a CSV file\ncattle &lt;- read_csv(\"data/raw/cattle_weights.csv\")\n\n\n\n2.6.3 Reading Your First CSV\nLet‚Äôs read the cattle weights data:\n\nlibrary(readr)\n\n# Read CSV file\ncattle &lt;- read_csv(here::here(\"data\", \"raw\", \"cattle_weights.csv\"))\n\nRows: 20 Columns: 7\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr  (4): animal_id, breed, sex, treatment\ndbl  (2): initial_weight_kg, final_weight_kg\ndate (1): birth_date\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View first few rows\nhead(cattle)\n\n# A tibble: 6 √ó 7\n  animal_id birth_date breed   sex   initial_weight_kg final_weight_kg treatment\n  &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1 C001      2023-03-15 Angus   M                   285             520 High_Pro‚Ä¶\n2 C002      2023-03-18 Herefo‚Ä¶ F                   270             485 Standard \n3 C003      2023-03-20 Angus   M                   290             535 High_Pro‚Ä¶\n4 C004      2023-03-22 Charol‚Ä¶ F                   295             510 Control  \n5 C005      2023-03-25 Angus   M                   280             500 Standard \n6 C006      2023-04-01 Herefo‚Ä¶ F                   275             495 High_Pro‚Ä¶\n\n\n\n\n\n\n\n\nNoteWhat Just Happened?\n\n\n\nWhen you run read_csv():\n\nR reads the file from disk\nParses column types automatically\nCreates a tibble (tidyverse data frame)\nPrints a message showing column types\nAssigns the result to cattle\n\n\n\n\n\n2.6.4 read_csv() vs read.csv()\nR has two functions for reading CSVs:\n\n\n\nFeature\nread.csv() (base R)\nread_csv() (readr)\n\n\n\n\nSpeed\nSlower\nFaster\n\n\nOutput\ndata.frame\ntibble (better)\n\n\nColumn types\nLess reliable\nSmart guessing\n\n\nProgress bar\nNo\nYes (for large files)\n\n\nStrings as factors\nYes (annoying)\nNo\n\n\n\n\n\n\n\n\n\nTipAlways Use read_csv()\n\n\n\nThe tidyverse version (read_csv() with underscore) is superior:\n\nFaster for large files\nBetter column type guessing\nMore consistent behavior\nBetter error messages\n\n\n\n\n\n2.6.5 Common read_csv() Options\n\n# Skip first row\ndata &lt;- read_csv(\"file.csv\", skip = 1)\n\n# Specify column types manually\ndata &lt;- read_csv(\"file.csv\",\n  col_types = cols(\n    animal_id = col_character(),\n    weight_kg = col_double(),\n    treatment = col_factor()\n  )\n)\n\n# Handle missing values\ndata &lt;- read_csv(\"file.csv\", na = c(\"\", \"NA\", \"N/A\", \"-\", \".\"))\n\n# No column names in file\ndata &lt;- read_csv(\"file.csv\", col_names = FALSE)\n\n# Custom column names\ndata &lt;- read_csv(\"file.csv\",\n  col_names = c(\"id\", \"weight\", \"treatment\")\n)\n\n\n\n2.6.6 File Paths\n\nAbsolute Path (Bad)Relative Path (Good)Using here() (Best)\n\n\n# DON'T DO THIS\ndata &lt;- read_csv(\"/Users/jane/Documents/project/data/cattle.csv\")\nProblem: Only works on Jane‚Äôs computer!\n\n\n# Better\ndata &lt;- read_csv(\"data/raw/cattle.csv\")\nWorks if: Your working directory is the project root\n\n\n# Best practice\nlibrary(here)\ndata &lt;- read_csv(here(\"data\", \"raw\", \"cattle.csv\"))\nWorks: Anywhere, any time!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#reading-excel-files",
    "href": "chapters/ch02-r_rstudio_reading_data.html#reading-excel-files",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.7 Reading Excel Files",
    "text": "2.7 Reading Excel Files\nWhile we prefer CSV for analysis, sometimes you need to read Excel files.\n\n2.7.1 The readxl Package\n\n# Install once\ninstall.packages(\"readxl\")\n\n# Load every session\nlibrary(readxl)\n\n\n\n2.7.2 Reading an Excel File\n\nlibrary(readxl)\n\n# Read Excel file\nfeed &lt;- read_excel(here::here(\"data\", \"raw\", \"feed_records.xlsx\"))\n\n# View first few rows\nhead(feed)\n\n# A tibble: 6 √ó 6\n  pen_id  week feed_type feed_consumed_kg avg_pen_weight_kg notes             \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;                &lt;dbl&gt;             &lt;dbl&gt; &lt;chr&gt;             \n1      1     1 Grain_A               419.              301. Normal consumption\n2      1     2 Grain_A               399.              378. Slight decrease   \n3      1     3 Grain_A               496               346. Normal            \n4      1     4 Grain_A               427               367. Increased intake  \n5      1     5 Grain_A               535.              341. Normal            \n6      2     1 Grain_B               475               406. Normal consumption\n\n\n\n\n2.7.3 Excel File Options\n\n# Specify sheet by name\ndata &lt;- read_excel(\"file.xlsx\", sheet = \"Sheet2\")\n\n# Specify sheet by number\ndata &lt;- read_excel(\"file.xlsx\", sheet = 2)\n\n# Skip rows\ndata &lt;- read_excel(\"file.xlsx\", skip = 3)\n\n# Specify column types\ndata &lt;- read_excel(\"file.xlsx\",\n  col_types = c(\"text\", \"numeric\", \"date\")\n)\n\n# Read specific range (like Excel: A1:D10)\ndata &lt;- read_excel(\"file.xlsx\", range = \"A1:D10\")\n\n# Handle missing values\ndata &lt;- read_excel(\"file.xlsx\", na = c(\"\", \"NA\", \"N/A\"))\n\n\n\n2.7.4 List Sheets in Excel File\n\n# See all sheet names\nexcel_sheets(here::here(\"data\", \"raw\", \"feed_records.xlsx\"))\n\n[1] \"Sheet1\"\n\n\n\n\n2.7.5 Read Multiple Sheets\n\n# Read all sheets into a list\nlibrary(purrr)\n\nsheets &lt;- excel_sheets(\"file.xlsx\")\nall_data &lt;- map(sheets, ~ read_excel(\"file.xlsx\", sheet = .x))\nnames(all_data) &lt;- sheets\n\n\n\n\n\n\n\nWarningExcel Issues to Watch For\n\n\n\n\nDate conversions: Excel loves turning things into dates\nFormula errors: read_excel() reads values, not formulas\nHidden rows/columns: Will be included\nMerged cells: Can cause problems\nFormatting: Lost when importing (colors, fonts)\n\nBest practice: Export to CSV from Excel before analysis!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#exploring-your-data",
    "href": "chapters/ch02-r_rstudio_reading_data.html#exploring-your-data",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.8 Exploring Your Data",
    "text": "2.8 Exploring Your Data\nOnce you‚Äôve loaded data, the first step is exploration.\n\n2.8.1 First Look Functions\n\nhead() and tail()glimpse()str()summary()\n\n\nView first or last rows:\n\n# First 6 rows (default)\nhead(cattle)\n\n# A tibble: 6 √ó 7\n  animal_id birth_date breed   sex   initial_weight_kg final_weight_kg treatment\n  &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1 C001      2023-03-15 Angus   M                   285             520 High_Pro‚Ä¶\n2 C002      2023-03-18 Herefo‚Ä¶ F                   270             485 Standard \n3 C003      2023-03-20 Angus   M                   290             535 High_Pro‚Ä¶\n4 C004      2023-03-22 Charol‚Ä¶ F                   295             510 Control  \n5 C005      2023-03-25 Angus   M                   280             500 Standard \n6 C006      2023-04-01 Herefo‚Ä¶ F                   275             495 High_Pro‚Ä¶\n\n# First 3 rows\nhead(cattle, n = 3)\n\n# A tibble: 3 √ó 7\n  animal_id birth_date breed   sex   initial_weight_kg final_weight_kg treatment\n  &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1 C001      2023-03-15 Angus   M                   285             520 High_Pro‚Ä¶\n2 C002      2023-03-18 Herefo‚Ä¶ F                   270             485 Standard \n3 C003      2023-03-20 Angus   M                   290             535 High_Pro‚Ä¶\n\n# Last 6 rows\ntail(cattle)\n\n# A tibble: 6 √ó 7\n  animal_id birth_date breed   sex   initial_weight_kg final_weight_kg treatment\n  &lt;chr&gt;     &lt;date&gt;     &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;           &lt;dbl&gt; &lt;chr&gt;    \n1 C015      2023-05-05 Angus   M                   288             520 Standard \n2 C016      2023-05-10 Herefo‚Ä¶ F                   275             485 Control  \n3 C017      2023-05-15 Angus   M                   292             540 High_Pro‚Ä¶\n4 C018      2023-05-18 Charol‚Ä¶ F                   302             545 Standard \n5 C019      2023-05-22 Herefo‚Ä¶ M                   280             500 Control  \n6 C020      2023-05-25 Angus   F                   270             480 High_Pro‚Ä¶\n\n\n\n\nTransposed view of data:\n\nlibrary(dplyr)\nglimpse(cattle)\n\nRows: 20\nColumns: 7\n$ animal_id         &lt;chr&gt; \"C001\", \"C002\", \"C003\", \"C004\", \"C005\", \"C006\", \"C00‚Ä¶\n$ birth_date        &lt;date&gt; 2023-03-15, 2023-03-18, 2023-03-20, 2023-03-22, 202‚Ä¶\n$ breed             &lt;chr&gt; \"Angus\", \"Hereford\", \"Angus\", \"Charolais\", \"Angus\", ‚Ä¶\n$ sex               &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M‚Ä¶\n$ initial_weight_kg &lt;dbl&gt; 285, 270, 290, 295, 280, 275, 285, 300, 278, 272, 30‚Ä¶\n$ final_weight_kg   &lt;dbl&gt; 520, 485, 535, 510, 500, 495, 525, 540, 490, 480, 55‚Ä¶\n$ treatment         &lt;chr&gt; \"High_Protein\", \"Standard\", \"High_Protein\", \"Control‚Ä¶\n\n\nShows: number of rows, columns, column names, types, and first few values\n\n\nStructure of object:\n\nstr(cattle)\n\nspc_tbl_ [20 √ó 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ animal_id        : chr [1:20] \"C001\" \"C002\" \"C003\" \"C004\" ...\n $ birth_date       : Date[1:20], format: \"2023-03-15\" \"2023-03-18\" ...\n $ breed            : chr [1:20] \"Angus\" \"Hereford\" \"Angus\" \"Charolais\" ...\n $ sex              : chr [1:20] \"M\" \"F\" \"M\" \"F\" ...\n $ initial_weight_kg: num [1:20] 285 270 290 295 280 275 285 300 278 272 ...\n $ final_weight_kg  : num [1:20] 520 485 535 510 500 495 525 540 490 480 ...\n $ treatment        : chr [1:20] \"High_Protein\" \"Standard\" \"High_Protein\" \"Control\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   animal_id = col_character(),\n  ..   birth_date = col_date(format = \"\"),\n  ..   breed = col_character(),\n  ..   sex = col_character(),\n  ..   initial_weight_kg = col_double(),\n  ..   final_weight_kg = col_double(),\n  ..   treatment = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nMore technical, shows object class and internal structure\n\n\nStatistical summary:\n\nsummary(cattle)\n\n  animal_id           birth_date            breed               sex           \n Length:20          Min.   :2023-03-15   Length:20          Length:20         \n Class :character   1st Qu.:2023-03-30   Class :character   Class :character  \n Mode  :character   Median :2023-04-16   Mode  :character   Mode  :character  \n                    Mean   :2023-04-17                                        \n                    3rd Qu.:2023-05-06                                        \n                    Max.   :2023-05-25                                        \n initial_weight_kg final_weight_kg  treatment        \n Min.   :268.0     Min.   :475.0   Length:20         \n 1st Qu.:275.0     1st Qu.:488.8   Class :character  \n Median :283.5     Median :507.5   Mode  :character  \n Mean   :284.5     Mean   :510.8                     \n 3rd Qu.:292.8     3rd Qu.:531.2                     \n Max.   :305.0     Max.   :555.0                     \n\n\nFor numeric columns: min, max, quartiles, mean For character/factor: counts\n\n\n\n\n\n2.8.2 Dimensions\n\n# Number of rows\nnrow(cattle)\n\n[1] 20\n\n# Number of columns\nncol(cattle)\n\n[1] 7\n\n# Both at once (rows, columns)\ndim(cattle)\n\n[1] 20  7\n\n\n\n\n2.8.3 Column Names\n\n# Get column names\nnames(cattle)\n\n[1] \"animal_id\"         \"birth_date\"        \"breed\"            \n[4] \"sex\"               \"initial_weight_kg\" \"final_weight_kg\"  \n[7] \"treatment\"        \n\n# Or\ncolnames(cattle)\n\n[1] \"animal_id\"         \"birth_date\"        \"breed\"            \n[4] \"sex\"               \"initial_weight_kg\" \"final_weight_kg\"  \n[7] \"treatment\"        \n\n\n\n\n2.8.4 Quick Counts\n\n# Count observations by treatment\nlibrary(dplyr)\ncattle %&gt;% count(treatment)\n\n# A tibble: 3 √ó 2\n  treatment        n\n  &lt;chr&gt;        &lt;int&gt;\n1 Control          6\n2 High_Protein     8\n3 Standard         6\n\n# Count by breed and sex\ncattle %&gt;% count(breed, sex)\n\n# A tibble: 6 √ó 3\n  breed     sex       n\n  &lt;chr&gt;     &lt;chr&gt; &lt;int&gt;\n1 Angus     F         3\n2 Angus     M         6\n3 Charolais F         4\n4 Charolais M         1\n5 Hereford  F         3\n6 Hereford  M         3",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#data-structures-data-frames-vs-tibbles",
    "href": "chapters/ch02-r_rstudio_reading_data.html#data-structures-data-frames-vs-tibbles",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.9 Data Structures: Data Frames vs Tibbles",
    "text": "2.9 Data Structures: Data Frames vs Tibbles\n\n2.9.1 Data Frames (Base R)\nA data frame is R‚Äôs traditional way to store tabular data:\n\nRows are observations\nColumns are variables\nColumns can be different types (numeric, character, factor)\n\n\n# Create a basic data frame\ndf &lt;- data.frame(\n  id = 1:3,\n  name = c(\"Bessie\", \"Daisy\", \"Buttercup\"),\n  weight_kg = c(450, 425, 475)\n)\n\nclass(df)\n\n[1] \"data.frame\"\n\ndf\n\n  id      name weight_kg\n1  1    Bessie       450\n2  2     Daisy       425\n3  3 Buttercup       475\n\n\n\n\n2.9.2 Tibbles (Tidyverse)\nA tibble is the tidyverse version of a data frame:\n\nlibrary(tibble)\n\n# Create a tibble\ntib &lt;- tibble(\n  id = 1:3,\n  name = c(\"Bessie\", \"Daisy\", \"Buttercup\"),\n  weight_kg = c(450, 425, 475)\n)\n\nclass(tib)\n\n[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n\ntib\n\n# A tibble: 3 √ó 3\n     id name      weight_kg\n  &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n1     1 Bessie          450\n2     2 Daisy           425\n3     3 Buttercup       475\n\n\n\n\n2.9.3 Data Frame vs Tibble\n\n\n\nFeature\nData Frame\nTibble\n\n\n\n\nPrinting\nPrints everything\nPrints first 10 rows\n\n\nColumn types\nShown with str()\nShown when printing\n\n\nStrings to factors\nYes (old R)\nNever\n\n\nSubsetting\nInconsistent\nConsistent\n\n\nModern tidyverse\n‚ùå\n‚úÖ\n\n\n\n\n\n\n\n\n\nTipPrefer Tibbles\n\n\n\nWhen using tidyverse packages, tibbles are better:\n\nBetter printing (won‚Äôt flood your console)\nBetter warnings when something goes wrong\nMore predictable behavior\nColumn types always visible\n\nConvert data frame to tibble:\n\ntib &lt;- as_tibble(df)\n\n\n\n\n\n2.9.4 Vectors\nBoth data frames and tibbles are built from vectors‚Äîthe most basic R data structure.\n\n# Numeric vector\nweights &lt;- c(250, 275, 300, 285)\nweights\n\n[1] 250 275 300 285\n\n# Character vector\nnames &lt;- c(\"Bessie\", \"Daisy\", \"Buttercup\", \"Moolinda\")\nnames\n\n[1] \"Bessie\"    \"Daisy\"     \"Buttercup\" \"Moolinda\" \n\n# Logical vector\nis_heavy &lt;- weights &gt; 280\nis_heavy\n\n[1] FALSE FALSE  TRUE  TRUE\n\n\nKey point: Each column in a data frame/tibble is a vector!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#the-r-help-system",
    "href": "chapters/ch02-r_rstudio_reading_data.html#the-r-help-system",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.10 The R Help System",
    "text": "2.10 The R Help System\nLearning to find help is crucial for becoming independent.\n\n2.10.1 Function Help\n\n# Get help for a function\n?read_csv\nhelp(read_csv)\n\n# Search for topic\n??csv\nhelp.search(\"csv\")\n\n\n\n2.10.2 Package Help\n\n# Overview of package\nhelp(package = \"readr\")\n\n# List all functions\nlibrary(help = \"readr\")\n\n\n\n2.10.3 Vignettes\nVignettes are long-form tutorials:\n\n# See available vignettes\nvignette(package = \"readr\")\n\n# Open a specific vignette\nvignette(\"readr\")\n\n\n\n2.10.4 Examples\nMost help files have examples at the bottom:\n\n# Run examples from help file\nexample(read_csv)\n\n\n\n2.10.5 Online Resources\nWhen stuck, try:\n\nGoogle: ‚ÄúR read csv with missing values‚Äù\nStack Overflow: stackoverflow.com/questions/tagged/r\nPackage websites: Often have better documentation than ?function\nR for Data Science: https://r4ds.hadley.nz/\n\n\n\n\n\n\n\nTipHow to Ask for Help\n\n\n\nWhen asking questions online:\n\nDescribe what you want to do\nShow your code (formatted!)\nInclude error messages (full text)\nProvide example data (use dput() or built-in datasets)\nSay what you‚Äôve tried\n\nGood question = fast, helpful answers!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#summary-1",
    "href": "chapters/ch02-r_rstudio_reading_data.html#summary-1",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.11 Summary",
    "text": "2.11 Summary\nThis chapter introduced the R ecosystem for data analysis:\n\nRStudio interface has four main panes: Source, Console, Environment, Files\nR Projects organize your work and use relative paths for reproducibility\nThe here package makes file paths work anywhere\nInstalling packages (install.packages()) is done once; loading packages (library()) is done every session\nQuarto documents combine code, text, and output for reproducible reports\nYAML headers control document settings; markdown formats text; code chunks execute R code\nread_csv() from readr is better than read.csv() for importing CSV files\nread_excel() from readxl imports Excel files\nExploration functions: head(), tail(), glimpse(), str(), summary(), names(), dim()\nTibbles are better than data frames for tidyverse workflows\nR‚Äôs help system: ?function, help(), vignette(), and online resources",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#homework-assignment",
    "href": "chapters/ch02-r_rstudio_reading_data.html#homework-assignment",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.12 Homework Assignment",
    "text": "2.12 Homework Assignment\n\n2.12.1 Assignment: Import, Explore, and Document Data\nDue: Before Week 3\n\n2.12.1.1 Part 1: Project Setup (20 points)\n\nCreate a new R Project called ans500_week2_yourname\nCreate this folder structure within your project:\ndata/\n  raw/\n  processed/\ncode/\noutput/\n  figures/\nDownload two data files (your instructor will provide links):\n\ncattle_weights.csv\nfeed_records.xlsx\n\nPlace both files in data/raw/\n\n\n\n2.12.1.2 Part 2: Data Import and Exploration (40 points)\nCreate a Quarto document called week2_homework.qmd that:\n\nLoads necessary packages (tidyverse, readxl, here)\nReads both data files using appropriate functions\nExplores the CSV data (cattle_weights.csv):\n\nShow dimensions (rows and columns)\nDisplay column names and types\nShow first 10 rows\nProvide a statistical summary\nCount observations by treatment and by breed\n\nExplores the Excel data (feed_records.xlsx):\n\nShow dimensions\nDisplay column names and types\nShow first 8 rows\nCalculate mean feed consumption by feed type\n\nDocuments your observations:\n\nWhat variables are in each dataset?\nWhat do you notice about the data?\nAny missing values or unusual patterns?\nAny potential data quality issues?\n\n\n\n\n2.12.1.3 Part 3: Quarto Skills (40 points)\nYour Quarto document must demonstrate:\n\nProper YAML header with:\n\nTitle: ‚ÄúWeek 2 Homework: Data Import and Exploration‚Äù\nYour name as author\nToday‚Äôs date\nHTML output with table of contents\nCode folding off\n\nMarkdown formatting:\n\nLevel 2 headers (##) for major sections\nLevel 3 headers (###) for subsections\nUse bold and italic for emphasis\nCreate a bulleted list\n\nCode chunks with appropriate options:\n\nName at least 3 code chunks\nUse #| message: false where appropriate\nUse #| echo: true to show code\n\nWritten interpretation:\n\nEach analysis section includes 2-3 sentences explaining what the output shows\nFinal section (150-200 words) comparing CSV vs Excel import process\n\n\nRender to HTML and submit both .qmd and .html files.\n\n\n\n2.12.2 Recommended YAML\n---\ntitle: \"Week 2 Homework: Data Import and Exploration\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n2.12.3 Grading Rubric\n\nProject Setup (20%):\n\nR Project created properly (5%)\nFolder structure correct (10%)\nData files in correct location (5%)\n\nData Import and Exploration (40%):\n\nBoth files imported correctly (10%)\nAll required exploration completed (20%)\nObservations documented (10%)\n\nQuarto Skills (40%):\n\nYAML header correct and complete (10%)\nMarkdown formatting used effectively (10%)\nCode chunks properly configured (10%)\nWritten interpretation clear and complete (10%)\n\n\n\n\n2.12.4 Bonus (5 points)\nCreate a simple visualization of either dataset using base R plotting:\n\n# Example: Histogram of initial weights\nhist(cattle$initial_weight_kg,\n     main = \"Distribution of Initial Cattle Weights\",\n     xlab = \"Weight (kg)\",\n     col = \"steelblue\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch02-r_rstudio_reading_data.html#additional-resources",
    "href": "chapters/ch02-r_rstudio_reading_data.html#additional-resources",
    "title": "2¬† Getting Started with R, RStudio, and Reading Data",
    "section": "2.13 Additional Resources",
    "text": "2.13 Additional Resources\n\n2.13.1 Required Reading\n\nR for Data Science (2e) - Chapters 6-8: Workflow, Data Import\nQuarto Tutorial - Complete the ‚ÄúHello, Quarto‚Äù tutorial\n\n\n\n2.13.2 Optional Reading\n\nRStudio IDE Cheat Sheet\nreadr documentation\nreadxl documentation\nWickham, H. (2014). ‚ÄúTidy Data.‚Äù Journal of Statistical Software, 59(10). Link\n\n\n\n2.13.3 Videos\n\n‚ÄúRStudio for the Total Beginner‚Äù by RStudio\n‚ÄúGetting Started with Quarto‚Äù by Posit\n‚ÄúHow to Import Data in R‚Äù by DataCamp\n\n\n\n2.13.4 Cheat Sheets\n\nData Import Cheat Sheet\nRStudio IDE Cheat Sheet\nQuarto Reference\n\n\n\n2.13.5 Useful Websites\n\nTidyverse - Homepage for tidyverse packages\nPosit Community - Forum for RStudio/Quarto questions\nStack Overflow - Q&A for R programming\n\n\nNext Chapter: Data Types, Strings, and Introduction to dplyr",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Getting Started with R, RStudio, and Reading Data</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html",
    "href": "chapters/ch03-data_types_strings_dplyr.html",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "",
    "text": "3.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#learning-objectives",
    "href": "chapters/ch03-data_types_strings_dplyr.html#learning-objectives",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "",
    "text": "Identify and work with R‚Äôs core data types: numeric, integer, character, logical, factor, and date\nCheck and coerce data types using functions like class(), typeof(), and as.*() functions\nCreate and manipulate factors using the forcats package\nPerform string operations using stringr functions\nUnderstand and apply basic regular expression patterns\nUse the pipe operator (%&gt;% and |&gt;) to create readable code workflows\nSelect columns from a dataset using dplyr::select() and helper functions\nFilter rows based on conditions using dplyr::filter()\nCombine multiple dplyr operations using pipes\nHandle missing values (NA) when filtering data",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#understanding-r-data-types",
    "href": "chapters/ch03-data_types_strings_dplyr.html#understanding-r-data-types",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.2 Understanding R Data Types",
    "text": "3.2 Understanding R Data Types\nEvery piece of data in R has a type. Understanding data types is crucial because:\n\nDifferent operations work on different types\nData import can misinterpret types\nType errors cause many frustrating bugs\nProper types enable correct statistical analysis\n\n\n3.2.1 The Six Essential Data Types\n\n\n\n\n\nflowchart TD\n    A[R Data Types] --&gt; B[Numeric&lt;br/&gt;Numbers with decimals]\n    A --&gt; C[Integer&lt;br/&gt;Whole numbers]\n    A --&gt; D[Character&lt;br/&gt;Text strings]\n    A --&gt; E[Logical&lt;br/&gt;TRUE/FALSE]\n    A --&gt; F[Factor&lt;br/&gt;Categorical data]\n    A --&gt; G[Date/POSIXct&lt;br/&gt;Dates and times]\n\n    B --&gt; B1[\"3.14, 500.5, -12.7\"]\n    C --&gt; C1[\"1L, 42L, 100L\"]\n    D --&gt; D1[\"'Holstein', 'A123'\"]\n    E --&gt; E1[\"TRUE, FALSE\"]\n    F --&gt; F1[\"Breed, Treatment\"]\n    G --&gt; G1[\"2024-01-15\"]\n\n\n\n\n\n\n\n\n3.2.2 1. Numeric (Double)\nNumbers with decimal points:\n\n# Numeric values\nweight &lt;- 450.5\ntemperature &lt;- 38.6\nmilk_yield &lt;- 25.3\n\nclass(weight)\n\n[1] \"numeric\"\n\ntypeof(weight)\n\n[1] \"double\"\n\n\n\n\n3.2.3 2. Integer\nWhole numbers (add L suffix):\n\n# Integer values\nn_animals &lt;- 100L\npen_number &lt;- 5L\n\nclass(n_animals)\n\n[1] \"integer\"\n\ntypeof(n_animals)\n\n[1] \"integer\"\n\n# Most numbers are numeric by default, not integer\nx &lt;- 42\nclass(x)  # numeric (double), not integer!\n\n[1] \"numeric\"\n\n\n\n\n\n\n\n\nNoteWhen to Use Integer vs Numeric?\n\n\n\nPractical answer: Almost never worry about it!\n\nR defaults to numeric (double precision)\nInteger saves memory (rarely matters)\nUse integer for:\n\nCounting things (number of animals)\nIDs that are whole numbers\nIndexing\n\n\n\n\n\n\n3.2.4 3. Character (Strings)\nText data, enclosed in quotes:\n\n# Character values\nanimal_id &lt;- \"A1234\"\nbreed &lt;- \"Holstein\"\nfarm_name &lt;- \"Green Valley Farm\"\n\nclass(breed)\n\n[1] \"character\"\n\ntypeof(breed)\n\n[1] \"character\"\n\n# Numbers can be characters too!\nid_as_string &lt;- \"12345\"\nclass(id_as_string)\n\n[1] \"character\"\n\n\n\n\n3.2.5 4. Logical (Boolean)\nTRUE or FALSE (note: all caps, no quotes):\n\n# Logical values\nis_lactating &lt;- TRUE\nhas_disease &lt;- FALSE\n\nclass(is_lactating)\n\n[1] \"logical\"\n\n# Logical from comparison\nweight &lt;- 500\nis_heavy &lt;- weight &gt; 450\nis_heavy\n\n[1] TRUE\n\nclass(is_heavy)\n\n[1] \"logical\"\n\n\n\n\n\n\n\n\nTipLogical Shortcuts\n\n\n\n\nTRUE can be abbreviated as T\nFALSE can be abbreviated as F\nBUT: Don‚Äôt use shortcuts! Write TRUE and FALSE for clarity\n\n\n\n\n\n3.2.6 5. Factor\nCategorical data with defined levels:\n\n# Create a factor\nbreed &lt;- factor(c(\"Holstein\", \"Jersey\", \"Holstein\", \"Angus\", \"Jersey\"))\nbreed\n\n[1] Holstein Jersey   Holstein Angus    Jersey  \nLevels: Angus Holstein Jersey\n\nclass(breed)\n\n[1] \"factor\"\n\nlevels(breed)\n\n[1] \"Angus\"    \"Holstein\" \"Jersey\"  \n\n\nWhen to use factors: - Categorical variables (breed, treatment, sex) - Ordered categories (small, medium, large) - Fixed set of possible values\nWhy factors matter: - Control order in plots and tables - Enable statistical modeling - Prevent typos (can‚Äôt add invalid levels)\n\n\n3.2.7 6. Date and POSIXct\nDates and date-times:\n\n# Date (just the date)\nbirth_date &lt;- as.Date(\"2024-01-15\")\nbirth_date\n\n[1] \"2024-01-15\"\n\nclass(birth_date)\n\n[1] \"Date\"\n\n# POSIXct (date + time)\nbreeding_time &lt;- as.POSIXct(\"2024-01-15 14:30:00\")\nbreeding_time\n\n[1] \"2024-01-15 14:30:00 CST\"\n\nclass(breeding_time)\n\n[1] \"POSIXct\" \"POSIXt\" \n\n\n\n\n\n\n\n\nImportantDate Format: YYYY-MM-DD\n\n\n\nAlways use ISO 8601 format: Year-Month-Day (2024-01-15)\n\nUnambiguous internationally\nSorts correctly\nStandard in R and databases\nAvoid: 01/15/2024 (ambiguous: US or Europe?)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#type-checking-and-coercion",
    "href": "chapters/ch03-data_types_strings_dplyr.html#type-checking-and-coercion",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.3 Type Checking and Coercion",
    "text": "3.3 Type Checking and Coercion\n\n3.3.1 Checking Types\n\nx &lt;- 42.5\n\n# What class is it?\nclass(x)\n\n[1] \"numeric\"\n\n# What is the internal type?\ntypeof(x)\n\n[1] \"double\"\n\n# Test if it's a specific type\nis.numeric(x)\n\n[1] TRUE\n\nis.character(x)\n\n[1] FALSE\n\nis.logical(x)\n\n[1] FALSE\n\nis.factor(x)\n\n[1] FALSE\n\n\n\n\n3.3.2 Common Type-Checking Functions\n\n\n\nFunction\nTests for\n\n\n\n\nis.numeric()\nNumeric (double)\n\n\nis.integer()\nInteger\n\n\nis.character()\nCharacter/string\n\n\nis.logical()\nLogical (TRUE/FALSE)\n\n\nis.factor()\nFactor\n\n\nis.na()\nMissing value (NA)\n\n\n\n\n\n3.3.3 Type Coercion\nCoercion = Converting from one type to another\n\n# Character to numeric\nage_string &lt;- \"5\"\nage_numeric &lt;- as.numeric(age_string)\nage_numeric\n\n[1] 5\n\nclass(age_numeric)\n\n[1] \"numeric\"\n\n# Numeric to character\nweight &lt;- 450\nweight_string &lt;- as.character(weight)\nweight_string\n\n[1] \"450\"\n\nclass(weight_string)\n\n[1] \"character\"\n\n# Numeric to integer\nn &lt;- 42.7\nn_integer &lt;- as.integer(n)  # Truncates decimal!\nn_integer\n\n[1] 42\n\n# Character to factor\ntreatments &lt;- c(\"Control\", \"TreatA\", \"TreatB\", \"Control\")\ntreatments_factor &lt;- as.factor(treatments)\ntreatments_factor\n\n[1] Control TreatA  TreatB  Control\nLevels: Control TreatA TreatB\n\n\n\n\n3.3.4 What Happens When Coercion Fails?\n\n# Can't convert text to number\nbad_conversion &lt;- as.numeric(\"not_a_number\")\n\nWarning: NAs introduced by coercion\n\nbad_conversion  # NA with warning\n\n[1] NA\n\n\n\n\n\n\n\n\nWarningWatch for Automatic Coercion!\n\n\n\nR silently coerces types in some situations:\n\n# Mixing types in a vector\nmixed &lt;- c(1, 2, \"three\", 4)\nmixed  # Everything became character!\n\n[1] \"1\"     \"2\"     \"three\" \"4\"    \n\nclass(mixed)\n\n[1] \"character\"\n\n\nRule: A vector can only hold one data type. R chooses the most flexible type that fits all values:\nlogical ‚Üí integer ‚Üí numeric ‚Üí character\n\n\n\n\n3.3.5 Real-World Example: Cleaning Animal IDs\n\nlibrary(tibble)\n\n# Animal IDs imported as numbers (lost leading zeros!)\nanimals &lt;- tibble(\n  animal_id = c(123, 456, 789),\n  weight = c(450, 525, 490)\n)\n\nanimals\n\n# A tibble: 3 √ó 2\n  animal_id weight\n      &lt;dbl&gt;  &lt;dbl&gt;\n1       123    450\n2       456    525\n3       789    490\n\n# Should be: \"0123\", \"0456\", \"0789\"\n# Fix by padding with zeros\nanimals$animal_id_fixed &lt;- sprintf(\"%04d\", animals$animal_id)\nanimals\n\n# A tibble: 3 √ó 3\n  animal_id weight animal_id_fixed\n      &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;          \n1       123    450 0123           \n2       456    525 0456           \n3       789    490 0789",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#working-with-factors",
    "href": "chapters/ch03-data_types_strings_dplyr.html#working-with-factors",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.4 Working with Factors",
    "text": "3.4 Working with Factors\nFactors are R‚Äôs way of handling categorical data. The forcats package (part of tidyverse) makes working with factors easier.\n\n3.4.1 Why Factors?\n\nProblemSolution\n\n\n\nlibrary(ggplot2)\n\n# Character vector for breed\ndf &lt;- data.frame(\n  breed = c(\"Holstein\", \"Jersey\", \"Angus\", \"Holstein\", \"Angus\", \"Jersey\"),\n  weight = c(600, 450, 550, 620, 580, 470)\n)\n\n# Default alphabetical order\nggplot(df, aes(x = breed, y = weight)) +\n  geom_boxplot() +\n  theme_minimal()\n\n\n\n\n\n\n\n\nOrder: Angus, Holstein, Jersey (alphabetical)\n\n\n\n# Use factors to control order\ndf$breed_factor &lt;- factor(df$breed,\n                          levels = c(\"Jersey\", \"Holstein\", \"Angus\"))\n\nggplot(df, aes(x = breed_factor, y = weight)) +\n  geom_boxplot() +\n  labs(x = \"Breed\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nOrder: Jersey, Holstein, Angus (our choice!)\n\n\n\n\n\n3.4.2 Creating Factors\n\n# From a character vector\ntreatments &lt;- c(\"Control\", \"Low\", \"High\", \"Control\", \"High\", \"Low\")\n\n# Convert to factor\ntreatment_factor &lt;- factor(treatments)\ntreatment_factor\n\n[1] Control Low     High    Control High    Low    \nLevels: Control High Low\n\n# Levels are alphabetical by default\nlevels(treatment_factor)\n\n[1] \"Control\" \"High\"    \"Low\"    \n\n# Specify level order\ntreatment_factor_ordered &lt;- factor(treatments,\n                                   levels = c(\"Control\", \"Low\", \"High\"))\nlevels(treatment_factor_ordered)\n\n[1] \"Control\" \"Low\"     \"High\"   \n\n\n\n\n3.4.3 Introduction to forcats\nThe forcats package (‚Äúfor categorical variables‚Äù) provides helpful factor functions:\n\nlibrary(forcats)\n\n# Example data\ndf &lt;- tibble(\n  treatment = c(\"Control\", \"TreatA\", \"TreatB\", \"Control\", \"TreatA\", \"TreatB\"),\n  response = c(10, 15, 20, 12, 18, 22)\n)\n\n# Convert to factor\ndf$treatment &lt;- as.factor(df$treatment)\nlevels(df$treatment)\n\n[1] \"Control\" \"TreatA\"  \"TreatB\" \n\n\n\n\n3.4.4 Useful forcats Functions\n\n3.4.4.1 1. fct_relevel() - Manually reorder levels\n\n# Put \"Control\" first\ndf$treatment &lt;- fct_relevel(df$treatment, \"Control\")\nlevels(df$treatment)\n\n[1] \"Control\" \"TreatA\"  \"TreatB\" \n\n\n\n\n3.4.4.2 2. fct_infreq() - Order by frequency\n\n# Most common first\ndf$treatment &lt;- fct_infreq(df$treatment)\nlevels(df$treatment)\n\n[1] \"Control\" \"TreatA\"  \"TreatB\" \n\n\n\n\n3.4.4.3 3. fct_recode() - Rename levels\n\n# Rename levels\ndf$treatment &lt;- fct_recode(df$treatment,\n  \"Control Group\" = \"Control\",\n  \"Treatment A\" = \"TreatA\",\n  \"Treatment B\" = \"TreatB\"\n)\nlevels(df$treatment)\n\n[1] \"Control Group\" \"Treatment A\"   \"Treatment B\"  \n\n\n\n\n\n\n\n\nTipWhen to Use Factors\n\n\n\n‚úÖ Use factors for: - Categorical variables (breed, sex, treatment) - Variables with a natural order (small &lt; medium &lt; large) - Modeling and statistical tests - Controlling plot order\n‚ùå Don‚Äôt use factors for: - Animal IDs (use character) - Free text (use character) - Data that might have new categories",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#string-manipulation-with-stringr",
    "href": "chapters/ch03-data_types_strings_dplyr.html#string-manipulation-with-stringr",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.5 String Manipulation with stringr",
    "text": "3.5 String Manipulation with stringr\nWorking with text is common in data cleaning. The stringr package provides consistent, intuitive functions for string manipulation.\n\n3.5.1 Why stringr?\nAll stringr functions: - Start with str_ - Take string as first argument (pipe-friendly) - Work with character vectors - Use consistent naming\n\n\n3.5.2 Loading stringr\n\nlibrary(stringr)\n\n\n\n3.5.3 Common String Operations\n\n3.5.3.1 1. Detecting Patterns: str_detect()\nCheck if a pattern exists in a string:\n\nanimal_ids &lt;- c(\"H001\", \"J002\", \"H003\", \"A004\", \"H005\")\n\n# Which IDs start with \"H\"?\nstr_detect(animal_ids, \"H\")\n\n[1]  TRUE FALSE  TRUE FALSE  TRUE\n\n# Filter to just Holstein (H prefix)\nanimal_ids[str_detect(animal_ids, \"H\")]\n\n[1] \"H001\" \"H003\" \"H005\"\n\n\n\n\n3.5.3.2 2. Extracting Substrings: str_subset()\nGet strings that match a pattern:\n\n# Get Holstein IDs\nstr_subset(animal_ids, \"H\")\n\n[1] \"H001\" \"H003\" \"H005\"\n\n# Get IDs ending in 0\nstr_subset(animal_ids, \"0\")\n\n[1] \"H001\" \"J002\" \"H003\" \"A004\" \"H005\"\n\n\n\n\n3.5.3.3 3. Changing Case: str_to_lower(), str_to_upper(), str_to_title()\n\nbreeds &lt;- c(\"HOLSTEIN\", \"jersey\", \"AnGuS\")\n\nstr_to_lower(breeds)\n\n[1] \"holstein\" \"jersey\"   \"angus\"   \n\nstr_to_upper(breeds)\n\n[1] \"HOLSTEIN\" \"JERSEY\"   \"ANGUS\"   \n\nstr_to_title(breeds)  # First letter capitalized\n\n[1] \"Holstein\" \"Jersey\"   \"Angus\"   \n\n\n\n\n\n\n\n\nTipStandardizing Text Data\n\n\n\nAlways standardize case when cleaning data:\n\n# Before comparing or grouping\ndata$breed &lt;- str_to_title(data$breed)\n\nPrevents: ‚Äúholstein‚Äù ‚â† ‚ÄúHolstein‚Äù ‚â† ‚ÄúHOLSTEIN‚Äù\n\n\n\n\n3.5.3.4 4. Trimming Whitespace: str_trim()\nRemove leading and trailing spaces:\n\nmessy_names &lt;- c(\"  Holstein  \", \"Jersey\", \"  Angus\")\nmessy_names\n\n[1] \"  Holstein  \" \"Jersey\"       \"  Angus\"     \n\n# Clean up\nstr_trim(messy_names)\n\n[1] \"Holstein\" \"Jersey\"   \"Angus\"   \n\n\n\n\n3.5.3.5 5. Replacing: str_replace() and str_replace_all()\n\nids &lt;- c(\"A-001\", \"A-002\", \"B-001\")\n\n# Replace first occurrence\nstr_replace(ids, \"-\", \"_\")\n\n[1] \"A_001\" \"A_002\" \"B_001\"\n\n# Replace all occurrences\ntext &lt;- \"the the cow cow\"\nstr_replace(text, \"the\", \"a\")      # First only\n\n[1] \"a the cow cow\"\n\nstr_replace_all(text, \"the\", \"a\")  # All\n\n[1] \"a a cow cow\"\n\n\n\n\n3.5.3.6 6. Combining Strings: str_c() (or paste())\n\nfirst &lt;- c(\"Animal\", \"Pen\", \"Feed\")\nsecond &lt;- c(\"ID\", \"Number\", \"Type\")\n\n# Concatenate\nstr_c(first, second, sep = \"_\")\n\n[1] \"Animal_ID\"  \"Pen_Number\" \"Feed_Type\" \n\n# Collapse into one string\nstr_c(first, collapse = \", \")\n\n[1] \"Animal, Pen, Feed\"\n\n\n\n\n3.5.3.7 7. Splitting Strings: str_split()\n\nids &lt;- \"H001,J002,A003\"\n\n# Split by comma\nstr_split(ids, \",\")\n\n[[1]]\n[1] \"H001\" \"J002\" \"A003\"\n\n# Returns a list! Get as character vector:\nstr_split(ids, \",\")[[1]]\n\n[1] \"H001\" \"J002\" \"A003\"\n\n\n\n\n\n3.5.4 Real-World Example: Cleaning Messy Farm Data\n\nlibrary(tibble)\n\n# Messy data from Excel\nfarm_data &lt;- tibble(\n  animal_id = c(\"  H-001\", \"h-002  \", \"H-003\"),\n  breed = c(\"HOLSTEIN\", \"holstein\", \"Holstein  \"),\n  weight_kg = c(450, 475, 460)\n)\n\nfarm_data\n\n# A tibble: 3 √ó 3\n  animal_id breed        weight_kg\n  &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;\n1 \"  H-001\" \"HOLSTEIN\"         450\n2 \"h-002  \" \"holstein\"         475\n3 \"H-003\"   \"Holstein  \"       460\n\n# Clean it up!\nlibrary(dplyr)\n\nfarm_data_clean &lt;- farm_data %&gt;%\n  mutate(\n    animal_id = str_trim(animal_id),           # Remove whitespace\n    animal_id = str_to_upper(animal_id),       # Uppercase\n    animal_id = str_replace(animal_id, \"-\", \"\"),  # Remove dash\n    breed = str_trim(breed),\n    breed = str_to_title(breed)\n  )\n\nfarm_data_clean\n\n# A tibble: 3 √ó 3\n  animal_id breed    weight_kg\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n1 H001      Holstein       450\n2 H002      Holstein       475\n3 H003      Holstein       460",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#regular-expressions-basics",
    "href": "chapters/ch03-data_types_strings_dplyr.html#regular-expressions-basics",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.6 Regular Expressions Basics",
    "text": "3.6 Regular Expressions Basics\nRegular expressions (regex) are patterns for matching text. They‚Äôre powerful but can look cryptic.\n\n3.6.1 Common Regex Patterns\n\n\n\n\n\n\n\n\nPattern\nMatches\nExample\n\n\n\n\n.\nAny single character\n\"a.c\" matches ‚Äúabc‚Äù, ‚Äúa1c‚Äù, ‚Äúa_c‚Äù\n\n\n^\nStart of string\n\"^H\" matches ‚ÄúHolstein‚Äù, not ‚ÄúThe Holstein‚Äù\n\n\n$\nEnd of string\n\"001$\" matches ‚ÄúH001‚Äù, not ‚ÄúH001A‚Äù\n\n\n[0-9]\nAny digit\n\"[0-9]\" matches any number\n\n\n[a-z]\nAny lowercase letter\n\"[a-z]+\" matches ‚Äúabc‚Äù\n\n\n[A-Z]\nAny uppercase letter\n\"[A-Z]\" matches ‚ÄúH‚Äù\n\n\n+\nOne or more\n\"[0-9]+\" matches ‚Äú123‚Äù\n\n\n*\nZero or more\n\"[0-9]*\" matches ‚Äú‚Äú,‚Äù1‚Äù, ‚Äú123‚Äù\n\n\n{n}\nExactly n times\n\"[0-9]{3}\" matches ‚Äú123‚Äù\n\n\n|\nOr\n\"H|J\" matches ‚ÄúH‚Äù or ‚ÄúJ‚Äù\n\n\n\n\n\n3.6.2 Regex Examples\n\nanimal_ids &lt;- c(\"H001\", \"J002\", \"H003\", \"A004\", \"H100\")\n\n# IDs starting with H\nstr_subset(animal_ids, \"^H\")\n\n[1] \"H001\" \"H003\" \"H100\"\n\n# IDs ending with 00-something\nstr_subset(animal_ids, \"00.$\")\n\n[1] \"H001\" \"J002\" \"H003\" \"A004\"\n\n# IDs with exactly 3 digits\nstr_detect(animal_ids, \"[0-9]{3}\")\n\n[1] TRUE TRUE TRUE TRUE TRUE\n\n# IDs starting with H or J\nstr_subset(animal_ids, \"^(H|J)\")\n\n[1] \"H001\" \"J002\" \"H003\" \"H100\"\n\n\n\n\n\n\n\n\nNoteLearning Regex\n\n\n\nRegular expressions take practice! Start simple:\n\nLiteral matches: \"Holstein\" matches ‚ÄúHolstein‚Äù\nSimple patterns: \"^H\" for starts with H\nCharacter classes: \"[0-9]\" for digits\nBuild gradually: Test patterns as you go\n\nResources: - regex101.com - Interactive regex tester - RegExr.com - Learn and test regex - stringr cheat sheet\n\n\n\n\n3.6.3 Practical Example: Extracting Numbers\n\npen_labels &lt;- c(\"Pen1\", \"Pen2\", \"Pen10\", \"Pen25\")\n\n# Extract the numbers\nstr_extract(pen_labels, \"[0-9]+\")\n\n[1] \"1\"  \"2\"  \"10\" \"25\"\n\n# Convert to numeric\nas.numeric(str_extract(pen_labels, \"[0-9]+\"))\n\n[1]  1  2 10 25",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#introduction-to-the-pipe",
    "href": "chapters/ch03-data_types_strings_dplyr.html#introduction-to-the-pipe",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.7 Introduction to the Pipe",
    "text": "3.7 Introduction to the Pipe\nThe pipe operator chains operations together, making code more readable.\n\n3.7.1 What is a Pipe?\nThe pipe takes the output of one function and passes it as the first argument to the next function.\nTwo versions: - Magrittr pipe: %&gt;% (requires magrittr or tidyverse) - Native pipe: |&gt; (built into R 4.1+)\n\nlibrary(dplyr)\n\n# Sample data\nweights &lt;- c(450, 475, 460, 490, 510, 425)\n\n# WITHOUT pipe (nested functions, hard to read)\nround(mean(weights), 1)\n\n[1] 468.3\n\n# WITH pipe (left to right, easy to read)\nweights %&gt;%\n  mean() %&gt;%\n  round(1)\n\n[1] 468.3\n\n# Read as: \"Take weights, THEN calculate mean, THEN round to 1 decimal\"\n\n\n\n3.7.2 Reading Pipes\nThink: ‚Äúand then‚Äù\n\ndata %&gt;%\n  filter(weight &gt; 400) %&gt;%\n  select(animal_id, weight) %&gt;%\n  arrange(weight)\n\nRead as: - Take data - and then filter to weight &gt; 400 - and then select animal_id and weight columns - and then arrange by weight\n\n\n3.7.3 %&gt;% vs |&gt;\n\nMagrittr Pipe %&gt;%Native Pipe |&gt;\n\n\n\n# From magrittr package (included in tidyverse)\nlibrary(dplyr)\n\ndata %&gt;%\n  filter(weight &gt; 400) %&gt;%\n  summarise(mean_weight = mean(weight))\n\n\nOlder, more established\nRequired before R 4.1\nMore features (. placeholder, %$%, %&lt;&gt;%)\n\n\n\n\n# Built into R 4.1+\ndata |&gt;\n  filter(weight &gt; 400) |&gt;\n  summarise(mean_weight = mean(weight))\n\n\nNewer (R 4.1+)\nSlightly faster\nNo extra package needed\nSimpler (fewer features)\n\n\n\n\n\n\n\n\n\n\nTipWhich Pipe Should You Use?\n\n\n\nShort answer: Either! They work the same 99% of the time.\nRecommendations: - Working with tidyverse? Use %&gt;% (consistency) - R 4.1+ and prefer native? Use |&gt; - Teaching beginners? %&gt;% (more examples online) - Most important: Be consistent within a project!\nKeyboard shortcuts: - %&gt;%: Cmd/Ctrl + Shift + M (customizable to |&gt; in RStudio settings)\n\n\n\n\n3.7.4 When NOT to Use Pipes\nPipes are great, but sometimes other approaches are clearer:\n\n# ‚ùå Too many steps (hard to debug)\nresult &lt;- data %&gt;%\n  step1() %&gt;%\n  step2() %&gt;%\n  step3() %&gt;%\n  step4() %&gt;%\n  step5() %&gt;%\n  step6() %&gt;%\n  step7()\n\n# ‚úÖ Break into intermediate steps\ndata_filtered &lt;- data %&gt;%\n  step1() %&gt;%\n  step2()\n\ndata_transformed &lt;- data_filtered %&gt;%\n  step3() %&gt;%\n  step4()\n\nresult &lt;- data_transformed %&gt;%\n  step5()\n\n# ‚ùå Only one step (unnecessary)\nresult &lt;- data %&gt;% mean()\n\n# ‚úÖ Just call the function\nresult &lt;- mean(data)\n\n# ‚ùå Multiple inputs (pipes work with one object)\n# Pipes don't work well here\nresult &lt;- merge(data1, data2, by = \"id\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#introduction-to-dplyr",
    "href": "chapters/ch03-data_types_strings_dplyr.html#introduction-to-dplyr",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.8 Introduction to dplyr",
    "text": "3.8 Introduction to dplyr\ndplyr is the tidyverse package for data manipulation. It provides intuitive ‚Äúverbs‚Äù for working with data frames.\n\n3.8.1 Why dplyr?\n\nBase Rdplyr\n\n\n\n# Subsetting in base R\ncattle_subset &lt;- cattle[cattle$weight &gt; 400 & cattle$breed == \"Holstein\",\n                        c(\"animal_id\", \"weight\")]\n\nHard to read! What‚Äôs happening?\n\n\n\n# Same operation with dplyr\ncattle_subset &lt;- cattle %&gt;%\n  filter(weight &gt; 400, breed == \"Holstein\") %&gt;%\n  select(animal_id, weight)\n\nClear and readable! ‚ÄúFilter rows, then select columns‚Äù\n\n\n\ndplyr verbs (functions): - select(): Pick columns - filter(): Pick rows - mutate(): Create/modify columns (next chapter) - arrange(): Sort rows (next chapter) - summarise(): Calculate summaries (next chapter) - group_by(): Group for operations (next chapter)\nThis chapter: select() and filter()\n\n\n3.8.2 Loading dplyr\n\nlibrary(dplyr)\n\n# Or load entire tidyverse (includes dplyr)\nlibrary(tidyverse)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#selecting-columns-with-select",
    "href": "chapters/ch03-data_types_strings_dplyr.html#selecting-columns-with-select",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.9 Selecting Columns with select()",
    "text": "3.9 Selecting Columns with select()\nselect() chooses which columns to keep in your dataset.\n\n3.9.1 Basic Usage\n\n# Create example data\ncattle &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"H003\"),\n  breed = c(\"Holstein\", \"Holstein\", \"Jersey\"),\n  weight_kg = c(450, 475, 460),\n  height_cm = c(140, 145, 135),\n  age_months = c(24, 30, 26)\n)\n\ncattle\n\n# A tibble: 3 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450       140         24\n2 H002      Holstein       475       145         30\n3 H003      Jersey         460       135         26\n\n# Select specific columns\ncattle %&gt;%\n  select(animal_id, weight_kg)\n\n# A tibble: 3 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 H001            450\n2 H002            475\n3 H003            460\n\n# Select range of columns\ncattle %&gt;%\n  select(animal_id:weight_kg)\n\n# A tibble: 3 √ó 3\n  animal_id breed    weight_kg\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n1 H001      Holstein       450\n2 H002      Holstein       475\n3 H003      Jersey         460\n\n# Select all EXCEPT certain columns\ncattle %&gt;%\n  select(-height_cm, -age_months)\n\n# A tibble: 3 √ó 3\n  animal_id breed    weight_kg\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;\n1 H001      Holstein       450\n2 H002      Holstein       475\n3 H003      Jersey         460\n\n\n\n\n3.9.2 Helper Functions\nselect() has powerful helper functions:\n\n# Create wider dataset\nanimals &lt;- tibble(\n  animal_id = c(\"001\", \"002\", \"003\"),\n  weight_kg = c(450, 475, 460),\n  weight_lb = c(992, 1047, 1014),\n  height_cm = c(140, 145, 135),\n  height_in = c(55, 57, 53),\n  temp_c = c(38.5, 38.8, 38.3)\n)\n\nanimals\n\n# A tibble: 3 √ó 6\n  animal_id weight_kg weight_lb height_cm height_in temp_c\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 001             450       992       140        55   38.5\n2 002             475      1047       145        57   38.8\n3 003             460      1014       135        53   38.3\n\n# Select columns starting with \"weight\"\nanimals %&gt;%\n  select(starts_with(\"weight\"))\n\n# A tibble: 3 √ó 2\n  weight_kg weight_lb\n      &lt;dbl&gt;     &lt;dbl&gt;\n1       450       992\n2       475      1047\n3       460      1014\n\n# Select columns ending with \"cm\"\nanimals %&gt;%\n  select(ends_with(\"cm\"))\n\n# A tibble: 3 √ó 1\n  height_cm\n      &lt;dbl&gt;\n1       140\n2       145\n3       135\n\n# Select columns containing \"weight\"\nanimals %&gt;%\n  select(contains(\"weight\"))\n\n# A tibble: 3 √ó 2\n  weight_kg weight_lb\n      &lt;dbl&gt;     &lt;dbl&gt;\n1       450       992\n2       475      1047\n3       460      1014\n\n# Select all numeric columns\nanimals %&gt;%\n  select(where(is.numeric))\n\n# A tibble: 3 √ó 5\n  weight_kg weight_lb height_cm height_in temp_c\n      &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1       450       992       140        55   38.5\n2       475      1047       145        57   38.8\n3       460      1014       135        53   38.3\n\n\n\n\n3.9.3 Common select() Helpers\n\n\n\nHelper\nSelects\n\n\n\n\nstarts_with(\"x\")\nColumns starting with ‚Äúx‚Äù\n\n\nends_with(\"x\")\nColumns ending with ‚Äúx‚Äù\n\n\ncontains(\"x\")\nColumns containing ‚Äúx‚Äù\n\n\nmatches(\"regex\")\nColumns matching regex pattern\n\n\nnum_range(\"x\", 1:3)\nx1, x2, x3\n\n\nwhere(is.numeric)\nAll numeric columns\n\n\nwhere(is.character)\nAll character columns\n\n\neverything()\nAll remaining columns\n\n\n\n\n\n3.9.4 Reordering Columns\n\n# Move animal_id to the end\ncattle %&gt;%\n  select(breed, weight_kg, height_cm, age_months, animal_id)\n\n# A tibble: 3 √ó 5\n  breed    weight_kg height_cm age_months animal_id\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;    \n1 Holstein       450       140         24 H001     \n2 Holstein       475       145         30 H002     \n3 Jersey         460       135         26 H003     \n\n# Or use everything() for remaining columns\ncattle %&gt;%\n  select(breed, everything())\n\n# A tibble: 3 √ó 5\n  breed    animal_id weight_kg height_cm age_months\n  &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Holstein H001            450       140         24\n2 Holstein H002            475       145         30\n3 Jersey   H003            460       135         26\n\n\n\n\n3.9.5 Renaming While Selecting\n\n# Rename during select\ncattle %&gt;%\n  select(id = animal_id, weight = weight_kg)\n\n# A tibble: 3 √ó 2\n  id    weight\n  &lt;chr&gt;  &lt;dbl&gt;\n1 H001     450\n2 H002     475\n3 H003     460\n\n\n\n\n\n\n\n\nTipselect() Pro Tips\n\n\n\n\nUse helper functions to avoid typing column names\nUse - to remove columns rather than listing many to keep\nUse everything() to move columns to front:\n\ndata %&gt;% select(important_col, everything())\n\nChain with other verbs for powerful workflows",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#filtering-rows-with-filter",
    "href": "chapters/ch03-data_types_strings_dplyr.html#filtering-rows-with-filter",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.10 Filtering Rows with filter()",
    "text": "3.10 Filtering Rows with filter()\nfilter() keeps rows that meet specified conditions.\n\n3.10.1 Basic Filtering\n\ncattle &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"J001\", \"A001\", \"H003\"),\n  breed = c(\"Holstein\", \"Holstein\", \"Jersey\", \"Angus\", \"Holstein\"),\n  weight_kg = c(450, 475, 460, 520, 490),\n  age_months = c(24, 30, 26, 36, 28)\n)\n\ncattle\n\n# A tibble: 5 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 J001      Jersey         460         26\n4 A001      Angus          520         36\n5 H003      Holstein       490         28\n\n# Filter to heavy animals (weight &gt; 470)\ncattle %&gt;%\n  filter(weight_kg &gt; 470)\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       475         30\n2 A001      Angus          520         36\n3 H003      Holstein       490         28\n\n# Filter to Holstein breed\ncattle %&gt;%\n  filter(breed == \"Holstein\")\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 H003      Holstein       490         28\n\n# Filter to young animals (age &lt; 30)\ncattle %&gt;%\n  filter(age_months &lt; 30)\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 J001      Jersey         460         26\n3 H003      Holstein       490         28\n\n\n\n\n3.10.2 Comparison Operators\n\n\n\nOperator\nMeaning\nExample\n\n\n\n\n==\nEqual to\nbreed == \"Holstein\"\n\n\n!=\nNot equal to\nbreed != \"Holstein\"\n\n\n&gt;\nGreater than\nweight &gt; 450\n\n\n&lt;\nLess than\nage &lt; 30\n\n\n&gt;=\nGreater than or equal\nweight &gt;= 450\n\n\n&lt;=\nLess than or equal\nage &lt;= 30\n\n\n%in%\nIn a set\nbreed %in% c(\"Holstein\", \"Jersey\")\n\n\n\n\n\n\n\n\n\nWarningCommon Mistake: = vs ==\n\n\n\n\n# ‚ùå WRONG (assignment, not comparison)\nfilter(cattle, breed = \"Holstein\")\n\n# ‚úÖ CORRECT (comparison)\nfilter(cattle, breed == \"Holstein\")\n\nRemember: - = assigns a value - == tests equality\n\n\n\n\n3.10.3 Combining Conditions\n\n3.10.3.1 AND: Multiple conditions (comma or &)\n\n# Heavy AND Holstein\ncattle %&gt;%\n  filter(weight_kg &gt; 460, breed == \"Holstein\")\n\n# A tibble: 2 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       475         30\n2 H003      Holstein       490         28\n\n# Same thing with &\ncattle %&gt;%\n  filter(weight_kg &gt; 460 & breed == \"Holstein\")\n\n# A tibble: 2 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       475         30\n2 H003      Holstein       490         28\n\n\n\n\n3.10.3.2 OR: Either condition (|)\n\n# Holstein OR Jersey\ncattle %&gt;%\n  filter(breed == \"Holstein\" | breed == \"Jersey\")\n\n# A tibble: 4 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 J001      Jersey         460         26\n4 H003      Holstein       490         28\n\n# Better way: use %in%\ncattle %&gt;%\n  filter(breed %in% c(\"Holstein\", \"Jersey\"))\n\n# A tibble: 4 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 J001      Jersey         460         26\n4 H003      Holstein       490         28\n\n\n\n\n3.10.3.3 NOT: Negate a condition (!)\n\n# NOT Holstein\ncattle %&gt;%\n  filter(breed != \"Holstein\")\n\n# A tibble: 2 √ó 4\n  animal_id breed  weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 J001      Jersey       460         26\n2 A001      Angus        520         36\n\n# Or\ncattle %&gt;%\n  filter(!breed == \"Holstein\")\n\n# A tibble: 2 √ó 4\n  animal_id breed  weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 J001      Jersey       460         26\n2 A001      Angus        520         36\n\n# NOT in the set\ncattle %&gt;%\n  filter(!breed %in% c(\"Holstein\", \"Jersey\"))\n\n# A tibble: 1 √ó 4\n  animal_id breed weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001      Angus       520         36\n\n\n\n\n\n3.10.4 Complex Logical Conditions\n\n# (Heavy OR old) AND not Angus\ncattle %&gt;%\n  filter(\n    (weight_kg &gt; 470 | age_months &gt; 30),\n    breed != \"Angus\"\n  )\n\n# A tibble: 2 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       475         30\n2 H003      Holstein       490         28\n\n\n\n\n3.10.5 The %in% Operator\nMatch any value in a set:\n\n# Select specific IDs\nselected_ids &lt;- c(\"H001\", \"H003\", \"A001\")\n\ncattle %&gt;%\n  filter(animal_id %in% selected_ids)\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 A001      Angus          520         36\n3 H003      Holstein       490         28\n\n# Select multiple breeds\ndairy_breeds &lt;- c(\"Holstein\", \"Jersey\", \"Guernsey\")\n\ncattle %&gt;%\n  filter(breed %in% dairy_breeds)\n\n# A tibble: 4 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 J001      Jersey         460         26\n4 H003      Holstein       490         28\n\n\n\n\n3.10.6 Filtering with Strings\nUse stringr functions inside filter():\n\n# Animals with IDs starting with \"H\"\ncattle %&gt;%\n  filter(str_detect(animal_id, \"^H\"))\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 H003      Holstein       490         28\n\n# Breeds containing \"Hol\"\ncattle %&gt;%\n  filter(str_detect(breed, \"Hol\"))\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       450         24\n2 H002      Holstein       475         30\n3 H003      Holstein       490         28\n\n\n\n\n3.10.7 Handling Missing Values\n\n# Data with missing values\ncattle_na &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"H003\", \"H004\"),\n  weight_kg = c(450, NA, 460, 490),\n  breed = c(\"Holstein\", \"Holstein\", NA, \"Jersey\")\n)\n\ncattle_na\n\n# A tibble: 4 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 H001            450 Holstein\n2 H002             NA Holstein\n3 H003            460 &lt;NA&gt;    \n4 H004            490 Jersey  \n\n# Filter to rows with missing weight\ncattle_na %&gt;%\n  filter(is.na(weight_kg))\n\n# A tibble: 1 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 H002             NA Holstein\n\n# Filter to rows with NON-missing weight\ncattle_na %&gt;%\n  filter(!is.na(weight_kg))\n\n# A tibble: 3 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 H001            450 Holstein\n2 H003            460 &lt;NA&gt;    \n3 H004            490 Jersey  \n\n# Remove all rows with ANY missing values\ncattle_na %&gt;%\n  filter(!is.na(weight_kg), !is.na(breed))\n\n# A tibble: 2 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 H001            450 Holstein\n2 H004            490 Jersey  \n\n# Or use drop_na() from tidyr\nlibrary(tidyr)\ncattle_na %&gt;%\n  drop_na()  # Removes rows with any NA\n\n# A tibble: 2 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 H001            450 Holstein\n2 H004            490 Jersey  \n\n\n\n\n\n\n\n\nImportantBe Careful with NA in Comparisons\n\n\n\n\nx &lt;- c(1, 2, NA, 4)\n\n# This doesn't work as expected!\nx == NA  # Returns NA, not TRUE/FALSE\n\n[1] NA NA NA NA\n\n# Use is.na() instead\nis.na(x)\n\n[1] FALSE FALSE  TRUE FALSE\n\n\nNA means ‚Äúunknown‚Äù, so: - NA == NA is NA (we don‚Äôt know if two unknowns are equal) - Always use is.na() to test for missingness",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#combining-select-and-filter-with-pipes",
    "href": "chapters/ch03-data_types_strings_dplyr.html#combining-select-and-filter-with-pipes",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.11 Combining select() and filter() with Pipes",
    "text": "3.11 Combining select() and filter() with Pipes\nThe real power comes from combining operations:\n\n# Create larger dataset\nfarm_data &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"J001\", \"A001\", \"H003\", \"J002\"),\n  breed = c(\"Holstein\", \"Holstein\", \"Jersey\", \"Angus\", \"Holstein\", \"Jersey\"),\n  weight_kg = c(450, 475, 420, 520, 490, 435),\n  height_cm = c(140, 145, 130, 155, 148, 132),\n  age_months = c(24, 30, 26, 36, 28, 25),\n  farm = c(\"North\", \"North\", \"South\", \"West\", \"North\", \"South\"),\n  sex = c(\"F\", \"F\", \"F\", \"M\", \"F\", \"F\")\n)\n\nfarm_data\n\n# A tibble: 6 √ó 7\n  animal_id breed    weight_kg height_cm age_months farm  sex  \n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 H001      Holstein       450       140         24 North F    \n2 H002      Holstein       475       145         30 North F    \n3 J001      Jersey         420       130         26 South F    \n4 A001      Angus          520       155         36 West  M    \n5 H003      Holstein       490       148         28 North F    \n6 J002      Jersey         435       132         25 South F    \n\n# Filter Holstein cows from North farm, select ID and weight\nfarm_data %&gt;%\n  filter(breed == \"Holstein\", farm == \"North\") %&gt;%\n  select(animal_id, weight_kg)\n\n# A tibble: 3 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 H001            450\n2 H002            475\n3 H003            490\n\n# Young, light animals: just ID, breed, weight\nfarm_data %&gt;%\n  filter(age_months &lt; 28, weight_kg &lt; 450) %&gt;%\n  select(animal_id, breed, weight_kg)\n\n# A tibble: 2 √ó 3\n  animal_id breed  weight_kg\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;\n1 J001      Jersey       420\n2 J002      Jersey       435\n\n# All females, excluding height column\nfarm_data %&gt;%\n  filter(sex == \"F\") %&gt;%\n  select(-height_cm)\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg age_months farm  sex  \n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 H001      Holstein       450         24 North F    \n2 H002      Holstein       475         30 North F    \n3 J001      Jersey         420         26 South F    \n4 H003      Holstein       490         28 North F    \n5 J002      Jersey         435         25 South F    \n\n\n\n3.11.1 Building Complex Workflows\n\n# Multi-step cleaning and filtering\ncleaned_data &lt;- farm_data %&gt;%\n  # Remove height (not needed)\n  select(-height_cm) %&gt;%\n  # Only females\n  filter(sex == \"F\") %&gt;%\n  # Only dairy breeds\n  filter(breed %in% c(\"Holstein\", \"Jersey\")) %&gt;%\n  # Only mature animals\n  filter(age_months &gt;= 26)\n\ncleaned_data\n\n# A tibble: 3 √ó 6\n  animal_id breed    weight_kg age_months farm  sex  \n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;\n1 H002      Holstein       475         30 North F    \n2 J001      Jersey         420         26 South F    \n3 H003      Holstein       490         28 North F    \n\n\n\n\n\n\n\n\nTipPipe Workflow Best Practices\n\n\n\n\nOne operation per line for readability:\n\ndata %&gt;%\n  filter(condition1) %&gt;%\n  select(col1, col2) %&gt;%\n  filter(condition2)\n\nComment complex filters:\n\ndata %&gt;%\n  # Remove animals that failed health check\n  filter(health_status == \"Pass\") %&gt;%\n  # Only lactating cows\n  filter(is_lactating == TRUE)\n\nIndent continued pipes (2 spaces):\n\nresult &lt;- data %&gt;%\n  filter(condition) %&gt;%\n  select(columns)\n\nSave intermediate results for debugging:\n\nfiltered &lt;- data %&gt;% filter(condition)\nfinal &lt;- filtered %&gt;% select(columns)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#summary",
    "href": "chapters/ch03-data_types_strings_dplyr.html#summary",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.12 Summary",
    "text": "3.12 Summary\nThis chapter introduced essential data manipulation skills:\n\nR data types include numeric, integer, character, logical, factor, and date/time\nType checking uses class(), typeof(), is.*() functions\nType coercion uses as.*() functions to convert between types\nFactors represent categorical data; use forcats package to manipulate them\nString manipulation with stringr: detect, replace, transform, split, combine text\nRegular expressions provide powerful pattern matching for text\nThe pipe (%&gt;% or |&gt;) chains operations for readable workflows\nselect() chooses columns; use helper functions like starts_with(), ends_with(), contains()\nfilter() chooses rows based on conditions; combine conditions with &, |, !\nCombining operations with pipes creates powerful data transformation workflows\n\nNext chapter: More dplyr verbs for data transformation!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#homework-assignment",
    "href": "chapters/ch03-data_types_strings_dplyr.html#homework-assignment",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.13 Homework Assignment",
    "text": "3.13 Homework Assignment\n\n3.13.1 Assignment: Data Cleaning and Manipulation\nDue: Before Week 4\n\n3.13.1.1 Part 1: Type Exploration and Conversion (25 points)\nDownload the dataset messy_cattle.csv (your instructor will provide the link).\nIn a Quarto document:\n\nRead the data and examine the structure\nCheck column types - are they correct?\nFix type issues:\n\nConvert animal_id to character (if needed)\nConvert breed and treatment to factors with appropriate levels\nConvert birth_date to Date type\nEnsure weights are numeric\n\nDocument the issues you found and how you fixed them\n\n\n\n3.13.1.2 Part 2: String Cleaning (30 points)\nThe dataset has messy text fields. Clean them:\n\nStandardize breed:\n\nRemove leading/trailing whitespace\nConvert to title case\nCount observations per breed\n\nFix animal_id:\n\nMake all IDs uppercase\nRemove any dashes or spaces\nPad with leading zeros to 4 digits (e.g., ‚Äú1‚Äù ‚Üí ‚Äú0001‚Äù)\n\nCreate new variable breed_code:\n\nExtract first letter of breed name\nExamples: ‚ÄúHolstein‚Äù ‚Üí ‚ÄúH‚Äù, ‚ÄúJersey‚Äù ‚Üí ‚ÄúJ‚Äù\n\nDocument your cleaning steps with before/after examples\n\n\n\n3.13.1.3 Part 3: Filtering and Selecting (35 points)\nUsing your cleaned data:\n\nCreate subset 1: Holstein and Jersey cows only\n\nSelect columns: animal_id, breed, weight_kg, age_months\nFilter to breeds: Holstein or Jersey\nFilter to sex: Female only\nSave as dairy_cows\n\nCreate subset 2: Heavy, mature animals\n\nFilter to weight &gt; 450 kg\nFilter to age &gt;= 24 months\nRemove the birth_date column\nSave as mature_heavy\n\nCreate subset 3: Treatment A recipients\n\nFilter to treatment == ‚ÄúTreatmentA‚Äù\nSelect only ID, weight, and treatment columns\nRemove any rows with missing values\nSave as treatment_a\n\nAnswer questions:\n\nHow many animals are in each subset?\nWhat is the mean weight in each subset?\nWhich breed is most common in dairy_cows?\n\n\n\n\n3.13.1.4 Part 4: Reflection on Pipes and Readability (10 points)\nWrite 150-200 words reflecting on: - How does using pipes change your code? - Compare nested functions vs piped operations‚Äîwhich is clearer? - When might you NOT want to use pipes?\n\n\n\n3.13.2 Recommended YAML\n---\ntitle: \"Week 3 Homework: Data Types and dplyr\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n3.13.3 Grading Rubric\n\nPart 1: Type Exploration (25%):\n\nData read and structure examined (5%)\nTypes checked correctly (5%)\nType conversions performed correctly (10%)\nIssues documented clearly (5%)\n\nPart 2: String Cleaning (30%):\n\nBreed standardization (10%)\nAnimal ID fixes (10%)\nBreed code creation (5%)\nDocumentation and examples (5%)\n\nPart 3: Filtering and Selecting (35%):\n\nThree subsets created correctly (21%)\nQuestions answered accurately (9%)\nCode is clean and well-commented (5%)\n\nPart 4: Reflection (10%):\n\nThoughtful reflection on pipes (5%)\nComparison of approaches (5%)\n\n\n\n\n3.13.4 Bonus (10 points)\nCreate a complex multi-step pipeline that: 1. Starts with the raw messy data 2. Cleans all text fields 3. Converts all types 4. Filters to a specific subset 5. Selects relevant columns 6. All in ONE pipe chain with clear comments\nShow before (first 5 rows of raw data) and after (final result).",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch03-data_types_strings_dplyr.html#additional-resources",
    "href": "chapters/ch03-data_types_strings_dplyr.html#additional-resources",
    "title": "3¬† Data Types, Strings, and Introduction to dplyr",
    "section": "3.14 Additional Resources",
    "text": "3.14 Additional Resources\n\n3.14.1 Required Reading\n\nR for Data Science (2e) - Chapters 13-15, 19-20: Strings, Factors, Pipes, Data Transformation\nstringr documentation\ndplyr documentation\n\n\n\n3.14.2 Optional Reading\n\nforcats documentation\nRegular Expressions in R\nWickham, H. (2014). ‚ÄúTidy Data.‚Äù Journal of Statistical Software, 59(10). Link\n\n\n\n3.14.3 Videos\n\n‚ÄúString Manipulation in R‚Äù by StatQuest\n‚ÄúIntroduction to dplyr‚Äù by RStudio\n‚ÄúRegular Expressions Demystified‚Äù by DataCamp\n\n\n\n3.14.4 Cheat Sheets\n\nstringr Cheat Sheet\ndplyr Cheat Sheet\nRegular Expressions Cheat Sheet\nforcats Cheat Sheet\n\n\n\n3.14.5 Interactive Learning\n\nregex101.com - Interactive regex tester and debugger\nRegExr.com - Learn, build, and test regex\nstringr in 5 minutes - Quick start guide\n\n\n\n3.14.6 Useful Websites\n\nTidyverse - Homepage for tidyverse packages\nStack Overflow: R - Q&A community\nRStudio Community - Friendly help forum\n\n\nNext Chapter: Data Manipulation with dplyr",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Data Types, Strings, and Introduction to dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html",
    "href": "chapters/ch04-dplyr.html",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "",
    "text": "4.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#learning-objectives",
    "href": "chapters/ch04-dplyr.html#learning-objectives",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "",
    "text": "Create and modify variables using dplyr::mutate()\nSort data using dplyr::arrange() in ascending and descending order\nCalculate summary statistics using dplyr::summarise()\nPerform grouped operations with group_by() and summarise()\nUse helper functions: count(), rename(), and relocate()\nApply conditional logic with if_else() and case_when()\nWork across multiple columns efficiently using across()\nHandle missing data with is.na(), drop_na(), replace_na(), and coalesce()\nUse window functions: lag(), lead(), cumsum(), and row_number()\nChain multiple dplyr operations into complex data transformation pipelines",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#introduction-to-dplyr-verbs",
    "href": "chapters/ch04-dplyr.html#introduction-to-dplyr-verbs",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.2 Introduction to dplyr Verbs",
    "text": "4.2 Introduction to dplyr Verbs\nIn the previous chapter, you learned select() and filter() for choosing columns and rows. This chapter covers the remaining core dplyr verbs that complete your data manipulation toolkit.\n\n4.2.1 The Five Main dplyr Verbs\n\n\n\n\n\nflowchart TD\n    A[dplyr Verbs] --&gt; B[select&lt;br/&gt;Pick columns]\n    A --&gt; C[filter&lt;br/&gt;Pick rows]\n    A --&gt; D[mutate&lt;br/&gt;Create/modify columns]\n    A --&gt; E[arrange&lt;br/&gt;Sort rows]\n    A --&gt; F[summarise&lt;br/&gt;Calculate summaries]\n\n    B --&gt; B1[\"select(animal_id, weight)\"]\n    C --&gt; C1[\"filter(weight &gt; 450)\"]\n    D --&gt; D1[\"mutate(bmi = weight/height)\"]\n    E --&gt; E1[\"arrange(weight)\"]\n    F --&gt; F1[\"summarise(mean_weight = mean(weight))\"]\n\n\n\n\n\n\nThis chapter focuses on: mutate(), arrange(), summarise(), and group_by()\n\n\n\n\n\n\nNoteReview: Data Used in This Chapter\n\n\n\nWe‚Äôll use animal science datasets throughout. Make sure you have these packages loaded:\n\nlibrary(tidyverse)  # Includes dplyr, ggplot2, tidyr, etc.\nlibrary(lubridate)  # For dates",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#creating-and-modifying-variables-with-mutate",
    "href": "chapters/ch04-dplyr.html#creating-and-modifying-variables-with-mutate",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.3 Creating and Modifying Variables with mutate()",
    "text": "4.3 Creating and Modifying Variables with mutate()\nmutate() creates new columns or modifies existing columns in your data frame.\n\n4.3.1 Basic Usage\n\n# Create example cattle data\ncattle &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"H003\", \"J001\", \"J002\"),\n  breed = c(\"Holstein\", \"Holstein\", \"Holstein\", \"Jersey\", \"Jersey\"),\n  weight_kg = c(600, 625, 580, 450, 470),\n  height_cm = c(145, 148, 142, 130, 133),\n  age_months = c(30, 36, 28, 30, 32)\n)\n\ncattle\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600       145         30\n2 H002      Holstein       625       148         36\n3 H003      Holstein       580       142         28\n4 J001      Jersey         450       130         30\n5 J002      Jersey         470       133         32\n\n# Create new column: weight in pounds\ncattle %&gt;%\n  mutate(weight_lb = weight_kg * 2.20462)\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months weight_lb\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 H001      Holstein       600       145         30     1323.\n2 H002      Holstein       625       148         36     1378.\n3 H003      Holstein       580       142         28     1279.\n4 J001      Jersey         450       130         30      992.\n5 J002      Jersey         470       133         32     1036.\n\n# Create multiple new columns\ncattle %&gt;%\n  mutate(\n    weight_lb = weight_kg * 2.20462,\n    height_m = height_cm / 100,\n    bmi = weight_kg / (height_m^2)\n  )\n\n# A tibble: 5 √ó 8\n  animal_id breed    weight_kg height_cm age_months weight_lb height_m   bmi\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 H001      Holstein       600       145         30     1323.     1.45  285.\n2 H002      Holstein       625       148         36     1378.     1.48  285.\n3 H003      Holstein       580       142         28     1279.     1.42  288.\n4 J001      Jersey         450       130         30      992.     1.3   266.\n5 J002      Jersey         470       133         32     1036.     1.33  266.\n\n\n\n\n\n\n\n\nImportantmutate() Adds Columns, Doesn‚Äôt Replace Data\n\n\n\nmutate() returns a new data frame. To keep changes, assign the result:\n\n# ‚ùå Changes are lost\ncattle %&gt;% mutate(weight_lb = weight_kg * 2.20462)\n\n# ‚úÖ Save the result\ncattle &lt;- cattle %&gt;% mutate(weight_lb = weight_kg * 2.20462)\n# OR\ncattle_with_lb &lt;- cattle %&gt;% mutate(weight_lb = weight_kg * 2.20462)\n\n\n\n\n\n4.3.2 Overwriting Existing Columns\n\n# Modify existing column: convert breed to uppercase\ncattle %&gt;%\n  mutate(breed = str_to_upper(breed))\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      HOLSTEIN       600       145         30\n2 H002      HOLSTEIN       625       148         36\n3 H003      HOLSTEIN       580       142         28\n4 J001      JERSEY         450       130         30\n5 J002      JERSEY         470       133         32\n\n# Round weight to nearest 10 kg\ncattle %&gt;%\n  mutate(weight_kg = round(weight_kg / 10) * 10)\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600       145         30\n2 H002      Holstein       620       148         36\n3 H003      Holstein       580       142         28\n4 J001      Jersey         450       130         30\n5 J002      Jersey         470       133         32\n\n\n\n\n4.3.3 Using New Columns Immediately\nYou can reference newly created columns within the same mutate():\n\ncattle %&gt;%\n  mutate(\n    height_m = height_cm / 100,       # Create height_m\n    bmi = weight_kg / (height_m^2)    # Use height_m immediately!\n  )\n\n# A tibble: 5 √ó 7\n  animal_id breed    weight_kg height_cm age_months height_m   bmi\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 H001      Holstein       600       145         30     1.45  285.\n2 H002      Holstein       625       148         36     1.48  285.\n3 H003      Holstein       580       142         28     1.42  288.\n4 J001      Jersey         450       130         30     1.3   266.\n5 J002      Jersey         470       133         32     1.33  266.\n\n\n\n\n4.3.4 Useful Functions Inside mutate()\nCommon operations you‚Äôll use with mutate():\n\n\n\n\n\n\n\n\nOperation\nFunction\nExample\n\n\n\n\nMath\n+, -, *, /, ^\nweight_kg * 2.20462\n\n\nRounding\nround(), floor(), ceiling()\nround(weight, 1)\n\n\nLogarithms\nlog(), log10(), exp()\nlog(concentration)\n\n\nRanking\nmin_rank(), dense_rank()\nmin_rank(desc(weight))\n\n\nString functions\nstr_*()\nstr_to_upper(breed)\n\n\nDate functions\nyear(), month(), day()\nyear(birth_date)\n\n\nConditional\nif_else(), case_when()\nif_else(weight &gt; 500, \"Heavy\", \"Light\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#conditional-operations",
    "href": "chapters/ch04-dplyr.html#conditional-operations",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.4 Conditional Operations",
    "text": "4.4 Conditional Operations\n\n4.4.1 Simple Conditions with if_else()\nif_else() creates values based on a TRUE/FALSE condition:\nSyntax: if_else(condition, value_if_true, value_if_false)\n\n# Classify animals as heavy or light\ncattle %&gt;%\n  mutate(\n    weight_class = if_else(weight_kg &gt; 500, \"Heavy\", \"Light\")\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months weight_class\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;       \n1 H001      Holstein       600       145         30 Heavy       \n2 H002      Holstein       625       148         36 Heavy       \n3 H003      Holstein       580       142         28 Heavy       \n4 J001      Jersey         450       130         30 Light       \n5 J002      Jersey         470       133         32 Light       \n\n# Create logical column\ncattle %&gt;%\n  mutate(\n    is_mature = if_else(age_months &gt;= 30, TRUE, FALSE)\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months is_mature\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;    \n1 H001      Holstein       600       145         30 TRUE     \n2 H002      Holstein       625       148         36 TRUE     \n3 H003      Holstein       580       142         28 FALSE    \n4 J001      Jersey         450       130         30 TRUE     \n5 J002      Jersey         470       133         32 TRUE     \n\n# Can use existing values\ncattle %&gt;%\n  mutate(\n    adjusted_weight = if_else(breed == \"Jersey\",\n                              weight_kg * 1.1,  # Boost Jersey weights by 10%\n                              weight_kg)        # Keep others same\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months adjusted_weight\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;           &lt;dbl&gt;\n1 H001      Holstein       600       145         30             600\n2 H002      Holstein       625       148         36             625\n3 H003      Holstein       580       142         28             580\n4 J001      Jersey         450       130         30             495\n5 J002      Jersey         470       133         32             517\n\n\n\n\n\n\n\n\nTipif_else() vs base R ifelse()\n\n\n\ndplyr‚Äôs if_else() is stricter and safer than base R‚Äôs ifelse():\n\nType safety: Both true and false values must be the same type\nNA handling: Explicit missing argument for NA values\nSpeed: Faster for large datasets\n\n\n# ‚úÖ Good (both character)\nif_else(condition, \"yes\", \"no\")\n\n# ‚ùå Error (different types)\nif_else(condition, \"yes\", 0)\n\n# ‚úÖ With NAs\nif_else(condition, \"yes\", \"no\", missing = \"unknown\")\n\n\n\n\n\n4.4.2 Multiple Conditions with case_when()\nFor multiple conditions, use case_when():\n\n# Classify into three weight categories\ncattle %&gt;%\n  mutate(\n    weight_category = case_when(\n      weight_kg &lt; 500 ~ \"Light\",\n      weight_kg &lt; 600 ~ \"Medium\",\n      weight_kg &gt;= 600 ~ \"Heavy\"\n    )\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months weight_category\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;          \n1 H001      Holstein       600       145         30 Heavy          \n2 H002      Holstein       625       148         36 Heavy          \n3 H003      Holstein       580       142         28 Medium         \n4 J001      Jersey         450       130         30 Light          \n5 J002      Jersey         470       133         32 Light          \n\n# Multiple factors\ncattle %&gt;%\n  mutate(\n    category = case_when(\n      breed == \"Jersey\" & weight_kg &gt; 450 ~ \"Large Jersey\",\n      breed == \"Jersey\" & weight_kg &lt;= 450 ~ \"Small Jersey\",\n      breed == \"Holstein\" & weight_kg &gt; 600 ~ \"Large Holstein\",\n      breed == \"Holstein\" & weight_kg &lt;= 600 ~ \"Small Holstein\",\n      TRUE ~ \"Other\"  # Catch-all (like \"else\")\n    )\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg height_cm age_months category      \n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;         \n1 H001      Holstein       600       145         30 Small Holstein\n2 H002      Holstein       625       148         36 Large Holstein\n3 H003      Holstein       580       142         28 Small Holstein\n4 J001      Jersey         450       130         30 Small Jersey  \n5 J002      Jersey         470       133         32 Large Jersey  \n\n\nHow case_when() works: 1. Evaluates conditions in order from top to bottom 2. Returns the value (~ right side) for the first TRUE condition 3. Stops checking once a match is found 4. Use TRUE ~ value as a catch-all for everything else\n\n\n\n\n\n\nWarningOrder Matters in case_when()!\n\n\n\n\n# ‚ùå WRONG: Everything becomes \"Heavy\"\ncattle %&gt;%\n  mutate(\n    wrong = case_when(\n      weight_kg &gt; 0 ~ \"Heavy\",      # This matches EVERYTHING!\n      weight_kg &gt; 500 ~ \"Medium\",   # Never reached\n      TRUE ~ \"Light\"                # Never reached\n    )\n  ) %&gt;%\n  select(animal_id, weight_kg, wrong)\n\n# A tibble: 5 √ó 3\n  animal_id weight_kg wrong\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;\n1 H001            600 Heavy\n2 H002            625 Heavy\n3 H003            580 Heavy\n4 J001            450 Heavy\n5 J002            470 Heavy\n\n# ‚úÖ CORRECT: Specific conditions first\ncattle %&gt;%\n  mutate(\n    correct = case_when(\n      weight_kg &gt; 600 ~ \"Heavy\",\n      weight_kg &gt; 500 ~ \"Medium\",\n      TRUE ~ \"Light\"\n    )\n  ) %&gt;%\n  select(animal_id, weight_kg, correct)\n\n# A tibble: 5 √ó 3\n  animal_id weight_kg correct\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  \n1 H001            600 Medium \n2 H002            625 Heavy  \n3 H003            580 Medium \n4 J001            450 Light  \n5 J002            470 Light  \n\n\nRule: Put specific conditions before general ones!\n\n\n\n\n4.4.3 Real-World Example: Creating Treatment Groups\n\n# Create feed trial data\nfeed_trial &lt;- tibble(\n  animal_id = sprintf(\"A%03d\", 1:10),\n  baseline_weight = c(450, 480, 445, 490, 455, 470, 460, 475, 465, 485),\n  age_months = c(24, 30, 22, 32, 26, 28, 24, 30, 26, 31),\n  sex = c(\"F\", \"F\", \"M\", \"F\", \"M\", \"F\", \"F\", \"M\", \"F\", \"M\")\n)\n\nfeed_trial\n\n# A tibble: 10 √ó 4\n   animal_id baseline_weight age_months sex  \n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;\n 1 A001                  450         24 F    \n 2 A002                  480         30 F    \n 3 A003                  445         22 M    \n 4 A004                  490         32 F    \n 5 A005                  455         26 M    \n 6 A006                  470         28 F    \n 7 A007                  460         24 F    \n 8 A008                  475         30 M    \n 9 A009                  465         26 F    \n10 A010                  485         31 M    \n\n# Assign animals to treatment groups based on multiple factors\nfeed_trial_assigned &lt;- feed_trial %&gt;%\n  mutate(\n    # Age category\n    age_group = case_when(\n      age_months &lt; 26 ~ \"Young\",\n      age_months &lt;= 30 ~ \"Adult\",\n      TRUE ~ \"Mature\"\n    ),\n    # Weight category\n    weight_group = if_else(baseline_weight &gt; 470, \"Heavy\", \"Light\"),\n    # Treatment assignment (balanced by sex and weight)\n    treatment = case_when(\n      sex == \"F\" & baseline_weight &gt; 470 ~ \"A\",\n      sex == \"F\" & baseline_weight &lt;= 470 ~ \"B\",\n      sex == \"M\" & baseline_weight &gt; 470 ~ \"B\",\n      sex == \"M\" & baseline_weight &lt;= 470 ~ \"A\"\n    )\n  )\n\nfeed_trial_assigned\n\n# A tibble: 10 √ó 7\n   animal_id baseline_weight age_months sex   age_group weight_group treatment\n   &lt;chr&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;        &lt;chr&gt;    \n 1 A001                  450         24 F     Young     Light        B        \n 2 A002                  480         30 F     Adult     Heavy        A        \n 3 A003                  445         22 M     Young     Light        A        \n 4 A004                  490         32 F     Mature    Heavy        A        \n 5 A005                  455         26 M     Adult     Light        A        \n 6 A006                  470         28 F     Adult     Light        B        \n 7 A007                  460         24 F     Young     Light        B        \n 8 A008                  475         30 M     Adult     Heavy        B        \n 9 A009                  465         26 F     Adult     Light        B        \n10 A010                  485         31 M     Mature    Heavy        B",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#sorting-data-with-arrange",
    "href": "chapters/ch04-dplyr.html#sorting-data-with-arrange",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.5 Sorting Data with arrange()",
    "text": "4.5 Sorting Data with arrange()\narrange() sorts rows by one or more columns.\n\n4.5.1 Basic Sorting\n\n# Sort by weight (ascending, lightest first)\ncattle %&gt;%\n  arrange(weight_kg)\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 J001      Jersey         450       130         30\n2 J002      Jersey         470       133         32\n3 H003      Holstein       580       142         28\n4 H001      Holstein       600       145         30\n5 H002      Holstein       625       148         36\n\n# Sort by weight (descending, heaviest first)\ncattle %&gt;%\n  arrange(desc(weight_kg))\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       625       148         36\n2 H001      Holstein       600       145         30\n3 H003      Holstein       580       142         28\n4 J002      Jersey         470       133         32\n5 J001      Jersey         450       130         30\n\n# Sort by breed, then by weight within breed\ncattle %&gt;%\n  arrange(breed, weight_kg)\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H003      Holstein       580       142         28\n2 H001      Holstein       600       145         30\n3 H002      Holstein       625       148         36\n4 J001      Jersey         450       130         30\n5 J002      Jersey         470       133         32\n\n# Sort by breed (ascending), weight (descending)\ncattle %&gt;%\n  arrange(breed, desc(weight_kg))\n\n# A tibble: 5 √ó 5\n  animal_id breed    weight_kg height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H002      Holstein       625       148         36\n2 H001      Holstein       600       145         30\n3 H003      Holstein       580       142         28\n4 J002      Jersey         470       133         32\n5 J001      Jersey         450       130         30\n\n\n\n\n4.5.2 Sorting with Missing Values\n\n# Data with missing weights\ncattle_na &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"H003\", \"H004\", \"H005\"),\n  weight_kg = c(600, NA, 580, 625, NA)\n)\n\ncattle_na\n\n# A tibble: 5 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 H001            600\n2 H002             NA\n3 H003            580\n4 H004            625\n5 H005             NA\n\n# By default, NAs go to the end\ncattle_na %&gt;%\n  arrange(weight_kg)\n\n# A tibble: 5 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 H003            580\n2 H001            600\n3 H004            625\n4 H002             NA\n5 H005             NA\n\n# Descending: NAs still at the end\ncattle_na %&gt;%\n  arrange(desc(weight_kg))\n\n# A tibble: 5 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 H004            625\n2 H001            600\n3 H003            580\n4 H002             NA\n5 H005             NA\n\n\n\n\n\n\n\n\nTipUse arrange() to Check Your Work\n\n\n\nWhen creating or modifying columns, sort to verify results:\n\n# Did the weight classification work correctly?\ncattle %&gt;%\n  mutate(weight_class = if_else(weight_kg &gt; 500, \"Heavy\", \"Light\")) %&gt;%\n  arrange(weight_kg) %&gt;%\n  select(animal_id, weight_kg, weight_class)\n\n# A tibble: 5 √ó 3\n  animal_id weight_kg weight_class\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       \n1 J001            450 Light       \n2 J002            470 Light       \n3 H003            580 Heavy       \n4 H001            600 Heavy       \n5 H002            625 Heavy       \n\n\nSorting makes it easy to spot errors!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#grouped-operations-with-group_by-and-summarise",
    "href": "chapters/ch04-dplyr.html#grouped-operations-with-group_by-and-summarise",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.6 Grouped Operations with group_by() and summarise()",
    "text": "4.6 Grouped Operations with group_by() and summarise()\nThe real power of dplyr comes from grouped operations: calculating summaries for each group separately.\n\n4.6.1 Understanding group_by()\ngroup_by() doesn‚Äôt change your data‚Äîit adds invisible grouping information:\n\n# Create farm data with multiple breeds and farms\nfarm_data &lt;- tibble(\n  animal_id = sprintf(\"A%03d\", 1:12),\n  breed = rep(c(\"Holstein\", \"Jersey\", \"Angus\"), each = 4),\n  farm = rep(c(\"North\", \"South\"), 6),\n  weight_kg = c(600, 620, 590, 610, 450, 470, 455, 465,\n                550, 570, 540, 560)\n)\n\nfarm_data\n\n# A tibble: 12 √ó 4\n   animal_id breed    farm  weight_kg\n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 A001      Holstein North       600\n 2 A002      Holstein South       620\n 3 A003      Holstein North       590\n 4 A004      Holstein South       610\n 5 A005      Jersey   North       450\n 6 A006      Jersey   South       470\n 7 A007      Jersey   North       455\n 8 A008      Jersey   South       465\n 9 A009      Angus    North       550\n10 A010      Angus    South       570\n11 A011      Angus    North       540\n12 A012      Angus    South       560\n\n# Group by breed\nfarm_data %&gt;%\n  group_by(breed)\n\n# A tibble: 12 √ó 4\n# Groups:   breed [3]\n   animal_id breed    farm  weight_kg\n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n 1 A001      Holstein North       600\n 2 A002      Holstein South       620\n 3 A003      Holstein North       590\n 4 A004      Holstein South       610\n 5 A005      Jersey   North       450\n 6 A006      Jersey   South       470\n 7 A007      Jersey   North       455\n 8 A008      Jersey   South       465\n 9 A009      Angus    North       550\n10 A010      Angus    South       570\n11 A011      Angus    North       540\n12 A012      Angus    South       560\n\n\nNotice: ‚ÄúGroups: breed [3]‚Äù in the output. The data looks the same, but it‚Äôs now grouped!\n\n\n4.6.2 Summarizing Data with summarise()\nsummarise() (or summarize()) calculates summary statistics:\n\n# Overall mean weight (no grouping)\nfarm_data %&gt;%\n  summarise(\n    mean_weight = mean(weight_kg),\n    sd_weight = sd(weight_kg),\n    n = n()  # n() counts rows\n  )\n\n# A tibble: 1 √ó 3\n  mean_weight sd_weight     n\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1         540      63.7    12\n\n# Mean weight BY BREED\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  summarise(\n    mean_weight = mean(weight_kg),\n    sd_weight = sd(weight_kg),\n    n = n()\n  )\n\n# A tibble: 3 √ó 4\n  breed    mean_weight sd_weight     n\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Angus            555     12.9      4\n2 Holstein         605     12.9      4\n3 Jersey           460      9.13     4\n\n\n\n\n\n\n\n\nImportantThe group_by() + summarise() Pattern\n\n\n\nThis is one of the most powerful patterns in data analysis:\n\ndata %&gt;%\n  group_by(category_column) %&gt;%\n  summarise(\n    summary_name = summary_function(numeric_column)\n  )\n\nRead as: ‚ÄúFor each category, calculate the summary‚Äù\n\n\n\n\n4.6.3 Common Summary Functions\n\n\n\nFunction\nWhat it does\nExample\n\n\n\n\nmean()\nAverage\nmean(weight)\n\n\nmedian()\nMedian (50th percentile)\nmedian(weight)\n\n\nsd()\nStandard deviation\nsd(weight)\n\n\nvar()\nVariance\nvar(weight)\n\n\nmin()\nMinimum value\nmin(weight)\n\n\nmax()\nMaximum value\nmax(weight)\n\n\nsum()\nSum of all values\nsum(milk_yield)\n\n\nn()\nCount of rows\nn()\n\n\nn_distinct()\nCount unique values\nn_distinct(animal_id)\n\n\nfirst()\nFirst value\nfirst(weight)\n\n\nlast()\nLast value\nlast(weight)\n\n\n\n\n\n4.6.4 Grouping by Multiple Variables\n\n# Mean weight by breed AND farm\nfarm_data %&gt;%\n  group_by(breed, farm) %&gt;%\n  summarise(\n    mean_weight = mean(weight_kg),\n    n = n(),\n    .groups = \"drop\"  # Remove grouping after summarise\n  )\n\n# A tibble: 6 √ó 4\n  breed    farm  mean_weight     n\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;\n1 Angus    North        545      2\n2 Angus    South        565      2\n3 Holstein North        595      2\n4 Holstein South        615      2\n5 Jersey   North        452.     2\n6 Jersey   South        468.     2\n\n\n\n\n\n\n\n\nNoteThe .groups Argument\n\n\n\nAfter summarise(), data remains grouped by all but the last grouping variable. This can cause unexpected behavior!\n\n# Recommended: explicitly drop groups\ndata %&gt;%\n  group_by(var1, var2) %&gt;%\n  summarise(mean_x = mean(x), .groups = \"drop\")\n\nOptions: - .groups = \"drop\": Remove all grouping (recommended) - .groups = \"keep\": Keep all grouping - .groups = \"drop_last\": Drop last grouping variable (default)\n\n\n\n\n4.6.5 Multiple Summaries at Once\n\n# Comprehensive summary by breed\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = mean(weight_kg),\n    sd_weight = sd(weight_kg),\n    min_weight = min(weight_kg),\n    max_weight = max(weight_kg),\n    median_weight = median(weight_kg),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 √ó 7\n  breed        n mean_weight sd_weight min_weight max_weight median_weight\n  &lt;chr&gt;    &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt;\n1 Angus        4         555     12.9         540        570           555\n2 Holstein     4         605     12.9         590        620           605\n3 Jersey       4         460      9.13        450        470           460\n\n\n\n\n4.6.6 Filtering on Grouped Data\nfilter() works with groups too:\n\n# Keep only animals heavier than the breed average\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  filter(weight_kg &gt; mean(weight_kg)) %&gt;%\n  ungroup()  # Remove grouping when done\n\n# A tibble: 6 √ó 4\n  animal_id breed    farm  weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n1 A002      Holstein South       620\n2 A004      Holstein South       610\n3 A006      Jersey   South       470\n4 A008      Jersey   South       465\n5 A010      Angus    South       570\n6 A012      Angus    South       560\n\n\n\n\n4.6.7 Mutating on Grouped Data\nmutate() calculates within groups:\n\n# Calculate deviation from breed mean\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  mutate(\n    breed_mean = mean(weight_kg),\n    deviation = weight_kg - breed_mean\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(breed, animal_id)\n\n# A tibble: 12 √ó 6\n   animal_id breed    farm  weight_kg breed_mean deviation\n   &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 A009      Angus    North       550        555        -5\n 2 A010      Angus    South       570        555        15\n 3 A011      Angus    North       540        555       -15\n 4 A012      Angus    South       560        555         5\n 5 A001      Holstein North       600        605        -5\n 6 A002      Holstein South       620        605        15\n 7 A003      Holstein North       590        605       -15\n 8 A004      Holstein South       610        605         5\n 9 A005      Jersey   North       450        460       -10\n10 A006      Jersey   South       470        460        10\n11 A007      Jersey   North       455        460        -5\n12 A008      Jersey   South       465        460         5",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#helper-functions",
    "href": "chapters/ch04-dplyr.html#helper-functions",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.7 Helper Functions",
    "text": "4.7 Helper Functions\n\n4.7.1 Counting with count()\ncount() is a shortcut for group_by() + summarise() + n():\n\n# How many animals per breed?\n# Long way\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  summarise(n = n(), .groups = \"drop\")\n\n# A tibble: 3 √ó 2\n  breed        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Angus        4\n2 Holstein     4\n3 Jersey       4\n\n# Short way with count()\nfarm_data %&gt;%\n  count(breed)\n\n# A tibble: 3 √ó 2\n  breed        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Angus        4\n2 Holstein     4\n3 Jersey       4\n\n# Count by multiple variables\nfarm_data %&gt;%\n  count(breed, farm)\n\n# A tibble: 6 √ó 3\n  breed    farm      n\n  &lt;chr&gt;    &lt;chr&gt; &lt;int&gt;\n1 Angus    North     2\n2 Angus    South     2\n3 Holstein North     2\n4 Holstein South     2\n5 Jersey   North     2\n6 Jersey   South     2\n\n# Sort by count (most common first)\nfarm_data %&gt;%\n  count(breed, sort = TRUE)\n\n# A tibble: 3 √ó 2\n  breed        n\n  &lt;chr&gt;    &lt;int&gt;\n1 Angus        4\n2 Holstein     4\n3 Jersey       4\n\n\n\n\n\n\n\n\nTipcount() Pro Tips\n\n\n\n\n# Rename the count column\nfarm_data %&gt;%\n  count(breed, name = \"number_of_animals\")\n\n# A tibble: 3 √ó 2\n  breed    number_of_animals\n  &lt;chr&gt;                &lt;int&gt;\n1 Angus                    4\n2 Holstein                 4\n3 Jersey                   4\n\n# Add a total row (using add_tally)\nfarm_data %&gt;%\n  count(breed) %&gt;%\n  mutate(proportion = n / sum(n))\n\n# A tibble: 3 √ó 3\n  breed        n proportion\n  &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 Angus        4      0.333\n2 Holstein     4      0.333\n3 Jersey       4      0.333\n\n\n\n\n\n\n4.7.2 Renaming Columns with rename()\nrename() changes column names:\nSyntax: rename(new_name = old_name)\n\n# Rename columns\ncattle %&gt;%\n  rename(\n    id = animal_id,\n    weight = weight_kg,\n    height = height_cm\n  )\n\n# A tibble: 5 √ó 5\n  id    breed    weight height age_months\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1 H001  Holstein    600    145         30\n2 H002  Holstein    625    148         36\n3 H003  Holstein    580    142         28\n4 J001  Jersey      450    130         30\n5 J002  Jersey      470    133         32\n\n# Rename with a function\ncattle %&gt;%\n  rename_with(str_to_upper)  # All columns to uppercase\n\n# A tibble: 5 √ó 5\n  ANIMAL_ID BREED    WEIGHT_KG HEIGHT_CM AGE_MONTHS\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600       145         30\n2 H002      Holstein       625       148         36\n3 H003      Holstein       580       142         28\n4 J001      Jersey         450       130         30\n5 J002      Jersey         470       133         32\n\n# Rename specific columns with a function\ncattle %&gt;%\n  rename_with(str_to_upper, starts_with(\"weight\"))\n\n# A tibble: 5 √ó 5\n  animal_id breed    WEIGHT_KG height_cm age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600       145         30\n2 H002      Holstein       625       148         36\n3 H003      Holstein       580       142         28\n4 J001      Jersey         450       130         30\n5 J002      Jersey         470       133         32\n\n\n\n\n4.7.3 Reordering Columns with relocate()\nrelocate() moves columns to different positions:\n\n# Move breed to the front\ncattle %&gt;%\n  relocate(breed)\n\n# A tibble: 5 √ó 5\n  breed    animal_id weight_kg height_cm age_months\n  &lt;chr&gt;    &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Holstein H001            600       145         30\n2 Holstein H002            625       148         36\n3 Holstein H003            580       142         28\n4 Jersey   J001            450       130         30\n5 Jersey   J002            470       133         32\n\n# Move weight_kg to the end\ncattle %&gt;%\n  relocate(weight_kg, .after = last_col())\n\n# A tibble: 5 √ó 5\n  animal_id breed    height_cm age_months weight_kg\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n1 H001      Holstein       145         30       600\n2 H002      Holstein       148         36       625\n3 H003      Holstein       142         28       580\n4 J001      Jersey         130         30       450\n5 J002      Jersey         133         32       470\n\n# Move height_cm before breed\ncattle %&gt;%\n  relocate(height_cm, .before = breed)\n\n# A tibble: 5 √ó 5\n  animal_id height_cm breed    weight_kg age_months\n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001            145 Holstein       600         30\n2 H002            148 Holstein       625         36\n3 H003            142 Holstein       580         28\n4 J001            130 Jersey         450         30\n5 J002            133 Jersey         470         32\n\n# Move all numeric columns to the front\ncattle %&gt;%\n  relocate(where(is.numeric))\n\n# A tibble: 5 √ó 5\n  weight_kg height_cm age_months animal_id breed   \n      &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;   \n1       600       145         30 H001      Holstein\n2       625       148         36 H002      Holstein\n3       580       142         28 H003      Holstein\n4       450       130         30 J001      Jersey  \n5       470       133         32 J002      Jersey",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#working-across-multiple-columns-with-across",
    "href": "chapters/ch04-dplyr.html#working-across-multiple-columns-with-across",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.8 Working Across Multiple Columns with across()",
    "text": "4.8 Working Across Multiple Columns with across()\nacross() applies the same operation to multiple columns at once.\n\n4.8.1 Basic Usage\n\n# Create test data\ntest_data &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  weight_kg = c(600.123, 450.456, 550.789),\n  height_cm = c(145.678, 130.234, 140.567),\n  age_months = c(30.5, 28.3, 32.1)\n)\n\ntest_data\n\n# A tibble: 3 √ó 4\n  animal_id weight_kg height_cm age_months\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001           600.      146.       30.5\n2 A002           450.      130.       28.3\n3 A003           551.      141.       32.1\n\n# Round all numeric columns to 1 decimal place\ntest_data %&gt;%\n  mutate(across(where(is.numeric), round, 1))\n\n# A tibble: 3 √ó 4\n  animal_id weight_kg height_cm age_months\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001           600.      146.       30.5\n2 A002           450.      130.       28.3\n3 A003           551.      141.       32.1\n\n# Convert all character columns to uppercase\nfarm_data %&gt;%\n  mutate(across(where(is.character), str_to_upper)) %&gt;%\n  head(3)\n\n# A tibble: 3 √ó 4\n  animal_id breed    farm  weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;     &lt;dbl&gt;\n1 A001      HOLSTEIN NORTH       600\n2 A002      HOLSTEIN SOUTH       620\n3 A003      HOLSTEIN NORTH       590\n\n\n\n\n4.8.2 Selecting Columns for across()\n\n# Apply to specific columns\ntest_data %&gt;%\n  mutate(across(c(weight_kg, height_cm), round, 1))\n\n# A tibble: 3 √ó 4\n  animal_id weight_kg height_cm age_months\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001           600.      146.       30.5\n2 A002           450.      130.       28.3\n3 A003           551.      141.       32.1\n\n# Apply to columns matching a pattern\ntest_data %&gt;%\n  mutate(across(ends_with(\"_kg\"), ~ . * 2.20462))  # Convert kg to lb\n\n# A tibble: 3 √ó 4\n  animal_id weight_kg height_cm age_months\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001          1323.      146.       30.5\n2 A002           993.      130.       28.3\n3 A003          1214.      141.       32.1\n\n# Apply to all except some columns\ntest_data %&gt;%\n  mutate(across(-animal_id, round, 0))\n\n# A tibble: 3 √ó 4\n  animal_id weight_kg height_cm age_months\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 A001            600       146         30\n2 A002            450       130         28\n3 A003            551       141         32\n\n\n\n\n\n\n\n\nNoteAnonymous Functions with across()\n\n\n\nThe ~ creates an anonymous function, and . represents each column:\n\n# These are equivalent:\nacross(cols, ~ round(., 1))\nacross(cols, round, 1)\nacross(cols, function(x) round(x, 1))\n\n# Use ~ when you need more complex operations:\nacross(cols, ~ . * 2 + 1)\nacross(cols, ~ if_else(. &gt; 100, ., . * 2))\n\n\n\n\n\n4.8.3 Multiple Summaries with across()\n\n# Calculate multiple summaries for multiple columns\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  summarise(\n    across(\n      weight_kg,\n      list(\n        mean = mean,\n        sd = sd,\n        min = min,\n        max = max\n      )\n    ),\n    n = n(),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 √ó 6\n  breed    weight_kg_mean weight_kg_sd weight_kg_min weight_kg_max     n\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Angus               555        12.9            540           570     4\n2 Holstein            605        12.9            590           620     4\n3 Jersey              460         9.13           450           470     4\n\n# Cleaner column names\nfarm_data %&gt;%\n  group_by(breed) %&gt;%\n  summarise(\n    across(\n      weight_kg,\n      list(\n        mean = mean,\n        sd = sd\n      ),\n      .names = \"{.col}_{.fn}\"  # Creates: weight_kg_mean, weight_kg_sd\n    ),\n    .groups = \"drop\"\n  )\n\n# A tibble: 3 √ó 3\n  breed    weight_kg_mean weight_kg_sd\n  &lt;chr&gt;             &lt;dbl&gt;        &lt;dbl&gt;\n1 Angus               555        12.9 \n2 Holstein            605        12.9 \n3 Jersey              460         9.13",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#handling-missing-data",
    "href": "chapters/ch04-dplyr.html#handling-missing-data",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.9 Handling Missing Data",
    "text": "4.9 Handling Missing Data\nMissing data (NA) is common in real datasets. dplyr provides several tools to handle it.\n\n4.9.1 Detecting Missing Values\n\n# Create data with missing values\ncattle_missing &lt;- tibble(\n  animal_id = c(\"H001\", \"H002\", \"H003\", \"H004\", \"H005\"),\n  breed = c(\"Holstein\", \"Jersey\", NA, \"Angus\", \"Holstein\"),\n  weight_kg = c(600, NA, 580, 625, NA),\n  age_months = c(30, 28, NA, 32, 26)\n)\n\ncattle_missing\n\n# A tibble: 5 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600         30\n2 H002      Jersey          NA         28\n3 H003      &lt;NA&gt;           580         NA\n4 H004      Angus          625         32\n5 H005      Holstein        NA         26\n\n# Check for missing values\ncattle_missing %&gt;%\n  mutate(\n    breed_is_missing = is.na(breed),\n    weight_is_missing = is.na(weight_kg)\n  )\n\n# A tibble: 5 √ó 6\n  animal_id breed    weight_kg age_months breed_is_missing weight_is_missing\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;            &lt;lgl&gt;            \n1 H001      Holstein       600         30 FALSE            FALSE            \n2 H002      Jersey          NA         28 FALSE            TRUE             \n3 H003      &lt;NA&gt;           580         NA TRUE             FALSE            \n4 H004      Angus          625         32 FALSE            FALSE            \n5 H005      Holstein        NA         26 FALSE            TRUE             \n\n# Count missing values per column\ncattle_missing %&gt;%\n  summarise(\n    across(everything(), ~ sum(is.na(.)))\n  )\n\n# A tibble: 1 √ó 4\n  animal_id breed weight_kg age_months\n      &lt;int&gt; &lt;int&gt;     &lt;int&gt;      &lt;int&gt;\n1         0     1         2          1\n\n\n\n\n4.9.2 Removing Missing Values with drop_na()\n\n# Remove rows with ANY missing values\ncattle_missing %&gt;%\n  drop_na()\n\n# A tibble: 2 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600         30\n2 H004      Angus          625         32\n\n# Remove rows with missing values in specific columns\ncattle_missing %&gt;%\n  drop_na(weight_kg)\n\n# A tibble: 3 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600         30\n2 H003      &lt;NA&gt;           580         NA\n3 H004      Angus          625         32\n\n# Remove rows missing BOTH weight and age\ncattle_missing %&gt;%\n  drop_na(weight_kg, age_months)\n\n# A tibble: 2 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600         30\n2 H004      Angus          625         32\n\n\n\n\n\n\n\n\nWarningBe Careful When Dropping NAs!\n\n\n\nRemoving missing data can: - Reduce sample size significantly - Introduce bias if missingness isn‚Äôt random - Lose valuable information in other columns\nBetter approach: Understand WHY data is missing, then decide how to handle it.\n\n\n\n\n4.9.3 Replacing Missing Values with replace_na()\n\n# Replace NAs with specific values\ncattle_missing %&gt;%\n  mutate(\n    breed = replace_na(breed, \"Unknown\"),\n    weight_kg = replace_na(weight_kg, 0)\n  )\n\n# A tibble: 5 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein       600         30\n2 H002      Jersey           0         28\n3 H003      Unknown        580         NA\n4 H004      Angus          625         32\n5 H005      Holstein         0         26\n\n# Replace NAs in multiple columns\ncattle_missing %&gt;%\n  mutate(\n    across(\n      where(is.numeric),\n      ~ replace_na(., mean(., na.rm = TRUE))  # Replace with column mean\n    )\n  )\n\n# A tibble: 5 √ó 4\n  animal_id breed    weight_kg age_months\n  &lt;chr&gt;     &lt;chr&gt;        &lt;dbl&gt;      &lt;dbl&gt;\n1 H001      Holstein      600          30\n2 H002      Jersey        602.         28\n3 H003      &lt;NA&gt;          580          29\n4 H004      Angus         625          32\n5 H005      Holstein      602.         26\n\n\n\n\n4.9.4 Filling Missing Values with coalesce()\ncoalesce() returns the first non-missing value:\n\n# Multiple weight measurements, use first available\nweights &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\", \"A004\"),\n  weight_measurement1 = c(600, NA, 580, NA),\n  weight_measurement2 = c(605, 450, NA, 625),\n  weight_measurement3 = c(NA, 455, 585, 630)\n)\n\nweights\n\n# A tibble: 4 √ó 4\n  animal_id weight_measurement1 weight_measurement2 weight_measurement3\n  &lt;chr&gt;                   &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n1 A001                      600                 605                  NA\n2 A002                       NA                 450                 455\n3 A003                      580                  NA                 585\n4 A004                       NA                 625                 630\n\n# Use first non-missing weight\nweights %&gt;%\n  mutate(\n    weight_final = coalesce(weight_measurement1,\n                           weight_measurement2,\n                           weight_measurement3)\n  )\n\n# A tibble: 4 √ó 5\n  animal_id weight_measurement1 weight_measurement2 weight_measurement3\n  &lt;chr&gt;                   &lt;dbl&gt;               &lt;dbl&gt;               &lt;dbl&gt;\n1 A001                      600                 605                  NA\n2 A002                       NA                 450                 455\n3 A003                      580                  NA                 585\n4 A004                       NA                 625                 630\n# ‚Ñπ 1 more variable: weight_final &lt;dbl&gt;\n\n\n\n\n4.9.5 Handling NAs in Calculations\nMost R functions return NA if any input is NA:\n\n# Mean returns NA if any value is NA\nmean(c(1, 2, NA, 4))\n\n[1] NA\n\n# Use na.rm = TRUE to remove NAs before calculating\nmean(c(1, 2, NA, 4), na.rm = TRUE)\n\n[1] 2.333333\n\n# In a summarise\ncattle_missing %&gt;%\n  summarise(\n    mean_weight_with_na = mean(weight_kg),           # Returns NA\n    mean_weight_removed = mean(weight_kg, na.rm = TRUE)  # Calculates mean\n  )\n\n# A tibble: 1 √ó 2\n  mean_weight_with_na mean_weight_removed\n                &lt;dbl&gt;               &lt;dbl&gt;\n1                  NA                602.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#window-functions",
    "href": "chapters/ch04-dplyr.html#window-functions",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.10 Window Functions",
    "text": "4.10 Window Functions\nWindow functions operate on groups of rows and return a value for each row (unlike summarise() which returns one value per group).\n\n4.10.1 Ranking Functions\n\n# Create competition data\ncompetition &lt;- tibble(\n  animal_id = sprintf(\"A%03d\", 1:8),\n  breed = rep(c(\"Holstein\", \"Jersey\"), each = 4),\n  score = c(92, 88, 88, 85, 78, 76, 76, 74)\n)\n\ncompetition\n\n# A tibble: 8 √ó 3\n  animal_id breed    score\n  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n1 A001      Holstein    92\n2 A002      Holstein    88\n3 A003      Holstein    88\n4 A004      Holstein    85\n5 A005      Jersey      78\n6 A006      Jersey      76\n7 A007      Jersey      76\n8 A008      Jersey      74\n\n# Rank animals by score\ncompetition %&gt;%\n  mutate(\n    rank = min_rank(desc(score)),        # Rank (ties get same rank, gaps after)\n    dense = dense_rank(desc(score)),     # Dense rank (ties, no gaps)\n    row_num = row_number(desc(score)),   # Row number (no ties, arbitrary order)\n    percentile = percent_rank(desc(score))  # Percentile (0 to 1)\n  )\n\n# A tibble: 8 √ó 7\n  animal_id breed    score  rank dense row_num percentile\n  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt;   &lt;int&gt;      &lt;dbl&gt;\n1 A001      Holstein    92     1     1       1      0    \n2 A002      Holstein    88     2     2       2      0.143\n3 A003      Holstein    88     2     2       3      0.143\n4 A004      Holstein    85     4     3       4      0.429\n5 A005      Jersey      78     5     4       5      0.571\n6 A006      Jersey      76     6     5       6      0.714\n7 A007      Jersey      76     6     5       7      0.714\n8 A008      Jersey      74     8     6       8      1    \n\n# Rank within breed\ncompetition %&gt;%\n  group_by(breed) %&gt;%\n  mutate(\n    breed_rank = min_rank(desc(score))\n  ) %&gt;%\n  ungroup() %&gt;%\n  arrange(breed, breed_rank)\n\n# A tibble: 8 √ó 4\n  animal_id breed    score breed_rank\n  &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;      &lt;int&gt;\n1 A001      Holstein    92          1\n2 A002      Holstein    88          2\n3 A003      Holstein    88          2\n4 A004      Holstein    85          4\n5 A005      Jersey      78          1\n6 A006      Jersey      76          2\n7 A007      Jersey      76          2\n8 A008      Jersey      74          4\n\n\n\n\n4.10.2 Offset Functions: lag() and lead()\nlag() and lead() access previous or next values:\n\n# Weight measurements over time\ngrowth &lt;- tibble(\n  animal_id = rep(\"A001\", 5),\n  week = 1:5,\n  weight_kg = c(450, 465, 478, 492, 505)\n)\n\ngrowth\n\n# A tibble: 5 √ó 3\n  animal_id  week weight_kg\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 A001          1       450\n2 A001          2       465\n3 A001          3       478\n4 A001          4       492\n5 A001          5       505\n\n# Calculate weight change from previous week\ngrowth %&gt;%\n  mutate(\n    previous_weight = lag(weight_kg),           # Previous row\n    weight_gain = weight_kg - lag(weight_kg),  # Change from previous\n    next_weight = lead(weight_kg)              # Next row\n  )\n\n# A tibble: 5 √ó 6\n  animal_id  week weight_kg previous_weight weight_gain next_weight\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 A001          1       450              NA          NA         465\n2 A001          2       465             450          15         478\n3 A001          3       478             465          13         492\n4 A001          4       492             478          14         505\n5 A001          5       505             492          13          NA\n\n# Lag by multiple rows\ngrowth %&gt;%\n  mutate(\n    two_weeks_ago = lag(weight_kg, n = 2)\n  )\n\n# A tibble: 5 √ó 4\n  animal_id  week weight_kg two_weeks_ago\n  &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n1 A001          1       450            NA\n2 A001          2       465            NA\n3 A001          3       478           450\n4 A001          4       492           465\n5 A001          5       505           478\n\n\n\n\n4.10.3 Cumulative Functions\n\n# Running totals\nmilk_production &lt;- tibble(\n  day = 1:7,\n  milk_liters = c(25, 28, 26, 29, 27, 30, 28)\n)\n\nmilk_production\n\n# A tibble: 7 √ó 2\n    day milk_liters\n  &lt;int&gt;       &lt;dbl&gt;\n1     1          25\n2     2          28\n3     3          26\n4     4          29\n5     5          27\n6     6          30\n7     7          28\n\n# Cumulative statistics\nmilk_production %&gt;%\n  mutate(\n    cumulative_milk = cumsum(milk_liters),      # Running total\n    running_mean = cummean(milk_liters),        # Running mean\n    running_min = cummin(milk_liters),          # Running minimum\n    running_max = cummax(milk_liters)           # Running maximum\n  )\n\n# A tibble: 7 √ó 6\n    day milk_liters cumulative_milk running_mean running_min running_max\n  &lt;int&gt;       &lt;dbl&gt;           &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1     1          25              25         25            25          25\n2     2          28              53         26.5          25          28\n3     3          26              79         26.3          25          28\n4     4          29             108         27            25          29\n5     5          27             135         27            25          29\n6     6          30             165         27.5          25          30\n7     7          28             193         27.6          25          30\n\n\n\n\n4.10.4 Real-World Example: Growth Rates\n\n# Multiple animals, multiple measurements\ngrowth_data &lt;- tibble(\n  animal_id = rep(c(\"A001\", \"A002\", \"A003\"), each = 4),\n  week = rep(c(0, 4, 8, 12), 3),\n  weight_kg = c(\n    # A001\n    450, 485, 518, 548,\n    # A002\n    445, 475, 502, 530,\n    # A003\n    455, 492, 525, 556\n  )\n)\n\ngrowth_data\n\n# A tibble: 12 √ó 3\n   animal_id  week weight_kg\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 A001          0       450\n 2 A001          4       485\n 3 A001          8       518\n 4 A001         12       548\n 5 A002          0       445\n 6 A002          4       475\n 7 A002          8       502\n 8 A002         12       530\n 9 A003          0       455\n10 A003          4       492\n11 A003          8       525\n12 A003         12       556\n\n# Calculate growth metrics for each animal\ngrowth_summary &lt;- growth_data %&gt;%\n  group_by(animal_id) %&gt;%\n  mutate(\n    previous_weight = lag(weight_kg),\n    weight_gain = weight_kg - lag(weight_kg),\n    weeks_elapsed = week - lag(week),\n    daily_gain = weight_gain / (weeks_elapsed * 7),\n    total_gain = weight_kg - first(weight_kg),\n    cumulative_gain = cumsum(replace_na(weight_gain, 0))\n  ) %&gt;%\n  ungroup()\n\ngrowth_summary\n\n# A tibble: 12 √ó 9\n   animal_id  week weight_kg previous_weight weight_gain weeks_elapsed\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;           &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1 A001          0       450              NA          NA            NA\n 2 A001          4       485             450          35             4\n 3 A001          8       518             485          33             4\n 4 A001         12       548             518          30             4\n 5 A002          0       445              NA          NA            NA\n 6 A002          4       475             445          30             4\n 7 A002          8       502             475          27             4\n 8 A002         12       530             502          28             4\n 9 A003          0       455              NA          NA            NA\n10 A003          4       492             455          37             4\n11 A003          8       525             492          33             4\n12 A003         12       556             525          31             4\n# ‚Ñπ 3 more variables: daily_gain &lt;dbl&gt;, total_gain &lt;dbl&gt;, cumulative_gain &lt;dbl&gt;",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#putting-it-all-together-complex-pipelines",
    "href": "chapters/ch04-dplyr.html#putting-it-all-together-complex-pipelines",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.11 Putting It All Together: Complex Pipelines",
    "text": "4.11 Putting It All Together: Complex Pipelines\nReal data analysis often requires chaining many operations. Here‚Äôs a comprehensive example:\n\n# Create realistic farm data\nset.seed(123)\nfarm_complete &lt;- tibble(\n  animal_id = sprintf(\"F%04d\", 1:100),\n  farm = sample(c(\"North\", \"South\", \"East\", \"West\"), 100, replace = TRUE),\n  breed = sample(c(\"Holstein\", \"Jersey\", \"Angus\", \"Hereford\"), 100,\n                 replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)),\n  sex = sample(c(\"M\", \"F\"), 100, replace = TRUE),\n  birth_date = as.Date(\"2022-01-01\") + sample(0:365, 100, replace = TRUE),\n  weight_kg = rnorm(100, mean = 500, sd = 80),\n  health_status = sample(c(\"Good\", \"Fair\", \"Poor\", NA), 100,\n                        replace = TRUE, prob = c(0.7, 0.2, 0.05, 0.05))\n)\n\n# Preview\nhead(farm_complete, 3)\n\n# A tibble: 3 √ó 7\n  animal_id farm  breed    sex   birth_date weight_kg health_status\n  &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt; &lt;date&gt;         &lt;dbl&gt; &lt;chr&gt;        \n1 F0001     East  Jersey   F     2022-08-06      397. Fair         \n2 F0002     East  Holstein F     2022-04-16      454. Good         \n3 F0003     East  Jersey   F     2022-07-05      549. Good         \n\n# Complex analysis pipeline\nanalysis_result &lt;- farm_complete %&gt;%\n  # Data cleaning\n  drop_na(health_status) %&gt;%                    # Remove missing health status\n  filter(health_status != \"Poor\") %&gt;%           # Exclude poor health\n  mutate(\n    # Calculate age\n    age_days = as.numeric(Sys.Date() - birth_date),\n    age_months = age_days / 30.44,\n    # Standardize breed names\n    breed = str_to_title(breed),\n    # Weight categories\n    weight_class = case_when(\n      weight_kg &lt; 450 ~ \"Light\",\n      weight_kg &lt; 550 ~ \"Medium\",\n      TRUE ~ \"Heavy\"\n    ),\n    # Round weight\n    weight_kg = round(weight_kg, 1)\n  ) %&gt;%\n  # Filter to mature animals only\n  filter(age_months &gt;= 10) %&gt;%\n  # Group analysis\n  group_by(farm, breed) %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = round(mean(weight_kg), 1),\n    sd_weight = round(sd(weight_kg), 1),\n    min_weight = min(weight_kg),\n    max_weight = max(weight_kg),\n    prop_heavy = mean(weight_class == \"Heavy\"),\n    .groups = \"drop\"\n  ) %&gt;%\n  # Filter to groups with at least 3 animals\n  filter(n &gt;= 3) %&gt;%\n  # Sort by mean weight\n  arrange(desc(mean_weight)) %&gt;%\n  # Add ranking\n  mutate(rank = row_number())\n\nanalysis_result\n\n# A tibble: 13 √ó 9\n   farm  breed        n mean_weight sd_weight min_weight max_weight prop_heavy\n   &lt;chr&gt; &lt;chr&gt;    &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 North Hereford     4        549.      47.1       485.       599.     0.75  \n 2 North Jersey       6        538.      64.3       438.       628.     0.5   \n 3 South Holstein     8        538.      89.1       443.       663      0.375 \n 4 North Holstein    13        532.      72.7       409        683.     0.385 \n 5 East  Angus        6        529.      70.5       440.       604      0.5   \n 6 East  Jersey      10        500       99.9       339.       649.     0.3   \n 7 West  Holstein     6        492.     105.        367.       652.     0.333 \n 8 South Jersey      12        482.      71.9       362.       660.     0.0833\n 9 West  Jersey       5        481.      60.4       395.       559.     0.2   \n10 West  Hereford     3        463.      50         419.       517      0     \n11 South Angus        3        461.      17.1       441.       471.     0     \n12 West  Angus        3        457.      59.1       392.       507.     0     \n13 East  Holstein     9        452.      77.7       360.       577      0.111 \n# ‚Ñπ 1 more variable: rank &lt;int&gt;\n\n\n\n\n\n\n\n\nTipComplex Pipeline Best Practices\n\n\n\n\nComment your steps: Explain what each section does\nOne operation per line: Easier to read and debug\nUse intermediate results: Break very long pipes into steps\nCheck intermediate output: Run the pipe up to a certain point to verify\nConsistent indentation: 2 spaces after pipe\n\n\n# Good structure\nresult &lt;- data %&gt;%\n  # Step 1: Clean data\n  filter(!is.na(important_var)) %&gt;%\n  mutate(clean_var = str_trim(var)) %&gt;%\n  # Step 2: Calculate new variables\n  mutate(\n    var1 = calculation1,\n    var2 = calculation2\n  ) %&gt;%\n  # Step 3: Summarize by group\n  group_by(category) %&gt;%\n  summarise(\n    mean_value = mean(value),\n    .groups = \"drop\"\n  )",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#summary",
    "href": "chapters/ch04-dplyr.html#summary",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.12 Summary",
    "text": "4.12 Summary\nThis chapter covered powerful dplyr verbs for data manipulation:\n\nmutate() creates and modifies columns, works with functions and calculations\nif_else() applies simple conditional logic (if/then/else)\ncase_when() handles multiple conditions elegantly (replaces nested if_else)\narrange() sorts data by one or more columns in ascending or descending order\nsummarise() calculates summary statistics (mean, sd, min, max, count, etc.)\ngroup_by() + summarise() performs grouped operations (summaries by category)\ncount() quickly counts observations per group\nrename() changes column names; relocate() reorders columns\nacross() applies functions to multiple columns efficiently\nMissing data can be detected (is.na()), removed (drop_na()), or replaced (replace_na(), coalesce())\nWindow functions (lag(), lead(), cumsum(), ranking) operate within groups\nComplex pipelines chain many operations together for complete data transformations\n\nKey Principle: Start simple, build complexity gradually, and always check your work!\nNext chapter: Data visualization with ggplot2!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#homework-assignment",
    "href": "chapters/ch04-dplyr.html#homework-assignment",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.13 Homework Assignment",
    "text": "4.13 Homework Assignment\n\n4.13.1 Assignment: Data Transformation and Analysis\nDue: Before Week 5\n\n4.13.1.1 Part 1: Creating Variables (25 points)\nYou will receive a dataset called pig_growth.csv with the following columns: - pig_id: Pig identifier - birth_date: Date of birth (YYYY-MM-DD) - breed: Pig breed - sex: M or F - weight_day0: Birth weight (kg) - weight_day28: Weight at 28 days (kg) - weight_day56: Weight at 56 days (kg) - feed_type: Type of feed given\nTasks:\n\nRead the data and examine its structure\nCreate new variables using mutate():\n\nage_days: Calculate age in days from birth_date to today\ngain_0_28: Weight gain from day 0 to day 28\ngain_28_56: Weight gain from day 28 to day 56\nadg_0_28: Average daily gain for first period (gain / 28)\nadg_28_56: Average daily gain for second period (gain / 28)\n\nCreate categorical variables:\n\nbirth_weight_class: ‚ÄúLow‚Äù (&lt;1.2 kg), ‚ÄúNormal‚Äù (1.2-1.6 kg), ‚ÄúHigh‚Äù (&gt;1.6 kg)\nsex_label: Convert ‚ÄúM‚Äù to ‚ÄúMale‚Äù, ‚ÄúF‚Äù to ‚ÄúFemale‚Äù\n\nRound all weight and gain variables to 2 decimal places\n\n\n\n4.13.1.2 Part 2: Conditional Logic (25 points)\nUsing your data with new variables:\n\nCreate a performance category using case_when():\n\n‚ÄúExcellent‚Äù: adg_28_56 &gt; 0.50 kg/day AND weight_day56 &gt; 18 kg\n‚ÄúGood‚Äù: adg_28_56 &gt; 0.45 kg/day\n‚ÄúFair‚Äù: adg_28_56 &gt; 0.40 kg/day\n‚ÄúPoor‚Äù: everything else\n\nCreate a treatment recommendation using conditional logic:\n\nIf performance is ‚ÄúPoor‚Äù AND sex is ‚ÄúM‚Äù: ‚ÄúSupplement + Monitor‚Äù\nIf performance is ‚ÄúPoor‚Äù AND sex is ‚ÄúF‚Äù: ‚ÄúSupplement‚Äù\nIf performance is ‚ÄúFair‚Äù: ‚ÄúMonitor‚Äù\nOtherwise: ‚ÄúContinue‚Äù\n\nIdentify concerning cases:\n\nCreate a logical variable needs_attention that is TRUE if:\n\nWeight gain in second period is LESS than first period, OR\nCurrent weight is below 12 kg\n\n\n\n\n\n4.13.1.3 Part 3: Grouped Summaries (30 points)\nCalculate comprehensive summary statistics:\n\nOverall summaries (no grouping):\n\nNumber of pigs\nMean birth weight, day 28 weight, day 56 weight\nMean ADG for both periods\nNumber and proportion needing attention\n\nSummaries by breed:\n\nCount of pigs per breed\nMean ADG (both periods) per breed\nSD of ADG per breed\nMin and max day 56 weight per breed\n\nSummaries by feed type AND sex:\n\nCount per combination\nMean day 56 weight\nProportion in each performance category\n\nSummaries by performance category:\n\nCount per category\nMean ADG in period 2\nWhat proportion of each category is male vs female?\n\n\nHint: Use group_by() + summarise() for each question. You may need across() for multiple columns.\n\n\n4.13.1.4 Part 4: Complex Pipeline (20 points)\nCreate ONE pipeline that:\n\nStarts with the raw data\nRemoves any pigs with missing weight measurements\nCalculates all new variables from Part 1\nCreates categories from Part 2\nFilters to pigs with birth_weight_class == ‚ÄúNormal‚Äù\nFilters to feed_type ‚ÄúA‚Äù or ‚ÄúB‚Äù only\nGroups by feed_type and sex\nCalculates mean day 56 weight and ADG for period 2\nArranges by mean day 56 weight (descending)\nAdds a rank column\n\nShow: - The complete pipeline (with comments explaining each step) - The final output - How many rows in the final result?\n\n\n\n4.13.2 Recommended YAML\n---\ntitle: \"Week 4 Homework: Data Manipulation with dplyr\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n4.13.3 Grading Rubric\n\nPart 1: Creating Variables (25%):\n\nAll variables created correctly (15%)\nProper use of mutate() (5%)\nRounding applied correctly (5%)\n\nPart 2: Conditional Logic (25%):\n\nPerformance category correct (10%)\nTreatment recommendation correct (8%)\nNeeds attention logic correct (7%)\n\nPart 3: Grouped Summaries (30%):\n\nOverall summaries (8%)\nBreed summaries (8%)\nFeed/sex summaries (7%)\nPerformance category summaries (7%)\n\nPart 4: Complex Pipeline (20%):\n\nPipeline executes correctly (12%)\nAll steps included (5%)\nClear comments and documentation (3%)\n\n\n\n\n4.13.4 Bonus (10 points)\n\nWindow functions:\n\nFor each pig, calculate the difference between their day 56 weight and the breed average day 56 weight\nRank pigs within their breed by day 56 weight\n\nAdvanced grouping:\n\nIdentify the top 3 breeds by mean ADG in period 2\nFor just those breeds, create a detailed summary with all statistics",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch04-dplyr.html#additional-resources",
    "href": "chapters/ch04-dplyr.html#additional-resources",
    "title": "4¬† Data Manipulation with dplyr",
    "section": "4.14 Additional Resources",
    "text": "4.14 Additional Resources\n\n4.14.1 Required Reading\n\nR for Data Science (2e) - Chapters 3-5: Data transformation\ndplyr documentation\ndplyr vignettes - Introduction to dplyr\n\n\n\n4.14.2 Optional Reading\n\nWickham, H., et al.¬†(2019). ‚ÄúWelcome to the Tidyverse.‚Äù Journal of Open Source Software, 4(43), 1686. Link\nProgramming with dplyr - Advanced techniques\nRow-wise operations - Working row-by-row\n\n\n\n4.14.3 Videos\n\n‚ÄúData Manipulation in R‚Äù by StatQuest with Josh Starmer\n‚Äúdplyr Tutorial‚Äù by RStudio / Posit\n‚ÄúTidy Data and tidyr‚Äù by RStudio / Posit\n‚Äúgrouped operations in dplyr‚Äù by Data Science Dojo\n\n\n\n4.14.4 Cheat Sheets\n\ndplyr Cheat Sheet\ntidyr Cheat Sheet\n\n\n\n4.14.5 Interactive Learning\n\ndplyr exercises on R for Data Science\nRStudio Primers: Work with Data\n\n\n\n4.14.6 Useful Websites\n\nTidyverse - Main tidyverse homepage\nStack Overflow: dplyr - Q&A community\nRStudio Community - Friendly help forum\ndplyr GitHub - Source code and issues\n\n\nNext Chapter: Introduction to Data Visualization with ggplot2",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Data Manipulation with dplyr</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html",
    "href": "chapters/ch05-ggplot2_intro.html",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "",
    "text": "5.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#learning-objectives",
    "href": "chapters/ch05-ggplot2_intro.html#learning-objectives",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "",
    "text": "Understand the Grammar of Graphics philosophy and how ggplot2 implements it\nCreate scatter plots, line plots, bar charts, histograms, box plots, and violin plots\nMap variables to aesthetic properties (color, size, shape, alpha)\nApply and customize themes for publication-ready plots\nAdd labels, titles, and customize scales\nSave plots with ggsave() for different output formats\nChoose the appropriate plot type for your data and research question",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#introduction-to-the-grammar-of-graphics",
    "href": "chapters/ch05-ggplot2_intro.html#introduction-to-the-grammar-of-graphics",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.2 Introduction to the Grammar of Graphics",
    "text": "5.2 Introduction to the Grammar of Graphics\n\n5.2.1 What is ggplot2?\nggplot2 is R‚Äôs most powerful and popular data visualization package. Created by Hadley Wickham, it implements Leland Wilkinson‚Äôs Grammar of Graphics ‚Äî a systematic way to describe and build any visualization.\n\n\n\n\n\n\nNoteWhy ‚Äúgg‚Äùplot2?\n\n\n\n\ngg = Grammar of Graphics\n2 = Second iteration (there was a ggplot1, but it‚Äôs obsolete)\n\nThe package is part of the tidyverse and integrates seamlessly with dplyr, tidyr, and other tidyverse tools.\n\n\n\n\n5.2.2 The Grammar of Graphics Philosophy\nTraditional plotting systems (like base R graphics) force you to choose from predefined plot types. The Grammar of Graphics takes a different approach: every visualization is built by combining independent components.\n\n\n\n\n\nflowchart LR\n    A[Data] --&gt; B[Aesthetic Mappings]\n    B --&gt; C[Geometric Objects]\n    C --&gt; D[Statistical Transformations]\n    D --&gt; E[Scales]\n    E --&gt; F[Coordinate System]\n    F --&gt; G[Facets]\n    G --&gt; H[Theme]\n    H --&gt; I[Final Plot]\n\n\n\n\n\n\n\n\n5.2.3 The Three Essential Components\nEvery ggplot2 plot requires three components:\n\nData: A data frame or tibble\nAesthetic mappings (aes()): How variables map to visual properties (x, y, color, size, etc.)\nGeometric objects (geom_*()): The type of plot (points, lines, bars, etc.)\n\nBasic syntax:\nggplot(data = &lt;DATA&gt;) +\n  aes(x = &lt;X_VAR&gt;, y = &lt;Y_VAR&gt;) +\n  geom_&lt;TYPE&gt;()\nOr more commonly:\nggplot(&lt;DATA&gt;, aes(x = &lt;X_VAR&gt;, y = &lt;Y_VAR&gt;)) +\n  geom_&lt;TYPE&gt;()\n\n\n\n\n\n\nImportantggplot2 Uses +, Not %&gt;%\n\n\n\nNotice we use + to add layers to a ggplot, NOT the pipe (%&gt;% or |&gt;). This is a common mistake!\n# CORRECT\nggplot(data, aes(x = var1, y = var2)) +\n  geom_point()\n\n# WRONG - don't use pipe!\nggplot(data, aes(x = var1, y = var2)) %&gt;%\n  geom_point()\n\n\n\n\n5.2.4 Setup\nLet‚Äôs load the necessary packages:\n\nlibrary(tidyverse)  # Includes ggplot2, dplyr, etc.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#your-first-ggplot-scatter-plots-with-geom_point",
    "href": "chapters/ch05-ggplot2_intro.html#your-first-ggplot-scatter-plots-with-geom_point",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.3 Your First ggplot: Scatter Plots with geom_point()",
    "text": "5.3 Your First ggplot: Scatter Plots with geom_point()\nScatter plots display the relationship between two continuous variables. Each observation is represented by a point.\n\n5.3.1 Basic Scatter Plot\nLet‚Äôs create example data on cattle feed intake and weight gain:\n\n# Create example cattle data\ncattle &lt;- tibble(\n  animal_id = paste0(\"C\", 1001:1030),\n  breed = rep(c(\"Holstein\", \"Jersey\", \"Angus\"), each = 10),\n  feed_intake_kg = c(\n    rnorm(10, mean = 25, sd = 3),  # Holstein\n    rnorm(10, mean = 18, sd = 2),  # Jersey\n    rnorm(10, mean = 22, sd = 2.5) # Angus\n  ),\n  weight_gain_kg = c(\n    rnorm(10, mean = 1.2, sd = 0.15),  # Holstein\n    rnorm(10, mean = 0.8, sd = 0.12),  # Jersey\n    rnorm(10, mean = 1.0, sd = 0.13)   # Angus\n  )\n)\n\n# View first few rows\nhead(cattle)\n\n# A tibble: 6 √ó 4\n  animal_id breed    feed_intake_kg weight_gain_kg\n  &lt;chr&gt;     &lt;chr&gt;             &lt;dbl&gt;          &lt;dbl&gt;\n1 C1001     Holstein           23.3          1.39 \n2 C1002     Holstein           24.0          1.17 \n3 C1003     Holstein           24.2          1.14 \n4 C1004     Holstein           19.4          0.991\n5 C1005     Holstein           26.0          0.976\n6 C1006     Holstein           23.5          1.16 \n\n# Create basic scatter plot\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg)) +\n  geom_point()\n\n\n\n\n\n\n\n\nReading this code:\n\nggplot(cattle, ...) ‚Äî Use the cattle data\naes(x = feed_intake_kg, y = weight_gain_kg) ‚Äî Map feed intake to x-axis, weight gain to y-axis\ngeom_point() ‚Äî Draw points\n\n\n\n5.3.2 Adding Color by Group\nWe can map the breed variable to color:\n\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point()\n\n\n\n\n\n\n\n\nNotice: - Each breed gets a different color automatically - A legend is created automatically - Colors are chosen from a default palette\n\n\n5.3.3 Customizing Point Aesthetics\nPoints have several aesthetic properties we can map to variables OR set manually:\n\n\n\nAesthetic\nWhat it controls\nExample variable type\n\n\n\n\nx\nHorizontal position\nContinuous\n\n\ny\nVertical position\nContinuous\n\n\ncolor\nPoint outline color\nCategorical or continuous\n\n\nsize\nPoint size\nContinuous\n\n\nshape\nPoint shape\nCategorical\n\n\nalpha\nTransparency (0-1)\nContinuous\n\n\n\n\n\n\n\n\n\nImportantMapping vs Setting\n\n\n\nMapping (inside aes()): Connects a variable to an aesthetic ‚Äî varies by data\naes(color = breed)  # Color depends on breed variable\nSetting (outside aes()): Sets a fixed value for all points\ngeom_point(color = \"blue\", size = 3)  # All points blue and size 3\n\n\n\n5.3.3.1 Example: Mapping Size\n\n# Map size to feed intake (redundant with x, but demonstrates the concept)\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg,\n                   color = breed, size = feed_intake_kg)) +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n5.3.3.2 Example: Setting Fixed Aesthetics\n\n# Make all points larger and semi-transparent\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4, alpha = 0.6)\n\n\n\n\n\n\n\n\n\n\n5.3.3.3 Example: Using Shape\n\n# Map shape to breed (in addition to color)\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg,\n                   color = breed, shape = breed)) +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n\n\n\n5.3.4 When to Use Scatter Plots\n‚úÖ Use scatter plots when: - You have two continuous variables - You want to explore relationships/correlations - You want to identify outliers or patterns - You want to show individual data points\n‚ùå Don‚Äôt use scatter plots when: - One or both variables are categorical (use bar charts or box plots) - You have too many overlapping points (consider hex bins or contour plots) - You want to show change over time for a single entity (use line plots)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#line-plots-with-geom_line",
    "href": "chapters/ch05-ggplot2_intro.html#line-plots-with-geom_line",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.4 Line Plots with geom_line()",
    "text": "5.4 Line Plots with geom_line()\nLine plots connect points in order, typically used for time series or sequential data.\n\n5.4.1 Example: Growth Curves\n\n# Create growth data for individual animals over time\ngrowth &lt;- tibble(\n  animal_id = rep(c(\"H001\", \"H002\", \"H003\"), each = 6),\n  week = rep(c(0, 2, 4, 6, 8, 10), times = 3),\n  weight_kg = c(\n    c(45, 58, 72, 88, 105, 122),  # H001\n    c(42, 55, 68, 83, 98, 115),   # H002\n    c(48, 60, 75, 91, 108, 126)   # H003\n  )\n)\n\ngrowth\n\n# A tibble: 18 √ó 3\n   animal_id  week weight_kg\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 H001          0        45\n 2 H001          2        58\n 3 H001          4        72\n 4 H001          6        88\n 5 H001          8       105\n 6 H001         10       122\n 7 H002          0        42\n 8 H002          2        55\n 9 H002          4        68\n10 H002          6        83\n11 H002          8        98\n12 H002         10       115\n13 H003          0        48\n14 H003          2        60\n15 H003          4        75\n16 H003          6        91\n17 H003          8       108\n18 H003         10       126\n\n# Create line plot\nggplot(growth, aes(x = week, y = weight_kg, color = animal_id)) +\n  geom_line()\n\n\n\n\n\n\n\n\n\n\n5.4.2 Combining Lines and Points\nOften, it‚Äôs helpful to show both the line and the data points:\n\nggplot(growth, aes(x = week, y = weight_kg, color = animal_id)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipLayer Order Matters\n\n\n\nLayers are drawn in the order you add them. Later layers appear on top of earlier layers.\n# Points on top of lines (usually preferred)\ngeom_line() + geom_point()\n\n# Lines on top of points (can obscure data)\ngeom_point() + geom_line()\n\n\n\n\n5.4.3 Customizing Lines\n\n# Thicker lines with different line types\nggplot(growth, aes(x = week, y = weight_kg, color = animal_id)) +\n  geom_line(linewidth = 1.2) +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n\n\n5.4.4 When to Use Line Plots\n‚úÖ Use line plots when: - Data represents a sequence (especially time) - You want to show trends or changes - Points should be connected in a specific order - You‚Äôre tracking individual subjects over time\n‚ùå Don‚Äôt use line plots when: - Data points are independent (no meaningful order) - You have many overlapping trajectories (hard to read) - X-axis is not continuous or sequential",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#bar-charts-with-geom_col-and-geom_bar",
    "href": "chapters/ch05-ggplot2_intro.html#bar-charts-with-geom_col-and-geom_bar",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.5 Bar Charts with geom_col() and geom_bar()",
    "text": "5.5 Bar Charts with geom_col() and geom_bar()\nBar charts display categorical data with rectangular bars.\n\n5.5.1 geom_col() vs geom_bar()\n\n\n\nFunction\nWhen to use\nY-axis comes from\n\n\n\n\ngeom_col()\nYou‚Äôve already calculated the values\nYour data\n\n\ngeom_bar()\nYou want to count occurrences\nAutomatic counting\n\n\n\n\n\n5.5.2 Using geom_col() with Pre-calculated Values\n\n# Average milk production by breed (already summarized)\nmilk_production &lt;- tibble(\n  breed = c(\"Holstein\", \"Jersey\", \"Guernsey\", \"Brown Swiss\"),\n  avg_liters_per_day = c(35, 24, 28, 30)\n)\n\nmilk_production\n\n# A tibble: 4 √ó 2\n  breed       avg_liters_per_day\n  &lt;chr&gt;                    &lt;dbl&gt;\n1 Holstein                    35\n2 Jersey                      24\n3 Guernsey                    28\n4 Brown Swiss                 30\n\n# Create bar chart\nggplot(milk_production, aes(x = breed, y = avg_liters_per_day)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n5.5.3 Using geom_bar() to Count\n\n# Create dataset with individual animals\nanimals &lt;- tibble(\n  breed = sample(c(\"Holstein\", \"Jersey\", \"Angus\"), size = 100, replace = TRUE)\n)\n\n# Count animals per breed automatically\nggplot(animals, aes(x = breed)) +\n  geom_bar()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNotegeom_bar() Counts for You\n\n\n\nWhen you use geom_bar(), you only specify x (the categorical variable). The y-axis (count) is calculated automatically using stat = \"count\".\n\n\n\n\n5.5.4 Adding Color\n\n# Color bars by breed\nggplot(milk_production, aes(x = breed, y = avg_liters_per_day, fill = breed)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantcolor vs fill for Bars\n\n\n\n\ncolor controls the outline of the bar\nfill controls the inside of the bar\n\nUsually, you want fill for bar charts:\naes(fill = breed)  # Fills the bars (typical)\naes(color = breed) # Just colors the outlines (usually not what you want)\n\n\n\n\n5.5.5 Grouped and Stacked Bars\n\n# Create data with two categorical variables\nfeed_trial &lt;- tibble(\n  breed = rep(c(\"Holstein\", \"Jersey\"), each = 3),\n  feed_type = rep(c(\"A\", \"B\", \"C\"), times = 2),\n  avg_gain_kg = c(1.2, 1.4, 1.1, 0.9, 1.0, 0.8)\n)\n\nfeed_trial\n\n# A tibble: 6 √ó 3\n  breed    feed_type avg_gain_kg\n  &lt;chr&gt;    &lt;chr&gt;           &lt;dbl&gt;\n1 Holstein A                 1.2\n2 Holstein B                 1.4\n3 Holstein C                 1.1\n4 Jersey   A                 0.9\n5 Jersey   B                 1  \n6 Jersey   C                 0.8\n\n# Grouped bars\nggplot(feed_trial, aes(x = breed, y = avg_gain_kg, fill = feed_type)) +\n  geom_col(position = \"dodge\")\n\n\n\n\n\n\n\n# Stacked bars (default)\nggplot(feed_trial, aes(x = breed, y = avg_gain_kg, fill = feed_type)) +\n  geom_col()\n\n\n\n\n\n\n\n\n\n\n5.5.6 Horizontal Bars\n\n# Use coord_flip() for horizontal bars\nggplot(milk_production, aes(x = breed, y = avg_liters_per_day)) +\n  geom_col() +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n5.5.7 When to Use Bar Charts\n‚úÖ Use bar charts when: - Comparing quantities across categories - Showing counts or frequencies - Categories are on the x-axis - Bars should start at zero\n‚ùå Don‚Äôt use bar charts when: - Showing distributions (use histograms) - Comparing many categories (can be hard to read) - Changes over time are important (use line plots)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#histograms-with-geom_histogram",
    "href": "chapters/ch05-ggplot2_intro.html#histograms-with-geom_histogram",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.6 Histograms with geom_histogram()",
    "text": "5.6 Histograms with geom_histogram()\nHistograms show the distribution of a single continuous variable by dividing the data into bins and counting how many observations fall into each bin.\n\n5.6.1 Basic Histogram\n\n# Create birth weight data\nbirth_weights &lt;- tibble(\n  calf_id = 1:200,\n  birth_weight_kg = rnorm(200, mean = 40, sd = 5)\n)\n\n# Create histogram\nggplot(birth_weights, aes(x = birth_weight_kg)) +\n  geom_histogram()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningPick the Right Number of Bins!\n\n\n\nThe default number of bins (30) may not be ideal for your data. Experiment with:\ngeom_histogram(bins = 20)      # Specify number of bins\ngeom_histogram(binwidth = 2)   # Specify width of each bin\nToo few bins ‚Üí lose detail; too many bins ‚Üí noisy\n\n\n\n\n5.6.2 Customizing Binwidth\n\n# Wider bins (3 kg)\nggplot(birth_weights, aes(x = birth_weight_kg)) +\n  geom_histogram(binwidth = 3, fill = \"steelblue\", color = \"white\")\n\n\n\n\n\n\n\n\n\n\n5.6.3 Overlapping Histograms by Group\n\n# Create breed data with different distributions\nset.seed(123)\nbreeds_vec &lt;- sample(c(\"Holstein\", \"Jersey\"), size = 200, replace = TRUE, prob = c(0.5, 0.5))\n\nbirth_weights_breed &lt;- tibble(\n  breed = breeds_vec,\n  birth_weight_kg = ifelse(\n    breed == \"Holstein\",\n    rnorm(sum(breed == \"Holstein\"), mean = 42, sd = 5),\n    rnorm(sum(breed == \"Jersey\"), mean = 37, sd = 4)\n  )\n)\n\n# Overlapping histograms\nggplot(birth_weights_breed, aes(x = birth_weight_kg, fill = breed)) +\n  geom_histogram(binwidth = 2, alpha = 0.6, position = \"identity\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipUse alpha for Overlapping Histograms\n\n\n\nWhen showing multiple distributions, use alpha (transparency) so you can see where they overlap:\ngeom_histogram(alpha = 0.5, position = \"identity\")\nposition = \"identity\" places histograms on top of each other (default is stacked).\n\n\n\n\n5.6.4 When to Use Histograms\n‚úÖ Use histograms when: - Showing the distribution of a continuous variable - Checking for normality - Identifying skewness or outliers - Understanding the shape of your data\n‚ùå Don‚Äôt use histograms when: - You have categorical data (use bar charts) - Comparing multiple groups precisely (use box plots or density plots) - You have very few data points (distributions won‚Äôt be clear)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#box-plots-with-geom_boxplot",
    "href": "chapters/ch05-ggplot2_intro.html#box-plots-with-geom_boxplot",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.7 Box Plots with geom_boxplot()",
    "text": "5.7 Box Plots with geom_boxplot()\nBox plots (box-and-whisker plots) display the distribution of data through five summary statistics: minimum, first quartile (Q1), median, third quartile (Q3), and maximum.\n\n5.7.1 Anatomy of a Box Plot\n\n\n\n\n\ngraph TD\n    A[Box Plot Components] --&gt; B[Box: IQR Q1 to Q3]\n    A --&gt; C[Line in box: Median]\n    A --&gt; D[Whiskers: 1.5 √ó IQR]\n    A --&gt; E[Points beyond whiskers: Outliers]\n\n\n\n\n\n\n\n\n5.7.2 Basic Box Plot\n\n# Create data for different treatment groups\ntreatments &lt;- tibble(\n  treatment = rep(c(\"Control\", \"Drug A\", \"Drug B\", \"Drug C\"), each = 25),\n  weight_gain_kg = c(\n    rnorm(25, mean = 50, sd = 8),   # Control\n    rnorm(25, mean = 58, sd = 7),   # Drug A\n    rnorm(25, mean = 62, sd = 9),   # Drug B\n    rnorm(25, mean = 55, sd = 6)    # Drug C\n  )\n)\n\n# Create box plot\nggplot(treatments, aes(x = treatment, y = weight_gain_kg)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.7.3 Adding Color\n\nggplot(treatments, aes(x = treatment, y = weight_gain_kg, fill = treatment)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n5.7.4 Combining Box Plots with Individual Points\nIt‚Äôs often useful to show the underlying data along with the box plot:\n\n# Add jittered points\nggplot(treatments, aes(x = treatment, y = weight_gain_kg, fill = treatment)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipgeom_jitter() Reduces Overplotting\n\n\n\nWhen points overlap, geom_jitter() adds small random horizontal noise to make them visible:\ngeom_jitter(width = 0.2)  # Jitter horizontally (adjust width as needed)\n\n\n\n\n5.7.5 When to Use Box Plots\n‚úÖ Use box plots when: - Comparing distributions across groups - You want to show median, quartiles, and outliers - You have multiple groups to compare - You want a compact summary of distributions\n‚ùå Don‚Äôt use box plots when: - Your data has multiple modes (bimodal, multimodal) ‚Äî box plots hide this - You have very few data points (&lt; ~10 per group) - You want to show exact distributions (use violin plots or histograms)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#violin-plots-with-geom_violin",
    "href": "chapters/ch05-ggplot2_intro.html#violin-plots-with-geom_violin",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.8 Violin Plots with geom_violin()",
    "text": "5.8 Violin Plots with geom_violin()\nViolin plots show the full distribution of data (like a density plot) rotated and mirrored. They‚Äôre useful for showing distributions that box plots might hide.\n\n5.8.1 Basic Violin Plot\n\nggplot(treatments, aes(x = treatment, y = weight_gain_kg, fill = treatment)) +\n  geom_violin()\n\n\n\n\n\n\n\n\n\n\n5.8.2 Combining Violin Plots with Box Plots\nA powerful combination: violin plot shows distribution, box plot shows summary statistics:\n\nggplot(treatments, aes(x = treatment, y = weight_gain_kg, fill = treatment)) +\n  geom_violin(alpha = 0.5) +\n  geom_boxplot(width = 0.2, fill = \"white\")\n\n\n\n\n\n\n\n\n\n\n5.8.3 Adding Points\n\nggplot(treatments, aes(x = treatment, y = weight_gain_kg, fill = treatment)) +\n  geom_violin(alpha = 0.5) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1)\n\n\n\n\n\n\n\n\n\n\n5.8.4 When to Use Violin Plots\n‚úÖ Use violin plots when: - You want to show full distribution shape - Your data might be bimodal or have multiple peaks - Comparing distributions across groups - You have enough data points (&gt; ~20 per group)\n‚ùå Don‚Äôt use violin plots when: - You have very few data points (distributions won‚Äôt be smooth) - Readers aren‚Äôt familiar with violin plots (less intuitive than box plots) - You only need summary statistics (use box plots)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#aesthetic-mappings-a-deeper-dive",
    "href": "chapters/ch05-ggplot2_intro.html#aesthetic-mappings-a-deeper-dive",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.9 Aesthetic Mappings: A Deeper Dive",
    "text": "5.9 Aesthetic Mappings: A Deeper Dive\nWe‚Äôve seen several aesthetics already. Let‚Äôs consolidate what we know:\n\n5.9.1 Common Aesthetics\n\n\n\nAesthetic\nType\nExample\nBest used with\n\n\n\n\nx\nPosition\naes(x = treatment)\nAll geoms\n\n\ny\nPosition\naes(y = weight)\nAll geoms\n\n\ncolor\nColor\naes(color = breed)\nPoints, lines, text\n\n\nfill\nFill\naes(fill = breed)\nBars, boxes, violins\n\n\nsize\nSize\naes(size = age)\nPoints, text\n\n\nshape\nShape\naes(shape = sex)\nPoints\n\n\nalpha\nTransparency\naes(alpha = density)\nAll geoms\n\n\nlinetype\nLine type\naes(linetype = group)\nLines\n\n\n\n\n\n5.9.2 Continuous vs Discrete Variables\nggplot2 handles continuous and discrete variables differently:\n\n# Create data with both variable types\nmixed_data &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 20),  # Discrete\n  score = rnorm(60, mean = 100, sd = 15),    # Continuous\n  intensity = rep(1:20, times = 3)           # Continuous\n)\n\n# Color by discrete variable (gets distinct colors)\nggplot(mixed_data, aes(x = intensity, y = score, color = group)) +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n# Color by continuous variable (gets gradient)\nggplot(mixed_data, aes(x = intensity, y = score, color = intensity)) +\n  geom_point(size = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantChoosing the Right Aesthetic\n\n\n\n\nCategorical data: Use color, fill, shape, linetype\nContinuous data: Use x, y, size, alpha, color (gradient)\nToo many categories? (&gt; ~7): Don‚Äôt use shape (hard to distinguish); consider faceting instead",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#working-with-scales",
    "href": "chapters/ch05-ggplot2_intro.html#working-with-scales",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.10 Working with Scales",
    "text": "5.10 Working with Scales\nScales control how data values map to visual properties. Every aesthetic has a scale.\n\n5.10.1 Scale Functions\nScale functions follow the pattern: scale_&lt;aesthetic&gt;_&lt;type&gt;()\nExamples: - scale_x_continuous() ‚Äî Continuous x-axis - scale_y_log10() ‚Äî Log10 transform y-axis - scale_color_manual() ‚Äî Manually specify colors - scale_fill_brewer() ‚Äî Use ColorBrewer palettes\n\n\n5.10.2 Modifying Continuous Scales\n\n# Create sample data\ngrowth_rate &lt;- tibble(\n  animal_id = 1:30,\n  weight_kg = runif(30, min = 400, max = 700),\n  gain_per_day_kg = runif(30, min = 0.8, max = 1.5)\n)\n\n# Modify axis limits and breaks\nggplot(growth_rate, aes(x = weight_kg, y = gain_per_day_kg)) +\n  geom_point() +\n  scale_x_continuous(limits = c(350, 750), breaks = seq(350, 750, by = 100)) +\n  scale_y_continuous(limits = c(0, 2), breaks = seq(0, 2, by = 0.5))\n\n\n\n\n\n\n\n\n\n\n5.10.3 Log Transformations\n\n# Data with exponential relationship\nbacteria &lt;- tibble(\n  time_hours = 0:24,\n  count = 100 * 2^(0:24 / 4)  # Exponential growth\n)\n\n# Regular scale (hard to see early values)\nggplot(bacteria, aes(x = time_hours, y = count)) +\n  geom_line() +\n  geom_point()\n\n\n\n\n\n\n\n# Log scale (easier to see pattern)\nggplot(bacteria, aes(x = time_hours, y = count)) +\n  geom_line() +\n  geom_point() +\n  scale_y_log10()\n\n\n\n\n\n\n\n\n\n\n5.10.4 Discrete Scales: Manual Colors\n\n# Specify custom colors\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  scale_color_manual(values = c(\"Holstein\" = \"blue\",\n                                  \"Jersey\" = \"orange\",\n                                  \"Angus\" = \"darkred\"))\n\n\n\n\n\n\n\n\n\n\n5.10.5 Using ColorBrewer Palettes\n\n# Use ColorBrewer palette (colorblind-friendly)\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipColorblind-Friendly Palettes\n\n\n\nUse scale_color_viridis_d() (discrete) or scale_color_viridis_c() (continuous) for accessible colors:\nscale_color_viridis_d()  # Discrete variable\nscale_color_viridis_c()  # Continuous variable",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#labels-and-titles-with-labs",
    "href": "chapters/ch05-ggplot2_intro.html#labels-and-titles-with-labs",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.11 Labels and Titles with labs()",
    "text": "5.11 Labels and Titles with labs()\nUse labs() to add or modify plot labels:\n\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Relationship Between Feed Intake and Weight Gain\",\n    subtitle = \"Data from 30 cattle across three breeds\",\n    x = \"Daily Feed Intake (kg)\",\n    y = \"Daily Weight Gain (kg)\",\n    color = \"Breed\",\n    caption = \"Source: Fictional farm data (2024)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteLabel Arguments in labs()\n\n\n\n\ntitle ‚Äî Main plot title\nsubtitle ‚Äî Secondary title (smaller font)\nx, y ‚Äî Axis labels\ncolor, fill, shape, size ‚Äî Legend titles (match the aesthetic name)\ncaption ‚Äî Small text at bottom (often for data source or notes)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#themes-customizing-plot-appearance",
    "href": "chapters/ch05-ggplot2_intro.html#themes-customizing-plot-appearance",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.12 Themes: Customizing Plot Appearance",
    "text": "5.12 Themes: Customizing Plot Appearance\nThemes control the non-data elements of your plot (background, grid lines, fonts, etc.).\n\n5.12.1 Built-in Themes\nggplot2 includes several complete themes:\n\n# Create base plot\np &lt;- ggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Feed Intake vs Weight Gain\")\n\n# Default theme (theme_gray)\np\n\n\n\n\n\n\n\n# theme_minimal() - Clean and modern\np + theme_minimal()\n\n\n\n\n\n\n\n# theme_classic() - No grid lines\np + theme_classic()\n\n\n\n\n\n\n\n# theme_bw() - Black and white\np + theme_bw()\n\n\n\n\n\n\n\n# theme_light() - Light gray grid\np + theme_light()\n\n\n\n\n\n\n\n\n\n\n5.12.2 Setting a Default Theme\nYou can set a theme for all plots in your session:\n\n# Set default theme for all subsequent plots\ntheme_set(theme_minimal())\n\n\n\n5.12.3 Customizing Themes\nUse theme() to modify specific elements:\n\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Feed Intake vs Weight Gain\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 16, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipCommon Theme Customizations\n\n\n\ntheme(\n  plot.title = element_text(size = 16, face = \"bold\"),\n  axis.text = element_text(size = 10),\n  legend.position = \"bottom\",  # or \"top\", \"left\", \"right\", \"none\"\n  panel.grid.major = element_line(color = \"gray90\"),\n  panel.grid.minor = element_blank()\n)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#coordinate-systems",
    "href": "chapters/ch05-ggplot2_intro.html#coordinate-systems",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.13 Coordinate Systems",
    "text": "5.13 Coordinate Systems\nCoordinate systems control how the x and y axes are rendered.\n\n5.13.1 coord_cartesian(): Zooming\ncoord_cartesian() is useful for zooming without removing data:\n\n# Zoom in on a specific region\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  coord_cartesian(xlim = c(18, 26), ylim = c(0.8, 1.2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantcoord_cartesian() vs scale_*_continuous(limits = ...)\n\n\n\n\ncoord_cartesian(xlim = c(a, b)) ‚Äî Zooms without removing data (stat calculations use all data)\nscale_x_continuous(limits = c(a, b)) ‚Äî Removes data outside limits before plotting\n\nUsually, you want coord_cartesian() for zooming.\n\n\n\n\n5.13.2 coord_flip(): Horizontal Plots\nWe saw this earlier with bar charts. It swaps x and y:\n\nggplot(milk_production, aes(x = breed, y = avg_liters_per_day)) +\n  geom_col(fill = \"steelblue\") +\n  coord_flip()\n\n\n\n\n\n\n\n\n\n\n5.13.3 Fixed Aspect Ratio\ncoord_fixed() ensures equal scales on both axes (1 unit on x = 1 unit on y visually):\n\n# Without fixed aspect ratio\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg)) +\n  geom_point()\n\n\n\n\n\n\n\n# With fixed aspect ratio\nggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg)) +\n  geom_point() +\n  coord_fixed(ratio = 20)  # 20 kg of feed = 1 kg weight gain visually",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#saving-plots-with-ggsave",
    "href": "chapters/ch05-ggplot2_intro.html#saving-plots-with-ggsave",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.14 Saving Plots with ggsave()",
    "text": "5.14 Saving Plots with ggsave()\nAfter creating a plot, save it with ggsave():\n\n# Create plot\nmy_plot &lt;- ggplot(cattle, aes(x = feed_intake_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Feed Intake vs Weight Gain\") +\n  theme_minimal()\n\n# Save to file\nggsave(\"cattle_plot.png\", plot = my_plot, width = 8, height = 6, dpi = 300)\n\n\n5.14.1 Common Arguments\n\n\n\nArgument\nWhat it does\nExample\n\n\n\n\nfilename\nFile path and name\n\"plots/figure1.png\"\n\n\nplot\nggplot object (defaults to last plot)\nmy_plot\n\n\nwidth\nWidth in inches (or cm, mm)\n8\n\n\nheight\nHeight in inches (or cm, mm)\n6\n\n\ndpi\nResolution (dots per inch)\n300 (publication quality)\n\n\nunits\nUnits for width/height\n\"in\", \"cm\", \"mm\"\n\n\n\n\n\n5.14.2 File Formats\nggplot2 supports many formats (determined by file extension):\n\nPNG (.png) ‚Äî Good for presentations, web (raster)\nPDF (.pdf) ‚Äî Best for publications (vector, scalable)\nJPEG (.jpg) ‚Äî Compressed (raster, smaller files)\nSVG (.svg) ‚Äî Vector format for web\nTIFF (.tiff) ‚Äî High quality (raster, large files)\n\n\n# Save as PDF (recommended for publications)\nggsave(\"figure1.pdf\", width = 7, height = 5)\n\n# Save as high-resolution PNG\nggsave(\"figure1.png\", width = 7, height = 5, dpi = 300)\n\n\n\n\n\n\n\nTipPublication Guidelines\n\n\n\nMost journals require: - 300 dpi minimum resolution - PDF or TIFF format for final submission - Specific dimensions (check journal guidelines) - Readable fonts (usually at least 8-10 pt in final figure)\nStart with:\nggsave(\"figure.pdf\", width = 7, height = 5, dpi = 300)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#summary-choosing-the-right-plot-type",
    "href": "chapters/ch05-ggplot2_intro.html#summary-choosing-the-right-plot-type",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.15 Summary: Choosing the Right Plot Type",
    "text": "5.15 Summary: Choosing the Right Plot Type\n\n\n\n\n\n\n\n\nData Type\nGoal\nRecommended Plot\n\n\n\n\n2 continuous variables\nShow relationship\nScatter plot (geom_point)\n\n\nTime series\nShow change over time\nLine plot (geom_line)\n\n\nCategorical + continuous\nCompare groups\nBar chart (geom_col), Box plot (geom_boxplot)\n\n\n1 continuous variable\nShow distribution\nHistogram (geom_histogram), Density plot\n\n\nMultiple groups + continuous\nCompare distributions\nBox plot (geom_boxplot), Violin plot (geom_violin)\n\n\nCounts of categories\nShow frequencies\nBar chart (geom_bar)\n\n\n\n\n5.15.1 Key Concepts Review\n\nThree essentials: Data + aes() + geom_*()\nLayers: Build plots by adding layers with +\nAesthetics: Map variables to visual properties (aes())\nGeoms: Choose the right plot type for your data\nScales: Control how data maps to visuals\nLabels: Always label axes and legends clearly (labs())\nThemes: Customize appearance for publication\nSave: Use ggsave() with appropriate format and resolution",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#week-5-homework-creating-publication-ready-plots",
    "href": "chapters/ch05-ggplot2_intro.html#week-5-homework-creating-publication-ready-plots",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.16 Week 5 Homework: Creating Publication-Ready Plots",
    "text": "5.16 Week 5 Homework: Creating Publication-Ready Plots\n\n5.16.1 Assignment Overview\nYou will create 5 different types of plots from a provided dataset and present them in a professional Quarto report. Focus on choosing appropriate plot types, applying proper labels, and using themes effectively.\n\n\n5.16.2 Dataset\nYou‚Äôll use simulated data from a sheep grazing study:\n\n# Generate dataset (include this in your homework)\nset.seed(2024)\n\nsheep_data &lt;- tibble(\n  sheep_id = rep(1:60, each = 4),\n  breed = rep(rep(c(\"Merino\", \"Suffolk\", \"Dorper\"), each = 20), each = 4),\n  pasture_type = rep(rep(c(\"Native\", \"Improved\"), each = 30), each = 4),\n  week = rep(c(0, 4, 8, 12), times = 60),\n  weight_kg = rep(rnorm(60, mean = 45, sd = 5), each = 4) +\n              rep(c(0, 5, 9, 12), times = 60) +\n              rnorm(240, mean = 0, sd = 1.5),\n  wool_growth_mm = rep(rnorm(60, mean = 8, sd = 1.5), each = 4) +\n                   rnorm(240, mean = 0, sd = 0.5)\n)\n\n# Add some additional variables\nsheep_data &lt;- sheep_data %&gt;%\n  group_by(sheep_id) %&gt;%\n  mutate(\n    weight_gain_kg = weight_kg - first(weight_kg),\n    avg_daily_gain_g = (weight_gain_kg * 1000) / (week * 7)\n  ) %&gt;%\n  ungroup() %&gt;%\n  filter(week &gt; 0)  # Remove week 0 for some analyses\n\n\n\n5.16.3 Part 1: Scatter Plot (20 points)\nCreate a scatter plot showing the relationship between weight_kg (x-axis) and wool_growth_mm (y-axis).\nRequirements: - Color points by breed - Set point size to 3 - Use theme_minimal() - Add appropriate title and axis labels - Include a subtitle indicating sample size\nBonus (+5 points): Add a trend line using geom_smooth(method = \"lm\") for each breed\n\n\n5.16.4 Part 2: Line Plot (20 points)\nCreate a line plot showing weight_kg over week for individual sheep.\nRequirements: - Show data for 6 randomly selected sheep (use filter(sheep_id %in% c(...))) - Color lines by breed - Add both lines and points - Use theme_classic() - Proper labels and title\n\n\n5.16.5 Part 3: Bar Chart (20 points)\nCreate a bar chart showing the average final weight (week 12) by breed and pasture type.\nRequirements: - Calculate average final weight using dplyr first - Use geom_col() with position = \"dodge\" for grouped bars - Fill bars by pasture_type - Add axis labels including units - Use a ColorBrewer palette\n\n\n5.16.6 Part 4: Histogram (15 points)\nCreate histograms showing the distribution of avg_daily_gain_g for each breed.\nRequirements: - Three separate histograms (one per breed) ‚Äî hint: use faceting or create 3 separate plots - Choose appropriate binwidth - Fill color should differ by breed - Use theme_bw() - Title should be descriptive\nNote: We‚Äôll cover faceting (facet_wrap()) in the next chapter, but you can try it now or create 3 separate plots.\n\n\n5.16.7 Part 5: Box Plot or Violin Plot (20 points)\nCompare the distribution of weight_gain_kg (total gain over the study) across breeds and pasture types.\nRequirements: - Use geom_boxplot() OR geom_violin() (your choice) - X-axis: breed, fill by pasture_type - Add jittered points with transparency - Use theme_light() - Clear labels and legend\n\n\n5.16.8 Part 6: Interpretation (5 points)\nFor each plot, write 2-3 sentences interpreting what the plot shows. What patterns, trends, or differences do you observe?\n\n\n5.16.9 Bonus Challenge (+10 points)\nCreate a single multi-panel figure combining your scatter plot and box plot using either: - Manual arrangement (creating two separate plots) - Looking ahead: Try library(patchwork) and combine with +\n\n# Example structure (you'll learn this next week!)\nlibrary(patchwork)\nplot1 + plot2\n\n\n\n5.16.10 Submission Requirements\n\nSubmit a rendered Quarto HTML document\nInclude all code and output\nAll plots should be clearly labeled\nUse professional formatting\nInclude interpretations\n\n\n\n5.16.11 Recommended YAML\n---\ntitle: \"Week 5 Homework: Data Visualization with ggplot2\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n5.16.12 Grading Rubric\n\nPart 1: Scatter Plot (20%)\n\nCorrect geom and aesthetics (8%)\nProper labels and theme (7%)\nOverall appearance (5%)\n\nPart 2: Line Plot (20%)\n\nCorrect data filtering and plotting (8%)\nLines and points shown (7%)\nLabels and styling (5%)\n\nPart 3: Bar Chart (20%)\n\nCorrect data summarization (8%)\nGrouped bars correctly displayed (7%)\nLabels and color palette (5%)\n\nPart 4: Histogram (15%)\n\nAppropriate binwidth chosen (6%)\nOne histogram per breed (5%)\nLabels and theme (4%)\n\nPart 5: Box/Violin Plot (20%)\n\nCorrect plot with multiple groupings (8%)\nPoints added with transparency (7%)\nLabels and theme (5%)\n\nPart 6: Interpretation (5%)\n\nClear, accurate interpretations for each plot",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch05-ggplot2_intro.html#additional-resources",
    "href": "chapters/ch05-ggplot2_intro.html#additional-resources",
    "title": "5¬† Introduction to Data Visualization with ggplot2",
    "section": "5.17 Additional Resources",
    "text": "5.17 Additional Resources\n\n5.17.1 Required Reading\n\nR for Data Science (2e) - Chapters 9-10: Visualize and Layers\nggplot2 documentation\nggplot2 book (3e) - Chapters 1-4\n\n\n\n5.17.2 Optional Reading\n\nWilkinson, L. (2005). The Grammar of Graphics (2nd ed.). Springer. (Original theory)\nWickham, H. (2010). ‚ÄúA Layered Grammar of Graphics.‚Äù Journal of Computational and Graphical Statistics, 19(1), 3-28.\nData Visualization: A Practical Introduction by Kieran Healy\n\n\n\n5.17.3 Videos\n\n‚Äúggplot2 Workshop‚Äù by Thomas Lin Pedersen (RStudio)\n‚ÄúA Quick Introduction to ggplot2‚Äù by StatQuest with Josh Starmer\n‚Äúggplot2 Tutorial‚Äù series by Data Science Dojo\n\n\n\n5.17.4 Cheat Sheets\n\nggplot2 Cheat Sheet ‚≠ê Most important!\nRStudio Cheatsheets Collection\n\n\n\n5.17.5 Galleries and Inspiration\n\nR Graph Gallery ‚Äî Hundreds of examples with code\nTop 50 ggplot2 Visualizations\nggplot2 extensions ‚Äî Additional packages\n\n\n\n5.17.6 Color Resources\n\nColorBrewer ‚Äî Color schemes for maps and data viz\nViridis color palettes ‚Äî Colorblind-friendly\nCoolors ‚Äî Color palette generator\n\n\n\n5.17.7 Interactive Learning\n\nRStudio Primers: Visualize Data\nggplot2 Exercises\n\n\n\n5.17.8 Useful Websites\n\nggplot2 Tidyverse Page\nStack Overflow: ggplot2 tag\nRStudio Community\n\n\nNext Chapter: Advanced ggplot2 and Multi-Panel Plots",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Introduction to Data Visualization with ggplot2</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html",
    "href": "chapters/ch06-ggplot2_advanced.html",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "",
    "text": "6.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#learning-objectives",
    "href": "chapters/ch06-ggplot2_advanced.html#learning-objectives",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "",
    "text": "Create small multiples using facet_wrap() and facet_grid()\nAdd statistical layers: trend lines, summaries, and error bars\nCustomize themes extensively with theme() for publication-ready plots\nApply advanced color palettes (ColorBrewer, Viridis) effectively\nAdd text annotations and labels to plots\nCombine multiple plots into complex figures using patchwork and cowplot\nPolish figures to meet journal publication standards",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#introduction",
    "href": "chapters/ch06-ggplot2_advanced.html#introduction",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.2 Introduction",
    "text": "6.2 Introduction\nIn Chapter 5, you learned the fundamentals of ggplot2: creating basic plots, mapping aesthetics, applying themes, and saving your work. This chapter builds on that foundation to teach advanced techniques for creating publication-quality, multi-panel figures.\n\n6.2.1 What You‚Äôll Learn\nThis chapter focuses on techniques you‚Äôll use when: - Creating figures for journal submissions - Making complex multi-panel layouts - Adding statistical summaries and trend lines - Customizing every aspect of plot appearance - Comparing patterns across multiple groups simultaneously\n\n\n\n\n\n\nNotePrerequisites\n\n\n\nThis chapter assumes you‚Äôve completed Chapter 5 and are comfortable with: - Basic ggplot2 syntax (data + aes() + geom_*()) - Common geoms (point, line, bar, boxplot, etc.) - Aesthetic mappings (color, fill, size, etc.) - Built-in themes\n\n\n\n\n6.2.2 Setup\n\nlibrary(tidyverse)  # Includes ggplot2, dplyr\nlibrary(patchwork)  # Combining plots\nlibrary(cowplot)    # Alternative for combining plots\n\n# Optional: install.packages(\"ggrepel\") for non-overlapping labels\n\n# Set default theme for all plots\ntheme_set(theme_minimal())\n\n\n\n6.2.3 Load Example Data\nWe‚Äôll use cattle weight data throughout this chapter:\n\n# Read cattle data\ncattle &lt;- read_csv(\"../data/raw/cattle_weights.csv\")\n\n# Calculate weight gain\ncattle &lt;- cattle %&gt;%\n  mutate(\n    weight_gain_kg = final_weight_kg - initial_weight_kg,\n    days_on_feed = as.numeric(as.Date(\"2023-12-01\") - as.Date(birth_date))\n  )\n\n# View data\nglimpse(cattle)\n\nRows: 20\nColumns: 9\n$ animal_id         &lt;chr&gt; \"C001\", \"C002\", \"C003\", \"C004\", \"C005\", \"C006\", \"C00‚Ä¶\n$ birth_date        &lt;date&gt; 2023-03-15, 2023-03-18, 2023-03-20, 2023-03-22, 202‚Ä¶\n$ breed             &lt;chr&gt; \"Angus\", \"Hereford\", \"Angus\", \"Charolais\", \"Angus\", ‚Ä¶\n$ sex               &lt;chr&gt; \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M\", \"F\", \"M‚Ä¶\n$ initial_weight_kg &lt;dbl&gt; 285, 270, 290, 295, 280, 275, 285, 300, 278, 272, 30‚Ä¶\n$ final_weight_kg   &lt;dbl&gt; 520, 485, 535, 510, 500, 495, 525, 540, 490, 480, 55‚Ä¶\n$ treatment         &lt;chr&gt; \"High_Protein\", \"Standard\", \"High_Protein\", \"Control‚Ä¶\n$ weight_gain_kg    &lt;dbl&gt; 235, 215, 245, 215, 220, 220, 240, 240, 212, 208, 25‚Ä¶\n$ days_on_feed      &lt;dbl&gt; 261, 258, 256, 254, 251, 244, 240, 237, 233, 230, 22‚Ä¶",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#faceting-creating-small-multiples",
    "href": "chapters/ch06-ggplot2_advanced.html#faceting-creating-small-multiples",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.3 Faceting: Creating Small Multiples",
    "text": "6.3 Faceting: Creating Small Multiples\nFaceting splits your data into subsets and creates a separate plot for each subset. This is powerful for comparing patterns across groups.\n\n6.3.1 Why Use Facets?\nInstead of putting everything on one plot with different colors:\n\n# All breeds on one plot (can be cluttered)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"All breeds together (can be hard to read)\")\n\n\n\n\n\n\n\n\nUse facets to give each group its own panel:\n\n# Each breed gets its own panel (clearer)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, color = \"steelblue\") +\n  facet_wrap(~breed) +\n  labs(title = \"Faceted by breed (clearer patterns)\")\n\n\n\n\n\n\n\n\n\n\n6.3.2 facet_wrap(): Faceting by One Variable\nfacet_wrap() creates a series of panels arranged in rows and columns.\nBasic syntax:\nfacet_wrap(~variable)\n\n6.3.2.1 Example: Facet by Breed\n\nggplot(cattle, aes(x = initial_weight_kg, y = final_weight_kg)) +\n  geom_point(aes(color = sex), size = 3) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\") +\n  facet_wrap(~breed) +\n  labs(\n    title = \"Initial vs Final Weight by Breed\",\n    x = \"Initial Weight (kg)\",\n    y = \"Final Weight (kg)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.3.2.2 Controlling Layout\n\n# Specify number of rows or columns\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_wrap(~breed, nrow = 1) +  # Force 1 row (3 columns)\n  labs(title = \"Single row layout\")\n\n\n\n\n\n\n\n# Or specify columns\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_wrap(~breed, ncol = 1) +  # Force 1 column (3 rows)\n  labs(title = \"Single column layout\")\n\n\n\n\n\n\n\n\n\n\n6.3.2.3 Free vs Fixed Scales\nBy default, all facets share the same x and y scales. You can make them independent:\n\n# Fixed scales (default - easier to compare)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3) +\n  facet_wrap(~breed, scales = \"fixed\") +\n  labs(title = \"Fixed scales (default)\")\n\n\n\n\n\n\n\n# Free scales (each panel optimized)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3) +\n  facet_wrap(~breed, scales = \"free\") +\n  labs(title = \"Free scales\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningBe Careful with Free Scales!\n\n\n\nFree scales can be misleading because viewers may not notice the different axis ranges. Use them only when: - Groups have very different ranges - You‚Äôre more interested in patterns within each group than comparisons between groups - You clearly communicate that scales differ\n\n\n\n\n6.3.2.4 Options for Free Scales\nscales = \"fixed\"    # Both x and y fixed (default)\nscales = \"free\"     # Both x and y free\nscales = \"free_x\"   # Only x-axis free\nscales = \"free_y\"   # Only y-axis free\n\n\n6.3.2.5 Customizing Facet Labels\n\n# Create cleaner labels\ncattle_labeled &lt;- cattle %&gt;%\n  mutate(\n    breed_label = case_when(\n      breed == \"Angus\" ~ \"Angus Cattle\",\n      breed == \"Hereford\" ~ \"Hereford Cattle\",\n      breed == \"Charolais\" ~ \"Charolais Cattle\"\n    )\n  )\n\nggplot(cattle_labeled, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_wrap(~breed_label) +\n  labs(title = \"Custom facet labels\")\n\n\n\n\n\n\n\n\n\n\n\n6.3.3 facet_grid(): Faceting by Two Variables\nfacet_grid() creates a grid of panels based on two variables (one for rows, one for columns).\nBasic syntax:\nfacet_grid(rows ~ columns)\nfacet_grid(var1 ~ var2)\n\n6.3.3.1 Example: Breed √ó Sex\n\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_grid(sex ~ breed) +\n  labs(\n    title = \"Weight Gain by Breed and Sex\",\n    subtitle = \"Rows = Sex, Columns = Breed\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.3.3.2 Just Rows or Just Columns\n\n# Facet only by rows (use . for columns)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_grid(breed ~ .) +\n  labs(title = \"Breed in rows\")\n\n\n\n\n\n\n\n# Facet only by columns (use . for rows)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  facet_grid(. ~ breed) +\n  labs(title = \"Breed in columns\")\n\n\n\n\n\n\n\n\n\n\n\n6.3.4 facet_wrap() vs facet_grid()\n\n\n\n\n\n\n\n\nFeature\nfacet_wrap()\nfacet_grid()\n\n\n\n\nVariables\n1 (usually)\n2 (rows √ó columns)\n\n\nLayout\nFlows into rows/columns\nStrict grid\n\n\nBest for\nMany levels of one variable\nCross-classifying two variables\n\n\nSpace\nMore efficient with space\nCan have empty cells\n\n\n\n\n\n\n\n\n\nTipWhen to Use Each\n\n\n\n\nfacet_wrap(): When you have one grouping variable with many levels (e.g., 10 different farms)\nfacet_grid(): When you want to compare combinations of two variables (e.g., treatment √ó time point)\n\n\n\n\n\n6.3.5 Faceting with Three Variables\nYou can facet by multiple variables with facet_wrap() using vars():\n\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 2, alpha = 0.6) +\n  facet_wrap(vars(breed, sex)) +\n  labs(title = \"Faceting by breed AND sex with facet_wrap()\")\n\n\n\n\n\n\n\n\nOr nest faceting:\n\n# Facet by treatment, then color by sex\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = sex)) +\n  geom_point(size = 3) +\n  facet_wrap(~treatment) +\n  labs(title = \"Treatment (facets) √ó Sex (color)\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#statistical-layers",
    "href": "chapters/ch06-ggplot2_advanced.html#statistical-layers",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.4 Statistical Layers",
    "text": "6.4 Statistical Layers\nStatistical layers add computed values to your plots: trend lines, means, error bars, etc.\n\n6.4.1 Adding Trend Lines with geom_smooth()\ngeom_smooth() adds a smoothed conditional mean to your plot.\n\n6.4.1.1 Linear Regression Line\n\nggplot(cattle, aes(x = initial_weight_kg, y = final_weight_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(\n    title = \"Linear Trend with Confidence Interval\",\n    x = \"Initial Weight (kg)\",\n    y = \"Final Weight (kg)\"\n  )\n\n\n\n\n\n\n\n\nArguments: - method = \"lm\" ‚Äî Linear regression - se = TRUE ‚Äî Show 95% confidence interval (default) - se = FALSE ‚Äî Hide confidence interval\n\n\n6.4.1.2 By Group\n\n# Separate trend line for each breed\nggplot(cattle, aes(x = initial_weight_kg, y = final_weight_kg, color = breed)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(title = \"Linear trends by breed\")\n\n\n\n\n\n\n\n\n\n\n6.4.1.3 LOESS Smoothing\nLOESS (Locally Estimated Scatterplot Smoothing) is better for non-linear patterns:\n\n# Create data with non-linear relationship\nset.seed(123)\ngrowth_curve &lt;- tibble(\n  age_days = rep(1:100, times = 3),\n  weight = 50 + age_days * 0.5 + 0.01 * age_days^2 + rnorm(300, sd = 10),\n  animal = rep(c(\"A\", \"B\", \"C\"), each = 100)\n)\n\nggplot(growth_curve, aes(x = age_days, y = weight)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"loess\", color = \"red\", se = TRUE) +\n  geom_smooth(method = \"lm\", color = \"blue\", linetype = \"dashed\", se = FALSE) +\n  labs(\n    title = \"LOESS (red) vs Linear (blue)\",\n    subtitle = \"LOESS captures non-linear growth pattern\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.4.1.4 GAM (Generalized Additive Model)\nFor very flexible smoothing:\n\nggplot(growth_curve, aes(x = age_days, y = weight)) +\n  geom_point(alpha = 0.3) +\n  geom_smooth(method = \"gam\", formula = y ~ s(x, bs = \"cs\"), color = \"darkgreen\") +\n  labs(title = \"GAM smoothing for flexible curves\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteMethod Options for geom_smooth()\n\n\n\n\nmethod = \"lm\" ‚Äî Linear regression\nmethod = \"loess\" ‚Äî LOESS smoothing (default for &lt; 1000 points)\nmethod = \"gam\" ‚Äî Generalized Additive Model (requires mgcv package)\nmethod = \"glm\" ‚Äî Generalized Linear Model\n\n\n\n\n\n\n6.4.2 Summary Statistics with stat_summary()\nstat_summary() calculates and displays summary statistics.\n\n6.4.2.1 Mean with Error Bars\n\n# Mean weight gain by breed\nggplot(cattle, aes(x = breed, y = weight_gain_kg)) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 2) +\n  stat_summary(\n    fun = mean,\n    geom = \"point\",\n    size = 4,\n    color = \"red\"\n  ) +\n  stat_summary(\n    fun.data = mean_se,\n    geom = \"errorbar\",\n    width = 0.2,\n    color = \"red\"\n  ) +\n  labs(title = \"Mean ¬± SE by breed\")\n\n\n\n\n\n\n\n\n\n\n6.4.2.2 Mean with Confidence Intervals\n\nggplot(cattle, aes(x = treatment, y = weight_gain_kg, color = treatment)) +\n  stat_summary(\n    fun.data = mean_cl_normal,  # Mean with 95% CI\n    geom = \"pointrange\",\n    size = 1\n  ) +\n  labs(\n    title = \"Mean Weight Gain by Treatment\",\n    subtitle = \"Points show mean, bars show 95% CI\",\n    y = \"Weight Gain (kg)\"\n  ) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSummary Functions\n\n\n\nCommon functions for stat_summary(): - mean, median, min, max - mean_se ‚Äî Mean ¬± standard error - mean_sdl ‚Äî Mean ¬± standard deviation - mean_cl_normal ‚Äî Mean ¬± 95% CI (assumes normality) - mean_cl_boot ‚Äî Mean ¬± 95% CI (bootstrap)\n\n\n\n\n\n6.4.3 Manual Error Bars\nIf you‚Äôve pre-calculated summaries, use geom_errorbar():\n\n# Calculate summaries\ncattle_summary &lt;- cattle %&gt;%\n  group_by(breed, treatment) %&gt;%\n  summarise(\n    mean_gain = mean(weight_gain_kg),\n    se_gain = sd(weight_gain_kg) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\ncattle_summary\n\n# A tibble: 9 √ó 4\n  breed     treatment    mean_gain se_gain\n  &lt;chr&gt;     &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n1 Angus     Control           224    16   \n2 Angus     High_Protein      234.    8.63\n3 Angus     Standard          220.    7.22\n4 Charolais Control           215    NA   \n5 Charolais High_Protein      241.    5.21\n6 Charolais Standard          243    NA   \n7 Hereford  Control           218.    3.93\n8 Hereford  High_Protein      220    NA   \n9 Hereford  Standard          214.    1.5 \n\n# Plot with error bars\nggplot(cattle_summary, aes(x = breed, y = mean_gain, fill = treatment)) +\n  geom_col(position = \"dodge\") +\n  geom_errorbar(\n    aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain),\n    position = position_dodge(width = 0.9),\n    width = 0.25\n  ) +\n  labs(\n    title = \"Mean Weight Gain ¬± SE\",\n    y = \"Mean Weight Gain (kg)\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.4.4 Other Error Bar Geoms\n\n# geom_linerange (no caps)\np1 &lt;- ggplot(cattle_summary, aes(x = breed, y = mean_gain, color = treatment)) +\n  geom_point(size = 3, position = position_dodge(width = 0.5)) +\n  geom_linerange(\n    aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain),\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"geom_linerange\")\n\n# geom_pointrange (combines point and range)\np2 &lt;- ggplot(cattle_summary, aes(x = breed, y = mean_gain, color = treatment)) +\n  geom_pointrange(\n    aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain),\n    position = position_dodge(width = 0.5)\n  ) +\n  labs(title = \"geom_pointrange\")\n\np1 + p2",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#advanced-theme-customization",
    "href": "chapters/ch06-ggplot2_advanced.html#advanced-theme-customization",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.5 Advanced Theme Customization",
    "text": "6.5 Advanced Theme Customization\nIn Chapter 5, you learned about built-in themes (theme_minimal(), theme_classic(), etc.). Now we‚Äôll use theme() to customize every aspect of plot appearance.\n\n6.5.1 The theme() Function\ntheme() controls non-data elements: text, backgrounds, grid lines, legends, etc.\n\n6.5.1.1 Basic Structure\ntheme(\n  element = element_function(arguments)\n)\nElement functions: - element_text() ‚Äî Text properties - element_line() ‚Äî Line properties - element_rect() ‚Äî Rectangle (background) properties - element_blank() ‚Äî Remove element entirely\n\n\n\n6.5.2 Customizing Text Elements\n\nggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = breed)) +\n  geom_boxplot() +\n  labs(title = \"Weight Gain by Breed\", x = \"Breed\", y = \"Weight Gain (kg)\") +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(size = 18, face = \"bold\", hjust = 0.5),\n    axis.title = element_text(size = 14, face = \"bold\"),\n    axis.text = element_text(size = 12),\n    legend.title = element_text(size = 12, face = \"bold\"),\n    legend.text = element_text(size = 10)\n  )\n\n\n\n\n\n\n\n\nText arguments: - size ‚Äî Font size in points - face ‚Äî ‚Äúplain‚Äù, ‚Äúbold‚Äù, ‚Äúitalic‚Äù, ‚Äúbold.italic‚Äù - hjust ‚Äî Horizontal justification (0 = left, 0.5 = center, 1 = right) - vjust ‚Äî Vertical justification - color ‚Äî Text color - family ‚Äî Font family\n\n\n6.5.3 Customizing Legends\n\n# Legend position\np1 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Legend on right (default)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Legend on bottom\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\np3 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"Legend inside plot\") +\n  theme_minimal() +\n  theme(legend.position = c(0.85, 0.2))  # x, y coordinates (0-1)\n\np4 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  labs(title = \"No legend\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\n6.5.3.1 Legend Box and Background\n\nggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = sex)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(\n    legend.position = c(0.9, 0.8),\n    legend.background = element_rect(fill = \"white\", color = \"black\", linewidth = 0.5),\n    legend.title = element_text(face = \"bold\"),\n    legend.key.size = unit(1, \"cm\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n6.5.4 Panel Elements\n\nggplot(cattle, aes(x = breed, y = weight_gain_kg)) +\n  geom_boxplot(fill = \"lightblue\") +\n  theme_minimal() +\n  theme(\n    panel.background = element_rect(fill = \"lightyellow\"),\n    panel.grid.major = element_line(color = \"gray70\", linewidth = 0.5),\n    panel.grid.minor = element_line(color = \"gray90\", linewidth = 0.25),\n    panel.border = element_rect(fill = NA, color = \"black\", linewidth = 1)\n  ) +\n  labs(title = \"Custom panel appearance\")\n\n\n\n\n\n\n\n\n\n\n6.5.5 Plot Background and Margins\n\nggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = breed)) +\n  geom_violin(alpha = 0.7) +\n  theme_minimal() +\n  theme(\n    plot.background = element_rect(fill = \"#f0f0f0\"),\n    plot.margin = margin(t = 20, r = 30, b = 20, l = 20, unit = \"pt\"),\n    plot.title = element_text(hjust = 0.5, size = 16, face = \"bold\")\n  ) +\n  labs(title = \"Custom plot background and margins\")\n\n\n\n\n\n\n\n\n\n\n6.5.6 Axis Customization\n\nggplot(cattle, aes(x = initial_weight_kg, y = final_weight_kg)) +\n  geom_point(size = 3, alpha = 0.6) +\n  theme_minimal() +\n  theme(\n    axis.line = element_line(color = \"black\", linewidth = 1),\n    axis.ticks = element_line(color = \"black\", linewidth = 0.5),\n    axis.ticks.length = unit(0.25, \"cm\"),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  ) +\n  labs(title = \"Custom axis styling\")\n\n\n\n\n\n\n\n\n\n\n6.5.7 Creating a Custom Theme\nSave your customizations as a reusable theme:\n\ntheme_publication &lt;- function(base_size = 12) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      # Text elements\n      plot.title = element_text(size = base_size + 4, face = \"bold\", hjust = 0.5),\n      plot.subtitle = element_text(size = base_size + 2, hjust = 0.5),\n      axis.title = element_text(size = base_size + 2, face = \"bold\"),\n      axis.text = element_text(size = base_size),\n\n      # Legend\n      legend.position = \"bottom\",\n      legend.title = element_text(face = \"bold\"),\n\n      # Panel\n      panel.grid.minor = element_blank(),\n      panel.border = element_rect(fill = NA, color = \"black\", linewidth = 0.5),\n\n      # Background\n      plot.background = element_rect(fill = \"white\", color = NA)\n    )\n}\n\n# Use custom theme\nggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = treatment)) +\n  geom_boxplot() +\n  labs(\n    title = \"Weight Gain by Breed and Treatment\",\n    subtitle = \"Using custom publication theme\",\n    y = \"Weight Gain (kg)\"\n  ) +\n  theme_publication()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipSave Your Theme\n\n\n\nSave custom themes in your R scripts or packages:\n# In your setup chunk\ntheme_publication &lt;- function(...) { ... }\ntheme_set(theme_publication())  # Set as default for all plots",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#advanced-color-palettes",
    "href": "chapters/ch06-ggplot2_advanced.html#advanced-color-palettes",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.6 Advanced Color Palettes",
    "text": "6.6 Advanced Color Palettes\nChapter 5 covered basic color scales. Now we‚Äôll explore professional color palettes.\n\n6.6.1 ColorBrewer Palettes\nColorBrewer provides carefully designed color schemes for different data types.\n\n6.6.1.1 Qualitative Palettes (Categorical Data)\n\nlibrary(RColorBrewer)\n\n# Display available ColorBrewer palettes\ndisplay.brewer.all()\n\n\n\n\n\n\n\n# Use Set1 palette\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"ColorBrewer Set1\")\n\n\n\n\n\n\n\n# Use Dark2 palette\nggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = breed)) +\n  geom_violin() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  theme(legend.position = \"none\") +\n  labs(title = \"ColorBrewer Dark2\")\n\n\n\n\n\n\n\n\n\n\n6.6.1.2 Sequential Palettes (Ordered Data)\n\n# Create ordered variable\ncattle &lt;- cattle %&gt;%\n  mutate(weight_category = cut(final_weight_kg,\n                                 breaks = c(0, 490, 510, 600),\n                                 labels = c(\"Light\", \"Medium\", \"Heavy\")))\n\nggplot(cattle, aes(x = breed, fill = weight_category)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_brewer(palette = \"Blues\", direction = 1) +\n  labs(\n    title = \"Weight Categories by Breed\",\n    y = \"Proportion\",\n    fill = \"Weight Category\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.6.1.3 Diverging Palettes\nGood for data with a meaningful midpoint:\n\n# Calculate deviation from mean\ncattle &lt;- cattle %&gt;%\n  mutate(gain_deviation = weight_gain_kg - mean(weight_gain_kg))\n\nggplot(cattle, aes(x = animal_id, y = 1, fill = gain_deviation)) +\n  geom_tile() +\n  scale_fill_distiller(palette = \"RdBu\", direction = -1) +\n  theme_void() +\n  theme(\n    legend.position = \"bottom\",\n    axis.text.x = element_text(angle = 90, hjust = 1)\n  ) +\n  labs(\n    title = \"Deviation from Mean Weight Gain\",\n    fill = \"Deviation (kg)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n6.6.2 Viridis Palettes (Colorblind-Friendly)\nViridis palettes are: - Colorblind-friendly - Perceptually uniform - Print well in grayscale\n\n# Discrete version\np1 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_viridis_d(option = \"viridis\") +\n  labs(title = \"Viridis (discrete)\")\n\np2 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_viridis_d(option = \"plasma\") +\n  labs(title = \"Plasma\")\n\np3 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_viridis_d(option = \"inferno\") +\n  labs(title = \"Inferno\")\n\np4 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_viridis_d(option = \"magma\") +\n  labs(title = \"Magma\")\n\n(p1 + p2) / (p3 + p4)\n\n\n\n\n\n\n\n\n\n6.6.2.1 Continuous Viridis\n\nggplot(cattle, aes(x = breed, y = sex, fill = weight_gain_kg)) +\n  geom_tile(color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(option = \"plasma\") +\n  labs(\n    title = \"Weight Gain Heatmap\",\n    fill = \"Weight Gain (kg)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantAlways Consider Colorblindness\n\n\n\nApproximately 8% of men and 0.5% of women have some form of color blindness. Use: - Viridis palettes - ColorBrewer palettes designed for colorblindness - Sufficient contrast - Shape or pattern in addition to color when possible\n\n\n\n\n\n6.6.3 Manual Color Specification\nFor precise control, specify colors manually:\n\n# Define custom colors\nbreed_colors &lt;- c(\n  \"Angus\" = \"#8B4513\",      # Brown\n  \"Hereford\" = \"#CD853F\",   # Tan\n  \"Charolais\" = \"#F5DEB3\"   # Wheat\n)\n\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 4) +\n  scale_color_manual(values = breed_colors) +\n  labs(title = \"Custom brand colors\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#text-and-annotations",
    "href": "chapters/ch06-ggplot2_advanced.html#text-and-annotations",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.7 Text and Annotations",
    "text": "6.7 Text and Annotations\nAdding text and annotations helps highlight important features in your plots.\n\n6.7.1 geom_text() and geom_label()\nAdd text for each data point:\n\n# Select a few animals to label\nlabeled_cattle &lt;- cattle %&gt;%\n  filter(animal_id %in% c(\"C001\", \"C011\", \"C020\"))\n\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_point(data = labeled_cattle, size = 4, color = \"red\") +\n  geom_text(data = labeled_cattle, aes(label = animal_id),\n            nudge_x = 5, nudge_y = 5, size = 4) +\n  labs(title = \"geom_text() - labels without boxes\")\n\n\n\n\n\n\n\n# With boxes (geom_label)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_point(data = labeled_cattle, size = 4, color = \"red\") +\n  geom_label(data = labeled_cattle, aes(label = animal_id),\n             nudge_x = 5, nudge_y = 5, size = 4) +\n  labs(title = \"geom_label() - labels with boxes\")\n\n\n\n\n\n\n\n\n\n\n6.7.2 Non-Overlapping Labels with ggrepel\nggrepel automatically adjusts label positions to avoid overlaps:\n\n# First install: install.packages(\"ggrepel\")\nlibrary(ggrepel)\n\n# Label all cattle (geom_text would overlap badly)\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, label = animal_id)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_text_repel(size = 3, max.overlaps = 10) +\n  labs(title = \"geom_text_repel() prevents overlaps\")\n\n\n\n\n\n\n\nNoteInstalling ggrepel\n\n\n\nThe ggrepel package is optional but very useful. Install it with:\ninstall.packages(\"ggrepel\")\n\n\n\n\n6.7.3 annotate() for Custom Annotations\nAdd text, shapes, or lines that aren‚Äôt tied to data:\n\nggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  # Add text annotation\n  annotate(\"text\", x = 305, y = 270, label = \"High performers\",\n           size = 5, fontface = \"bold\") +\n  # Add arrow\n  annotate(\"segment\", x = 303, xend = 298, y = 268, yend = 260,\n           arrow = arrow(length = unit(0.3, \"cm\")), linewidth = 1) +\n  # Add rectangle\n  annotate(\"rect\", xmin = 295, xmax = 308, ymin = 255, ymax = 275,\n           alpha = 0.2, fill = \"yellow\") +\n  labs(title = \"Using annotate() for custom elements\")\n\n\n\n\n\n\n\n\n\n\n6.7.4 Highlighting Regions\n\n# Highlight a target zone\nggplot(cattle, aes(x = initial_weight_kg, y = final_weight_kg)) +\n  # Target zone (add first so it's behind points)\n  annotate(\"rect\", xmin = 280, xmax = 295, ymin = 510, ymax = 540,\n           alpha = 0.2, fill = \"green\") +\n  annotate(\"text\", x = 287.5, y = 545, label = \"Target Range\",\n           fontface = \"bold\", size = 4) +\n  # Data points\n  geom_point(aes(color = breed), size = 3) +\n  labs(\n    title = \"Target Weight Range\",\n    x = \"Initial Weight (kg)\",\n    y = \"Final Weight (kg)\"\n  )",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#combining-plots",
    "href": "chapters/ch06-ggplot2_advanced.html#combining-plots",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.8 Combining Plots",
    "text": "6.8 Combining Plots\nCreating multi-panel figures is essential for publications. We‚Äôll use two packages: patchwork and cowplot.\n\n6.8.1 The patchwork Package\npatchwork makes combining plots incredibly easy with intuitive operators.\n\n6.8.1.1 Basic Operators\n\n# Create individual plots\np1 &lt;- ggplot(cattle, aes(x = breed, fill = breed)) +\n  geom_bar() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Count by Breed\", y = \"Count\")\n\np2 &lt;- ggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = breed)) +\n  geom_boxplot() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Weight Gain by Breed\", y = \"Weight Gain (kg)\")\n\np3 &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 3) +\n  theme_minimal() +\n  labs(title = \"Initial Weight vs Gain\")\n\n# Side by side with +\np1 + p2\n\n\n\n\n\n\n\n# Stack vertically with /\np1 / p2\n\n\n\n\n\n\n\n# Complex layouts\n(p1 + p2) / p3\n\n\n\n\n\n\n\n\n\n\n6.8.1.2 More Complex Layouts\n\np4 &lt;- ggplot(cattle, aes(x = treatment, fill = treatment)) +\n  geom_bar() +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  labs(title = \"Treatment Distribution\")\n\n# | for side-by-side (same as +)\np1 | p2 | p4\n\n\n\n\n\n\n\n# Nested layouts\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n# One plot taking more space\np1 + p2 + p3 + plot_layout(widths = c(2, 1, 1))\n\n\n\n\n\n\n\n\n\n\n6.8.1.3 Collecting Legends\n\n# All plots have their own legends (cluttered)\np1_legend &lt;- p1 + theme(legend.position = \"right\")\np2_legend &lt;- p2 + theme(legend.position = \"right\")\np3_legend &lt;- p3 + theme(legend.position = \"right\")\n\n(p1_legend + p2_legend + p3_legend)\n\n\n\n\n\n\n\n# Collect legends into one\n(p1_legend + p2_legend + p3_legend) +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\n\n6.8.1.4 Adding Plot Labels\n\n# Add A, B, C labels\n(p1 + p2) / p3 +\n  plot_annotation(\n    tag_levels = \"A\",\n    title = \"Cattle Weight Analysis\",\n    subtitle = \"Three perspectives on weight gain data\",\n    caption = \"Data from 20 cattle across 3 breeds\"\n  )\n\n\n\n\n\n\n\n\n\n\n6.8.1.5 Controlling Dimensions\n\n# Control relative widths and heights\n(p1 + p2) / p3 +\n  plot_layout(heights = c(1, 2))  # Bottom plot twice as tall\n\n\n\n\n\n\n\np1 + p2 + p3 +\n  plot_layout(widths = c(2, 1, 1))  # First plot twice as wide\n\n\n\n\n\n\n\n\n\n\n\n6.8.2 The cowplot Package\ncowplot provides more control over plot alignment and layouts.\n\n6.8.2.1 Basic plot_grid()\n\nlibrary(cowplot)\n\n# Simple grid\nplot_grid(p1, p2, p3, ncol = 2, labels = c(\"A\", \"B\", \"C\"))\n\n\n\n\n\n\n\n# Adjust relative sizes\nplot_grid(\n  p1, p2, p3,\n  ncol = 2,\n  rel_widths = c(1, 1.5),\n  rel_heights = c(1, 1.2),\n  labels = \"AUTO\"\n)\n\n\n\n\n\n\n\n\n\n\n6.8.2.2 Aligning Plots\n\n# Align axes\nplot_grid(p1, p2, ncol = 1, align = \"v\")\n\n\n\n\n\n\n\n# Align both axes\nplot_grid(p1, p2, p3, ncol = 3, align = \"hv\")\n\n\n\n\n\n\n\n\n\n\n6.8.2.3 Complex Layouts with Nesting\n\n# Create nested layout\ntop_row &lt;- plot_grid(p1, p2, ncol = 2, labels = c(\"A\", \"B\"))\nbottom_row &lt;- plot_grid(p3, p4, ncol = 2, labels = c(\"C\", \"D\"))\n\nplot_grid(top_row, bottom_row, ncol = 1, rel_heights = c(1, 1.2))\n\n\n\n\n\n\n\n\n\n\n\n6.8.3 patchwork vs cowplot\n\n\n\n\n\n\n\n\nFeature\npatchwork\ncowplot\n\n\n\n\nSyntax\nVery intuitive (+, /, \\|)\nFunction-based (plot_grid())\n\n\nQuick layouts\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê\n\n\nComplex control\n‚≠ê‚≠ê‚≠ê‚≠ê\n‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n\n\nLearning curve\nEasy\nModerate\n\n\nLegend handling\nExcellent\nGood\n\n\n\n\n\n\n\n\n\nTipWhich Package to Use?\n\n\n\n\npatchwork: For most cases ‚Äî intuitive and quick\ncowplot: When you need precise control over alignment and spacing\nBoth: They work well together! Use patchwork for layout, cowplot for fine-tuning",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#publication-ready-workflow",
    "href": "chapters/ch06-ggplot2_advanced.html#publication-ready-workflow",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.9 Publication-Ready Workflow",
    "text": "6.9 Publication-Ready Workflow\nCreating journal-quality figures requires attention to detail. Here‚Äôs a complete workflow.\n\n6.9.1 Checklist for Publication Figures\n‚úÖ Data Quality - [ ] Data cleaned and verified - [ ] Outliers investigated - [ ] Sample sizes appropriate\n‚úÖ Plot Choice - [ ] Plot type appropriate for data - [ ] Multiple views if needed (e.g., raw data + summary)\n‚úÖ Visual Elements - [ ] Axis labels clear and include units - [ ] Title informative (or caption in figure legend) - [ ] Legend easy to understand - [ ] Colors colorblind-friendly - [ ] Text readable at publication size\n‚úÖ Statistics - [ ] Error bars clearly defined (SE, SD, CI?) - [ ] Sample sizes shown or stated - [ ] Statistical tests results shown if relevant\n‚úÖ Style - [ ] Font size: 8-12pt in final figure - [ ] Line widths: 0.5-1pt - [ ] Resolution: 300+ DPI - [ ] Format: PDF (vector) or high-res PNG\n\n\n6.9.2 Complete Example: Publication Figure\n\n# Step 1: Prepare data with summary statistics\ncattle_stats &lt;- cattle %&gt;%\n  group_by(breed, treatment) %&gt;%\n  summarise(\n    n = n(),\n    mean_gain = mean(weight_gain_kg),\n    se_gain = sd(weight_gain_kg) / sqrt(n()),\n    .groups = \"drop\"\n  )\n\n# Step 2: Create individual panels\n\n# Panel A: Raw data with trend lines\npanel_a &lt;- ggplot(cattle, aes(x = initial_weight_kg, y = weight_gain_kg, color = breed)) +\n  geom_point(size = 2, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1) +\n  scale_color_viridis_d(option = \"plasma\") +\n  labs(\n    title = \"Individual data with linear trends\",\n    x = \"Initial Weight (kg)\",\n    y = \"Weight Gain (kg)\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    plot.title = element_text(size = 10, face = \"bold\"),\n    legend.position = c(0.85, 0.2),\n    panel.border = element_rect(fill = NA, color = \"black\", linewidth = 0.5)\n  )\n\n# Panel B: Summary by breed\npanel_b &lt;- ggplot(cattle, aes(x = breed, y = weight_gain_kg, fill = breed)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.2, size = 1.5, alpha = 0.4) +\n  scale_fill_viridis_d(option = \"plasma\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"red\") +\n  labs(\n    title = \"Distribution by breed\",\n    x = \"Breed\",\n    y = \"Weight Gain (kg)\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    plot.title = element_text(size = 10, face = \"bold\"),\n    legend.position = \"none\",\n    panel.border = element_rect(fill = NA, color = \"black\", linewidth = 0.5)\n  )\n\n# Panel C: Interaction between breed and treatment\npanel_c &lt;- ggplot(cattle_stats, aes(x = breed, y = mean_gain, fill = treatment)) +\n  geom_col(position = \"dodge\", alpha = 0.8) +\n  geom_errorbar(\n    aes(ymin = mean_gain - se_gain, ymax = mean_gain + se_gain),\n    position = position_dodge(width = 0.9),\n    width = 0.25\n  ) +\n  geom_text(\n    aes(label = paste0(\"n=\", n)),\n    position = position_dodge(width = 0.9),\n    vjust = -0.5,\n    size = 2.5\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Breed √ó Treatment interaction\",\n    x = \"Breed\",\n    y = \"Mean Weight Gain ¬± SE (kg)\",\n    fill = \"Treatment\"\n  ) +\n  theme_minimal(base_size = 10) +\n  theme(\n    plot.title = element_text(size = 10, face = \"bold\"),\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 9),\n    legend.text = element_text(size = 8),\n    panel.border = element_rect(fill = NA, color = \"black\", linewidth = 0.5)\n  )\n\n# Step 3: Combine with patchwork\nfinal_figure &lt;- (panel_a + panel_b) / panel_c +\n  plot_annotation(\n    tag_levels = \"A\",\n    title = \"Weight gain response to diet treatments in three cattle breeds\",\n    caption = \"Figure 1. (A) Initial weight vs weight gain showing breed-specific responses. (B) Distribution of weight gain by breed; red diamonds show means. (C) Treatment effects within each breed; error bars show ¬±SE, sample sizes indicated above bars.\",\n    theme = theme(\n      plot.title = element_text(size = 12, face = \"bold\", hjust = 0),\n      plot.caption = element_text(size = 8, hjust = 0)\n    )\n  )\n\nfinal_figure\n\n\n\n\n\n\n\n# Step 4: Save for publication\n# ggsave(\"figures/figure1_cattle_weight_gain.pdf\",\n#        plot = final_figure,\n#        width = 7, height = 7, dpi = 300)\n\n\n\n6.9.3 Journal-Specific Requirements\nDifferent journals have different requirements. Common specifications:\nNature/Science: - Size: 89 mm (single column) or 183 mm (double column) - Format: PDF or EPS (vector) - Font: Arial, 5-7 pt minimum - Resolution: 300+ DPI\nPLOS: - Size: Up to 190 mm width - Format: TIFF, EPS, or PDF - Font: 8-12 pt - Resolution: 300-600 DPI\nJournal of Animal Science: - Size: 3.5‚Äù (single) or 7‚Äù (double column) - Format: TIFF, EPS, PDF - Font: Times or Arial, 8 pt minimum - Resolution: 300 DPI minimum\n\n\n\n\n\n\nImportantAlways Check Journal Guidelines\n\n\n\nBefore creating final figures: 1. Check journal‚Äôs ‚ÄúInstructions for Authors‚Äù 2. Look at recently published figures in that journal 3. Verify file format, dimensions, and resolution 4. Test that fonts are readable at final size",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#summary",
    "href": "chapters/ch06-ggplot2_advanced.html#summary",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.10 Summary",
    "text": "6.10 Summary\nThis chapter covered advanced ggplot2 techniques for creating publication-quality figures:\n\n6.10.1 Key Concepts\n\nFaceting allows you to create small multiples for comparing groups:\n\nfacet_wrap() for one variable\nfacet_grid() for two variables (rows √ó columns)\nControl scales with scales = \"free\", \"fixed\", etc.\n\nStatistical layers add computed values:\n\ngeom_smooth() for trend lines\nstat_summary() for custom summaries\ngeom_errorbar(), geom_pointrange() for uncertainty\n\nTheme customization with theme():\n\nText elements: element_text()\nLines: element_line()\nRectangles: element_rect()\nRemove: element_blank()\n\nColor palettes:\n\nColorBrewer for qualitative, sequential, and diverging data\nViridis for colorblind-friendly, perceptually uniform colors\nManual specification for precise control\n\nText and annotations:\n\ngeom_text() and geom_label() for labeling data points\ngeom_text_repel() to avoid overlaps\nannotate() for custom text, shapes, and arrows\n\nCombining plots:\n\npatchwork: Intuitive operators (+, /, |)\ncowplot: Precise control with plot_grid()\nCollect legends, add labels, control dimensions\n\nPublication workflow:\n\nCheck journal requirements early\nUse appropriate dimensions and resolution\nEnsure text is readable at final size\nSave as PDF (vector) when possible",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#week-6-homework-replicating-a-journal-figure",
    "href": "chapters/ch06-ggplot2_advanced.html#week-6-homework-replicating-a-journal-figure",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.11 Week 6 Homework: Replicating a Journal Figure",
    "text": "6.11 Week 6 Homework: Replicating a Journal Figure\n\n6.11.1 Assignment Overview\nYour task is to replicate a multi-panel figure from a published paper using advanced ggplot2 techniques. This will test your ability to combine everything you‚Äôve learned in Chapters 5 and 6.\n\n\n6.11.2 Part 1: Select a Figure (0 points, but required)\nFind a multi-panel figure (2-4 panels) from a published paper in animal science, agriculture, or related field. The figure should include: - At least 2 different plot types - Multiple groups or categories - Clear data patterns to replicate\nSubmit: PDF of the original figure with citation\n\n\n6.11.3 Part 2: Generate Similar Data (10 points)\nCreate simulated data that will produce similar patterns to the published figure. Your data should: - Have similar sample sizes - Show similar trends or patterns - Include appropriate grouping variables\nSubmit: R code creating your dataset and showing head() and summary()\n\n\n6.11.4 Part 3: Replicate Panels (50 points)\nCreate each panel of the figure using ggplot2. Requirements: - Panel types: Match the original plot types (scatter, bar, box, etc.) - Faceting: Use if present in original - Statistical layers: Include trend lines, error bars, or summaries as shown - Colors: Use colorblind-friendly palettes - Labels: Clear axis labels with units\nSubmit: Code creating each individual panel\n\n\n6.11.5 Part 4: Combine Panels (20 points)\nCombine your panels into a single multi-panel figure using patchwork or cowplot. Requirements: - Layout matches original figure structure - Panel labels (A, B, C, etc.) - Shared or collected legends - Overall title or caption\nSubmit: Code combining panels and the final combined figure\n\n\n6.11.6 Part 5: Polish for Publication (15 points)\nApply final touches: - Custom theme appropriate for publication - Consistent font sizes and styling across panels - Professional-looking overall appearance - Proper spacing and alignment\nSubmit: Final polished figure saved as PDF (300 DPI, 7‚Äù width)\n\n\n6.11.7 Part 6: Reflection (5 points)\nWrite 200-300 words addressing: - What was most challenging about replicating the figure? - What design choices did you make differently from the original? - What did you learn about creating publication-quality figures?\n\n\n6.11.8 Example Workflow\n\n# Part 2: Generate data\nset.seed(42)\nmy_data &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 30),\n  value1 = rnorm(90, mean = c(50, 55, 60), sd = 5),\n  value2 = value1 * 1.2 + rnorm(90, sd = 3)\n)\n\n# Part 3: Create panels\npanel_a &lt;- ggplot(my_data, aes(x = value1, y = value2, color = group)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  # ... more customization\n\npanel_b &lt;- ggplot(my_data, aes(x = group, y = value1, fill = group)) +\n  geom_boxplot() +\n  # ... more customization\n\n# Part 4: Combine\nlibrary(patchwork)\nfinal_fig &lt;- panel_a + panel_b +\n  plot_annotation(tag_levels = \"A\") +\n  plot_layout(guides = \"collect\") &\n  theme(legend.position = \"bottom\")\n\n# Part 5: Save\nggsave(\"final_figure.pdf\", final_fig, width = 7, height = 5, dpi = 300)\n\n\n\n6.11.9 Recommended YAML\n---\ntitle: \"Week 6 Homework: Replicating a Journal Figure\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 2\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n6.11.10 Grading Rubric\n\nPart 2: Data Generation (10%)\n\nData structure appropriate (5%)\nSimilar patterns to original (5%)\n\nPart 3: Individual Panels (50%)\n\nCorrect plot types (15%)\nStatistical layers correct (10%)\nColors and themes (10%)\nLabels and clarity (10%)\nOverall appearance (5%)\n\nPart 4: Combining Panels (20%)\n\nLayout matches original (8%)\nPanel labels correct (6%)\nLegends handled well (6%)\n\nPart 5: Publication Polish (15%)\n\nProfessional appearance (8%)\nConsistent styling (4%)\nAppropriate dimensions (3%)\n\nPart 6: Reflection (5%)\n\nThoughtful analysis of challenges and decisions\n\n\n\n\n6.11.11 Bonus (+10 points)\nAdd one panel that wasn‚Äôt in the original figure but provides additional insight into the data. Justify why you added it and what it shows.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch06-ggplot2_advanced.html#additional-resources",
    "href": "chapters/ch06-ggplot2_advanced.html#additional-resources",
    "title": "6¬† Advanced ggplot2 and Multi-Panel Plots",
    "section": "6.12 Additional Resources",
    "text": "6.12 Additional Resources\n\n6.12.1 Required Reading\n\nR for Data Science (2e) - Chapter 11: Communication\nggplot2 book (3e) - Chapters 7-10 (Layers, Annotations, Networks)\npatchwork documentation\n\n\n\n6.12.2 Optional Reading\n\nWilke, C.O. (2019). Fundamentals of Data Visualization. O‚ÄôReilly. Free online\nTufte, E.R. (2001). The Visual Display of Quantitative Information (2nd ed.). Graphics Press.\nColorblind-Friendly Palettes by Paul Tol\n\n\n\n6.12.3 Videos\n\n‚ÄúThe Grammar of Graphics‚Äù by Hadley Wickham (RStudio Conference)\n‚Äúggplot2 Workshop Part 2: Customization‚Äù by Thomas Lin Pedersen\n‚ÄúMaking Beautiful Figures in R‚Äù by Rafael Irizarry\n\n\n\n6.12.4 Cheat Sheets\n\nggplot2 Extensions ‚≠ê\npatchwork Cheat Sheet\ncowplot Vignettes\n\n\n\n6.12.5 Color Resources\n\nColorBrewer ‚Äî Test color schemes\nCoolors ‚Äî Generate palettes\nCoblis ‚Äî Simulate colorblindness\nViz Palette ‚Äî Test palette accessibility\n\n\n\n6.12.6 Galleries and Examples\n\nR Graph Gallery: ggplot2 section\nTop 50 ggplot2 Visualizations\nBBC Visual and Data Journalism cookbook for R graphics\n\n\n\n6.12.7 Packages Worth Exploring\n\nggpubr ‚Äî Publication-ready plots\nggthemes ‚Äî Additional themes\ngganimate ‚Äî Animated plots\nplotly ‚Äî Interactive plots\nggridges ‚Äî Ridgeline plots\n\n\n\n6.12.8 Useful Websites\n\nggplot2 Tidyverse Page\nStack Overflow: ggplot2 tag\nRStudio Community\nggplot2 GitHub Issues ‚Äî Report bugs or request features\n\n\nNext Chapter: Data Reshaping, Joining, and Iteration",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Advanced ggplot2 and Multi-Panel Plots</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html",
    "href": "chapters/ch07-tidyr_joins_purrr.html",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "",
    "text": "7.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#learning-objectives",
    "href": "chapters/ch07-tidyr_joins_purrr.html#learning-objectives",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "",
    "text": "Understand and apply tidy data principles to organize datasets effectively\nReshape data between wide and long formats using pivot_longer() and pivot_wider()\nSplit and combine columns using separate() and unite()\nJoin multiple datasets using SQL-style joins (left_join(), inner_join(), etc.)\nUnderstand when to use different join types and filtering joins\nBind datasets together with bind_rows() and bind_cols()\nUse functional programming with purrr::map() for iteration\nWork with list columns and nested data structures",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#introduction",
    "href": "chapters/ch07-tidyr_joins_purrr.html#introduction",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nIn previous chapters, you learned to manipulate single datasets with dplyr. Real-world analysis often requires: - Reshaping data between different formats - Combining multiple related datasets - Iterating operations across many groups or files\nThis chapter teaches these essential data wrangling skills using tidyr for reshaping and purrr for iteration.\n\n7.2.1 Setup\n\nlibrary(tidyverse)  # Includes tidyr, dplyr, purrr\nlibrary(readxl)     # For reading Excel files\n\n# Set default theme for plots\ntheme_set(theme_minimal())",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#tidy-data-principles",
    "href": "chapters/ch07-tidyr_joins_purrr.html#tidy-data-principles",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.3 Tidy Data Principles",
    "text": "7.3 Tidy Data Principles\n\n7.3.1 What is Tidy Data?\nTidy data follows three interrelated rules:\n\nEach variable forms a column\nEach observation forms a row\nEach type of observational unit forms a table\n\n\n\n\n\n\n\nNoteHadley Wickham‚Äôs Tidy Data\n\n\n\nThe concept of tidy data comes from Hadley Wickham‚Äôs 2014 paper in the Journal of Statistical Software. Tidy data makes analysis easier because: - Variables are easy to access as columns - Observations are easy to filter as rows - Most tidyverse functions expect tidy data\n\n\n\n\n7.3.2 Tidy vs Untidy Data\nExample 1: Animal weight measurements\nUntidy (wide format):\n\n# Each time point is a separate column\nuntidy_weights &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  week_0 = c(45, 48, 43),\n  week_4 = c(52, 54, 50),\n  week_8 = c(58, 61, 56),\n  week_12 = c(64, 67, 62)\n)\n\nuntidy_weights\n\n# A tibble: 3 √ó 5\n  animal_id week_0 week_4 week_8 week_12\n  &lt;chr&gt;      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1 A001          45     52     58      64\n2 A002          48     54     61      67\n3 A003          43     50     56      62\n\n\nTidy (long format):\n\n# Each observation (animal √ó time) is a separate row\ntidy_weights &lt;- tibble(\n  animal_id = rep(c(\"A001\", \"A002\", \"A003\"), each = 4),\n  week = rep(c(0, 4, 8, 12), times = 3),\n  weight_kg = c(45, 52, 58, 64,  # A001\n                48, 54, 61, 67,  # A002\n                43, 50, 56, 62)  # A003\n)\n\ntidy_weights\n\n# A tibble: 12 √ó 3\n   animal_id  week weight_kg\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 A001          0        45\n 2 A001          4        52\n 3 A001          8        58\n 4 A001         12        64\n 5 A002          0        48\n 6 A002          4        54\n 7 A002          8        61\n 8 A002         12        67\n 9 A003          0        43\n10 A003          4        50\n11 A003          8        56\n12 A003         12        62\n\n\nWhy is tidy better?\n\n# Easy to plot with ggplot2\nggplot(tidy_weights, aes(x = week, y = weight_kg, color = animal_id)) +\n  geom_line(linewidth = 1) +\n  geom_point(size = 3) +\n  labs(title = \"Animal Growth Curves\",\n       x = \"Week\",\n       y = \"Weight (kg)\",\n       color = \"Animal ID\")\n\n\n\n\n\n\n\n# Easy to calculate summaries\ntidy_weights %&gt;%\n  group_by(week) %&gt;%\n  summarise(\n    mean_weight = mean(weight_kg),\n    sd_weight = sd(weight_kg)\n  )\n\n# A tibble: 4 √ó 3\n   week mean_weight sd_weight\n  &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1     0        45.3      2.52\n2     4        52        2   \n3     8        58.3      2.52\n4    12        64.3      2.52\n\n\n\n\n7.3.3 Common Messy Data Problems\n\nColumn headers are values, not variable names (like week_0, week_4)\nMultiple variables in one column (like ‚ÄúHolstein-Male‚Äù)\nVariables stored in both rows and columns\nMultiple types of observational units in the same table\nSingle observational unit stored in multiple tables\n\nThis chapter teaches how to fix these problems.",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#data-reshaping-with-tidyr",
    "href": "chapters/ch07-tidyr_joins_purrr.html#data-reshaping-with-tidyr",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.4 Data Reshaping with tidyr",
    "text": "7.4 Data Reshaping with tidyr\n\n7.4.1 pivot_longer(): Wide to Long\npivot_longer() transforms wide data (many columns) into long data (many rows).\nBasic syntax:\npivot_longer(\n  data,\n  cols = &lt;columns to pivot&gt;,\n  names_to = \"&lt;name for new column with old column names&gt;\",\n  values_to = \"&lt;name for new column with values&gt;\"\n)\n\n7.4.1.1 Example: Pivoting Weight Measurements\n\n# Start with wide format\nwide_weights &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  breed = c(\"Holstein\", \"Jersey\", \"Angus\"),\n  week_0 = c(45, 42, 48),\n  week_4 = c(52, 48, 54),\n  week_8 = c(58, 54, 61)\n)\n\nwide_weights\n\n# A tibble: 3 √ó 5\n  animal_id breed    week_0 week_4 week_8\n  &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 A001      Holstein     45     52     58\n2 A002      Jersey       42     48     54\n3 A003      Angus        48     54     61\n\n# Pivot to long format\nlong_weights &lt;- wide_weights %&gt;%\n  pivot_longer(\n    cols = starts_with(\"week\"),\n    names_to = \"time_point\",\n    values_to = \"weight_kg\"\n  )\n\nlong_weights\n\n# A tibble: 9 √ó 4\n  animal_id breed    time_point weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 A001      Holstein week_0            45\n2 A001      Holstein week_4            52\n3 A001      Holstein week_8            58\n4 A002      Jersey   week_0            42\n5 A002      Jersey   week_4            48\n6 A002      Jersey   week_8            54\n7 A003      Angus    week_0            48\n8 A003      Angus    week_4            54\n9 A003      Angus    week_8            61\n\n\n\n\n7.4.1.2 Selecting Columns to Pivot\nMultiple ways to specify which columns to pivot:\n\n# By name\nwide_weights %&gt;%\n  pivot_longer(\n    cols = c(week_0, week_4, week_8),\n    names_to = \"time_point\",\n    values_to = \"weight_kg\"\n  )\n\n# A tibble: 9 √ó 4\n  animal_id breed    time_point weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 A001      Holstein week_0            45\n2 A001      Holstein week_4            52\n3 A001      Holstein week_8            58\n4 A002      Jersey   week_0            42\n5 A002      Jersey   week_4            48\n6 A002      Jersey   week_8            54\n7 A003      Angus    week_0            48\n8 A003      Angus    week_4            54\n9 A003      Angus    week_8            61\n\n# By position\nwide_weights %&gt;%\n  pivot_longer(\n    cols = 3:5,\n    names_to = \"time_point\",\n    values_to = \"weight_kg\"\n  )\n\n# A tibble: 9 √ó 4\n  animal_id breed    time_point weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 A001      Holstein week_0            45\n2 A001      Holstein week_4            52\n3 A001      Holstein week_8            58\n4 A002      Jersey   week_0            42\n5 A002      Jersey   week_4            48\n6 A002      Jersey   week_8            54\n7 A003      Angus    week_0            48\n8 A003      Angus    week_4            54\n9 A003      Angus    week_8            61\n\n# Everything except ID columns\nwide_weights %&gt;%\n  pivot_longer(\n    cols = -c(animal_id, breed),\n    names_to = \"time_point\",\n    values_to = \"weight_kg\"\n  )\n\n# A tibble: 9 √ó 4\n  animal_id breed    time_point weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 A001      Holstein week_0            45\n2 A001      Holstein week_4            52\n3 A001      Holstein week_8            58\n4 A002      Jersey   week_0            42\n5 A002      Jersey   week_4            48\n6 A002      Jersey   week_8            54\n7 A003      Angus    week_0            48\n8 A003      Angus    week_4            54\n9 A003      Angus    week_8            61\n\n\n\n\n7.4.1.3 Cleaning Column Names During Pivot\nUse names_prefix to remove prefixes and names_transform to change types:\n\n# Remove \"week_\" prefix and convert to numeric\nwide_weights %&gt;%\n  pivot_longer(\n    cols = starts_with(\"week\"),\n    names_to = \"week\",\n    values_to = \"weight_kg\",\n    names_prefix = \"week_\",\n    names_transform = list(week = as.integer)\n  )\n\n# A tibble: 9 √ó 4\n  animal_id breed     week weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;int&gt;     &lt;dbl&gt;\n1 A001      Holstein     0        45\n2 A001      Holstein     4        52\n3 A001      Holstein     8        58\n4 A002      Jersey       0        42\n5 A002      Jersey       4        48\n6 A002      Jersey       8        54\n7 A003      Angus        0        48\n8 A003      Angus        4        54\n9 A003      Angus        8        61\n\n\n\n\n7.4.1.4 Multiple Value Columns\nWhat if you have multiple measurements at each time point?\n\n# Wide data with multiple measurements\nwide_multi &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\"),\n  weight_week0 = c(45, 42),\n  weight_week4 = c(52, 48),\n  height_week0 = c(80, 78),\n  height_week4 = c(85, 82)\n)\n\nwide_multi\n\n# A tibble: 2 √ó 5\n  animal_id weight_week0 weight_week4 height_week0 height_week4\n  &lt;chr&gt;            &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;        &lt;dbl&gt;\n1 A001                45           52           80           85\n2 A002                42           48           78           82\n\n# Alternative: create separate columns for each measurement\nwide_multi %&gt;%\n  pivot_longer(\n    cols = -animal_id,\n    names_to = c(\".value\", \"week\"),\n    names_pattern = \"(.+)_week(.+)\"\n  )\n\n# A tibble: 4 √ó 4\n  animal_id week  weight height\n  &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 A001      0         45     80\n2 A001      4         52     85\n3 A002      0         42     78\n4 A002      4         48     82\n\n\n\n\n\n\n\n\nTipnames_pattern with Regular Expressions\n\n\n\nUse names_pattern with regex to extract parts of column names: - .+ matches one or more characters - (.+) creates a capture group - week(.+) matches ‚Äúweek‚Äù followed by captured characters\n\n\n\n\n\n7.4.2 pivot_wider(): Long to Wide\npivot_wider() transforms long data (many rows) into wide data (many columns). It‚Äôs the opposite of pivot_longer().\nBasic syntax:\npivot_wider(\n  data,\n  names_from = &lt;column with names for new columns&gt;,\n  values_from = &lt;column with values to fill new columns&gt;\n)\n\n7.4.2.1 Example: Creating a Summary Table\n\n# Long format data\nanimals_long &lt;- tibble(\n  animal_id = rep(c(\"A001\", \"A002\", \"A003\"), each = 3),\n  breed = rep(c(\"Holstein\", \"Jersey\", \"Angus\"), each = 3),\n  treatment = rep(c(\"Control\", \"Treatment_A\", \"Treatment_B\"), times = 3),\n  weight_gain_kg = c(15, 22, 18, 12, 18, 15, 16, 21, 17)\n)\n\nanimals_long\n\n# A tibble: 9 √ó 4\n  animal_id breed    treatment   weight_gain_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;chr&gt;                &lt;dbl&gt;\n1 A001      Holstein Control                 15\n2 A001      Holstein Treatment_A             22\n3 A001      Holstein Treatment_B             18\n4 A002      Jersey   Control                 12\n5 A002      Jersey   Treatment_A             18\n6 A002      Jersey   Treatment_B             15\n7 A003      Angus    Control                 16\n8 A003      Angus    Treatment_A             21\n9 A003      Angus    Treatment_B             17\n\n# Pivot wider: treatments become columns\nanimals_wide &lt;- animals_long %&gt;%\n  pivot_wider(\n    names_from = treatment,\n    values_from = weight_gain_kg\n  )\n\nanimals_wide\n\n# A tibble: 3 √ó 5\n  animal_id breed    Control Treatment_A Treatment_B\n  &lt;chr&gt;     &lt;chr&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 A001      Holstein      15          22          18\n2 A002      Jersey        12          18          15\n3 A003      Angus         16          21          17\n\n\n\n\n7.4.2.2 Handling Multiple Value Columns\n\n# Long data with multiple measurements\nmeasurements_long &lt;- tibble(\n  animal_id = rep(c(\"A001\", \"A002\"), each = 4),\n  week = rep(c(0, 4, 8, 12), times = 2),\n  weight_kg = c(45, 52, 58, 64, 42, 48, 54, 60),\n  height_cm = c(80, 85, 88, 91, 78, 82, 85, 88)\n)\n\nmeasurements_long\n\n# A tibble: 8 √ó 4\n  animal_id  week weight_kg height_cm\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 A001          0        45        80\n2 A001          4        52        85\n3 A001          8        58        88\n4 A001         12        64        91\n5 A002          0        42        78\n6 A002          4        48        82\n7 A002          8        54        85\n8 A002         12        60        88\n\n# Pivot wider: weeks become columns for each measurement\nmeasurements_long %&gt;%\n  pivot_wider(\n    names_from = week,\n    values_from = c(weight_kg, height_cm),\n    names_glue = \"{.value}_week{week}\"\n  )\n\n# A tibble: 2 √ó 9\n  animal_id weight_kg_week0 weight_kg_week4 weight_kg_week8 weight_kg_week12\n  &lt;chr&gt;               &lt;dbl&gt;           &lt;dbl&gt;           &lt;dbl&gt;            &lt;dbl&gt;\n1 A001                   45              52              58               64\n2 A002                   42              48              54               60\n# ‚Ñπ 4 more variables: height_cm_week0 &lt;dbl&gt;, height_cm_week4 &lt;dbl&gt;,\n#   height_cm_week8 &lt;dbl&gt;, height_cm_week12 &lt;dbl&gt;\n\n\n\n\n7.4.2.3 Dealing with Duplicates\nWhat if you have duplicate id-time combinations?\n\n# Data with duplicates\nduplicated_data &lt;- tibble(\n  animal_id = c(\"A001\", \"A001\", \"A002\", \"A002\"),\n  week = c(0, 0, 0, 0),  # Same animal measured twice\n  weight_kg = c(45, 46, 42, 43)\n)\n\nduplicated_data\n\n# A tibble: 4 √ó 3\n  animal_id  week weight_kg\n  &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 A001          0        45\n2 A001          0        46\n3 A002          0        42\n4 A002          0        43\n\n# Summarize first to remove duplicates\nduplicated_data %&gt;%\n  group_by(animal_id, week) %&gt;%\n  summarise(weight_kg = mean(weight_kg), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = week,\n    values_from = weight_kg\n  )\n\n# A tibble: 2 √ó 2\n  animal_id   `0`\n  &lt;chr&gt;     &lt;dbl&gt;\n1 A001       45.5\n2 A002       42.5\n\n\n\n\n\n\n\n\nWarningWatch Out for Duplicates!\n\n\n\npivot_wider() creates list-columns when there are multiple values for the same id √ó names_from combination. Either: - Summarize first (take mean, max, etc.) - Keep as list-column and unnest() later - Check your data for unexpected duplicates\n\n\n\n\n\n7.4.3 separate(): Split One Column into Many\nseparate() splits a single column into multiple columns based on a separator.\nBasic syntax:\nseparate(\n  data,\n  col = &lt;column to split&gt;,\n  into = c(\"&lt;new_col1&gt;\", \"&lt;new_col2&gt;\"),\n  sep = \"&lt;separator&gt;\"\n)\n\n7.4.3.1 Example: Splitting Composite IDs\n\n# Data with composite identifiers\nanimals_composite &lt;- tibble(\n  animal_id = c(\"Holstein_M_001\", \"Jersey_F_002\", \"Angus_M_003\"),\n  weight_kg = c(550, 420, 580)\n)\n\nanimals_composite\n\n# A tibble: 3 √ó 2\n  animal_id      weight_kg\n  &lt;chr&gt;              &lt;dbl&gt;\n1 Holstein_M_001       550\n2 Jersey_F_002         420\n3 Angus_M_003          580\n\n# Separate into components\nanimals_composite %&gt;%\n  separate(\n    col = animal_id,\n    into = c(\"breed\", \"sex\", \"id_number\"),\n    sep = \"_\"\n  )\n\n# A tibble: 3 √ó 4\n  breed    sex   id_number weight_kg\n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Holstein M     001             550\n2 Jersey   F     002             420\n3 Angus    M     003             580\n\n\n\n\n7.4.3.2 Automatic Type Conversion\n\n# Separate and convert types automatically\nanimals_composite %&gt;%\n  separate(\n    col = animal_id,\n    into = c(\"breed\", \"sex\", \"id_number\"),\n    sep = \"_\",\n    convert = TRUE  # Automatically converts to appropriate types\n  )\n\n# A tibble: 3 √ó 4\n  breed    sex   id_number weight_kg\n  &lt;chr&gt;    &lt;chr&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 Holstein M             1       550\n2 Jersey   F             2       420\n3 Angus    M             3       580\n\n\n\n\n7.4.3.3 Keeping the Original Column\n\n# Keep original column\nanimals_composite %&gt;%\n  separate(\n    col = animal_id,\n    into = c(\"breed\", \"sex\", \"id_number\"),\n    sep = \"_\",\n    remove = FALSE  # Don't remove original column\n  )\n\n# A tibble: 3 √ó 5\n  animal_id      breed    sex   id_number weight_kg\n  &lt;chr&gt;          &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 Holstein_M_001 Holstein M     001             550\n2 Jersey_F_002   Jersey   F     002             420\n3 Angus_M_003    Angus    M     003             580\n\n\n\n\n\n7.4.4 unite(): Combine Columns into One\nunite() is the opposite of separate() ‚Äî it combines multiple columns into one.\nBasic syntax:\nunite(\n  data,\n  col = \"&lt;new_column_name&gt;\",\n  ...,\n  sep = \"&lt;separator&gt;\"\n)\n\n7.4.4.1 Example: Creating Unique IDs\n\n# Separate columns\nanimals_separated &lt;- tibble(\n  farm = c(\"North\", \"South\", \"East\"),\n  breed = c(\"Holstein\", \"Jersey\", \"Angus\"),\n  id_num = c(1, 2, 3),\n  weight_kg = c(550, 420, 580)\n)\n\nanimals_separated\n\n# A tibble: 3 √ó 4\n  farm  breed    id_num weight_kg\n  &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 North Holstein      1       550\n2 South Jersey        2       420\n3 East  Angus         3       580\n\n# Unite into single ID\nanimals_separated %&gt;%\n  unite(\n    col = \"animal_id\",\n    farm, breed, id_num,\n    sep = \"_\"\n  )\n\n# A tibble: 3 √ó 2\n  animal_id        weight_kg\n  &lt;chr&gt;                &lt;dbl&gt;\n1 North_Holstein_1       550\n2 South_Jersey_2         420\n3 East_Angus_3           580\n\n\n\n\n7.4.4.2 Keeping Original Columns\n\n# Keep original columns\nanimals_separated %&gt;%\n  unite(\n    col = \"animal_id\",\n    farm, breed, id_num,\n    sep = \"_\",\n    remove = FALSE\n  )\n\n# A tibble: 3 √ó 5\n  animal_id        farm  breed    id_num weight_kg\n  &lt;chr&gt;            &lt;chr&gt; &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 North_Holstein_1 North Holstein      1       550\n2 South_Jersey_2   South Jersey        2       420\n3 East_Angus_3     East  Angus         3       580\n\n\n\n\n\n7.4.5 When to Use Each Function\n\n\n\n\n\n\n\n\nGoal\nFunction\nExample\n\n\n\n\nMany columns ‚Üí few rows\npivot_longer()\nweek_0, week_4, week_8 ‚Üí week column\n\n\nFew columns ‚Üí many rows\npivot_wider()\nweek column ‚Üí week_0, week_4, week_8\n\n\nOne column ‚Üí many columns\nseparate()\n‚ÄúHolstein_M_001‚Äù ‚Üí breed, sex, id\n\n\nMany columns ‚Üí one column\nunite()\nbreed, sex, id ‚Üí ‚ÄúHolstein_M_001‚Äù",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#joining-datasets",
    "href": "chapters/ch07-tidyr_joins_purrr.html#joining-datasets",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.5 Joining Datasets",
    "text": "7.5 Joining Datasets\nIn real-world analysis, data is often spread across multiple tables. Joins combine tables based on shared keys.\n\n7.5.1 Relational Data Example\n\n# Table 1: Animal information\nanimals &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\", \"A004\"),\n  breed = c(\"Holstein\", \"Jersey\", \"Angus\", \"Holstein\"),\n  birth_date = as.Date(c(\"2023-01-15\", \"2023-02-20\", \"2023-01-10\", \"2023-03-05\"))\n)\n\n# Table 2: Weight records\nweights &lt;- tibble(\n  animal_id = c(\"A001\", \"A001\", \"A002\", \"A002\", \"A005\"),\n  date = as.Date(c(\"2023-06-01\", \"2023-09-01\", \"2023-06-01\", \"2023-09-01\", \"2023-06-01\")),\n  weight_kg = c(180, 220, 150, 185, 200)\n)\n\n# Table 3: Treatment assignments\ntreatments &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  treatment = c(\"Control\", \"Treatment_A\", \"Treatment_B\")\n)\n\nanimals\n\n# A tibble: 4 √ó 3\n  animal_id breed    birth_date\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;    \n1 A001      Holstein 2023-01-15\n2 A002      Jersey   2023-02-20\n3 A003      Angus    2023-01-10\n4 A004      Holstein 2023-03-05\n\nweights\n\n# A tibble: 5 √ó 3\n  animal_id date       weight_kg\n  &lt;chr&gt;     &lt;date&gt;         &lt;dbl&gt;\n1 A001      2023-06-01       180\n2 A001      2023-09-01       220\n3 A002      2023-06-01       150\n4 A002      2023-09-01       185\n5 A005      2023-06-01       200\n\ntreatments\n\n# A tibble: 3 √ó 2\n  animal_id treatment  \n  &lt;chr&gt;     &lt;chr&gt;      \n1 A001      Control    \n2 A002      Treatment_A\n3 A003      Treatment_B\n\n\n\n\n7.5.2 Types of Joins\n\n7.5.2.1 Mutating Joins: Add Columns\nVisual representation:\n\n\n\n\n\nflowchart TD\n    A[Mutating Joins&lt;br/&gt;Add columns from y to x] --&gt; B[left_join]\n    A --&gt; C[right_join]\n    A --&gt; D[inner_join]\n    A --&gt; E[full_join]\n\n    B --&gt; B1[Keep all x rows&lt;br/&gt;Add matching y columns]\n    C --&gt; C1[Keep all y rows&lt;br/&gt;Add matching x columns]\n    D --&gt; D1[Keep only matching rows&lt;br/&gt;from both x and y]\n    E --&gt; E1[Keep all rows&lt;br/&gt;from both x and y]\n\n\n\n\n\n\n\n\n\n7.5.3 left_join(): Keep All Left Rows\nVenn Diagram concept: All of LEFT + matching from RIGHT\n\n# Left join: keep all animals, add treatment info\nanimals %&gt;%\n  left_join(treatments, by = \"animal_id\")\n\n# A tibble: 4 √ó 4\n  animal_id breed    birth_date treatment  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      \n1 A001      Holstein 2023-01-15 Control    \n2 A002      Jersey   2023-02-20 Treatment_A\n3 A003      Angus    2023-01-10 Treatment_B\n4 A004      Holstein 2023-03-05 &lt;NA&gt;       \n\n\nNotice: - All 4 animals kept (even A004 with no treatment) - A004 gets NA for treatment - Unmatched rows from treatments (if any) are dropped\n\n7.5.3.1 Use left_join() when:\n\nYou want to add information to your main table\nYou want to keep all rows from the main table\nMissing matches should be NA\n\n\n\n\n7.5.4 right_join(): Keep All Right Rows\nVenn Diagram concept: All of RIGHT + matching from LEFT\n\n# Right join: keep all treatments, add animal info\nanimals %&gt;%\n  right_join(treatments, by = \"animal_id\")\n\n# A tibble: 3 √ó 4\n  animal_id breed    birth_date treatment  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      \n1 A001      Holstein 2023-01-15 Control    \n2 A002      Jersey   2023-02-20 Treatment_A\n3 A003      Angus    2023-01-10 Treatment_B\n\n\n\n\n\n\n\n\nNoteright_join() is Rarely Used\n\n\n\nright_join(x, y) gives the same result as left_join(y, x). Most people prefer to use left_join() and think about the ‚Äúmain‚Äù table being on the left.\n# These are equivalent:\nanimals %&gt;% right_join(treatments)\ntreatments %&gt;% left_join(animals)\n\n\n\n\n7.5.5 inner_join(): Keep Only Matching Rows\nVenn Diagram concept: Only the INTERSECTION\n\n# Inner join: keep only animals with treatments\nanimals %&gt;%\n  inner_join(treatments, by = \"animal_id\")\n\n# A tibble: 3 √ó 4\n  animal_id breed    birth_date treatment  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      \n1 A001      Holstein 2023-01-15 Control    \n2 A002      Jersey   2023-02-20 Treatment_A\n3 A003      Angus    2023-01-10 Treatment_B\n\n\nNotice: - Only 3 animals kept (A001, A002, A003) - A004 dropped (no treatment) - Perfect when you only want complete cases\n\n7.5.5.1 Use inner_join() when:\n\nYou only want rows that match in both tables\nUnmatched rows should be excluded\nYou‚Äôre analyzing complete cases only\n\n\n\n\n7.5.6 full_join(): Keep All Rows from Both\nVenn Diagram concept: UNION of LEFT and RIGHT\n\n# Full join: keep everything\nanimals %&gt;%\n  full_join(treatments, by = \"animal_id\")\n\n# A tibble: 4 √ó 4\n  animal_id breed    birth_date treatment  \n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;      \n1 A001      Holstein 2023-01-15 Control    \n2 A002      Jersey   2023-02-20 Treatment_A\n3 A003      Angus    2023-01-10 Treatment_B\n4 A004      Holstein 2023-03-05 &lt;NA&gt;       \n\n\n\n7.5.6.1 Use full_join() when:\n\nYou want all data from both tables\nYou want to see what‚Äôs missing from each table\nYou‚Äôll handle NA values later\n\n\n\n\n7.5.7 Joining with Multiple Keys\nWhat if matching requires multiple columns?\n\n# Farm data with state\nfarms &lt;- tibble(\n  farm_name = c(\"Green Acres\", \"Sunny Farm\", \"Green Acres\"),\n  state = c(\"Iowa\", \"Kansas\", \"Nebraska\"),\n  size_acres = c(500, 350, 600)\n)\n\n# Animal locations\nanimal_locations &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  farm_name = c(\"Green Acres\", \"Sunny Farm\", \"Green Acres\"),\n  state = c(\"Iowa\", \"Kansas\", \"Iowa\")\n)\n\n# Join on both farm_name AND state\nanimal_locations %&gt;%\n  left_join(farms, by = c(\"farm_name\", \"state\"))\n\n# A tibble: 3 √ó 4\n  animal_id farm_name   state  size_acres\n  &lt;chr&gt;     &lt;chr&gt;       &lt;chr&gt;       &lt;dbl&gt;\n1 A001      Green Acres Iowa          500\n2 A002      Sunny Farm  Kansas        350\n3 A003      Green Acres Iowa          500\n\n\n\n\n7.5.8 Joining with Different Column Names\n\n# Different ID column names\ntable1 &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  weight_kg = c(180, 150, 200)\n)\n\ntable2 &lt;- tibble(\n  id = c(\"A001\", \"A002\", \"A004\"),\n  breed = c(\"Holstein\", \"Jersey\", \"Angus\")\n)\n\n# Specify which columns to join on\ntable1 %&gt;%\n  left_join(table2, by = c(\"animal_id\" = \"id\"))\n\n# A tibble: 3 √ó 3\n  animal_id weight_kg breed   \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;   \n1 A001            180 Holstein\n2 A002            150 Jersey  \n3 A003            200 &lt;NA&gt;    \n\n\n\n\n7.5.9 Filtering Joins: Filter Rows (Don‚Äôt Add Columns)\n\n7.5.9.1 semi_join(): Keep Rows with Matches\n\n# Semi join: keep animals that have treatments (but don't add treatment column)\nanimals %&gt;%\n  semi_join(treatments, by = \"animal_id\")\n\n# A tibble: 3 √ó 3\n  animal_id breed    birth_date\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;    \n1 A001      Holstein 2023-01-15\n2 A002      Jersey   2023-02-20\n3 A003      Angus    2023-01-10\n\n\nUse semi_join() to answer: ‚ÄúWhich x rows have a match in y?‚Äù\n\n\n7.5.9.2 anti_join(): Keep Rows WITHOUT Matches\n\n# Anti join: keep animals that DON'T have treatments\nanimals %&gt;%\n  anti_join(treatments, by = \"animal_id\")\n\n# A tibble: 1 √ó 3\n  animal_id breed    birth_date\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;    \n1 A004      Holstein 2023-03-05\n\n\nUse anti_join() to answer: ‚ÄúWhich x rows DON‚ÄôT have a match in y?‚Äù\nThis is incredibly useful for finding missing data or orphaned records.\n\n\n\n7.5.10 Join Summary Table\n\n\n\n\n\n\n\n\n\nJoin Type\nRows Kept\nColumns Added\nUse When\n\n\n\n\nleft_join(x, y)\nAll from x\nFrom y\nAdding info to main table\n\n\nright_join(x, y)\nAll from y\nFrom x\nRarely used (use left_join instead)\n\n\ninner_join(x, y)\nOnly matches\nFrom both\nOnly want complete cases\n\n\nfull_join(x, y)\nAll from both\nFrom both\nKeep everything\n\n\nsemi_join(x, y)\nMatches from x\nNone\nFilter x by presence in y\n\n\nanti_join(x, y)\nNon-matches from x\nNone\nFilter x by absence in y\n\n\n\n\n\n7.5.11 Common Join Problems and Solutions\n\n7.5.11.1 Problem 1: Duplicate Keys\n\n# Multiple weights per animal\nweights_dup &lt;- tibble(\n  animal_id = c(\"A001\", \"A001\", \"A002\"),\n  weight_kg = c(180, 185, 150)\n)\n\n# Join creates all combinations\nanimals %&gt;%\n  left_join(weights_dup, by = \"animal_id\")\n\n# A tibble: 5 √ó 4\n  animal_id breed    birth_date weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;         &lt;dbl&gt;\n1 A001      Holstein 2023-01-15       180\n2 A001      Holstein 2023-01-15       185\n3 A002      Jersey   2023-02-20       150\n4 A003      Angus    2023-01-10        NA\n5 A004      Holstein 2023-03-05        NA\n\n\nSolution: If you only want one weight per animal, summarize first:\n\nweights_dup %&gt;%\n  group_by(animal_id) %&gt;%\n  summarise(avg_weight_kg = mean(weight_kg), .groups = \"drop\") %&gt;%\n  left_join(animals, ., by = \"animal_id\")\n\n# A tibble: 4 √ó 4\n  animal_id breed    birth_date avg_weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;             &lt;dbl&gt;\n1 A001      Holstein 2023-01-15          182.\n2 A002      Jersey   2023-02-20          150 \n3 A003      Angus    2023-01-10           NA \n4 A004      Holstein 2023-03-05           NA \n\n\n\n\n\n\n\n\nWarningAlways Specify by =\n\n\n\nIf you don‚Äôt specify by, joins use all columns with the same name. This can lead to: - Unintended matches - Silent errors - Confusing results\nBest practice: Always explicitly state by = \"column_name\"\n\n\n\n\n\n7.5.12 Real-World Example: Combining Multiple Tables\n\n# Start with animals\nresult &lt;- animals %&gt;%\n  # Add treatments\n  left_join(treatments, by = \"animal_id\") %&gt;%\n  # Add weights (keeping all weight records)\n  left_join(weights, by = \"animal_id\") %&gt;%\n  # Calculate age at weighing\n  mutate(\n    age_days = as.numeric(date - birth_date),\n    .after = date\n  )\n\nresult\n\n# A tibble: 6 √ó 7\n  animal_id breed    birth_date treatment   date       age_days weight_kg\n  &lt;chr&gt;     &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;       &lt;date&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 A001      Holstein 2023-01-15 Control     2023-06-01      137       180\n2 A001      Holstein 2023-01-15 Control     2023-09-01      229       220\n3 A002      Jersey   2023-02-20 Treatment_A 2023-06-01      101       150\n4 A002      Jersey   2023-02-20 Treatment_A 2023-09-01      193       185\n5 A003      Angus    2023-01-10 Treatment_B NA               NA        NA\n6 A004      Holstein 2023-03-05 &lt;NA&gt;        NA               NA        NA",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#binding-datasets",
    "href": "chapters/ch07-tidyr_joins_purrr.html#binding-datasets",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.6 Binding Datasets",
    "text": "7.6 Binding Datasets\nSometimes you need to combine datasets by stacking (rows) or attaching (columns), not by joining on keys.\n\n7.6.1 bind_rows(): Stack Datasets Vertically\nUse when you have the same structure in multiple datasets.\n\n# Data from multiple farms\nfarm_a &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\"),\n  weight_kg = c(180, 150),\n  farm = \"Farm_A\"\n)\n\nfarm_b &lt;- tibble(\n  animal_id = c(\"B001\", \"B002\"),\n  weight_kg = c(170, 160),\n  farm = \"Farm_B\"\n)\n\n# Stack them\nbind_rows(farm_a, farm_b)\n\n# A tibble: 4 √ó 3\n  animal_id weight_kg farm  \n  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt; \n1 A001            180 Farm_A\n2 A002            150 Farm_A\n3 B001            170 Farm_B\n4 B002            160 Farm_B\n\n\n\n7.6.1.1 Handling Missing Columns\n\n# Different columns\ndf1 &lt;- tibble(animal_id = \"A001\", weight_kg = 180)\ndf2 &lt;- tibble(animal_id = \"B001\", height_cm = 125)\n\n# bind_rows() fills missing columns with NA\nbind_rows(df1, df2)\n\n# A tibble: 2 √ó 3\n  animal_id weight_kg height_cm\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 A001            180        NA\n2 B001             NA       125\n\n\n\n\n7.6.1.2 Adding Source Identifier\n\n# Or use named list\nbind_rows(\n  \"Iowa\" = farm_a,\n  \"Kansas\" = farm_b,\n  .id = \"state\"\n)\n\n# A tibble: 4 √ó 4\n  state  animal_id weight_kg farm  \n  &lt;chr&gt;  &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt; \n1 Iowa   A001            180 Farm_A\n2 Iowa   A002            150 Farm_A\n3 Kansas B001            170 Farm_B\n4 Kansas B002            160 Farm_B\n\n\n\n\n\n7.6.2 bind_cols(): Attach Datasets Horizontally\n\n\n\n\n\n\nWarningUse bind_cols() with Caution!\n\n\n\nbind_cols() simply attaches columns side-by-side based on row position. It does NOT match by keys!\nProblems: - No checking if rows actually match - Row order must be identical - Easy to create mismatched data\nBetter alternative: Use a join with a row number or ID column.\n\n\n\n# Two datasets with same row order (risky!)\nids &lt;- tibble(animal_id = c(\"A001\", \"A002\", \"A003\"))\nweights &lt;- tibble(weight_kg = c(180, 150, 200))\n\n# Attach side-by-side\nbind_cols(ids, weights)\n\n# A tibble: 3 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 A001            180\n2 A002            150\n3 A003            200\n\n# MUCH SAFER: Use mutate instead if they're truly row-aligned\nids %&gt;%\n  mutate(weight_kg = weights$weight_kg)\n\n# A tibble: 3 √ó 2\n  animal_id weight_kg\n  &lt;chr&gt;         &lt;dbl&gt;\n1 A001            180\n2 A002            150\n3 A003            200\n\n# Or use a proper join if you have keys!\n\n\n\n7.6.3 When to Use Binding vs Joining\n\n\n\nUse\nbind_rows()\nbind_cols()\nJoin\n\n\n\n\nSame structure, stack vertically\n‚úÖ\n‚ùå\n‚ùå\n\n\nDifferent farms/time periods\n‚úÖ\n‚ùå\n‚ùå\n\n\nAdd columns based on keys\n‚ùå\n‚ùå\n‚úÖ\n\n\nRow-aligned data (same order)\n‚ùå\n‚ö†Ô∏è Use with caution\n‚ùå\n\n\nRelational data (shared IDs)\n‚ùå\n‚ùå\n‚úÖ",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#iteration-with-purrr",
    "href": "chapters/ch07-tidyr_joins_purrr.html#iteration-with-purrr",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.7 Iteration with purrr",
    "text": "7.7 Iteration with purrr\nThe purrr package provides tools for functional programming ‚Äî applying functions across elements of a list or vector.\n\n7.7.1 Why Use map() Instead of Loops?\n\n# Example: Calculate mean of each column\n\n# Base R loop approach\ncol_means_loop &lt;- numeric(ncol(mtcars))\nfor (i in seq_along(mtcars)) {\n  col_means_loop[i] &lt;- mean(mtcars[[i]])\n}\ncol_means_loop\n\n [1]  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250\n [7]  17.848750   0.437500   0.406250   3.687500   2.812500\n\n# purrr approach\ncol_means_map &lt;- map_dbl(mtcars, mean)\ncol_means_map\n\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb \n  0.437500   0.406250   3.687500   2.812500 \n\n# They're the same!\nall.equal(col_means_loop, col_means_map)\n\n[1] \"names for current but not for target\"\n\n\nBenefits of map(): - More concise - Less error-prone (no index management) - Easier to read intent - Consistent return types\n\n\n\n\n\n\nNoteWhen to Use Loops vs map()\n\n\n\nUse loops when: - You need to modify objects in place - The operation depends on previous iterations - The logic is complex and loops are clearer\nUse map() when: - Applying the same operation to each element - You want consistent output types - You‚Äôre working with lists or data frames\n\n\n\n\n7.7.2 Basic map() Functions\n\n# Create example data\nnumbers &lt;- list(\n  a = c(1, 2, 3, 4, 5),\n  b = c(10, 20, 30),\n  c = c(100, 200, 300, 400)\n)\n\n# map() returns a list\nmap(numbers, mean)\n\n$a\n[1] 3\n\n$b\n[1] 20\n\n$c\n[1] 250\n\n# map_dbl() returns a numeric vector\nmap_dbl(numbers, mean)\n\n  a   b   c \n  3  20 250 \n\n# map_chr() returns a character vector\nmap_chr(numbers, \\(x) paste(\"Mean:\", round(mean(x), 1)))\n\n          a           b           c \n  \"Mean: 3\"  \"Mean: 20\" \"Mean: 250\" \n\n# map_int() returns an integer vector\nmap_int(numbers, length)\n\na b c \n5 3 4 \n\n# map_lgl() returns a logical vector\nmap_lgl(numbers, \\(x) mean(x) &gt; 50)\n\n    a     b     c \nFALSE FALSE  TRUE \n\n\n\n\n7.7.3 map() with Data Frames\n\n# Create example animal data\nanimals_list &lt;- list(\n  farm_a = tibble(animal_id = c(\"A001\", \"A002\"), weight_kg = c(180, 150)),\n  farm_b = tibble(animal_id = c(\"B001\", \"B002\"), weight_kg = c(170, 160)),\n  farm_c = tibble(animal_id = c(\"C001\", \"C002\"), weight_kg = c(190, 165))\n)\n\n# Calculate mean weight for each farm\nmap_dbl(animals_list, \\(df) mean(df$weight_kg))\n\nfarm_a farm_b farm_c \n 165.0  165.0  177.5 \n\n# Count rows in each data frame\nmap_int(animals_list, nrow)\n\nfarm_a farm_b farm_c \n     2      2      2 \n\n# Add a new column to each data frame\nmap(animals_list, \\(df) mutate(df, weight_lb = weight_kg * 2.20462))\n\n$farm_a\n# A tibble: 2 √ó 3\n  animal_id weight_kg weight_lb\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 A001            180      397.\n2 A002            150      331.\n\n$farm_b\n# A tibble: 2 √ó 3\n  animal_id weight_kg weight_lb\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 B001            170      375.\n2 B002            160      353.\n\n$farm_c\n# A tibble: 2 √ó 3\n  animal_id weight_kg weight_lb\n  &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 C001            190      419.\n2 C002            165      364.\n\n\n\n\n7.7.4 Anonymous Functions in map()\nThree ways to write anonymous functions:\n\nnumbers &lt;- list(a = 1:5, b = 10:15, c = 20:25)\n\n# 1. Traditional function() syntax\nmap_dbl(numbers, function(x) mean(x) * 2)\n\n a  b  c \n 6 25 45 \n\n# 2. Formula shorthand with ~ and .x\nmap_dbl(numbers, ~ mean(.x) * 2)\n\n a  b  c \n 6 25 45 \n\n# 3. Lambda function with \\() (R &gt;= 4.1)\nmap_dbl(numbers, \\(x) mean(x) * 2)\n\n a  b  c \n 6 25 45 \n\n\nAll three are equivalent. Use whichever you find most readable.\n\n\n7.7.5 map2(): Iterate Over Two Inputs\nWhen you need to iterate over two vectors/lists in parallel:\n\n# Two vectors to iterate over\nweights_kg &lt;- c(180, 150, 200)\nprices_per_kg &lt;- c(5, 5.50, 4.80)\n\n# Calculate total value for each animal\nmap2_dbl(weights_kg, prices_per_kg, \\(w, p) w * p)\n\n[1] 900 825 960\n\n# With names\nanimals &lt;- c(\"A001\", \"A002\", \"A003\")\nmap2_chr(animals, weights_kg, \\(id, wt) paste(id, \"weighs\", wt, \"kg\"))\n\n[1] \"A001 weighs 180 kg\" \"A002 weighs 150 kg\" \"A003 weighs 200 kg\"\n\n\n\n\n7.7.6 pmap(): Iterate Over Many Inputs\nFor more than two inputs, use pmap():\n\n# Data frame with multiple columns\nsales &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\"),\n  weight_kg = c(180, 150, 200),\n  price_per_kg = c(5, 5.50, 4.80),\n  bonus = c(50, 30, 60)\n)\n\n# Calculate total value: (weight * price) + bonus\nsales %&gt;%\n  mutate(\n    total_value = pmap_dbl(\n      list(weight_kg, price_per_kg, bonus),\n      \\(w, p, b) (w * p) + b\n    )\n  )\n\n# A tibble: 3 √ó 5\n  animal_id weight_kg price_per_kg bonus total_value\n  &lt;chr&gt;         &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;\n1 A001            180          5      50         950\n2 A002            150          5.5    30         855\n3 A003            200          4.8    60        1020\n\n\n\n\n7.7.7 Practical Example: Reading Multiple Files\n\n# Get list of CSV files\nfile_list &lt;- list.files(path = \"data/\", pattern = \"*.csv\", full.names = TRUE)\n\n# Read all files into a list\nall_data &lt;- map(file_list, read_csv)\n\n# Combine into single data frame with source file\ncombined_data &lt;- map(file_list, read_csv) %&gt;%\n  set_names(basename(file_list)) %&gt;%\n  bind_rows(.id = \"source_file\")\n\n\n\n7.7.8 Handling Errors with safely() and possibly()\nSometimes operations fail for some elements. Handle errors gracefully:\n\n# Some operations might fail\nmixed_list &lt;- list(\n  a = c(1, 2, 3),\n  b = c(\"a\", \"b\", \"c\"),  # This will cause an error with mean()\n  c = c(10, 20, 30)\n)\n\n# This would error:\n# map_dbl(mixed_list, mean)\n\n# Use safely() to catch errors\nsafe_mean &lt;- safely(mean)\nmap(mixed_list, safe_mean)\n\n$a\n$a$result\n[1] 2\n\n$a$error\nNULL\n\n\n$b\n$b$result\n[1] NA\n\n$b$error\nNULL\n\n\n$c\n$c$result\n[1] 20\n\n$c$error\nNULL\n\n# Use possibly() to return a default value on error\npossibly_mean &lt;- possibly(mean, otherwise = NA_real_)\nmap_dbl(mixed_list, possibly_mean)\n\n a  b  c \n 2 NA 20",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#list-columns-and-nested-data",
    "href": "chapters/ch07-tidyr_joins_purrr.html#list-columns-and-nested-data",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.8 List Columns and Nested Data",
    "text": "7.8 List Columns and Nested Data\nList columns allow you to store complex objects (like data frames) inside data frame cells.\n\n7.8.1 Creating Nested Data\n\n# Example: Group and nest\nanimals_nested &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\", \"A001\", \"A002\", \"A003\"),\n  week = c(0, 0, 0, 4, 4, 4),\n  weight_kg = c(45, 42, 48, 52, 48, 54)\n) %&gt;%\n  group_by(animal_id) %&gt;%\n  nest()\n\nanimals_nested\n\n# A tibble: 3 √ó 2\n# Groups:   animal_id [3]\n  animal_id data            \n  &lt;chr&gt;     &lt;list&gt;          \n1 A001      &lt;tibble [2 √ó 2]&gt;\n2 A002      &lt;tibble [2 √ó 2]&gt;\n3 A003      &lt;tibble [2 √ó 2]&gt;\n\n# Each row now contains a data frame in the 'data' column\nanimals_nested$data[[1]]  # Data for first animal\n\n# A tibble: 2 √ó 2\n   week weight_kg\n  &lt;dbl&gt;     &lt;dbl&gt;\n1     0        45\n2     4        52\n\n\n\n\n7.8.2 Working with Nested Data\n\n# Calculate statistics for each animal's data\nanimals_nested %&gt;%\n  mutate(\n    n_measurements = map_int(data, nrow),\n    mean_weight = map_dbl(data, ~mean(.x$weight_kg)),\n    weight_gain = map_dbl(data, ~diff(range(.x$weight_kg)))\n  )\n\n# A tibble: 3 √ó 5\n# Groups:   animal_id [3]\n  animal_id data             n_measurements mean_weight weight_gain\n  &lt;chr&gt;     &lt;list&gt;                    &lt;int&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 A001      &lt;tibble [2 √ó 2]&gt;              2        48.5           7\n2 A002      &lt;tibble [2 √ó 2]&gt;              2        45             6\n3 A003      &lt;tibble [2 √ó 2]&gt;              2        51             6\n\n\n\n\n7.8.3 Fitting Models to Nested Data\n\n# Create more complete data\nset.seed(123)\ngrowth_data &lt;- tibble(\n  animal_id = rep(c(\"A001\", \"A002\", \"A003\"), each = 5),\n  week = rep(c(0, 2, 4, 6, 8), times = 3),\n  weight_kg = c(\n    45 + (0:4) * 2 + rnorm(5, 0, 1),  # A001\n    42 + (0:4) * 1.8 + rnorm(5, 0, 1),  # A002\n    48 + (0:4) * 2.2 + rnorm(5, 0, 1)   # A003\n  )\n)\n\n# Nest by animal\ngrowth_nested &lt;- growth_data %&gt;%\n  group_by(animal_id) %&gt;%\n  nest()\n\n# Fit linear model to each animal's growth\ngrowth_nested &lt;- growth_nested %&gt;%\n  mutate(\n    model = map(data, ~lm(weight_kg ~ week, data = .x)),\n    growth_rate = map_dbl(model, ~coef(.x)[2])\n  )\n\ngrowth_nested\n\n# A tibble: 3 √ó 4\n# Groups:   animal_id [3]\n  animal_id data             model  growth_rate\n  &lt;chr&gt;     &lt;list&gt;           &lt;list&gt;       &lt;dbl&gt;\n1 A001      &lt;tibble [5 √ó 2]&gt; &lt;lm&gt;         1.08 \n2 A002      &lt;tibble [5 √ó 2]&gt; &lt;lm&gt;         0.627\n3 A003      &lt;tibble [5 √ó 2]&gt; &lt;lm&gt;         0.910\n\n# Extract coefficients\ngrowth_nested %&gt;%\n  select(animal_id, growth_rate)\n\n# A tibble: 3 √ó 2\n# Groups:   animal_id [3]\n  animal_id growth_rate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 A001            1.08 \n2 A002            0.627\n3 A003            0.910\n\n\n\n\n7.8.4 Unnesting\nConvert list columns back to regular rows:\n\n# Unnest data column\ngrowth_nested %&gt;%\n  select(animal_id, data) %&gt;%\n  unnest(cols = data)\n\n# A tibble: 15 √ó 3\n# Groups:   animal_id [3]\n   animal_id  week weight_kg\n   &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 A001          0      44.4\n 2 A001          2      46.8\n 3 A001          4      50.6\n 4 A001          6      51.1\n 5 A001          8      53.1\n 6 A002          0      43.7\n 7 A002          2      44.3\n 8 A002          4      44.3\n 9 A002          6      46.7\n10 A002          8      48.8\n11 A003          0      49.2\n12 A003          2      50.6\n13 A003          4      52.8\n14 A003          6      54.7\n15 A003          8      56.2",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#summary",
    "href": "chapters/ch07-tidyr_joins_purrr.html#summary",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.9 Summary",
    "text": "7.9 Summary\nThis chapter covered three major topics for advanced data wrangling:\n\n7.9.1 Data Reshaping (tidyr)\n\npivot_longer(): Wide ‚Üí Long format\n\nConvert column names to values\nStack measurements\n\npivot_wider(): Long ‚Üí Wide format\n\nCreate summary tables\nSpread values across columns\n\nseparate() & unite(): Split or combine columns\n\nExtract information from composite columns\nCreate unique identifiers\n\n\n\n\n7.9.2 Joining Datasets\n\nMutating joins (add columns):\n\nleft_join(): Keep all left, add matching right\ninner_join(): Keep only matches\nfull_join(): Keep everything\n\nFiltering joins (filter rows):\n\nsemi_join(): Keep rows with matches\nanti_join(): Keep rows without matches\n\nBinding:\n\nbind_rows(): Stack vertically\nbind_cols(): Attach horizontally (use cautiously)\n\n\n\n\n7.9.3 Iteration (purrr)\n\nmap() functions: Apply functions to each element\n\nmap(): Returns list\nmap_dbl(), map_chr(), map_int(), map_lgl(): Return vectors\n\nMultiple inputs:\n\nmap2(): Two inputs\npmap(): Many inputs\n\nList columns: Store complex objects in data frames\n\nnest(): Create nested data\nunnest(): Expand nested data",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#week-7-homework-multi-table-data-integration",
    "href": "chapters/ch07-tidyr_joins_purrr.html#week-7-homework-multi-table-data-integration",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.10 Week 7 Homework: Multi-Table Data Integration",
    "text": "7.10 Week 7 Homework: Multi-Table Data Integration\n\n7.10.1 Assignment Overview\nYou will work with three related datasets about a pig feeding trial. Your task is to reshape, join, and analyze the data using techniques from this chapter.\n\n\n7.10.2 Part 1: Data Generation (0 points, but required)\nRun this code to create your datasets:\n\nlibrary(tidyverse)\nset.seed(2024)\n\n# Dataset 1: Pig information\npigs &lt;- tibble(\n  pig_id = paste0(\"P\", 1001:1050),\n  breed = sample(c(\"Yorkshire\", \"Duroc\", \"Hampshire\"), 50, replace = TRUE),\n  birth_date = as.Date(\"2023-03-01\") + sample(0:30, 50, replace = TRUE),\n  initial_weight_kg = rnorm(50, mean = 8, sd = 1)\n)\n\n# Dataset 2: Feed records (wide format - needs reshaping!)\nfeed_wide &lt;- tibble(\n  pig_id = paste0(\"P\", 1001:1050),\n  week_1_kg = rnorm(50, mean = 5, sd = 0.5),\n  week_2_kg = rnorm(50, mean = 6, sd = 0.6),\n  week_3_kg = rnorm(50, mean = 7, sd = 0.7),\n  week_4_kg = rnorm(50, mean = 8, sd = 0.8)\n)\n\n# Dataset 3: Treatment assignments (some pigs, not all)\ntreatments &lt;- tibble(\n  pig_id = sample(paste0(\"P\", 1001:1050), 40),\n  treatment = sample(c(\"Standard\", \"HighProtein\", \"Supplement\"), 40, replace = TRUE),\n  treatment_start = as.Date(\"2023-03-15\")\n)\n\n# Dataset 4: Health records (long format, multiple records per pig)\nhealth &lt;- tibble(\n  pig_id = rep(sample(paste0(\"P\", 1001:1050), 30), each = 3),\n  check_date = rep(as.Date(\"2023-03-15\") + c(7, 14, 21), times = 30),\n  health_score = sample(1:10, 90, replace = TRUE),\n  temperature_C = rnorm(90, mean = 39, sd = 0.5)\n)\n\n# Save datasets\nwrite_csv(pigs, \"pigs.csv\")\nwrite_csv(feed_wide, \"feed_wide.csv\")\nwrite_csv(treatments, \"treatments.csv\")\nwrite_csv(health, \"health.csv\")\n\n\n\n7.10.3 Part 2: Data Reshaping (25 points)\nTask 1 (10 points): Reshape feed_wide from wide to long format - Create columns: pig_id, week, feed_consumed_kg - Convert week to numeric (remove ‚Äúweek_‚Äù prefix and ‚Äú_kg‚Äù suffix) - Show first 10 rows\nTask 2 (10 points): Create a summary table showing average feed by week - Calculate mean feed consumed for each week - Reshape to wide format where weeks are columns - Add a ‚Äútotal‚Äù column with sum across weeks\nTask 3 (5 points): In the health dataset, separate check_date into year, month, day\n\n\n7.10.4 Part 3: Joining Datasets (30 points)\nTask 1 (10 points): Create a master dataset - Start with pigs - Add treatment information (keep all pigs) - Add the reshaped feed data (long format) - Show structure and first 15 rows\nTask 2 (10 points): Analyze which pigs DON‚ÄôT have health records - Use an appropriate filtering join - Show pig_id, breed, and initial_weight_kg - How many pigs lack health records?\nTask 3 (10 points): Create a complete cases dataset - Only include pigs that have: - Treatment assignments - Feed records for all 4 weeks - At least one health record - How many pigs remain? - What‚Äôs the average initial weight?\n\n\n7.10.5 Part 4: Complex Joins (20 points)\nTask 1 (10 points): Calculate total feed consumption per pig - Sum feed_consumed_kg across all weeks for each pig - Join with pig information and treatment - Create a table showing mean total feed by breed and treatment - Which breed √ó treatment combination consumed most feed?\nTask 2 (10 points): Identify treatment effects - Calculate average health_score for each pig - Join with treatment information - Compare average health scores across treatments - Create a visualization (boxplot or violin plot)\n\n\n7.10.6 Part 5: Iteration with purrr (20 points)\nTask 1 (10 points): Calculate summary statistics for each breed - Nest data by breed - Use map() to calculate for each breed: - Number of pigs - Mean initial weight - Mean total feed consumed (if you calculated it in Part 4) - Mean health score (if available) - Unnest results into a summary table\nTask 2 (10 points): Fit models to nested data - Nest feed data by pig_id - Fit a linear model of feed_consumed_kg ~ week for each pig - Extract the slope (growth rate of feed consumption) - Which pig has the highest feed growth rate? - Which breed has the highest average feed growth rate?\n\n\n7.10.7 Part 6: Reflection (5 points)\nWrite 200-300 words addressing: - Which type of join did you use most and why? - What was the most challenging aspect of reshaping the data? - When did you choose map() vs a loop (if at all)? - What insights did you discover about the pig feeding trial?\n\n\n7.10.8 Recommended YAML\n---\ntitle: \"Week 7 Homework: Multi-Table Data Integration\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: false\n    theme: cosmo\n    embed-resources: true\nexecute:\n  warning: false\n  message: false\n---\n\n\n7.10.9 Grading Rubric\n\nPart 2: Data Reshaping (25%)\n\nCorrect pivoting to long format (10%)\nSummary table correctly reshaped (10%)\nDate separation correct (5%)\n\nPart 3: Joining Datasets (30%)\n\nMaster dataset correctly created (10%)\nAnti-join to find missing health records (10%)\nComplete cases filtering correct (10%)\n\nPart 4: Complex Joins (20%)\n\nTotal feed calculation and table (10%)\nTreatment effects analysis (10%)\n\nPart 5: Iteration with purrr (20%)\n\nNested summaries by breed (10%)\nModel fitting with map (10%)\n\nPart 6: Reflection (5%)\n\nThoughtful analysis of methods and insights\n\n\n\n\n7.10.10 Bonus (+10 points)\n\nCreate a function that takes a pig_id and returns a plot of their feed consumption over time\nUse map() to generate plots for all pigs in one breed\nCombine plots using patchwork (from Chapter 6)",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch07-tidyr_joins_purrr.html#additional-resources",
    "href": "chapters/ch07-tidyr_joins_purrr.html#additional-resources",
    "title": "7¬† Data Reshaping, Joining, and Iteration",
    "section": "7.11 Additional Resources",
    "text": "7.11 Additional Resources\n\n7.11.1 Required Reading\n\nR for Data Science (2e) - Chapters 19-21 (Joins, Tidy Data, Iteration)\ntidyr documentation\npurrr documentation\nWickham, H. (2014). ‚ÄúTidy Data.‚Äù Journal of Statistical Software, 59(10). Link\n\n\n\n7.11.2 Optional Reading\n\nTidy Data Tutor - Interactive visual guide\npurrr tutorial by Jenny Bryan\nRelational Data chapter from R4DS\n\n\n\n7.11.3 Videos\n\n‚ÄúTidy Data and tidyr‚Äù by RStudio / Posit\n‚ÄúIteration with purrr‚Äù by RStudio / Posit\n‚ÄúRow-oriented workflows‚Äù by Jenny Bryan\n\n\n\n7.11.4 Cheat Sheets\n\ntidyr Cheat Sheet\npurrr Cheat Sheet\nData Import Cheat Sheet\n\n\n\n7.11.5 Interactive Learning\n\nRStudio Primers: Tidy Your Data\nRStudio Primers: Iterate\n\n\n\n7.11.6 Useful Websites\n\nTidy Data Principles\ndplyr join vignette\npurrr GitHub\nStack Overflow: tidyr tag\n\n\nNext Chapter: Special Data Formats, Integration, and Course Wrap-up",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Data Reshaping, Joining, and Iteration</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html",
    "href": "chapters/ch08-special_formats.html",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "",
    "text": "8.1 Learning Objectives\nBy the end of this chapter, you will be able to:",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#learning-objectives",
    "href": "chapters/ch08-special_formats.html#learning-objectives",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "",
    "text": "Read data from SAS (.sas7bdat), SPSS (.sav), and Stata (.dta) files using the haven package\nWork with Excel files for reading (simple and advanced) and writing with formatting\nUse janitor for data cleaning tasks like fixing column names and removing duplicates\nManipulate dates and times effectively with lubridate\nCreate dynamic strings with the glue package\nApply the complete data science workflow from raw data to final report\nImplement reproducibility best practices in your analysis projects\nUnderstand how data science skills enable statistical analysis (Part 2 preview)\nIdentify career paths in data science and resources for continued learning\nComplete a capstone project integrating all skills from Part 1",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#introduction",
    "href": "chapters/ch08-special_formats.html#introduction",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.2 Introduction",
    "text": "8.2 Introduction\nCongratulations on reaching the final chapter of Part 1! Over the past seven weeks, you‚Äôve built a comprehensive foundation in data science using R, RStudio, and the tidyverse ecosystem. You‚Äôve learned to read data, clean and transform it, visualize patterns, and create reproducible reports.\nThis chapter serves three important purposes:\n\nTechnical Skills: Introduce additional tools for working with data in formats you‚Äôll encounter in the real world\nIntegration: Review and synthesize the complete data science workflow\nTransition: Prepare you for Part 2 (statistics) and point you toward continued learning\n\nIn the real world, data rarely arrives in clean CSV files. You might receive:\n\nData from colleagues using SAS, SPSS, or Stata (common in animal science research)\nExcel files with multiple sheets, formatting, and formulas\nMessy column names that need standardization\nDate and time data in various formats\nThe need to create formatted Excel reports for stakeholders\n\nThis chapter will equip you to handle these situations confidently.\n\n\n\n\n\n\nNoteInstalling Packages for This Chapter\n\n\n\nYou‚Äôll need several packages that may not be installed yet:\ninstall.packages(c(\n  \"haven\",      # Read SAS, SPSS, Stata files\n  \"readxl\",     # Read Excel files (you should have this)\n  \"writexl\",    # Write simple Excel files\n  \"openxlsx\",   # Advanced Excel operations\n  \"janitor\",    # Data cleaning utilities\n  \"lubridate\",  # Date/time manipulation\n  \"glue\"        # String interpolation\n))\n\n\n\n# Load packages\nlibrary(tidyverse)  # Core data science tools\nlibrary(haven)      # Statistical software files\nlibrary(readxl)     # Read Excel\nlibrary(writexl)    # Write simple Excel\nlibrary(janitor)    # Data cleaning\nlibrary(lubridate)  # Dates and times\nlibrary(glue)       # String interpolation\n\n# Optional: Install openxlsx if you need advanced Excel formatting\n# install.packages(\"openxlsx\")",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#reading-data-from-statistical-software",
    "href": "chapters/ch08-special_formats.html#reading-data-from-statistical-software",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.3 Reading Data from Statistical Software",
    "text": "8.3 Reading Data from Statistical Software\nMany researchers in animal science and related fields use SAS, SPSS, or Stata for their analyses. If you need to collaborate with these researchers or access legacy data, you‚Äôll need to read these file formats. The haven package makes this straightforward.\n\n8.3.1 The haven Package\nhaven can read:\n\nSAS: .sas7bdat (data files), .sas7bcat (catalog files)\nSPSS: .sav files\nStata: .dta files\n\nAll three software packages support variable labels (longer descriptions) and value labels (like factor levels in R). haven preserves this metadata.\n\n\n8.3.2 Reading SAS Files\n\n# Create a simulated SAS-style dataset for demonstration\n# In reality, you'd receive this file from a colleague\nsas_cattle_data &lt;- tibble(\n  id = 1:10,\n  treatment = c(1, 1, 1, 1, 1, 2, 2, 2, 2, 2),\n  weight_gain_kg = c(125, 130, 118, 135, 128, 145, 152, 138, 148, 143)\n)\n\n# Save as SAS file (for demonstration)\nwrite_sas(sas_cattle_data, \"../data/raw/cattle_trial.sas7bdat\")\n\n# Now read it back\ncattle_sas &lt;- read_sas(\"../data/raw/cattle_trial.sas7bdat\")\ncattle_sas\n\n# A tibble: 10 √ó 3\n      id treatment weight_gain_kg\n   &lt;dbl&gt;     &lt;dbl&gt;          &lt;dbl&gt;\n 1     1         1            125\n 2     2         1            130\n 3     3         1            118\n 4     4         1            135\n 5     5         1            128\n 6     6         2            145\n 7     7         2            152\n 8     8         2            138\n 9     9         2            148\n10    10         2            143\n\n\n\n\n8.3.3 Reading SPSS Files\n\n# Create labeled data similar to SPSS format\nspss_feeding_data &lt;- tibble(\n  animal_id = 1:15,\n  feed_type = c(rep(1, 5), rep(2, 5), rep(3, 5)),\n  daily_gain_kg = rnorm(15, mean = 1.5, sd = 0.3)\n)\n\n# Write as SPSS file\nwrite_sav(spss_feeding_data, \"../data/raw/feeding_study.sav\")\n\n# Read it back\nfeeding_spss &lt;- read_sav(\"../data/raw/feeding_study.sav\")\nfeeding_spss\n\n# A tibble: 15 √ó 3\n   animal_id feed_type daily_gain_kg\n       &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;\n 1         1         1         1.48 \n 2         2         1         1.73 \n 3         3         1         1.56 \n 4         4         1         1.43 \n 5         5         1         1.90 \n 6         6         2         1.40 \n 7         7         2         1.89 \n 8         8         2         1.74 \n 9         9         2         1.32 \n10        10         2         0.741\n11        11         3         1.38 \n12        12         3         1.28 \n13        13         3         1.37 \n14        14         3         1.16 \n15        15         3         1.23 \n\n\n\n\n8.3.4 Reading Stata Files\n\n# Create Stata-style dataset\nstata_breeding_data &lt;- tibble(\n  cow_id = 1001:1020,\n  parity = sample(1:4, 20, replace = TRUE),\n  conception = sample(0:1, 20, replace = TRUE)\n)\n\n# Write as Stata file\nwrite_dta(stata_breeding_data, \"../data/raw/breeding_records.dta\")\n\n# Read it back\nbreeding_stata &lt;- read_dta(\"../data/raw/breeding_records.dta\")\nbreeding_stata\n\n# A tibble: 20 √ó 3\n   cow_id parity conception\n    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n 1   1001      3          1\n 2   1002      1          1\n 3   1003      1          1\n 4   1004      4          1\n 5   1005      2          1\n 6   1006      1          0\n 7   1007      1          0\n 8   1008      1          0\n 9   1009      3          0\n10   1010      4          1\n11   1011      3          0\n12   1012      3          0\n13   1013      1          1\n14   1014      1          0\n15   1015      4          1\n16   1016      3          0\n17   1017      3          0\n18   1018      2          1\n19   1019      4          0\n20   1020      1          0\n\n\n\n\n8.3.5 Working with Labeled Data\nStatistical software often uses numeric codes with text labels. For example, 1 = \"Control\" and 2 = \"Treatment\". haven preserves these as labeled vectors.\n\n# Create data with value labels (like SPSS)\nlabeled_data &lt;- tibble(\n  id = 1:6,\n  sex = c(1, 2, 1, 2, 1, 2),\n  breed = c(1, 1, 2, 2, 3, 3)\n) %&gt;%\n  mutate(\n    sex = labelled(sex, c(\"Male\" = 1, \"Female\" = 2)),\n    breed = labelled(breed, c(\"Angus\" = 1, \"Hereford\" = 2, \"Charolais\" = 3))\n  )\n\n# View the structure\nstr(labeled_data)\n\ntibble [6 √ó 3] (S3: tbl_df/tbl/data.frame)\n $ id   : int [1:6] 1 2 3 4 5 6\n $ sex  : dbl+lbl [1:6] 1, 2, 1, 2, 1, 2\n   ..@ labels: Named num [1:2] 1 2\n   .. ..- attr(*, \"names\")= chr [1:2] \"Male\" \"Female\"\n $ breed: dbl+lbl [1:6] 1, 1, 2, 2, 3, 3\n   ..@ labels: Named num [1:3] 1 2 3\n   .. ..- attr(*, \"names\")= chr [1:3] \"Angus\" \"Hereford\" \"Charolais\"\n\n# Convert labeled columns to factors for analysis\nlabeled_data_clean &lt;- labeled_data %&gt;%\n  mutate(\n    sex = as_factor(sex),\n    breed = as_factor(breed)\n  )\n\nlabeled_data_clean\n\n# A tibble: 6 √ó 3\n     id sex    breed    \n  &lt;int&gt; &lt;fct&gt;  &lt;fct&gt;    \n1     1 Male   Angus    \n2     2 Female Angus    \n3     3 Male   Hereford \n4     4 Female Hereford \n5     5 Male   Charolais\n6     6 Female Charolais\n\n\n\n\n\n\n\n\nTipConverting Labeled Data\n\n\n\nWhen you read SAS/SPSS/Stata files with labels, use as_factor() from haven to convert labeled columns to regular R factors. This makes them easier to work with in tidyverse functions.\n# Convert all labeled columns at once\ndata_clean &lt;- data %&gt;%\n  mutate(across(where(is.labelled), as_factor))\n\n\n\n\n8.3.6 When You Might Need haven\n\nCollaborating with researchers who use SAS/SPSS/Stata\nAccessing institutional databases stored in these formats\nConverting legacy data to R for new analyses\nMaintaining compatibility with established workflows",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#working-with-excel-files",
    "href": "chapters/ch08-special_formats.html#working-with-excel-files",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.4 Working with Excel Files",
    "text": "8.4 Working with Excel Files\nExcel is ubiquitous in business and research settings. While we generally prefer CSV files for data storage, you‚Äôll often need to read from or write to Excel files.\n\n8.4.1 Reading Excel Files (Review + Advanced)\nWe covered readxl basics in Week 2. Let‚Äôs review and add advanced features:\n\n# Create a sample Excel file for demonstration\ncattle_summary &lt;- tibble(\n  breed = c(\"Angus\", \"Hereford\", \"Charolais\", \"Limousin\"),\n  n_animals = c(45, 38, 42, 35),\n  avg_weight_kg = c(520, 495, 535, 510),\n  avg_gain_kg = c(1.45, 1.38, 1.52, 1.41)\n)\n\n# Write to Excel\nwrite_xlsx(cattle_summary, \"../data/raw/cattle_summary.xlsx\")\n\n# Read back - basic\nread_excel(\"../data/raw/cattle_summary.xlsx\")\n\n# A tibble: 4 √ó 4\n  breed     n_animals avg_weight_kg avg_gain_kg\n  &lt;chr&gt;         &lt;dbl&gt;         &lt;dbl&gt;       &lt;dbl&gt;\n1 Angus            45           520        1.45\n2 Hereford         38           495        1.38\n3 Charolais        42           535        1.52\n4 Limousin         35           510        1.41\n\n\n\n\n8.4.2 Reading Specific Ranges and Sheets\n\n\n\n\n\n\nNoteNote on openxlsx\n\n\n\nThe code below uses openxlsx to create multi-sheet Excel files. If you want to run this code, first install the package:\ninstall.packages(\"openxlsx\")\nlibrary(openxlsx)\n\n\n\n# Create multi-sheet Excel file\nweight_data &lt;- tibble(\n  week = 0:12,\n  weight_kg = c(42, 48, 54, 60, 66, 72, 78, 84, 90, 96, 102, 108, 114)\n)\n\nhealth_data &lt;- tibble(\n  week = 0:12,\n  health_score = sample(1:5, 13, replace = TRUE)\n)\n\n# Write multiple sheets (requires openxlsx package)\nlibrary(openxlsx)\nwb &lt;- createWorkbook()\naddWorksheet(wb, \"Weights\")\naddWorksheet(wb, \"Health\")\nwriteData(wb, \"Weights\", weight_data)\nwriteData(wb, \"Health\", health_data)\nsaveWorkbook(wb, \"../data/raw/calf_data.xlsx\", overwrite = TRUE)\n\n# Read specific sheet\nread_excel(\"../data/raw/calf_data.xlsx\", sheet = \"Weights\")\n\n# List all sheets first\nexcel_sheets(\"../data/raw/calf_data.xlsx\")\n\n# Read specific range (e.g., first 5 rows)\nread_excel(\"../data/raw/calf_data.xlsx\",\n           sheet = \"Weights\",\n           range = \"A1:B6\")\n\n\n\n8.4.3 Writing Simple Excel Files with writexl\nFor basic Excel export without formatting, writexl is fast and simple:\n\n# Write single sheet\nwrite_xlsx(cattle_summary, \"output_cattle.xlsx\")\n\n# Write multiple sheets as a list\ncattle_list &lt;- list(\n  Summary = cattle_summary,\n  Weights = weight_data,\n  Health = health_data\n)\n\nwrite_xlsx(cattle_list, \"output_multi_sheet.xlsx\")\n\n\n\n8.4.4 Advanced Excel Operations with openxlsx\nFor formatted reports, use openxlsx:\n\n# Create a formatted Excel report\nwb &lt;- createWorkbook()\n\n# Add a worksheet\naddWorksheet(wb, \"Performance Report\")\n\n# Write title\nwriteData(wb, \"Performance Report\",\n          \"Cattle Performance Summary - 2024\",\n          startRow = 1, startCol = 1)\n\n# Format title (bold, larger, colored)\naddStyle(wb, \"Performance Report\",\n         style = createStyle(fontSize = 16, textDecoration = \"bold\",\n                            fgFill = \"#4F81BD\", fontColour = \"#FFFFFF\"),\n         rows = 1, cols = 1:4, gridExpand = TRUE)\n\n# Write data starting at row 3\nwriteData(wb, \"Performance Report\", cattle_summary, startRow = 3)\n\n# Format header row\naddStyle(wb, \"Performance Report\",\n         style = createStyle(textDecoration = \"bold\",\n                            fgFill = \"#D9E1F2\"),\n         rows = 3, cols = 1:4, gridExpand = TRUE)\n\n# Add borders\naddStyle(wb, \"Performance Report\",\n         style = createStyle(border = \"TopBottomLeftRight\"),\n         rows = 3:7, cols = 1:4, gridExpand = TRUE, stack = TRUE)\n\n# Set column widths\nsetColWidths(wb, \"Performance Report\", cols = 1:4, widths = c(15, 12, 15, 15))\n\n# Add a formula (total animals)\nwriteFormula(wb, \"Performance Report\",\n             x = \"=SUM(B4:B7)\",\n             startRow = 8, startCol = 2)\n\n# Save\nsaveWorkbook(wb, \"../data/raw/formatted_cattle_report.xlsx\", overwrite = TRUE)\n\n\n\n\n\n\n\nImportantWhen to Use Excel vs CSV\n\n\n\nUse CSV when: - Data will be read by R, Python, or databases - You want simplicity and version control (text files) - No special formatting needed\nUse Excel when: - Sharing with non-technical stakeholders - Multiple related tables (sheets) belong together - Formatting enhances readability (colors, bold, borders) - Client specifically requests Excel format",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#essential-utility-packages",
    "href": "chapters/ch08-special_formats.html#essential-utility-packages",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.5 Essential Utility Packages",
    "text": "8.5 Essential Utility Packages\nThree packages that will make your data cleaning life much easier:\n\n8.5.1 The janitor Package\njanitor provides simple functions for common data cleaning tasks.\n\n8.5.1.1 clean_names(): Fix Messy Column Names\n\n# Messy data with terrible column names (realistic!)\nmessy_data &lt;- tibble(\n  `Animal ID` = 1:5,\n  `Birth Weight (kg)` = c(42, 45, 39, 44, 41),\n  `Final Weight-KG` = c(520, 535, 498, 528, 512),\n  `%Daily Gain` = c(1.45, 1.48, 1.39, 1.46, 1.43),\n  `Dam's ID` = c(101, 102, 103, 104, 105)\n)\n\nmessy_data\n\n# A tibble: 5 √ó 5\n  `Animal ID` `Birth Weight (kg)` `Final Weight-KG` `%Daily Gain` `Dam's ID`\n        &lt;int&gt;               &lt;dbl&gt;             &lt;dbl&gt;         &lt;dbl&gt;      &lt;dbl&gt;\n1           1                  42               520          1.45        101\n2           2                  45               535          1.48        102\n3           3                  39               498          1.39        103\n4           4                  44               528          1.46        104\n5           5                  41               512          1.43        105\n\n# Clean it up!\nclean_data &lt;- messy_data %&gt;%\n  clean_names()\n\nclean_data\n\n# A tibble: 5 √ó 5\n  animal_id birth_weight_kg final_weight_kg percent_daily_gain dams_id\n      &lt;int&gt;           &lt;dbl&gt;           &lt;dbl&gt;              &lt;dbl&gt;   &lt;dbl&gt;\n1         1              42             520               1.45     101\n2         2              45             535               1.48     102\n3         3              39             498               1.39     103\n4         4              44             528               1.46     104\n5         5              41             512               1.43     105\n\n# Now you have snake_case names that work well with R\n# animal_id, birth_weight_kg, final_weight_kg, percent_daily_gain, dam_s_id\n\n\n\n\n\n\n\nTipAlways Use clean_names()\n\n\n\nMake it a habit to run clean_names() immediately after reading data from external sources. It prevents countless headaches with spaces, special characters, and inconsistent naming.\ndata &lt;- read_excel(\"messy_file.xlsx\") %&gt;%\n  clean_names()\n\n\n\n\n8.5.1.2 tabyl(): Better Frequency Tables\n\n# Create sample data\nanimals &lt;- tibble(\n  breed = sample(c(\"Angus\", \"Hereford\", \"Charolais\"), 100, replace = TRUE),\n  sex = sample(c(\"M\", \"F\"), 100, replace = TRUE),\n  treatment = sample(c(\"Control\", \"Treatment\"), 100, replace = TRUE)\n)\n\n# Base R table (basic)\ntable(animals$breed)\n\n\n    Angus Charolais  Hereford \n       40        30        30 \n\n# janitor's tabyl (better!)\nanimals %&gt;%\n  tabyl(breed)\n\n     breed  n percent\n     Angus 40     0.4\n Charolais 30     0.3\n  Hereford 30     0.3\n\n# Two-way table with totals\nanimals %&gt;%\n  tabyl(breed, sex) %&gt;%\n  adorn_totals(c(\"row\", \"col\"))\n\n     breed  F  M Total\n     Angus 21 19    40\n Charolais 16 14    30\n  Hereford 16 14    30\n     Total 53 47   100\n\n# Add percentages\nanimals %&gt;%\n  tabyl(breed, sex) %&gt;%\n  adorn_percentages(\"row\") %&gt;%\n  adorn_pct_formatting(digits = 1) %&gt;%\n  adorn_ns()  # Add counts in parentheses\n\n     breed          F          M\n     Angus 52.5% (21) 47.5% (19)\n Charolais 53.3% (16) 46.7% (14)\n  Hereford 53.3% (16) 46.7% (14)\n\n\n\n\n8.5.1.3 remove_empty() and get_dupes()\n\n# Data with empty rows/columns\ndata_with_empties &lt;- tibble(\n  id = c(1, 2, NA, 4, 5),\n  weight = c(50, 55, NA, 60, 65),\n  empty_col = rep(NA, 5),\n  breed = c(\"Angus\", \"Angus\", NA, \"Hereford\", \"Hereford\")\n)\n\n# Remove empty rows and columns\ndata_with_empties %&gt;%\n  remove_empty(c(\"rows\", \"cols\"))\n\n# A tibble: 4 √ó 3\n     id weight breed   \n  &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n1     1     50 Angus   \n2     2     55 Angus   \n3     4     60 Hereford\n4     5     65 Hereford\n\n# Find duplicates\ndup_data &lt;- tibble(\n  animal_id = c(\"A001\", \"A002\", \"A003\", \"A001\", \"A004\"),\n  weight = c(50, 55, 60, 50, 65)\n)\n\ndup_data %&gt;%\n  get_dupes(animal_id)\n\n# A tibble: 2 √ó 3\n  animal_id dupe_count weight\n  &lt;chr&gt;          &lt;int&gt;  &lt;dbl&gt;\n1 A001               2     50\n2 A001               2     50\n\n\n\n\n\n8.5.2 The lubridate Package\nWorking with dates and times in base R is painful. lubridate makes it intuitive.\n\n8.5.2.1 Parsing Dates\n\n# Different date formats\ndates_various &lt;- c(\"2024-01-15\", \"January 20, 2024\", \"03/25/2024\", \"2024-04-10\")\n\n# lubridate parsing functions\nymd(\"2024-01-15\")  # Year-Month-Day\n\n[1] \"2024-01-15\"\n\nmdy(\"03/25/2024\")  # Month-Day-Year\n\n[1] \"2024-03-25\"\n\ndmy(\"15-01-2024\")  # Day-Month-Year\n\n[1] \"2024-01-15\"\n\n# Parse with times\nymd_hms(\"2024-01-15 14:30:00\")\n\n[1] \"2024-01-15 14:30:00 UTC\"\n\n\n\n\n8.5.2.2 Extracting Components\n\n# Sample birthdates\ncattle &lt;- tibble(\n  animal_id = c(\"C001\", \"C002\", \"C003\", \"C004\"),\n  birth_date = ymd(c(\"2023-03-15\", \"2023-04-02\", \"2023-03-28\", \"2023-05-10\"))\n)\n\ncattle &lt;- cattle %&gt;%\n  mutate(\n    birth_year = year(birth_date),\n    birth_month = month(birth_date, label = TRUE),  # Jan, Feb, etc.\n    birth_day = day(birth_date),\n    birth_week = week(birth_date),\n    birth_quarter = quarter(birth_date)\n  )\n\ncattle\n\n# A tibble: 4 √ó 7\n  animal_id birth_date birth_year birth_month birth_day birth_week birth_quarter\n  &lt;chr&gt;     &lt;date&gt;          &lt;dbl&gt; &lt;ord&gt;           &lt;int&gt;      &lt;dbl&gt;         &lt;int&gt;\n1 C001      2023-03-15       2023 Mar                15         11             1\n2 C002      2023-04-02       2023 Apr                 2         14             2\n3 C003      2023-03-28       2023 Mar                28         13             1\n4 C004      2023-05-10       2023 May                10         19             2\n\n\n\n\n8.5.2.3 Date Arithmetic\n\n# Calculate age in days\ncattle &lt;- cattle %&gt;%\n  mutate(\n    today_date = ymd(\"2024-11-14\"),\n    age_days = as.numeric(today_date - birth_date),\n    age_weeks = age_days / 7,\n    age_months = age_days / 30.44  # Approximate\n  )\n\ncattle %&gt;%\n  select(animal_id, birth_date, age_days, age_weeks)\n\n# A tibble: 4 √ó 4\n  animal_id birth_date age_days age_weeks\n  &lt;chr&gt;     &lt;date&gt;        &lt;dbl&gt;     &lt;dbl&gt;\n1 C001      2023-03-15      610      87.1\n2 C002      2023-04-02      592      84.6\n3 C003      2023-03-28      597      85.3\n4 C004      2023-05-10      554      79.1\n\n# Add time intervals\ncattle %&gt;%\n  mutate(\n    weaning_date = birth_date + weeks(8),\n    yearling_date = birth_date + years(1)\n  ) %&gt;%\n  select(animal_id, birth_date, weaning_date, yearling_date)\n\n# A tibble: 4 √ó 4\n  animal_id birth_date weaning_date yearling_date\n  &lt;chr&gt;     &lt;date&gt;     &lt;date&gt;       &lt;date&gt;       \n1 C001      2023-03-15 2023-05-10   2024-03-15   \n2 C002      2023-04-02 2023-05-28   2024-04-02   \n3 C003      2023-03-28 2023-05-23   2024-03-28   \n4 C004      2023-05-10 2023-07-05   2024-05-10   \n\n\n\n\n8.5.2.4 Working with Periods and Durations\n\n# Periods (human units: 1 month = 1 month regardless of days)\ntoday() + months(1)\n\n[1] \"2025-12-15\"\n\ntoday() + weeks(2)\n\n[1] \"2025-11-29\"\n\n# Durations (exact seconds)\ntoday() + ddays(30)  # Exactly 30 days\n\n[1] \"2025-12-15\"\n\ntoday() + dhours(24)  # Exactly 24 hours\n\n[1] \"2025-11-16\"\n\n# Time intervals\nbirth &lt;- ymd(\"2023-03-15\")\nweaning &lt;- ymd(\"2023-05-10\")\n\ninterval(birth, weaning) / days(1)  # Days between dates\n\n[1] 56\n\ninterval(birth, weaning) / weeks(1)  # Weeks between dates\n\n[1] 8\n\n\n\n\n\n\n\n\nTipCommon lubridate Functions\n\n\n\n\nParsing: ymd(), mdy(), dmy(), ymd_hms()\nExtracting: year(), month(), day(), week(), quarter(), wday()\nArithmetic: + / - with days(), weeks(), months(), years()\nCurrent date/time: today(), now()\nIntervals: interval(), time_length()\n\n\n\n\n\n\n8.5.3 The glue Package\nglue provides elegant string interpolation (inserting variables into strings).\n\n# Instead of paste() or paste0()\nanimal_id &lt;- \"C001\"\nweight &lt;- 520\nbreed &lt;- \"Angus\"\n\n# Base R way (clunky)\npaste0(\"Animal \", animal_id, \" (\", breed, \") weighs \", weight, \" kg\")\n\n[1] \"Animal C001 (Angus) weighs 520 kg\"\n\n# glue way (elegant!)\nglue(\"Animal {animal_id} ({breed}) weighs {weight} kg\")\n\nAnimal C001 (Angus) weighs 520 kg\n\n\n\n8.5.3.1 glue_data() for Data Frames\n\n# Create reports for each row\ncattle_report &lt;- tibble(\n  animal_id = c(\"C001\", \"C002\", \"C003\"),\n  breed = c(\"Angus\", \"Hereford\", \"Charolais\"),\n  weight = c(520, 495, 535),\n  gain = c(1.45, 1.38, 1.52)\n)\n\ncattle_report %&gt;%\n  mutate(\n    summary = glue_data(.,\n      \"Animal {animal_id} is a {breed} weighing {weight} kg with a daily gain of {gain} kg/day.\"\n    )\n  ) %&gt;%\n  pull(summary)\n\nAnimal C001 is a Angus weighing 520 kg with a daily gain of 1.45 kg/day.\nAnimal C002 is a Hereford weighing 495 kg with a daily gain of 1.38 kg/day.\nAnimal C003 is a Charolais weighing 535 kg with a daily gain of 1.52 kg/day.\n\n\n\n\n8.5.3.2 Dynamic Column Names\n\n# Create dynamic column names in summaries\ntreatment_var &lt;- \"treatment\"\nmetric &lt;- \"weight_gain\"\n\n# Use glue for dynamic naming\nsummary_data &lt;- tibble(\n  treatment = c(\"Control\", \"Treatment A\"),\n  avg_gain = c(1.35, 1.48)\n)\n\nsummary_data %&gt;%\n  rename(\n    \"{treatment_var}\" := treatment,\n    \"avg_{metric}\" := avg_gain\n  )\n\n# A tibble: 2 √ó 2\n  treatment   avg_weight_gain\n  &lt;chr&gt;                 &lt;dbl&gt;\n1 Control                1.35\n2 Treatment A            1.48",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#the-complete-data-science-workflow",
    "href": "chapters/ch08-special_formats.html#the-complete-data-science-workflow",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.6 The Complete Data Science Workflow",
    "text": "8.6 The Complete Data Science Workflow\nLet‚Äôs synthesize everything you‚Äôve learned into a comprehensive workflow. This is what you do when tackling a real data analysis project:\n\nWorkflow DiagramEight-Week Review\n\n\n\n\n\n\n\nflowchart TD\n    A[1. Define Question] --&gt; B[2. Import Data]\n    B --&gt; C[3. Explore & Clean]\n    C --&gt; D[4. Transform & Reshape]\n    D --&gt; E[5. Visualize]\n    E --&gt; F[6. Model/Analyze]\n    F --&gt; G[7. Communicate Results]\n    G --&gt; H{Need to Refine?}\n    H --&gt;|Yes| C\n    H --&gt;|No| I[Final Report]\n\n    style A fill:#e1f5ff\n    style I fill:#d4edda\n\n\n\n\n\n\n\n\n\nWeek 1: Foundations\n\nWhat is data science\nBest practices (project structure, file naming, database concepts)\nR, RStudio, Quarto setup\nVersion control basics\n\nWeek 2: Getting Started\n\nRStudio interface\nR Projects and working directories\nReading CSV (readr::read_csv()) and Excel (readxl::read_excel())\nFirst look at data: glimpse(), summary()\n\nWeek 3: Data Types & Strings\n\nR data types (numeric, character, factor, logical, dates)\nString manipulation (stringr)\nRegular expressions\nThe pipe operator: %&gt;% and |&gt;\nFirst dplyr verbs: select(), filter()\n\nWeek 4: Data Manipulation\n\nCore dplyr verbs: mutate(), arrange(), group_by(), summarise()\nacross() for multiple columns\nConditional logic: if_else(), case_when()\nHandling missing data: drop_na(), replace_na()\nWindow functions: lag(), lead(), cumsum()\n\nWeek 5: Visualization Basics\n\nGrammar of Graphics\nEssential geoms: geom_point(), geom_line(), geom_col(), geom_histogram(), geom_boxplot(), geom_violin()\nAesthetic mappings: x, y, color, fill, size, shape\nScales, themes, labels\nSaving plots with ggsave()\n\nWeek 6: Advanced Visualization\n\nFaceting: facet_wrap(), facet_grid()\nStatistical layers: geom_smooth(), stat_summary()\nCustom themes and color palettes\nText annotations\nCombining plots: cowplot, patchwork\n\nWeek 7: Reshaping & Joining\n\nTidy data principles\nReshaping: pivot_longer(), pivot_wider(), separate(), unite()\nJoining: left_join(), right_join(), inner_join(), full_join()\nFunctional programming: purrr::map() functions\nList columns and nested data\n\nWeek 8: Integration (This Week!)\n\nReading statistical software files (haven)\nExcel workflows (readxl, writexl, openxlsx)\nData cleaning (janitor)\nDate/time handling (lubridate)\nString interpolation (glue)\nComplete workflow synthesis\n\n\n\n\n\n\n8.6.1 Real-World Example: Complete Analysis Pipeline\nLet‚Äôs work through a complete example from raw messy data to final report.\nScenario: You receive cattle weight data from three sources: 1. Birth records from a colleague (SAS file) 2. Monthly weight measurements (messy Excel file) 3. Treatment assignments (CSV file)\nYour task: Analyze weight gain by treatment and create a report.\n\n8.6.1.1 Step 1: Import from Multiple Sources\n\n# Source 1: Birth records (SAS file - simulated)\nbirth_records &lt;- tibble(\n  animal_id = sprintf(\"C%03d\", 1:20),\n  birth_date = ymd(\"2023-03-01\") + days(sample(0:30, 20, replace = TRUE)),\n  dam_id = sample(1001:1010, 20, replace = TRUE),\n  birth_weight_kg = rnorm(20, mean = 42, sd = 5)\n)\n\n# Source 2: Monthly weights (messy Excel - simulated)\nweight_measurements &lt;- tibble(\n  `Animal ID` = rep(sprintf(\"C%03d\", 1:20), each = 4),\n  `Measurement Date` = rep(c(\"2023-04-01\", \"2023-05-01\", \"2023-06-01\", \"2023-07-01\"), times = 20),\n  `Body Weight (kg)` = rnorm(80, mean = 150, sd = 30) + rep(0:3, times = 20) * 50,\n  `Notes` = sample(c(NA, \"healthy\", \"\"), 80, replace = TRUE)\n)\n\n# Source 3: Treatment assignments (CSV)\ntreatments &lt;- tibble(\n  animal_id = sprintf(\"C%03d\", 1:20),\n  treatment = sample(c(\"Control\", \"High Protein\", \"Standard\"), 20, replace = TRUE),\n  assigned_date = ymd(\"2023-03-15\")\n)\n\n\n\n8.6.1.2 Step 2: Clean Each Dataset\n\n# Clean birth records (already clean in this case)\nbirth_clean &lt;- birth_records\n\n# Clean weight measurements\nweights_clean &lt;- weight_measurements %&gt;%\n  clean_names() %&gt;%  # Fix column names\n  mutate(\n    measurement_date = ymd(measurement_date),\n    body_weight_kg = round(body_weight_kg, 1)\n  ) %&gt;%\n  select(-notes)  # Remove empty notes column\n\n# Clean treatments (already clean)\ntreatments_clean &lt;- treatments\n\nglimpse(weights_clean)\n\nRows: 80\nColumns: 3\n$ animal_id        &lt;chr&gt; \"C001\", \"C001\", \"C001\", \"C001\", \"C002\", \"C002\", \"C002‚Ä¶\n$ measurement_date &lt;date&gt; 2023-04-01, 2023-05-01, 2023-06-01, 2023-07-01, 2023‚Ä¶\n$ body_weight_kg   &lt;dbl&gt; 150.1, 219.3, 260.1, 278.9, 152.4, 260.7, 292.4, 300.‚Ä¶\n\n\n\n\n8.6.1.3 Step 3: Join Datasets\n\n# Join everything together\ncomplete_data &lt;- birth_clean %&gt;%\n  left_join(treatments_clean, by = \"animal_id\") %&gt;%\n  left_join(weights_clean, by = \"animal_id\") %&gt;%\n  # Calculate age at measurement\n  mutate(\n    age_days = as.numeric(measurement_date - birth_date),\n    age_weeks = age_days / 7\n  )\n\nglimpse(complete_data)\n\nRows: 80\nColumns: 10\n$ animal_id        &lt;chr&gt; \"C001\", \"C001\", \"C001\", \"C001\", \"C002\", \"C002\", \"C002‚Ä¶\n$ birth_date       &lt;date&gt; 2023-03-16, 2023-03-16, 2023-03-16, 2023-03-16, 2023‚Ä¶\n$ dam_id           &lt;int&gt; 1004, 1004, 1004, 1004, 1009, 1009, 1009, 1009, 1001,‚Ä¶\n$ birth_weight_kg  &lt;dbl&gt; 42.34456, 42.34456, 42.34456, 42.34456, 48.63042, 48.‚Ä¶\n$ treatment        &lt;chr&gt; \"High Protein\", \"High Protein\", \"High Protein\", \"High‚Ä¶\n$ assigned_date    &lt;date&gt; 2023-03-15, 2023-03-15, 2023-03-15, 2023-03-15, 2023‚Ä¶\n$ measurement_date &lt;date&gt; 2023-04-01, 2023-05-01, 2023-06-01, 2023-07-01, 2023‚Ä¶\n$ body_weight_kg   &lt;dbl&gt; 150.1, 219.3, 260.1, 278.9, 152.4, 260.7, 292.4, 300.‚Ä¶\n$ age_days         &lt;dbl&gt; 16, 46, 77, 107, 26, 56, 87, 117, 31, 61, 92, 122, 9,‚Ä¶\n$ age_weeks        &lt;dbl&gt; 2.285714, 6.571429, 11.000000, 15.285714, 3.714286, 8‚Ä¶\n\n\n\n\n8.6.1.4 Step 4: Transform and Summarize\n\n# Calculate growth rates\ngrowth_analysis &lt;- complete_data %&gt;%\n  group_by(animal_id, treatment) %&gt;%\n  summarise(\n    birth_weight = first(birth_weight_kg),\n    final_weight = last(body_weight_kg),\n    total_gain = final_weight - birth_weight,\n    days_measured = max(age_days),\n    avg_daily_gain = total_gain / days_measured,\n    .groups = \"drop\"\n  )\n\n# Summary by treatment\ntreatment_summary &lt;- growth_analysis %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    n = n(),\n    mean_adg = mean(avg_daily_gain),\n    sd_adg = sd(avg_daily_gain),\n    min_adg = min(avg_daily_gain),\n    max_adg = max(avg_daily_gain)\n  )\n\ntreatment_summary\n\n# A tibble: 3 √ó 6\n  treatment        n mean_adg sd_adg min_adg max_adg\n  &lt;chr&gt;        &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 Control          7     2.49  0.341    1.85    2.96\n2 High Protein     5     2.36  0.318    1.96    2.76\n3 Standard         8     2.34  0.307    1.89    2.75\n\n\n\n\n8.6.1.5 Step 5: Visualize\n\n# Growth curves by treatment\ncomplete_data %&gt;%\n  ggplot(aes(x = age_weeks, y = body_weight_kg,\n             color = treatment, group = animal_id)) +\n  geom_line(alpha = 0.5) +\n  geom_smooth(aes(group = treatment), method = \"lm\", se = TRUE, size = 1.5) +\n  labs(\n    title = \"Cattle Growth Curves by Treatment\",\n    x = \"Age (weeks)\",\n    y = \"Body Weight (kg)\",\n    color = \"Treatment\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n# Distribution of daily gains\ngrowth_analysis %&gt;%\n  ggplot(aes(x = treatment, y = avg_daily_gain, fill = treatment)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.5) +\n  labs(\n    title = \"Average Daily Gain by Treatment\",\n    x = \"Treatment\",\n    y = \"Average Daily Gain (kg/day)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n8.6.1.6 Step 6: Create Summary Report\n\n# Generate text summaries with glue\ntreatment_summary %&gt;%\n  mutate(\n    report_text = glue(\n      \"The {treatment} group (n={n}) had an average daily gain of {round(mean_adg, 3)} kg/day (SD = {round(sd_adg, 3)}).\"\n    )\n  ) %&gt;%\n  pull(report_text)\n\nThe Control group (n=7) had an average daily gain of 2.49 kg/day (SD = 0.341).\nThe High Protein group (n=5) had an average daily gain of 2.359 kg/day (SD = 0.318).\nThe Standard group (n=8) had an average daily gain of 2.342 kg/day (SD = 0.307).\n\n\n\n\n\n\n\n\nNoteComplete Workflow in Action\n\n\n\nThis example demonstrates: 1. Importing from multiple sources 2. Cleaning with janitor::clean_names() and date parsing 3. Joining with left_join() 4. Transforming with mutate() and group_by() %&gt;% summarise() 5. Visualizing with ggplot2 6. Communicating with summary statistics and glue\nThis is exactly what you‚Äôll do in your capstone project!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#reproducibility-best-practices",
    "href": "chapters/ch08-special_formats.html#reproducibility-best-practices",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.7 Reproducibility Best Practices",
    "text": "8.7 Reproducibility Best Practices\nReproducibility means someone else (including future you!) can re-run your analysis and get the same results. Here‚Äôs a checklist:\n\n8.7.1 Project Organization Checklist\nmy_project/\n‚îú‚îÄ‚îÄ README.md              # Project overview, how to run\n‚îú‚îÄ‚îÄ my_project.Rproj       # RStudio project file\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/               # Original, untouched data (read-only!)\n‚îÇ   ‚îî‚îÄ‚îÄ processed/         # Cleaned data (created by scripts)\n‚îú‚îÄ‚îÄ code/\n‚îÇ   ‚îú‚îÄ‚îÄ 01_import.R        # Data import scripts\n‚îÇ   ‚îú‚îÄ‚îÄ 02_clean.R         # Cleaning scripts\n‚îÇ   ‚îî‚îÄ‚îÄ 03_analysis.R      # Analysis scripts\n‚îú‚îÄ‚îÄ output/\n‚îÇ   ‚îú‚îÄ‚îÄ figures/           # Generated plots\n‚îÇ   ‚îî‚îÄ‚îÄ tables/            # Generated tables\n‚îú‚îÄ‚îÄ reports/\n‚îÇ   ‚îî‚îÄ‚îÄ final_report.qmd   # Quarto reports\n‚îî‚îÄ‚îÄ renv.lock              # Package versions (if using renv)\n\n\n\n\n\n\nImportantGolden Rules\n\n\n\n\nNever modify raw data files - Keep originals pristine\nUse R Projects - Avoid setwd(), use relative paths\nDocument everything - Comments, README, codebooks\nVersion control - Use Git from the start\nExplicit packages - Always use library() at the top of scripts\n\n\n\n\n\n8.7.2 File Naming Conventions\nGood file names: - 2024-11-14_cattle-weights_final.csv - 01_import_data.R - fig01_growth_curves.png\nBad file names: - data.csv (not descriptive) - analysis final FINAL v2.R (version chaos) - my file with spaces.csv (spaces cause problems)\nPrinciples: - Use dates: YYYY-MM-DD_description.ext - Use numbers for order: 01_, 02_, 03_ - Use hyphens or underscores (no spaces) - Be descriptive but concise - Use lowercase\n\n\n8.7.3 Code Style Best Practices\n\n# GOOD: Clear, readable, consistent\ncattle_data &lt;- read_csv(\"../data/raw/cattle_weights.csv\") %&gt;%\n  clean_names() %&gt;%\n  filter(weight_kg &gt; 0) %&gt;%\n  mutate(\n    weight_lb = weight_kg * 2.20462,\n    weight_category = case_when(\n      weight_kg &lt; 400 ~ \"Light\",\n      weight_kg &lt; 500 ~ \"Medium\",\n      TRUE ~ \"Heavy\"\n    )\n  )\n\n# BAD: Hard to read, inconsistent spacing\ncattle_data&lt;-read_csv(\"../data/raw/cattle_weights.csv\")%&gt;%clean_names()%&gt;%filter(weight_kg&gt;0)%&gt;%mutate(weight_lb=weight_kg*2.20462,weight_category=case_when(weight_kg&lt;400~\"Light\",weight_kg&lt;500~\"Medium\",TRUE~\"Heavy\"))\n\nStyle Guidelines: - Use &lt;- for assignment (not =) - Space around operators: x + y not x+y - Space after commas: f(x, y) not f(x,y) - Indent code blocks (2 or 4 spaces) - Break long pipes into multiple lines - Use meaningful variable names: cattle_weights not cw or x\n\n\n8.7.4 Package Version Management\nR packages update frequently. To ensure reproducibility across time:\n\n# Option 1: Document package versions in your report\nsessionInfo()\n\n# Option 2: Use renv for package management (recommended for important projects)\n# install.packages(\"renv\")\n# renv::init()    # Initialize renv for project\n# renv::snapshot() # Save package versions\n# renv::restore()  # Restore exact package versions\n\n\n\n8.7.5 Quarto/R Markdown Best Practices\n---\ntitle: \"Cattle Growth Analysis\"\nauthor: \"Your Name\"\ndate: today  # Auto-updates to current date\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    code-fold: show  # Or 'false' to hide code\n    code-tools: true\n    embed-resources: true  # Single-file output\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n  cache: true  # Cache results for faster re-rendering\n---\nCode Chunk Options: - #| eval: true - Run the code - #| echo: true - Show the code - #| warning: false - Hide warnings - #| message: false - Hide messages - #| fig-width: 8 - Control figure dimensions - #| fig-cap: \"Caption text\" - Add figure captions\n\n\n\n\n\n\nTipReproducibility Checklist\n\n\n\nBefore sharing your analysis:\n\nAll file paths are relative (no hard-coded /Users/yourname/)\nRaw data files are unchanged and documented\nAll required packages are loaded at the top\nCode runs from start to finish without errors\nRandom processes use set.seed() for reproducibility\nREADME explains how to run the analysis\nOutput folder contains generated figures/tables\nQuarto document renders successfully",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#transitioning-to-part-2-how-data-science-enables-statistics",
    "href": "chapters/ch08-special_formats.html#transitioning-to-part-2-how-data-science-enables-statistics",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.8 Transitioning to Part 2: How Data Science Enables Statistics",
    "text": "8.8 Transitioning to Part 2: How Data Science Enables Statistics\nYou‚Äôve spent 8 weeks building essential data manipulation and visualization skills. Now you might wonder: Why was this necessary? How does this connect to statistics?\n\n8.8.1 The Foundation for Statistical Analysis\nStatistical analyses require clean, well-structured data. Consider what you need before running a t-test or regression:\n\nData must be imported - You can‚Äôt analyze data you can‚Äôt load (Weeks 2, 8)\nData must be clean - Missing values, wrong types, messy names cause errors (Weeks 3, 4, 8)\nData must be in the right format - Wide vs long, joined tables (Week 7)\nYou must understand your data - Distributions, outliers, patterns (Weeks 5, 6)\nYou must communicate results - Tables, plots, reports (Weeks 5, 6, Quarto throughout)\n\nThe reality: 80% of a data scientist‚Äôs time is spent on steps 1-4. Only 20% is the ‚Äúsexy‚Äù statistical modeling.\n\n\n8.8.2 Example: What Part 1 Enables in Part 2\nLet‚Äôs say you want to test if a new feed supplement affects cattle weight gain (Week 4 of Part 2: t-tests).\nPart 1 skills needed:\n\n# Import data (Week 2)\ncattle &lt;- read_csv(\"cattle_trial.csv\")\n\n# Clean and prepare (Weeks 3, 4)\ncattle_clean &lt;- cattle %&gt;%\n  clean_names() %&gt;%\n  filter(!is.na(weight_gain_kg)) %&gt;%\n  mutate(\n    treatment = factor(treatment, levels = c(\"Control\", \"Supplement\")),\n    weight_gain_kg = as.numeric(weight_gain_kg)\n  )\n\n# Check assumptions with visualization (Week 5)\ncattle_clean %&gt;%\n  ggplot(aes(x = treatment, y = weight_gain_kg)) +\n  geom_boxplot()\n\n# NOW you're ready for Part 2: Statistical test\nt.test(weight_gain_kg ~ treatment, data = cattle_clean)\n\nWithout Part 1 skills, you‚Äôd be stuck at the import or cleaning stage!\n\n\n8.8.3 Preview of Part 2 Topics\nYou‚Äôll learn to answer questions like:\n\nWeek 1-2: Are these two groups really different, or is it just random variation?\nWeek 3: What‚Äôs the probability of observing this result by chance?\nWeek 4: Is this treatment effective? (t-tests)\nWeek 5: Do these three feed types differ? (ANOVA)\nWeek 6: Are these two variables associated? (Chi-square tests)\nWeek 7: Can I predict weight from age? (Simple regression)\nWeek 8: How do multiple factors affect the outcome? (Multiple regression)\n\nThe connection: Every statistical test in Part 2 builds on the data wrangling and visualization skills you‚Äôve mastered in Part 1.\n\n\n8.8.4 Your Advantage\nBy learning data science before statistics, you have a huge advantage over traditional statistics courses:\n\nYou won‚Äôt be stuck fighting with data import and cleaning\nYou can visualize data to build intuition before testing\nYou can create professional reports to communicate findings\nYou understand reproducibility from the start\n\nMany students learn statistics first and struggle to apply it because they lack these skills. You‚Äôre ahead of the curve!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#career-paths-in-data-science",
    "href": "chapters/ch08-special_formats.html#career-paths-in-data-science",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.9 Career Paths in Data Science",
    "text": "8.9 Career Paths in Data Science\nData science skills are in high demand across industries. Here are potential career paths, especially relevant to animal science backgrounds:\n\n8.9.1 1. Data Analyst\nWhat they do: - Explore data to find patterns - Create dashboards and reports - Support business/research decisions with data\nSkills emphasized: - SQL and database queries - Data visualization (Tableau, Power BI, or R/ggplot2) - Basic statistics - Communication with non-technical stakeholders\nAnimal science examples: - Analyzing herd performance data for dairy operations - Market analysis for livestock commodities - Reporting on feed efficiency metrics\nTypical salary: $60,000 - $90,000 USD\n\n\n\n8.9.2 2. Data Scientist\nWhat they do: - Build predictive models - Design experiments (A/B testing) - Extract insights from complex data - Communicate findings to guide strategy\nSkills emphasized: - Statistical modeling and machine learning - Programming (R, Python) - Data visualization and storytelling - Domain expertise (animal science is valuable!)\nAnimal science examples: - Predicting disease outbreaks in livestock populations - Optimizing breeding programs with genomic data - Forecasting feed prices and supply chain logistics\nTypical salary: $90,000 - $140,000 USD\n\n\n\n8.9.3 3. Quantitative Analyst (Animal Science Focus)\nWhat they do: - Analyze breeding values and genetic data - Model growth curves and production traits - Support precision livestock farming initiatives\nSkills emphasized: - Statistical genetics - R programming (especially for quantitative genetics) - Knowledge of linear models and BLUP - Domain expertise in animal breeding/nutrition\nAnimal science examples: - Calculating EPDs (Expected Progeny Differences) for breed associations - Analyzing feed efficiency and genetic correlations - Supporting genomic selection programs\nTypical salary: $70,000 - $110,000 USD\n\n\n\n8.9.4 4. Machine Learning Engineer (Industry/Tech)\nWhat they do: - Build and deploy ML models at scale - Work on computer vision, NLP, recommendation systems - Optimize model performance and infrastructure\nSkills emphasized: - Python programming - Deep learning frameworks (TensorFlow, PyTorch) - Cloud platforms (AWS, GCP, Azure) - Software engineering practices\nAnimal science examples: - Computer vision for livestock monitoring (activity, health scoring) - Predictive models for precision agriculture - Sensor data analysis (wearables for animals)\nTypical salary: $110,000 - $180,000 USD (tech companies)\n\n\n\n8.9.5 5. Researcher / Academic Data Scientist\nWhat they do: - Conduct research using quantitative methods - Publish peer-reviewed papers - Teach statistics and data science - Collaborate on grants and interdisciplinary projects\nSkills emphasized: - Advanced statistics and experimental design - R and Quarto for reproducible research - Domain expertise in animal science - Scientific writing and communication\nAnimal science examples: - University faculty studying animal nutrition with data-driven approaches - USDA researchers analyzing agricultural data - Industry R&D scientists at feed companies\nTypical salary: $65,000 - $120,000 USD (varies widely by institution)\n\n\n\n8.9.6 What Skills Do You Need Beyond This Course?\n\n8.9.6.1 For Most Data Science Roles:\n\nSQL: Querying databases (critical!)\nMore statistics: Hypothesis testing, regression, time series (Part 2 covers basics)\nMachine learning basics: Classification, regression, clustering\nPython (optional but valuable): Complements R skills\nVersion control: Git and GitHub (introduced in Week 1)\nCommunication: Writing reports, presenting to stakeholders\n\n\n\n8.9.6.2 For Specialized Roles:\n\nQuantitative genetics: BLUP, genomic selection (for breeding/genetics roles)\nCloud computing: AWS, GCP for scaling analyses\nBig data tools: Spark, Hadoop (for massive datasets)\nDeep learning: Neural networks for computer vision or complex prediction\n\n\n\n\n8.9.7 Resources for Continued Learning\nR Programming: - R for Data Science (2e) by Hadley Wickham & Garrett Grolemund - Advanced R by Hadley Wickham (when you‚Äôre ready to level up)\nStatistics: - Learning Statistics with R by Danielle Navarro - Statistical Rethinking by Richard McElreath (Bayesian focus)\nMachine Learning: - An Introduction to Statistical Learning (ISLR) - Free textbook with R code - Tidy Modeling with R by Max Kuhn & Julia Silge\nSQL: - SQL for Data Scientists - Mode SQL Tutorial (interactive)\nPython (if desired): - Python for Data Analysis by Wes McKinney - Python Data Science Handbook\nQuarto & Reproducibility: - Quarto Documentation - Happy Git with R by Jenny Bryan\nCommunities: - R for Data Science (R4DS) Online Learning Community - Slack community - RStudio Community - Forum for questions - Stack Overflow - Technical Q&A - Twitter/X #RStats hashtag - R community updates",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#summary",
    "href": "chapters/ch08-special_formats.html#summary",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.10 Summary",
    "text": "8.10 Summary\nCongratulations! You‚Äôve completed Part 1 of this course. Let‚Äôs recap what you‚Äôve accomplished:\n\n8.10.1 Skills You‚Äôve Mastered\n\nData Import: CSV, Excel, SAS, SPSS, Stata files\nData Cleaning: Fixing names, handling missing data, removing duplicates\nData Transformation: dplyr verbs, grouped summaries, conditional logic\nData Reshaping: Wide to long, long to wide, separating and uniting columns\nData Joining: Combining multiple datasets with all join types\nString Manipulation: stringr functions and regular expressions\nDate/Time Handling: Parsing, extracting, arithmetic with lubridate\nData Visualization: Creating publication-quality plots with ggplot2\nFunctional Programming: Iteration with purrr::map() functions\nReproducible Reporting: Professional reports with Quarto\nBest Practices: Project organization, version control, code style\n\n\n\n8.10.2 The Journey Ahead\nYou‚Äôve built a solid foundation in data science. These skills will serve you throughout your career, whether you pursue:\n\nGraduate research in animal science\nIndustry roles in livestock/agriculture\nData science positions in any field\nTeaching and extension work\n\nPart 2 (Statistics) will build directly on these skills. You‚Äôll learn to: - Test hypotheses with confidence - Quantify uncertainty - Build predictive models - Make data-driven decisions\n\n\n8.10.3 A Note of Encouragement\nData science is a journey, not a destination. You won‚Äôt master everything immediately, and that‚Äôs okay. What matters is:\n\nYou know what‚Äôs possible\nYou can find help when stuck (documentation, communities, Google/Stack Overflow)\nYou understand reproducibility and best practices\nYou‚Äôre comfortable enough with R to keep learning\n\nKeep practicing! The more you use these skills on real projects, the more natural they‚Äôll become.\n\n\n\n\n\n\nTipNext Steps\n\n\n\n\nComplete the capstone project below (this integrates everything!)\nReview any topics from Weeks 1-8 that feel unclear\nExplore the additional resources section\nGet ready for Part 2: Introduction to Statistics!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#homework-final-part-1-capstone-project",
    "href": "chapters/ch08-special_formats.html#homework-final-part-1-capstone-project",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.11 Homework: Final Part 1 Capstone Project",
    "text": "8.11 Homework: Final Part 1 Capstone Project\n\n8.11.1 Overview\nThis is your opportunity to demonstrate everything you‚Äôve learned in Part 1. You‚Äôll work with messy, multi-source data from a realistic animal science scenario, clean it, analyze it, visualize it, and present your findings in a professional Quarto report.\nTotal Points: 100\n\n\n\n8.11.2 Scenario\nYou are a data analyst for a large cattle feedlot operation. Management wants to understand factors affecting cattle performance to optimize feeding strategies. You‚Äôve been given data from three sources:\n\nAnimal records (SAS file): Birth dates, breeds, initial weights\nWeight measurements (messy Excel file): Monthly weight measurements with inconsistent formatting\nFeed treatments (CSV file): Which animals received which feed rations\n\nYour task is to integrate these datasets, clean them, analyze growth patterns, and report your findings.\n\n\n\n8.11.3 Part 1: Data Import and Initial Exploration (20 points)\nTasks:\n\nCreate a well-organized R Project with appropriate folder structure:\ncapstone_project/\n‚îú‚îÄ‚îÄ data/\n‚îÇ   ‚îú‚îÄ‚îÄ raw/          # Original data files\n‚îÇ   ‚îî‚îÄ‚îÄ processed/    # Cleaned data\n‚îú‚îÄ‚îÄ code/\n‚îÇ   ‚îî‚îÄ‚îÄ analysis.qmd  # Your Quarto report\n‚îú‚îÄ‚îÄ output/\n‚îÇ   ‚îî‚îÄ‚îÄ figures/      # Generated plots\n‚îî‚îÄ‚îÄ README.md         # Project description\nImport all three datasets:\n\nanimal_records.sas7bdat (haven)\nweight_measurements.xlsx (readxl)\nfeed_treatments.csv (readr)\n\nUse glimpse(), summary(), and head() to explore each dataset\nDocument any initial observations about data quality issues\n\nDeliverable: R code showing import and initial exploration with commentary\nData files will be provided separately with: - 200 cattle - 6 monthly weight measurements per animal - 3 feed treatment groups - Intentional messiness: missing values, inconsistent column names, date formats, etc.\n\n\n\n8.11.4 Part 2: Data Cleaning (25 points)\nTasks:\n\nClean column names in the Excel file using janitor::clean_names()\nFix date formats using lubridate functions\nHandle missing values:\n\nIdentify which variables have NAs\nDecide on an appropriate strategy (remove, impute, or flag)\nDocument your decisions\n\nRemove duplicates if any exist (use janitor::get_dupes() to check)\nFix data types:\n\nEnsure weights are numeric\nEnsure breeds and treatments are factors with meaningful levels\nEnsure dates are Date class\n\nCreate derived variables:\n\nCalculate age (days) at each weight measurement\nCalculate weight gain between consecutive measurements\nCreate a birth season variable (Winter/Spring/Summer/Fall)\n\n\nDeliverable: Clean, well-structured datasets with code and explanations\n\n\n\n8.11.5 Part 3: Data Integration (15 points)\nTasks:\n\nJoin the three datasets into a single analysis-ready dataset\n\nChoose appropriate join types (likely left_join())\nEnsure you don‚Äôt lose data unintentionally\nCheck the dimensions before and after joining\n\nReshape data if needed:\n\nYou may need pivot_longer() or pivot_wider() depending on your analysis approach\n\nCreate a summary table showing:\n\nNumber of animals per treatment\nNumber of animals per breed\nNumber of measurements per animal\n\n\nDeliverable: A single tidy dataset ready for analysis and summary tables\n\n\n\n8.11.6 Part 4: Exploratory Data Analysis (20 points)\nTasks:\nCreate at least 5 visualizations addressing these questions:\n\nGrowth curves: How do cattle weights change over time?\n\nLine plot with time on x-axis, weight on y-axis\nColor by treatment or breed\n\nTreatment comparison: Do treatments affect final weights?\n\nBox plots or violin plots comparing treatments\n\nBreed differences: Are there breed differences in growth?\n\nFaceted plots or colored lines\n\nWeight gain distribution: What‚Äôs the distribution of daily gain?\n\nHistogram or density plot\n\nRelationships: Is initial weight related to final weight?\n\nScatter plot with trend line\n\n\nRequirements for each plot: - Appropriate geom choice - Clear axis labels and titles - Proper use of color/fill - Clean theme - Saved to output/figures/ with ggsave()\nDeliverable: Five publication-quality plots with interpretation\n\n\n\n8.11.7 Part 5: Summary Statistics (10 points)\nTasks:\n\nCalculate summary statistics by treatment:\n\nMean and SD of initial weight\nMean and SD of final weight\nMean and SD of total weight gain\nMean and SD of average daily gain\n\nCreate a nicely formatted table (use knitr::kable() or gt package)\nWrite 2-3 sentences interpreting the results:\n\nWhich treatment performed best?\nIs the difference substantial?\n\n\nDeliverable: Summary table and written interpretation\n\n\n\n8.11.8 Part 6: Professional Quarto Report (10 points)\nTasks:\nStructure your report with these sections:\n\nTitle and Author Information\nExecutive Summary (2-3 paragraphs summarizing goals, methods, key findings)\nIntroduction (background and objectives)\nMethods:\n\nData sources\nCleaning steps\nAnalysis approach\n\nResults:\n\nSummary statistics table\nFigures with captions\nWritten interpretation of each figure\n\nDiscussion:\n\nWhat do these results mean for feedlot management?\nLimitations of the analysis\nRecommendations for future work\n\nConclusion (1 paragraph)\nAppendix: Session info (sessionInfo())\n\nYAML header requirements:\n---\ntitle: \"Cattle Feedlot Performance Analysis\"\nauthor: \"Your Name\"\ndate: today\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    number-sections: true\n    code-fold: true\n    code-tools: true\n    embed-resources: true\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\nDeliverable: Fully rendered HTML report that tells a cohesive story\n\n\n\n8.11.9 Grading Rubric\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nData Import\n20\nAll datasets imported correctly; initial exploration documented\n\n\nData Cleaning\n25\nThorough cleaning with appropriate techniques; well-documented decisions\n\n\nData Integration\n15\nDatasets joined correctly; appropriate reshaping; summary tables\n\n\nVisualizations\n20\n5+ plots addressing key questions; clear, publication-quality formatting\n\n\nSummary Statistics\n10\nCorrect calculations; well-formatted table; thoughtful interpretation\n\n\nReport Quality\n10\nProfessional structure; clear writing; reproducible; proper YAML\n\n\nTOTAL\n100\n\n\n\n\n\n\n\n8.11.10 Submission Instructions\nDue Date: [To be announced by instructor]\nSubmission:\n\nRender your Quarto document to HTML\nCompress your entire project folder (including data, code, output) into a ZIP file\nSubmit via the course learning management system (Canvas/Blackboard/etc.)\n\nChecklist before submitting: - [ ] All code runs from top to bottom without errors - [ ] Quarto document renders successfully to HTML - [ ] All required sections are present - [ ] Figures are saved in output/figures/ - [ ] README.md describes the project - [ ] File paths are relative (no hard-coded paths like /Users/yourname/) - [ ] Code is well-commented and easy to follow\n\n\n\n8.11.11 Tips for Success\n\nStart early! This is a substantial project.\nWork incrementally:\n\nImport ‚Üí Clean ‚Üí Join ‚Üí Visualize ‚Üí Report\nDon‚Äôt try to do everything at once\n\nTest your code frequently:\n\nRender your Quarto document often to catch errors early\n\nUse comments generously:\n\nExplain why you made decisions, not just what the code does\n\nMake it your own:\n\nFeel free to go beyond the minimum requirements\nAdditional analyses, creative visualizations, and deeper insights are encouraged!\n\nAsk for help if stuck:\n\nInstructor office hours\nR4DS Slack community\nStack Overflow (search first, then ask)\n\nProofread your writing:\n\nYour report should be polished and professional",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch08-special_formats.html#additional-resources",
    "href": "chapters/ch08-special_formats.html#additional-resources",
    "title": "8¬† Special Data Formats, Integration, and Course Wrap-up",
    "section": "8.12 Additional Resources",
    "text": "8.12 Additional Resources\n\n8.12.1 R Packages Documentation\nData Import: - haven - Read SAS, SPSS, Stata - readxl - Read Excel - writexl - Write Excel (simple) - openxlsx - Advanced Excel operations\nData Cleaning: - janitor - Data cleaning utilities - lubridate - Date/time manipulation - glue - String interpolation\nCore Tidyverse: - dplyr - Data manipulation - tidyr - Data reshaping - ggplot2 - Data visualization - readr - Read CSV files - stringr - String manipulation - purrr - Functional programming\n\n\n8.12.2 Cheat Sheets\n\nData Import Cheat Sheet\nData Transformation (dplyr) Cheat Sheet\nData Tidying (tidyr) Cheat Sheet\nData Visualization (ggplot2) Cheat Sheet\nDates and Times (lubridate) Cheat Sheet\nApply Functions (purrr) Cheat Sheet\n\n\n\n8.12.3 Books (Free Online)\nR Programming: - R for Data Science (2e) - Wickham & Grolemund (comprehensive reference) - Hands-On Programming with R - Grolemund (beginner-friendly) - Advanced R - Wickham (when you‚Äôre ready to level up)\nStatistics (Part 2 Preview): - Learning Statistics with R - Navarro (statistics fundamentals) - Introduction to Modern Statistics - OpenIntro (free textbook)\nQuarto: - Quarto Guide - Official documentation\nData Science Career: - Build a Career in Data Science - Robinson & Nolis\n\n\n8.12.4 Video Tutorials\n\nStatQuest with Josh Starmer - R and statistics (excellent explanations!)\nData Science Dojo - R tutorials\nRStudio Webinars - Advanced topics\n\n\n\n8.12.5 Online Courses\n\nDataCamp: Data Scientist with R Track (paid)\nCoursera: R Programming (Johns Hopkins)\nedX: Data Science: R Basics (Harvard)\n\n\n\n8.12.6 Communities\n\nR for Data Science Online Learning Community - Slack workspace, book club\nRStudio Community - Forum for questions\nStack Overflow [r] tag - Technical Q&A\n#RStats on Twitter/X - R community news and tips\n\n\n\n8.12.7 Blogs to Follow\n\nR-bloggers - Aggregator of R blogs\nSimply Statistics - Statistics and data science\nJulia Silge‚Äôs Blog - Tidy modeling and text analysis\nDavid Robinson‚Äôs Blog - Data analysis and visualization\n\n\nNext Chapter: Part 2, Chapter 1: Statistical Foundations\n\n\n\n\n\n\n\nNoteüéâ Congratulations on Completing Part 1!\n\n\n\nYou‚Äôve learned an incredible amount in 8 weeks. Take a moment to be proud of what you‚Äôve accomplished. You now have the skills to:\n\nImport data from any source\nClean and transform messy real-world data\nCreate publication-quality visualizations\nJoin and reshape complex datasets\nProduce reproducible reports\n\nThese skills will serve you throughout your career. Well done, and see you in Part 2!",
    "crumbs": [
      "Part 1: Introduction to Data Science",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Special Data Formats, Integration, and Course Wrap-up</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html",
    "href": "chapters/ch09-statistical_foundations.html",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "",
    "text": "10 Introduction: Why Statistics Matter in Animal Science\nImagine you‚Äôre a swine nutritionist testing a new feed additive that claims to improve growth rates. After a 90-day trial, you observe that pigs fed the new additive weigh an average of 5 kg more than the control group. Is this difference real, or just random variation? Should you recommend this expensive additive to producers?\nOr perhaps you‚Äôre a beef geneticist comparing two breeding programs. Bulls from Program A seem to produce offspring with slightly better marbling scores. But is the difference large enough to justify changing breeding protocols?\nThese are the types of questions statistics helps us answer. Statistics is fundamentally about making decisions in the presence of uncertainty. In animal science, we deal with biological variation constantly‚Äîno two animals are exactly alike, even if they‚Äôre raised identically. Statistics gives us a framework to:\nIn this course, we‚Äôll build your statistical toolkit step by step, always connecting concepts back to real problems in animal agriculture.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-frequentist",
    "href": "chapters/ch09-statistical_foundations.html#sec-frequentist",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "11.1 Frequentist Statistics",
    "text": "11.1 Frequentist Statistics\nThe frequentist approach defines probability as long-run frequency. If we say ‚Äúthe probability of getting heads is 0.5,‚Äù we mean that if we flipped a coin infinitely many times, about half would be heads.\n\n11.1.1 Key Principles\n\nParameters are fixed but unknown: The true average weight of pigs on a new diet is a fixed number‚Äîwe just don‚Äôt know it. Our job is to estimate it from data.\nProbability describes data, not hypotheses: We calculate ‚Äúthe probability of observing data this extreme if the null hypothesis were true,‚Äù NOT ‚Äúthe probability that the hypothesis is true.‚Äù\nRepetition is key: Frequentist inference imagines repeating the same experiment many times. Confidence intervals and p-values only make sense in this framework of repeated sampling.\n\n\n\n11.1.2 Example: Feed Trial\nSuppose we test a new feed supplement in pigs. The frequentist asks:\n\n‚ÄúIf this supplement truly had no effect (null hypothesis), what‚Äôs the probability we‚Äôd observe a difference this large just by chance?‚Äù\n\nIf that probability is very small (say, p &lt; 0.05), we conclude the data are incompatible with the null hypothesis, and we reject it in favor of the alternative (the supplement does have an effect).\n\n\n\n\n\n\nImportantCritical Point\n\n\n\nA p-value of 0.03 does NOT mean ‚Äúthere‚Äôs a 3% chance the null hypothesis is true.‚Äù It means ‚Äúif the null were true, we‚Äôd see data this extreme only 3% of the time by chance alone.‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-bayesian",
    "href": "chapters/ch09-statistical_foundations.html#sec-bayesian",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "11.2 Bayesian Statistics",
    "text": "11.2 Bayesian Statistics\nThe Bayesian approach defines probability as degree of belief. It explicitly incorporates prior knowledge and updates that knowledge with data.\n\n11.2.1 Key Principles\n\nParameters have probability distributions: Instead of saying ‚Äúthe true effect is unknown,‚Äù Bayesians say ‚Äúour belief about the effect can be described by a probability distribution.‚Äù\nPrior + Data = Posterior: Bayesian analysis combines:\n\nPrior: What we believed before seeing the data\nLikelihood: What the data tell us\nPosterior: Updated beliefs after seeing the data\n\nDirect probability statements about hypotheses: Bayesians can say things like ‚Äúthere‚Äôs an 85% probability the effect is positive‚Äù or ‚Äúthe treatment effect is between 2 and 8 kg with 95% probability.‚Äù\n\n\n\n11.2.2 Bayes‚Äô Theorem\nThe mathematical foundation of Bayesian statistics is Bayes‚Äô Theorem:\n\\[\nP(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\times P(\\theta)}{P(\\text{data})}\n\\]\nWhere:\n\n\\(P(\\theta | \\text{data})\\) = Posterior: Our updated belief about parameter \\(\\theta\\) after seeing the data\n\\(P(\\text{data} | \\theta)\\) = Likelihood: How probable our data are under different values of \\(\\theta\\)\n\\(P(\\theta)\\) = Prior: Our belief about \\(\\theta\\) before seeing the data\n\\(P(\\text{data})\\) = Marginal likelihood: A normalizing constant (probability of data across all possible \\(\\theta\\))\n\n\n\n11.2.3 Example: Same Feed Trial, Bayesian Perspective\nA Bayesian might start with prior knowledge: ‚ÄúPrevious studies suggest feed supplements increase growth by 0-10 kg, with most around 3-5 kg.‚Äù After seeing the data, they update this prior to a posterior distribution and can make statements like:\n\n‚ÄúBased on our data, there‚Äôs a 92% probability that the supplement increases weight by at least 2 kg, and a 70% probability the increase is between 4 and 8 kg.‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#comparing-the-approaches",
    "href": "chapters/ch09-statistical_foundations.html#comparing-the-approaches",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "11.3 Comparing the Approaches",
    "text": "11.3 Comparing the Approaches\n\n\n\n\n\nAspect\nFrequentist\nBayesian\n\n\n\n\nDefinition of Probability\nLong-run frequency\nDegree of belief\n\n\nParameters\nFixed, unknown constants\nRandom variables with distributions\n\n\nPrior Knowledge\nNot formally incorporated\nExplicitly incorporated via priors\n\n\nOutput\np-values, confidence intervals\nPosterior distributions, credible intervals\n\n\nInterpretation\nBased on hypothetical repetition\nDirect probability statements\n\n\n\n\n\n\n\n\n\n\n\nTipWhy Frequentist in This Course?\n\n\n\nFrequentist methods are:\n\nMore commonly used in animal science journals\nRequired by many regulatory bodies (FDA, EPA)\nComputationally simpler for basic analyses\nThe foundation for most statistical software defaults\n\nHowever, Bayesian methods are growing in popularity, especially for complex models. Being fluent in frequentist thinking first makes learning Bayesian approaches easier later.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-pvalue-definition",
    "href": "chapters/ch09-statistical_foundations.html#sec-pvalue-definition",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "12.1 Definition and Meaning",
    "text": "12.1 Definition and Meaning\nA p-value is defined as:\n\\[\np = P(\\text{data as extreme or more extreme} \\mid H_0 \\text{ is true})\n\\]\nWhere:\n\n\\(P(\\cdot)\\) = Probability\n\\(\\mid\\) = ‚Äúgiven that‚Äù or ‚Äúconditional on‚Äù\n\\(H_0\\) = The null hypothesis (typically ‚Äúno effect‚Äù or ‚Äúno difference‚Äù)\n\nIn plain English: The p-value is the probability of observing results at least as extreme as what we actually observed, assuming the null hypothesis is true.\n\n12.1.1 Breaking Down the Definition\nLet‚Äôs unpack each part:\n\n‚ÄúProbability of observing results‚Ä¶‚Äù ‚Äì We‚Äôre talking about data, not hypotheses\n‚Äú‚Ä¶at least as extreme‚Ä¶‚Äù ‚Äì Not just exactly what we saw, but anything further from what we‚Äôd expect under the null\n‚Äú‚Ä¶assuming the null hypothesis is true‚Äù ‚Äì This is a conditional probability; we‚Äôre starting with an assumption\n\n\n\n\n\n\n\nWarningThe p-value is NOT:\n\n\n\n\n‚ùå The probability that the null hypothesis is true: \\(P(H_0 | \\text{data})\\)\n‚ùå The probability that the result occurred by chance\n‚ùå The probability of making a wrong decision\n‚ùå The size or importance of an effect\n‚ùå The probability that replicating the study would give the same result",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-pvalue-visual",
    "href": "chapters/ch09-statistical_foundations.html#sec-pvalue-visual",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "12.2 Visualizing What P-Values Mean",
    "text": "12.2 Visualizing What P-Values Mean\nLet‚Äôs simulate a situation to build intuition. Suppose we‚Äôre comparing two groups of beef cattle (Control vs Treatment), and in reality, there‚Äôs no true difference between them (null hypothesis is true).\n\n\nCode\n# Simulation parameters\nn_per_group &lt;- 25\ntrue_mean &lt;- 600  # kg, both groups\ntrue_sd &lt;- 40\n\n# Generate ONE sample where null is true\nset.seed(123)\ncontrol &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\ntreatment &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n\n# Combine into data frame\nsample_data &lt;- tibble(\n  weight = c(control, treatment),\n  group = rep(c(\"Control\", \"Treatment\"), each = n_per_group)\n)\n\n# Visualize\np1 &lt;- ggplot(sample_data, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.6, size = 2) +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Treatment\" = \"#56B4E9\")) +\n  labs(\n    title = \"One Sample: Weights Under the Null (No True Difference)\",\n    subtitle = sprintf(\"Control mean: %.1f kg | Treatment mean: %.1f kg\",\n                      mean(control), mean(treatment)),\n    y = \"Final Weight (kg)\",\n    x = \"Group\"\n  ) +\n  theme(legend.position = \"none\") +\n  ylim(500, 700)\n\nprint(p1)\n\n\n\n\n\n\n\n\n\nCode\n# Run t-test\ntest_result &lt;- t.test(control, treatment)\ncat(sprintf(\"\\nObserved difference: %.1f kg\\n\", mean(treatment) - mean(control)))\n\n\n\nObserved difference: 5.4 kg\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", test_result$p.value))\n\n\nP-value: 0.6100\n\n\nEven though the null hypothesis is true (both groups have the same mean), we observe a difference just due to random sampling. The p-value tells us how ‚Äúsurprising‚Äù this observed difference would be if the null were true.\n\n12.2.1 The Distribution of P-Values Under the Null\nNow, what happens if we repeat this experiment 1,000 times, always with no true difference?\n\n\nCode\n# Simulate 1000 experiments where null is true\nn_simulations &lt;- 1000\n\nsimulate_study &lt;- function() {\n  control &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n  treatment &lt;- rnorm(n_per_group, mean = true_mean, sd = true_sd)\n  t.test(control, treatment)$p.value\n}\n\np_values &lt;- replicate(n_simulations, simulate_study())\n\n# Visualize distribution\np2 &lt;- tibble(p_value = p_values) %&gt;%\n  ggplot(aes(x = p_value)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0.05, color = \"red\", linetype = \"dashed\", linewidth = 1.2) +\n  annotate(\"text\", x = 0.05, y = 70, label = \"Œ± = 0.05\",\n           color = \"red\", hjust = -0.1, size = 5) +\n  labs(\n    title = \"Distribution of P-Values When the Null Hypothesis is TRUE\",\n    subtitle = sprintf(\"%d simulations: each time, both groups truly have mean = %d kg\",\n                      n_simulations, true_mean),\n    x = \"P-value\",\n    y = \"Count (out of 1,000 studies)\"\n  ) +\n  scale_x_continuous(breaks = seq(0, 1, 0.1)) +\n  theme_minimal(base_size = 13)\n\nprint(p2)\n\n\n\n\n\n\n\n\n\nCode\n# Calculate proportion \"significant\"\nprop_sig &lt;- mean(p_values &lt; 0.05)\ncat(sprintf(\"\\nProportion of p-values &lt; 0.05: %.3f (expected: 0.05)\\n\", prop_sig))\n\n\n\nProportion of p-values &lt; 0.05: 0.062 (expected: 0.05)\n\n\nCode\ncat(sprintf(\"Out of %d studies where null is TRUE, %d (%.1f%%) would be \\\"significant\\\" at p &lt; 0.05\\n\",\n            n_simulations, sum(p_values &lt; 0.05), 100 * prop_sig))\n\n\nOut of 1000 studies where null is TRUE, 62 (6.2%) would be \"significant\" at p &lt; 0.05\n\n\n\n\n\n\n\n\nImportantCritical Insight\n\n\n\nWhen the null hypothesis is true, p-values are uniformly distributed between 0 and 1. This means about 5% of studies will produce p &lt; 0.05 purely by chance‚Äîthis is the Type I error rate (false positive rate).\nIf you use Œ± = 0.05 as your threshold, you‚Äôre accepting that 5% of the time, you‚Äôll incorrectly reject a true null hypothesis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-pvalue-misconceptions",
    "href": "chapters/ch09-statistical_foundations.html#sec-pvalue-misconceptions",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "12.3 Common P-Value Misconceptions",
    "text": "12.3 Common P-Value Misconceptions\nLet‚Äôs address the most common misinterpretations with specific examples from animal science.\n\n12.3.1 Misconception 1: ‚Äúp = 0.03 means 3% chance null is true‚Äù\nWRONG. The p-value is \\(P(\\text{data} \\mid H_0)\\), not \\(P(H_0 \\mid \\text{data})\\).\nExample: In a swine growth study, you find p = 0.03 when comparing two diets. This means:\n\n‚úÖ Correct: ‚ÄúIf both diets were truly identical, we‚Äôd see a difference this large in only 3% of similar studies, just by chance.‚Äù\n‚ùå Incorrect: ‚ÄúThere‚Äôs a 3% chance the diets are really the same.‚Äù\n\nTo know \\(P(H_0 \\mid \\text{data})\\), you‚Äôd need to know the prior probability that \\(H_0\\) is true‚Äîthat requires Bayesian analysis.\n\n\n12.3.2 Misconception 2: ‚Äúp = 0.06 means no effect‚Äù\nWRONG. Absence of evidence is not evidence of absence.\nExample: You test a new probiotic in beef cattle and get p = 0.06 for weight gain.\n\n‚ùå Incorrect: ‚ÄúThe probiotic doesn‚Äôt work.‚Äù\n‚úÖ Correct: ‚ÄúOur data don‚Äôt provide strong evidence against the null hypothesis. The effect might be real but small, or our sample size might be too small to detect it.‚Äù\n\nConsider these two scenarios that both give p = 0.06:\n\n\nCode\nset.seed(456)\n\n# Scenario A: Large sample, small effect\nn_large &lt;- 100\neffect_small &lt;- 3  # kg difference\ncattle_a_control &lt;- rnorm(n_large, mean = 600, sd = 40)\ncattle_a_treat &lt;- rnorm(n_large, mean = 600 + effect_small, sd = 40)\n\n# Scenario B: Small sample, large effect\nn_small &lt;- 15\neffect_large &lt;- 15  # kg difference\ncattle_b_control &lt;- rnorm(n_small, mean = 600, sd = 40)\ncattle_b_treat &lt;- rnorm(n_small, mean = 600 + effect_large, sd = 40)\n\n# T-tests\np_a &lt;- t.test(cattle_a_treat, cattle_a_control)$p.value\np_b &lt;- t.test(cattle_b_treat, cattle_b_control)$p.value\n\n# Visualize\ndata_a &lt;- tibble(weight = c(cattle_a_control, cattle_a_treat),\n                 group = rep(c(\"Control\", \"Probiotic\"), each = n_large),\n                 scenario = \"A\")\ndata_b &lt;- tibble(weight = c(cattle_b_control, cattle_b_treat),\n                 group = rep(c(\"Control\", \"Probiotic\"), each = n_small),\n                 scenario = \"B\")\n\nplot_a &lt;- ggplot(data_a, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 1.5) +\n  labs(title = sprintf(\"Scenario A: Large Sample, Small Effect\\nn=%d per group, p=%.3f\",\n                       n_large, p_a),\n       y = \"Weight (kg)\", x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(450, 750)\n\nplot_b &lt;- ggplot(data_b, aes(x = group, y = weight, fill = group)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 2) +\n  labs(title = sprintf(\"Scenario B: Small Sample, Large Effect\\nn=%d per group, p=%.3f\",\n                       n_small, p_b),\n       y = \"Weight (kg)\", x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(450, 750)\n\nplot_a + plot_b\n\n\n\n\n\n\n\n\n\nBoth studies have p ‚âà 0.05-0.07, but they tell very different stories! Always report effect sizes and confidence intervals, not just p-values.\n\n\n12.3.3 Misconception 3: ‚Äúp &lt; 0.001 means a large/important effect‚Äù\nWRONG. Statistical significance ‚â† practical significance.\nExample: In a massive database of 10,000 pigs, you find that pigs born on Mondays weigh 0.3 kg less at market than pigs born on other days (p &lt; 0.001).\n\nStatistically significant: Yes! With huge sample sizes, even tiny effects become ‚Äúsignificant.‚Äù\nPractically significant: Probably not. A 0.3 kg difference is unlikely to matter economically.\n\n\n\nCode\n# Simulation: huge sample, tiny effect\nset.seed(789)\nn_huge &lt;- 5000\ntiny_effect &lt;- 0.3  # kg\n\nmonday_pigs &lt;- rnorm(n_huge, mean = 280, sd = 20)\nother_pigs &lt;- rnorm(n_huge, mean = 280 + tiny_effect, sd = 20)\n\ntest_huge &lt;- t.test(other_pigs, monday_pigs)\n\ncat(sprintf(\"Sample size: %d per group\\n\", n_huge))\n\n\nSample size: 5000 per group\n\n\nCode\ncat(sprintf(\"Mean difference: %.2f kg\\n\", mean(other_pigs) - mean(monday_pigs)))\n\n\nMean difference: -0.17 kg\n\n\nCode\ncat(sprintf(\"P-value: %.2e (highly significant!)\\n\", test_huge$p.value))\n\n\nP-value: 6.67e-01 (highly significant!)\n\n\nCode\ncat(sprintf(\"But effect size: %.2f kg (%.1f%% of mean weight)\\n\",\n            tiny_effect, 100 * tiny_effect / 280))\n\n\nBut effect size: 0.30 kg (0.1% of mean weight)\n\n\n\n\n\n\n\n\nTipAlways Ask Two Questions\n\n\n\n\nIs it statistically significant? (p-value)\nIs it practically significant? (effect size, confidence intervals, domain knowledge)\n\nA difference can be statistically significant without being biologically or economically meaningful.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-threshold",
    "href": "chapters/ch09-statistical_foundations.html#sec-threshold",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "12.4 The Arbitrary Nature of p < 0.05",
    "text": "12.4 The Arbitrary Nature of p &lt; 0.05\nWhere did p &lt; 0.05 come from? It was popularized by statistician R.A. Fisher in the 1920s as a convenient convention, not a law of nature. He even cautioned against treating it as a bright-line rule.\n\n12.4.1 The Problem with Bright Lines\nConsider three studies comparing the same feed additive:\n\nStudy A: p = 0.049 ‚Üí ‚ÄúSignificant! The additive works!‚Äù\nStudy B: p = 0.051 ‚Üí ‚ÄúNot significant. No evidence it works.‚Äù\nStudy C: p = 0.048 ‚Üí ‚ÄúSignificant! Definitely works!‚Äù\n\nDoes it really make sense that Study A and C lead to completely different conclusions than Study B, when the p-values are nearly identical?\n\n\nCode\n# Visualize the arbitrary threshold\ntibble(\n  study = c(\"A\", \"B\", \"C\"),\n  p_value = c(0.049, 0.051, 0.048),\n  significant = p_value &lt; 0.05\n) %&gt;%\n  ggplot(aes(x = study, y = p_value, fill = significant)) +\n  geom_col(alpha = 0.7) +\n  geom_hline(yintercept = 0.05, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  geom_text(aes(label = sprintf(\"p = %.3f\", p_value)), vjust = -0.5, size = 5) +\n  annotate(\"text\", x = 2, y = 0.05, label = \"Œ± = 0.05 threshold\",\n           color = \"red\", vjust = -0.5, size = 4) +\n  scale_fill_manual(values = c(\"TRUE\" = \"darkgreen\", \"FALSE\" = \"gray50\"),\n                    labels = c(\"TRUE\" = \"Significant\", \"FALSE\" = \"Not Significant\")) +\n  labs(\n    title = \"The Arbitrary Nature of p &lt; 0.05\",\n    subtitle = \"Should Study B really lead to a completely different conclusion?\",\n    x = \"Study\",\n    y = \"P-value\",\n    fill = \"\"\n  ) +\n  theme_minimal(base_size = 13) +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Modern Perspectives\nMany scientific fields are moving away from rigid thresholds:\n\nReport exact p-values (e.g., p = 0.03, not just ‚Äúp &lt; 0.05‚Äù)\nFocus on effect sizes and confidence intervals more than p-values\nConsider p-values as continuous measures of evidence, not binary decisions\nSome journals now ban the term ‚Äústatistically significant‚Äù entirely\n\n\n\n\n\n\n\nNoteWhat We‚Äôll Do in This Course\n\n\n\nWe‚Äôll calculate p-values because they‚Äôre standard in animal science, but we‚Äôll always interpret them alongside:\n\nEffect sizes (how big is the difference?)\nConfidence intervals (what‚Äôs the range of plausible values?)\nPractical significance (does the effect size matter in the real world?)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-causation",
    "href": "chapters/ch09-statistical_foundations.html#sec-causation",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "13.1 The Gold Standard: Causation vs Association",
    "text": "13.1 The Gold Standard: Causation vs Association\n\nAssociation (correlation): Two variables change together, but we don‚Äôt know if one causes the other\nCausation: Changing one variable causes changes in the other\n\nThe single most important question when reading research: Can this study establish causation, or only association?\n\n13.1.1 Observational Studies\nIn an observational study, the researcher simply observes and records data without manipulating any variables. You measure what‚Äôs already happening naturally.\nExamples in animal science:\n\nCross-sectional survey: Measure backfat thickness in pigs across different farms at one point in time\nCohort study: Follow beef cattle over time and record which ones develop health issues\nCase-control study: Compare diet history of cattle with vs without liver abscesses\n\nStrengths:\n\nCan study things we can‚Äôt (or shouldn‚Äôt) experimentally manipulate\nOften cheaper and faster than experiments\nReflects real-world conditions\nGood for exploratory research and hypothesis generation\n\nLimitations:\n\nCannot establish causation (only association)\nConfounding variables can bias results (more on this below)\nDifficult to control for all alternative explanations\n\n\n13.1.1.1 Example: Farm Size and Pig Health\nImagine you survey 100 swine farms and find that larger farms have lower mortality rates.\nCan you conclude that increasing farm size causes better health outcomes?\nNo! Many confounding variables could explain this:\n\nLarger farms might have better veterinary care\nThey might use better biosecurity protocols\nThey might have more experienced managers\nThey might be in regions with different disease pressures\nHealthier farms might have expanded to become larger (reverse causation!)\n\n\n\nCode\n# Simulate observational data with confounding\nset.seed(321)\nn_farms &lt;- 100\n\n# Management quality is a confounder\nmanagement_quality &lt;- rnorm(n_farms, mean = 50, sd = 15)\n\n# Better-managed farms tend to be larger (confounding)\nfarm_size &lt;- 500 + 8 * management_quality + rnorm(n_farms, mean = 0, sd = 200)\n\n# Mortality is affected by management quality, NOT farm size directly\nmortality_rate &lt;- 8 - 0.08 * management_quality + rnorm(n_farms, mean = 0, sd = 1.5)\nmortality_rate &lt;- pmax(0, mortality_rate)  # Can't be negative\n\nfarm_data &lt;- tibble(\n  farm_id = 1:n_farms,\n  size = farm_size,\n  mortality = mortality_rate,\n  management = management_quality\n)\n\n# Naive analysis (ignoring confounding)\np3 &lt;- ggplot(farm_data, aes(x = size, y = mortality)) +\n  geom_point(alpha = 0.6, size = 3, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", linewidth = 1.2) +\n  labs(\n    title = \"Observational Study: Farm Size vs Mortality Rate\",\n    subtitle = \"Appears that larger farms have lower mortality - but is this causal?\",\n    x = \"Farm Size (number of sows)\",\n    y = \"Mortality Rate (%)\"\n  )\n\nprint(p3)\n\n\n\n\n\n\n\n\n\nCode\ncor_size_mort &lt;- cor(farm_data$size, farm_data$mortality)\ncat(sprintf(\"\\nCorrelation between farm size and mortality: %.3f\\n\", cor_size_mort))\n\n\n\nCorrelation between farm size and mortality: -0.383\n\n\nCode\ncat(\"But this is driven by a confounder: management quality!\\n\")\n\n\nBut this is driven by a confounder: management quality!\n\n\nThis is association, not causation. To establish that farm size itself affects mortality, you‚Äôd need an experimental design.\n\n\n\n\n13.1.2 Experimental Studies\nIn an experimental study, the researcher actively manipulates one or more variables (the ‚Äútreatment‚Äù or ‚Äúintervention‚Äù) and measures the effect on an outcome.\nKey features:\n\nResearcher controls who receives which treatment\nIdeally uses randomization to assign treatments\nControls other variables to isolate the effect of the treatment\nCan establish causation (if designed properly)\n\nExamples in animal science:\n\nFeed trial: Randomly assign piglets to Diet A vs Diet B, measure growth\nDrug efficacy trial: Randomly assign cattle to antibiotic vs placebo, measure recovery\nBreeding experiment: Randomly assign boars to breeding groups, compare offspring traits\n\n\n13.1.2.1 Example: Does Lysine Supplementation Improve Growth?\nStudy design: Take 60 pigs, randomly assign 30 to a control diet and 30 to a lysine-supplemented diet. Raise them identically otherwise. Measure final weight.\n\n\nCode\n# Simulate experimental data\nset.seed(654)\nn_pigs &lt;- 60\n\n# Randomly assign treatment\npig_data &lt;- tibble(\n  pig_id = 1:n_pigs,\n  treatment = rep(c(\"Control\", \"Lysine\"), each = n_pigs/2),\n  # Lysine truly increases weight by ~8 kg\n  final_weight = ifelse(treatment == \"Control\",\n                       rnorm(n_pigs/2, mean = 115, sd = 12),\n                       rnorm(n_pigs/2, mean = 115 + 8, sd = 12))\n)\n\n# Visualize\np4 &lt;- ggplot(pig_data, aes(x = treatment, y = final_weight, fill = treatment)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4, fill = \"red\") +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Lysine\" = \"#009E73\")) +\n  labs(\n    title = \"Experimental Study: Effect of Lysine Supplementation on Pig Growth\",\n    subtitle = \"Random assignment allows causal inference\",\n    y = \"Final Weight (kg)\",\n    x = \"Treatment Group\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(p4)\n\n\n\n\n\n\n\n\n\nCode\n# Test for difference\nexp_test &lt;- t.test(final_weight ~ treatment, data = pig_data)\ncat(sprintf(\"\\nMean difference: %.2f kg\\n\",\n            mean(pig_data$final_weight[pig_data$treatment == \"Lysine\"]) -\n            mean(pig_data$final_weight[pig_data$treatment == \"Control\"])))\n\n\n\nMean difference: 6.86 kg\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", exp_test$p.value))\n\n\nP-value: 0.0044\n\n\nCode\ncat(\"\\nBecause we RANDOMLY assigned treatments, we can conclude:\\n\")\n\n\n\nBecause we RANDOMLY assigned treatments, we can conclude:\n\n\nCode\ncat(\"Lysine supplementation CAUSES increased growth in pigs.\\n\")\n\n\nLysine supplementation CAUSES increased growth in pigs.\n\n\nWhy can we claim causation here?\nBecause of randomization (discussed in detail in the next section). Random assignment ensures that the two groups are equivalent on average at the start‚Äîany difference at the end must be due to the treatment.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-confounding",
    "href": "chapters/ch09-statistical_foundations.html#sec-confounding",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "13.2 Confounding Variables",
    "text": "13.2 Confounding Variables\nA confounding variable (or confounder) is a variable that:\n\nIs associated with the treatment/exposure\nIndependently affects the outcome\nIs not on the causal pathway between treatment and outcome\n\nConfounding creates spurious associations‚Äîrelationships that appear causal but aren‚Äôt.\n\n13.2.1 The Classic Example: Ice Cream and Drowning\nThis non-agricultural example illustrates confounding perfectly:\nObservation: Ice cream sales are strongly correlated with drowning deaths.\nConclusion: Ice cream causes drowning?! Should we ban ice cream to save lives?\nReality: Both are caused by a confounder: temperature/summer season\n\nHot weather ‚Üí people buy ice cream\nHot weather ‚Üí people go swimming ‚Üí more drownings\n\nIce cream and drowning are associated but not causally related.\n\n\n\n\n\n\n\n\n\n\n\n13.2.2 Agricultural Example: Pasture Type and Weight Gain\nScenario: You visit 20 beef farms. Some use Pasture A (fescue), others use Pasture B (mixed grass). You record average daily gain (ADG) for cattle on each farm.\nObservation: Cattle on Pasture A have higher ADG.\nCan you conclude Pasture A is better?\nProbably not! Possible confounders:\n\nFarm quality: Better-managed farms might choose Pasture A (and also have better nutrition, genetics, health)\nSoil quality: Farms with better soil grow Pasture A, but soil quality also affects other forages\nRegion: Pasture A might be used in regions with better climate for cattle\nGenetics: Farms using Pasture A might also use superior genetics\n\n\n\nCode\n# Simulate pasture study with confounding\nset.seed(987)\nn_farms &lt;- 20\n\npasture_data &lt;- tibble(\n  farm = 1:n_farms,\n  pasture_type = rep(c(\"Fescue\", \"Mixed Grass\"), each = 10)\n) %&gt;%\n  mutate(\n    # Farm quality is confounder: better farms choose fescue\n    farm_quality = ifelse(pasture_type == \"Fescue\",\n                         rnorm(n_farms/2, mean = 75, sd = 8),\n                         rnorm(n_farms/2, mean = 60, sd = 8)),\n    # ADG depends on farm quality, NOT pasture type!\n    adg = 1.2 + 0.012 * farm_quality + rnorm(n_farms, mean = 0, sd = 0.15)\n  )\n\n# Visualize the confounding\np6 &lt;- ggplot(pasture_data, aes(x = farm_quality, y = adg, color = pasture_type, shape = pasture_type)) +\n  geom_point(size = 4, alpha = 0.8) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.2) +\n  scale_color_manual(values = c(\"Fescue\" = \"#D55E00\", \"Mixed Grass\" = \"#0072B2\")) +\n  labs(\n    title = \"Confounding Example: Farm Quality Affects Both Pasture Choice and ADG\",\n    subtitle = \"Better farms choose fescue AND have higher ADG (but pasture isn't the cause)\",\n    x = \"Farm Quality Score\",\n    y = \"Average Daily Gain (kg/day)\",\n    color = \"Pasture Type\",\n    shape = \"Pasture Type\"\n  ) +\n  theme(legend.position = \"top\")\n\nprint(p6)\n\n\n\n\n\n\n\n\n\nCode\n# Naive comparison\npasture_data %&gt;%\n  group_by(pasture_type) %&gt;%\n  summarise(mean_adg = mean(adg), .groups = 'drop') %&gt;%\n  knitr::kable(digits = 3, col.names = c(\"Pasture Type\", \"Mean ADG (kg/day)\"))\n\n\n\n\n\nPasture Type\nMean ADG (kg/day)\n\n\n\n\nFescue\n2.028\n\n\nMixed Grass\n2.026\n\n\n\n\n\nThe fescue group has higher ADG, but it‚Äôs because better farms choose fescue‚Äînot because fescue itself is superior.\n\n\n\n\n\n\nWarningHow to Address Confounding\n\n\n\nIn observational studies:\n\nStatistical adjustment (multiple regression, matching, stratification)\nCareful measurement of potential confounders\nAcknowledge limitations in conclusions\n\nIn experimental studies:\n\nRandomization (the gold standard‚Äîdiscussed next!)\nBlocking/stratification\nStandardizing all other conditions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-randomization",
    "href": "chapters/ch09-statistical_foundations.html#sec-randomization",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "14.1 Why Randomization is Powerful",
    "text": "14.1 Why Randomization is Powerful\nRandom assignment ensures that treatment groups are balanced on all variables‚Äîboth measured and unmeasured‚Äîon average.\nThis is crucial because:\n\nYou can‚Äôt measure every potential confounder\nYou don‚Äôt always know what the confounders are\nRandomization balances them automatically (in expectation)\n\n\n14.1.1 Mathematical Intuition\nWhen you randomly assign \\(n\\) animals to groups, every animal has an equal probability of being in any group. This means:\n\\[\nE[\\text{Confounder}_{\\text{Treatment}}] = E[\\text{Confounder}_{\\text{Control}}]\n\\]\nWhere \\(E[\\cdot]\\) denotes expected value (average across many repetitions).\nIn plain English: On average, the treatment and control groups will have the same distribution of age, weight, genetics, health status, etc.‚Äîeven if you don‚Äôt measure these variables!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-rct-features",
    "href": "chapters/ch09-statistical_foundations.html#sec-rct-features",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "14.2 Key Features of RCTs",
    "text": "14.2 Key Features of RCTs\n\n14.2.1 1. Random Assignment\nNot ‚Äúhaphazard‚Äù or ‚Äúarbitrary‚Äù‚Äîrandom using a chance mechanism (coin flip, random number generator, etc.).\nExample: 60 pigs, 30 to each group\n\n\nCode\nset.seed(2025)\n\n# Start with 60 pigs with various characteristics\npigs &lt;- tibble(\n  pig_id = 1:60,\n  initial_weight = rnorm(60, mean = 25, sd = 4),\n  age_days = round(runif(60, min = 50, max = 70)),\n  sex = sample(c(\"Male\", \"Female\"), 60, replace = TRUE),\n  litter = sample(1:15, 60, replace = TRUE)\n)\n\n# RANDOMLY assign to treatment\npigs &lt;- pigs %&gt;%\n  mutate(treatment = sample(rep(c(\"Control\", \"Probiotic\"), each = 30)))\n\n# Check balance\npigs %&gt;%\n  group_by(treatment) %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = mean(initial_weight),\n    mean_age = mean(age_days),\n    prop_male = mean(sex == \"Male\"),\n    .groups = 'drop'\n  ) %&gt;%\n  knitr::kable(digits = 2,\n               col.names = c(\"Treatment\", \"N\", \"Mean Weight (kg)\",\n                            \"Mean Age (days)\", \"Proportion Male\"))\n\n\n\n\n\nTreatment\nN\nMean Weight (kg)\nMean Age (days)\nProportion Male\n\n\n\n\nControl\n30\n25.19\n60.97\n0.47\n\n\nProbiotic\n30\n25.89\n59.77\n0.37\n\n\n\n\n\nNotice how the groups are similar on all measured characteristics‚Äîthat‚Äôs randomization working!\n\n\n14.2.2 2. Control Group\nThe control group provides the counterfactual: what would have happened without the treatment?\nTypes of controls:\n\nNegative control: No treatment (or placebo)\nPositive control: Standard treatment (if testing a new alternative)\nMultiple controls: Compare several treatments\n\n\n\n14.2.3 3. Blinding (when possible)\nBlinding means keeping the treatment assignment hidden to reduce bias:\n\nSingle-blind: Animals (or caretakers) don‚Äôt know which group receives which treatment\nDouble-blind: Neither caretakers nor researchers analyzing data know\n\nExample: In a drug trial for cattle, identical-looking pills (one with drug, one placebo) prevent the farm workers from treating groups differently.\nNote: Blinding isn‚Äôt always possible in animal science (e.g., you can‚Äôt hide which diet an animal is eating), but controlling for observer bias is still important.\n\n\n14.2.4 4. Standardization\nKeep all other conditions identical between groups:\n\nSame housing\nSame feeding schedule\nSame environmental conditions\nSame outcome measurement procedures",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-rct-example",
    "href": "chapters/ch09-statistical_foundations.html#sec-rct-example",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "14.3 Example RCT: Feed Additive Trial in Swine",
    "text": "14.3 Example RCT: Feed Additive Trial in Swine\nResearch question: Does a novel feed additive improve average daily gain (ADG) in growing pigs?\nDesign:\n\nPopulation: 120 pigs (60-day-old, weaned)\nRandomization: Randomly assign 60 to control diet, 60 to additive diet\nControl: Standard corn-soybean diet\nTreatment: Same diet + 0.5% additive\nBlinding: Farm workers don‚Äôt know which pens get which diet (feed is labeled A/B)\nStandardization: All pigs housed in identical pens, same schedule, same health protocols\nDuration: 90 days\nOutcome: Average daily gain (kg/day)\n\n\n\nCode\nset.seed(111)\n\n# Simulate RCT data\nrct_pigs &lt;- tibble(\n  pig_id = 1:120,\n  # Randomize first\n  treatment = sample(rep(c(\"Control\", \"Additive\"), each = 60)),\n  # Baseline characteristics are balanced (due to randomization)\n  initial_weight = rnorm(120, mean = 20, sd = 3),\n  # Outcome: additive truly improves ADG by 0.05 kg/day\n  adg = ifelse(treatment == \"Control\",\n              rnorm(60, mean = 0.75, sd = 0.10),\n              rnorm(60, mean = 0.75 + 0.05, sd = 0.10))\n)\n\n# Visualize\np7 &lt;- ggplot(rct_pigs, aes(x = treatment, y = adg, fill = treatment)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 5, fill = \"white\", color = \"black\") +\n  scale_fill_manual(values = c(\"Control\" = \"#E69F00\", \"Additive\" = \"#56B4E9\")) +\n  labs(\n    title = \"RCT Results: Feed Additive Effect on Average Daily Gain\",\n    subtitle = \"White diamond = group mean\",\n    y = \"Average Daily Gain (kg/day)\",\n    x = \"Treatment Group\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(p7)\n\n\n\n\n\n\n\n\n\nCode\n# Statistical test\nrct_test &lt;- t.test(adg ~ treatment, data = rct_pigs)\neffect_size &lt;- mean(rct_pigs$adg[rct_pigs$treatment == \"Additive\"]) -\n               mean(rct_pigs$adg[rct_pigs$treatment == \"Control\"])\n\ncat(sprintf(\"\\nControl mean ADG: %.3f kg/day\\n\",\n            mean(rct_pigs$adg[rct_pigs$treatment == \"Control\"])))\n\n\n\nControl mean ADG: 0.767 kg/day\n\n\nCode\ncat(sprintf(\"Additive mean ADG: %.3f kg/day\\n\",\n            mean(rct_pigs$adg[rct_pigs$treatment == \"Additive\"])))\n\n\nAdditive mean ADG: 0.786 kg/day\n\n\nCode\ncat(sprintf(\"Difference: %.3f kg/day\\n\", effect_size))\n\n\nDifference: 0.019 kg/day\n\n\nCode\ncat(sprintf(\"95%% CI: [%.3f, %.3f]\\n\", rct_test$conf.int[1], rct_test$conf.int[2]))\n\n\n95% CI: [-0.019, 0.056]\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", rct_test$p.value))\n\n\nP-value: 0.3196\n\n\nCode\ncat(\"\\n‚úì Because of RANDOM ASSIGNMENT, we can conclude:\\n\")\n\n\n\n‚úì Because of RANDOM ASSIGNMENT, we can conclude:\n\n\nCode\ncat(\"  The additive CAUSES a ~0.05 kg/day increase in growth rate.\\n\")\n\n\n  The additive CAUSES a ~0.05 kg/day increase in growth rate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#sec-rct-limitations",
    "href": "chapters/ch09-statistical_foundations.html#sec-rct-limitations",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "14.4 Limitations of RCTs",
    "text": "14.4 Limitations of RCTs\nDespite being the gold standard, RCTs have limitations:\n\nCost: Experiments are expensive and time-consuming\nEthics: Some treatments can‚Äôt be tested experimentally (e.g., exposing animals to disease)\nPracticality: Long-term outcomes (years) may be infeasible\nExternal validity: Controlled conditions may not reflect real-world settings\nSample size: May need large numbers to detect small effects\n\nWhen RCTs aren‚Äôt possible, observational studies remain valuable‚Äîbut we must be cautious about causal claims and carefully consider confounding.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#main-concepts",
    "href": "chapters/ch09-statistical_foundations.html#main-concepts",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "15.1 Main Concepts",
    "text": "15.1 Main Concepts\n\n\n\n\n\n\nTip1. Two Statistical Philosophies\n\n\n\n\nFrequentist: Probability as long-run frequency; focus on \\(P(\\text{data} \\mid H_0)\\)\nBayesian: Probability as degree of belief; focus on \\(P(H_0 \\mid \\text{data})\\)\nThis course uses frequentist methods (standard in animal science)\n\n\n\n\n\n\n\n\n\nTip2. P-Values Are Widely Misunderstood\n\n\n\n\nDefinition: \\(p = P(\\text{data as extreme or more} \\mid H_0 \\text{ true})\\)\nNOT the probability the null hypothesis is true\nNOT the size or importance of an effect\np &lt; 0.05 is an arbitrary convention, not a law of nature\nAlways report effect sizes and confidence intervals alongside p-values\n\n\n\n\n\n\n\n\n\nTip3. Study Design Determines What You Can Conclude\n\n\n\n\nObservational studies: Can show association, NOT causation (confounding!)\nExperimental studies: Can establish causation (if designed properly)\nConfounding variables create spurious associations in observational data\n\n\n\n\n\n\n\n\n\nTip4. Randomization is Powerful\n\n\n\n\nRCTs are the gold standard for causal inference\nRandom assignment balances confounders automatically (on average)\nControl groups provide the counterfactual\nBut RCTs have limitations (cost, ethics, practicality)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#looking-ahead",
    "href": "chapters/ch09-statistical_foundations.html#looking-ahead",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "15.2 Looking Ahead",
    "text": "15.2 Looking Ahead\nNext week, we‚Äôll move from big-picture philosophy to practical tools: how to describe and summarize data using descriptive statistics and exploratory data analysis. We‚Äôll learn to:\n\nCalculate and interpret measures of central tendency and variability\nVisualize distributions effectively\nIdentify outliers and unusual patterns\nCreate publication-quality summary tables\n\nThese foundational skills will prepare us for inferential statistics (hypothesis testing, confidence intervals, regression) in subsequent weeks.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#reflection-questions",
    "href": "chapters/ch09-statistical_foundations.html#reflection-questions",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "15.3 Reflection Questions",
    "text": "15.3 Reflection Questions\nBefore next week‚Äôs class, think about:\n\nFind a recent paper in your area of animal science. Is it observational or experimental? If observational, what are potential confounders?\nLook at the p-values reported in the paper. Are effect sizes and confidence intervals also reported? If not, what information is missing?\nIf the paper claims causation, is that claim justified by the study design?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#additional-resources",
    "href": "chapters/ch09-statistical_foundations.html#additional-resources",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "15.4 Additional Resources",
    "text": "15.4 Additional Resources\n\n15.4.1 Recommended Reading\n\nASA Statement on P-values and Statistical Significance (2016) ‚Äì required reading\nGreenland et al.¬†(2016): ‚ÄúStatistical tests, P values, confidence intervals, and power: a guide to misinterpretations‚Äù ‚Äì comprehensive list of common errors\nIoannidis (2005): ‚ÄúWhy Most Published Research Findings Are False‚Äù ‚Äì provocative but important\n\n\n\n15.4.2 Videos\n\nStatQuest by Josh Starmer (YouTube): ‚ÄúP-values, clearly explained‚Äù\n‚ÄúDance of the p-values‚Äù (YouTube): Visual demonstration of p-value behavior\n\n\n\n15.4.3 Books\n\nThe Lady Tasting Tea by David Salsburg ‚Äì history of statistics, very readable\nNaked Statistics by Charles Wheelan ‚Äì conceptual introduction, no equations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch09-statistical_foundations.html#session-info",
    "href": "chapters/ch09-statistical_foundations.html#session-info",
    "title": "9¬† Week 9: Statistical Foundations and Study Design",
    "section": "15.5 Session Info",
    "text": "15.5 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.4.0    patchwork_1.3.2 broom_1.0.7     lubridate_1.9.3\n [5] forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4     purrr_1.0.4    \n [9] readr_2.1.5     tidyr_1.3.1     tibble_3.2.1    ggplot2_4.0.0  \n[13] tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] utf8_1.2.4         generics_0.1.3     xml2_1.3.6         lattice_0.22-6    \n [5] stringi_1.8.4      hms_1.1.3          digest_0.6.37      magrittr_2.0.3    \n [9] evaluate_1.0.1     grid_4.4.2         timechange_0.3.0   RColorBrewer_1.1-3\n[13] fastmap_1.2.0      Matrix_1.7-1       jsonlite_1.8.9     backports_1.5.0   \n[17] mgcv_1.9-1         fansi_1.0.6        viridisLite_0.4.2  textshaping_0.4.0 \n[21] cli_3.6.4          rlang_1.1.6        splines_4.4.2      withr_3.0.2       \n[25] yaml_2.3.10        tools_4.4.2        tzdb_0.4.0         kableExtra_1.4.0  \n[29] vctrs_0.6.5        R6_2.5.1           lifecycle_1.0.4    htmlwidgets_1.6.4 \n[33] pkgconfig_2.0.3    pillar_1.9.0       gtable_0.3.6       glue_1.8.0        \n[37] systemfonts_1.3.1  xfun_0.53          tidyselect_1.2.1   rstudioapi_0.17.1 \n[41] knitr_1.49         farver_2.1.2       nlme_3.1-166       htmltools_0.5.8.1 \n[45] labeling_0.4.3     rmarkdown_2.29     svglite_2.2.1      compiler_4.4.2    \n[49] S7_0.2.0          \n\n\n\nEnd of Week 1: Statistical Foundations and Study Design",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Week 9: Statistical Foundations and Study Design</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html",
    "href": "chapters/ch10-descriptive_statistics.html",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "",
    "text": "11 Introduction: The Foundation of Data Analysis\nBefore you can run sophisticated statistical tests or build complex models, you must understand your data. This seemingly simple step is where many analyses go wrong. Jumping straight to hypothesis tests without thoroughly exploring your data is like performing surgery without examining the patient first.\nConsider a swine nutritionist who receives data from a 12-week growth trial involving 200 pigs across four different diets. What should be the first step? Running an ANOVA? No! The first step is exploratory data analysis (EDA): looking at the data, understanding its structure, identifying patterns, and checking for potential issues.\nDescriptive statistics help us summarize data with numbers (means, standard deviations, percentiles), while exploratory data analysis uses visualization and summary techniques to understand patterns, spot outliers, and generate hypotheses.\nIn this chapter, we‚Äôll learn to:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-mean",
    "href": "chapters/ch10-descriptive_statistics.html#sec-mean",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.1 The Mean (Arithmetic Average)",
    "text": "12.1 The Mean (Arithmetic Average)\nThe mean is the sum of all values divided by the number of observations. It‚Äôs the most commonly used measure of central tendency.\n\n12.1.1 Mathematical Definition\nFor a sample of \\(n\\) observations \\(x_1, x_2, \\ldots, x_n\\), the sample mean is:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{x_1 + x_2 + \\cdots + x_n}{n}\n\\]\nWhere:\n\n\\(\\bar{x}\\) (pronounced ‚Äúx-bar‚Äù) = the sample mean\n\\(\\sum\\) = summation symbol (add up all values)\n\\(i=1\\) to \\(n\\) = index from the first to the \\(n\\)-th observation\n\\(x_i\\) = the \\(i\\)-th observation\n\n\n\n12.1.2 Example: Weaning Weights in Pigs\nSuppose we have 8 piglets with the following weaning weights (kg):\n\n\nCode\n# Weaning weights of 8 piglets\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# Calculate mean manually\nmean_manual &lt;- sum(weights) / length(weights)\n\n# Calculate mean using R function\nmean_r &lt;- mean(weights)\n\ncat(\"Piglet weights (kg):\", paste(weights, collapse = \", \"), \"\\n\")\n\n\nPiglet weights (kg): 6.2, 5.8, 6.5, 6, 5.9, 6.3, 6.1, 5.7 \n\n\nCode\ncat(sprintf(\"Manual calculation: (%.1f + %.1f + ... + %.1f) / 8 = %.2f kg\\n\",\n            weights[1], weights[2], weights[8], mean_manual))\n\n\nManual calculation: (6.2 + 5.8 + ... + 5.7) / 8 = 6.06 kg\n\n\nCode\ncat(sprintf(\"Using mean(): %.2f kg\\n\", mean_r))\n\n\nUsing mean(): 6.06 kg\n\n\n\n\n12.1.3 Properties of the Mean\nStrengths:\n\nUses all data points (every value contributes)\nAlgebraically convenient (works well in formulas)\nFamiliar and widely understood\n\nWeaknesses:\n\nSensitive to outliers: One extreme value can dramatically shift the mean\nRequires numerical data: Can‚Äôt use with categorical data\nNot robust: May not represent ‚Äútypical‚Äù value if distribution is skewed\n\n\n\n12.1.4 The Mean and Outliers\nLet‚Äôs see how outliers affect the mean:\n\n\nCode\n# Normal piglet weights\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\n# One piglet has a data entry error (67 kg instead of 6.7 kg!)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean = 6.06 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean = %.2f kg\\n\", mean(weights_with_outlier)))\n\n\n  Mean = 13.72 kg\n\n\nCode\ncat(sprintf(\"  Difference: %.2f kg\\n\\n\",\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Difference: 7.66 kg\n\n\nCode\ncat(\"The outlier increased the mean by ~7.5 kg!\\n\")\n\n\nThe outlier increased the mean by ~7.5 kg!\n\n\nCode\ncat(\"This clearly doesn't represent the 'typical' piglet weight.\\n\")\n\n\nThis clearly doesn't represent the 'typical' piglet weight.\n\n\nThis is why we need other measures of central tendency that are more robust to outliers.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-median",
    "href": "chapters/ch10-descriptive_statistics.html#sec-median",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.2 The Median",
    "text": "12.2 The Median\nThe median is the middle value when data are ordered from smallest to largest. Half the observations are below the median, half are above.\n\n12.2.1 How to Calculate the Median\n\nSort the data from smallest to largest\nIf \\(n\\) is odd: median = the middle value\nIf \\(n\\) is even: median = average of the two middle values\n\nMathematically, for sorted data \\(x_{(1)} \\leq x_{(2)} \\leq \\cdots \\leq x_{(n)}\\):\n\\[\n\\text{Median} =\n\\begin{cases}\nx_{((n+1)/2)} & \\text{if } n \\text{ is odd} \\\\\n\\frac{x_{(n/2)} + x_{(n/2+1)}}{2} & \\text{if } n \\text{ is even}\n\\end{cases}\n\\]\n\n\n12.2.2 Example: Median Calculation\n\n\nCode\n# Odd number of observations (9 pigs)\nweights_odd &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7, 6.4)\n\n# Even number of observations (8 pigs)\nweights_even &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\n\ncat(\"Odd sample (n=9):\\n\")\n\n\nOdd sample (n=9):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_odd), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.4, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (5th value): %.1f kg\\n\", median(weights_odd)))\n\n\n  Median (5th value): 6.1 kg\n\n\nCode\ncat(\"\\nEven sample (n=8):\\n\")\n\n\n\nEven sample (n=8):\n\n\nCode\ncat(\"  Sorted:\", paste(sort(weights_even), collapse = \", \"), \"\\n\")\n\n\n  Sorted: 5.7, 5.8, 5.9, 6, 6.1, 6.2, 6.3, 6.5 \n\n\nCode\ncat(sprintf(\"  Median (average of 4th and 5th): (%.1f + %.1f)/2 = %.2f kg\\n\",\n            sort(weights_even)[4], sort(weights_even)[5], median(weights_even)))\n\n\n  Median (average of 4th and 5th): (6.0 + 6.1)/2 = 6.05 kg\n\n\n\n\n12.2.3 The Median is Robust to Outliers\nLet‚Äôs revisit our outlier example:\n\n\nCode\nnormal_weights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 5.7)\nweights_with_outlier &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9, 6.3, 6.1, 67.0)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg\\n\", mean(normal_weights)))\n\n\n  Mean:   6.06 kg\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg\\n\", median(normal_weights)))\n\n\n  Median: 6.05 kg\n\n\nCode\ncat(\"\\nWith outlier (67 kg):\\n\")\n\n\n\nWith outlier (67 kg):\n\n\nCode\ncat(sprintf(\"  Mean:   %.2f kg (changed by %.2f kg)\\n\",\n            mean(weights_with_outlier),\n            mean(weights_with_outlier) - mean(normal_weights)))\n\n\n  Mean:   13.72 kg (changed by 7.66 kg)\n\n\nCode\ncat(sprintf(\"  Median: %.2f kg (changed by %.2f kg)\\n\",\n            median(weights_with_outlier),\n            median(weights_with_outlier) - median(normal_weights)))\n\n\n  Median: 6.15 kg (changed by 0.10 kg)\n\n\nCode\ncat(\"\\nThe median barely changed, while the mean shifted dramatically!\\n\")\n\n\n\nThe median barely changed, while the mean shifted dramatically!\n\n\n\n\n\n\n\n\nTipWhen to Use Mean vs Median\n\n\n\nUse the mean when:\n\nData are roughly symmetric (no strong skew)\nNo extreme outliers\nYou need the mathematical properties of the mean (e.g., for further calculations)\n\nUse the median when:\n\nData are skewed (right-skewed or left-skewed)\nOutliers are present\nYou want a measure resistant to extreme values\nReporting income, house prices, or other variables with long tails",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-mode",
    "href": "chapters/ch10-descriptive_statistics.html#sec-mode",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.3 The Mode",
    "text": "12.3 The Mode\nThe mode is the most frequently occurring value in a dataset. Unlike mean and median, the mode can be used with categorical data.\n\n12.3.1 Example: Mode in Categorical Data\n\n\nCode\n# Breed types in a beef herd\nbreeds &lt;- c(\"Angus\", \"Angus\", \"Hereford\", \"Angus\", \"Charolais\",\n            \"Angus\", \"Hereford\", \"Angus\", \"Angus\")\n\n# Find mode (most common breed)\nbreed_counts &lt;- table(breeds)\nmode_breed &lt;- names(breed_counts)[which.max(breed_counts)]\n\ncat(\"Breed counts:\\n\")\n\n\nBreed counts:\n\n\nCode\nprint(breed_counts)\n\n\nbreeds\n    Angus Charolais  Hereford \n        6         1         2 \n\n\nCode\ncat(sprintf(\"\\nMode: %s (most common breed)\\n\", mode_breed))\n\n\n\nMode: Angus (most common breed)\n\n\n\n\n12.3.2 Mode in Continuous Data\nFor continuous data, the mode is less useful because values rarely repeat exactly. Instead, we look at the peak of the distribution using histograms or density plots.\n\n\nCode\n# Birth weights of 100 calves\nset.seed(123)\ncalf_weights &lt;- rnorm(100, mean = 40, sd = 5)\n\n# Visualize\nggplot(tibble(weight = calf_weights), aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20, fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(calf_weights), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = mean(calf_weights) + 2, y = 0.07,\n           label = sprintf(\"Mean = %.1f kg\", mean(calf_weights)),\n           color = \"darkgreen\", hjust = 0) +\n  labs(\n    title = \"Distribution of Calf Birth Weights\",\n    subtitle = \"Red curve shows density; mode is near the peak\",\n    x = \"Birth Weight (kg)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteKey Point: Mode\n\n\n\nThe mode is most useful for:\n\nCategorical variables (breed, sex, treatment group)\nDiscrete counts (number of piglets per litter)\nMultimodal distributions (distributions with multiple peaks, suggesting subgroups)\n\nFor continuous measurements, mean and median are usually more informative.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-skewness",
    "href": "chapters/ch10-descriptive_statistics.html#sec-skewness",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "12.4 Comparing Measures in Skewed Distributions",
    "text": "12.4 Comparing Measures in Skewed Distributions\nThe relationship between mean, median, and mode reveals the shape of the distribution.\n\n12.4.1 Symmetric Distribution\nWhen data are symmetric (like a normal distribution):\n\\[\n\\text{Mean} \\approx \\text{Median} \\approx \\text{Mode}\n\\]\n\n\nCode\nset.seed(456)\nsymmetric_data &lt;- rnorm(500, mean = 100, sd = 15)\n\np_sym &lt;- ggplot(tibble(x = symmetric_data), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(symmetric_data), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(symmetric_data), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  labs(title = \"Symmetric Distribution\",\n       subtitle = sprintf(\"Mean (green) = %.1f | Median (purple) = %.1f\",\n                         mean(symmetric_data), median(symmetric_data)),\n       x = \"Value\", y = \"Density\")\n\nprint(p_sym)\n\n\n\n\n\n\n\n\n\n\n\n12.4.2 Right-Skewed Distribution\nWhen data have a long tail to the right (positive skew):\n\\[\n\\text{Mean} &gt; \\text{Median} &gt; \\text{Mode}\n\\]\nThe mean is ‚Äúpulled‚Äù toward the tail by extreme high values.\nExample: Days to market for pigs (most finish quickly, some take much longer)\n\n\nCode\nset.seed(789)\n# Simulate right-skewed data (e.g., days to market)\nright_skewed &lt;- rgamma(500, shape = 2, rate = 0.02)\n\np_right &lt;- ggplot(tibble(x = right_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(right_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(right_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(right_skewed), y = 0.012,\n           label = sprintf(\"Mean = %.1f\", mean(right_skewed)),\n           color = \"darkgreen\", hjust = -0.1, size = 3.5) +\n  annotate(\"text\", x = median(right_skewed), y = 0.011,\n           label = sprintf(\"Median = %.1f\", median(right_skewed)),\n           color = \"purple\", hjust = 1.1, size = 3.5) +\n  labs(title = \"Right-Skewed Distribution (Positive Skew)\",\n       subtitle = \"Mean &gt; Median (mean pulled toward long right tail)\",\n       x = \"Days to Market\", y = \"Density\")\n\nprint(p_right)\n\n\n\n\n\n\n\n\n\n\n\n12.4.3 Left-Skewed Distribution\nWhen data have a long tail to the left (negative skew):\n\\[\n\\text{Mean} &lt; \\text{Median} &lt; \\text{Mode}\n\\]\nExample: Carcass yield percentage (most are high, some are unusually low)\n\n\nCode\nset.seed(321)\n# Simulate left-skewed data\nleft_skewed &lt;- 100 - rgamma(500, shape = 2, rate = 0.2)\n\np_left &lt;- ggplot(tibble(x = left_skewed), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.6) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(left_skewed), color = \"darkgreen\",\n             linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = median(left_skewed), color = \"purple\",\n             linetype = \"dotted\", linewidth = 1) +\n  annotate(\"text\", x = mean(left_skewed), y = 0.04,\n           label = sprintf(\"Mean = %.1f\", mean(left_skewed)),\n           color = \"darkgreen\", hjust = 1.1, size = 3.5) +\n  annotate(\"text\", x = median(left_skewed), y = 0.042,\n           label = sprintf(\"Median = %.1f\", median(left_skewed)),\n           color = \"purple\", hjust = -0.1, size = 3.5) +\n  labs(title = \"Left-Skewed Distribution (Negative Skew)\",\n       subtitle = \"Mean &lt; Median (mean pulled toward long left tail)\",\n       x = \"Carcass Yield (%)\", y = \"Density\")\n\nprint(p_left)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportantRemember\n\n\n\n\nSymmetric: Mean ‚âà Median\nRight-skewed: Mean &gt; Median (use median to describe center)\nLeft-skewed: Mean &lt; Median (use median to describe center)\n\nAlways visualize your data to understand its shape before choosing which measure to report!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-range",
    "href": "chapters/ch10-descriptive_statistics.html#sec-range",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.1 Range",
    "text": "13.1 Range\nThe range is the simplest measure of spread:\n\\[\n\\text{Range} = \\text{Maximum} - \\text{Minimum}\n\\]\n\n\nCode\n# Two herds with same mean, different range\nherd_a &lt;- c(5.8, 5.9, 6.0, 6.1, 6.2)\nherd_b &lt;- c(3.5, 5.0, 6.0, 7.0, 8.5)\n\ncat(\"Herd A:\", paste(herd_a, collapse = \", \"), \"\\n\")\n\n\nHerd A: 5.8, 5.9, 6, 6.1, 6.2 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_a), min(herd_a), max(herd_a), max(herd_a) - min(herd_a)))\n\n\n  Mean: 6.0 kg | Range: 5.8 - 6.2 kg (0.4 kg)\n\n\nCode\ncat(\"\\nHerd B:\", paste(herd_b, collapse = \", \"), \"\\n\")\n\n\n\nHerd B: 3.5, 5, 6, 7, 8.5 \n\n\nCode\ncat(sprintf(\"  Mean: %.1f kg | Range: %.1f - %.1f kg (%.1f kg)\\n\",\n            mean(herd_b), min(herd_b), max(herd_b), max(herd_b) - min(herd_b)))\n\n\n  Mean: 6.0 kg | Range: 3.5 - 8.5 kg (5.0 kg)\n\n\nLimitation: The range uses only two values (min and max) and is extremely sensitive to outliers. We need better measures.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-variance-sd",
    "href": "chapters/ch10-descriptive_statistics.html#sec-variance-sd",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.2 Variance and Standard Deviation",
    "text": "13.2 Variance and Standard Deviation\nThe variance and standard deviation are the most important measures of variability in statistics.\n\n13.2.1 Variance\nThe variance measures the average squared deviation from the mean:\n\\[\ns^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\n\\]\nWhere:\n\n\\(s^2\\) = sample variance\n\\(n\\) = sample size\n\\(x_i\\) = each observation\n\\(\\bar{x}\\) = sample mean\n\\((x_i - \\bar{x})\\) = deviation of observation \\(i\\) from the mean\n\\(n-1\\) = degrees of freedom (we use \\(n-1\\) instead of \\(n\\) for sample variance)\n\nWhy \\(n-1\\) instead of \\(n\\)? This is called Bessel‚Äôs correction. Using \\(n-1\\) makes the sample variance an unbiased estimator of the population variance. Since we estimated the mean from the same data, we ‚Äúlose‚Äù one degree of freedom.\n\n\n13.2.2 Standard Deviation\nThe standard deviation is the square root of the variance:\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\nWhy take the square root? Variance is in squared units (e.g., kg¬≤), which is hard to interpret. Standard deviation is in the original units (kg), making it much more intuitive.\n\n\n13.2.3 Calculating by Hand\nLet‚Äôs calculate variance and SD step by step:\n\n\nCode\n# Piglet weights\nweights &lt;- c(6.2, 5.8, 6.5, 6.0, 5.9)\n\n# Step 1: Calculate mean\nmean_w &lt;- mean(weights)\n\n# Step 2: Calculate deviations from mean\ndeviations &lt;- weights - mean_w\n\n# Step 3: Square the deviations\nsquared_devs &lt;- deviations^2\n\n# Step 4: Sum squared deviations\nsum_sq_devs &lt;- sum(squared_devs)\n\n# Step 5: Divide by n-1\nvariance &lt;- sum_sq_devs / (length(weights) - 1)\n\n# Step 6: Take square root for SD\nstd_dev &lt;- sqrt(variance)\n\n# Create summary table\ntibble(\n  Weight = weights,\n  Deviation = round(deviations, 2),\n  `Squared Dev` = round(squared_devs, 3)\n) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Variance Calculation: Step by Step\") %&gt;%\n  tab_source_note(sprintf(\"Mean = %.2f kg\", mean_w)) %&gt;%\n  tab_source_note(sprintf(\"Sum of squared deviations = %.3f\", sum_sq_devs)) %&gt;%\n  tab_source_note(sprintf(\"Variance = %.3f / %d = %.3f kg¬≤\",\n                         sum_sq_devs, length(weights)-1, variance)) %&gt;%\n  tab_source_note(sprintf(\"Standard Deviation = ‚àö%.3f = %.3f kg\",\n                         variance, std_dev))\n\n\n\n\n\n\n\n\nVariance Calculation: Step by Step\n\n\nWeight\nDeviation\nSquared Dev\n\n\n\n\n6.2\n0.12\n0.014\n\n\n5.8\n-0.28\n0.078\n\n\n6.5\n0.42\n0.176\n\n\n6.0\n-0.08\n0.006\n\n\n5.9\n-0.18\n0.032\n\n\n\nMean = 6.08 kg\n\n\nSum of squared deviations = 0.308\n\n\nVariance = 0.308 / 4 = 0.077 kg¬≤\n\n\nStandard Deviation = ‚àö0.077 = 0.277 kg\n\n\n\n\n\n\n\n\n\n\nCode\n# Compare to R's built-in functions\ncat(sprintf(\"\\nUsing R functions:\\n\"))\n\n\n\nUsing R functions:\n\n\nCode\ncat(sprintf(\"  Variance: %.3f kg¬≤\\n\", var(weights)))\n\n\n  Variance: 0.077 kg¬≤\n\n\nCode\ncat(sprintf(\"  Standard Deviation: %.3f kg\\n\", sd(weights)))\n\n\n  Standard Deviation: 0.277 kg\n\n\n\n\n13.2.4 Interpreting Standard Deviation\nStandard deviation tells us, on average, how far observations deviate from the mean.\n\nSmall SD: Data are clustered tightly around the mean (low variability)\nLarge SD: Data are spread out widely (high variability)\n\n\n\nCode\nset.seed(999)\n\n# Generate two datasets with same mean, different SD\nlow_sd &lt;- rnorm(500, mean = 100, sd = 5)\nhigh_sd &lt;- rnorm(500, mean = 100, sd = 20)\n\np_low &lt;- ggplot(tibble(x = low_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"steelblue\", alpha = 0.7) +\n  geom_vline(xintercept = mean(low_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"Low Variability: SD = %.1f\", sd(low_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_high &lt;- ggplot(tibble(x = high_sd), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"darkorange\", alpha = 0.7) +\n  geom_vline(xintercept = mean(high_sd), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(title = sprintf(\"High Variability: SD = %.1f\", sd(high_sd)),\n       x = \"Weight (kg)\", y = \"Count\") +\n  xlim(20, 180)\n\np_low + p_high\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipThe Empirical Rule (68-95-99.7 Rule)\n\n\n\nFor data that are approximately normally distributed:\n\nAbout 68% of values fall within 1 SD of the mean\nAbout 95% of values fall within 2 SD of the mean\nAbout 99.7% of values fall within 3 SD of the mean\n\nThis rule helps you quickly assess whether an observation is unusual.\n\n\n\n\nCode\n# Demonstrate empirical rule\nset.seed(2025)\ndata_norm &lt;- rnorm(10000, mean = 100, sd = 15)\nmean_val &lt;- 100\nsd_val &lt;- 15\n\nggplot(tibble(x = data_norm), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 50,\n                 fill = \"lightblue\", alpha = 0.7) +\n  geom_density(color = \"darkblue\", linewidth = 1.5) +\n  # Mark mean\n  geom_vline(xintercept = mean_val, color = \"red\",\n             linetype = \"solid\", linewidth = 1.2) +\n  # Mark ¬±1 SD\n  geom_vline(xintercept = c(mean_val - sd_val, mean_val + sd_val),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ¬±2 SD\n  geom_vline(xintercept = c(mean_val - 2*sd_val, mean_val + 2*sd_val),\n             color = \"orange\", linetype = \"dashed\", linewidth = 1) +\n  # Mark ¬±3 SD\n  geom_vline(xintercept = c(mean_val - 3*sd_val, mean_val + 3*sd_val),\n             color = \"purple\", linetype = \"dashed\", linewidth = 1) +\n  # Annotations\n  annotate(\"text\", x = mean_val, y = 0.025, label = \"Mean\",\n           color = \"red\", fontface = \"bold\") +\n  annotate(\"text\", x = mean_val + sd_val, y = 0.020,\n           label = \"¬±1 SD\\n(68%)\", color = \"darkgreen\", size = 3) +\n  annotate(\"text\", x = mean_val + 2*sd_val, y = 0.015,\n           label = \"¬±2 SD\\n(95%)\", color = \"orange\", size = 3) +\n  annotate(\"text\", x = mean_val + 3*sd_val, y = 0.010,\n           label = \"¬±3 SD\\n(99.7%)\", color = \"purple\", size = 3) +\n  labs(\n    title = \"The Empirical Rule (68-95-99.7 Rule)\",\n    subtitle = \"For normal distributions: most data fall within 3 SD of the mean\",\n    x = \"Value\",\n    y = \"Density\"\n  ) +\n  xlim(25, 175)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-iqr",
    "href": "chapters/ch10-descriptive_statistics.html#sec-iqr",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "13.3 Interquartile Range (IQR)",
    "text": "13.3 Interquartile Range (IQR)\nThe interquartile range (IQR) is a robust measure of spread that‚Äôs not affected by outliers.\n\n13.3.1 Quartiles\nQuartiles divide data into four equal parts:\n\nQ1 (First quartile / 25th percentile): 25% of data are below this value\nQ2 (Second quartile / 50th percentile): The median\nQ3 (Third quartile / 75th percentile): 75% of data are below this value\n\n\\[\n\\text{IQR} = Q3 - Q1\n\\]\nThe IQR represents the range containing the middle 50% of the data.\n\n\nCode\n# Beef cattle weights\ncattle_weights &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\nq1 &lt;- quantile(cattle_weights, 0.25)\nq2 &lt;- quantile(cattle_weights, 0.50)  # Median\nq3 &lt;- quantile(cattle_weights, 0.75)\niqr_val &lt;- IQR(cattle_weights)\n\ncat(\"Cattle weights (kg):\", paste(cattle_weights, collapse = \", \"), \"\\n\\n\")\n\n\nCattle weights (kg): 450, 480, 490, 510, 520, 530, 540, 560, 580, 620 \n\n\nCode\ncat(sprintf(\"Q1 (25th percentile): %.1f kg\\n\", q1))\n\n\nQ1 (25th percentile): 495.0 kg\n\n\nCode\ncat(sprintf(\"Q2 (Median):          %.1f kg\\n\", q2))\n\n\nQ2 (Median):          525.0 kg\n\n\nCode\ncat(sprintf(\"Q3 (75th percentile): %.1f kg\\n\", q3))\n\n\nQ3 (75th percentile): 555.0 kg\n\n\nCode\ncat(sprintf(\"\\nIQR = Q3 - Q1 = %.1f - %.1f = %.1f kg\\n\", q3, q1, iqr_val))\n\n\n\nIQR = Q3 - Q1 = 555.0 - 495.0 = 60.0 kg\n\n\nCode\ncat(\"\\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\\n\")\n\n\n\nInterpretation: The middle 50% of cattle weigh between 495 and 565 kg.\n\n\n\n\n13.3.2 IQR is Robust to Outliers\n\n\nCode\n# Normal data\nnormal_data &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 620)\n\n# Add extreme outlier\nwith_outlier &lt;- c(450, 480, 490, 510, 520, 530, 540, 560, 580, 900)\n\ncat(\"Without outlier:\\n\")\n\n\nWithout outlier:\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg\\n\", sd(normal_data)))\n\n\n  SD:  50.1 kg\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg\\n\", IQR(normal_data)))\n\n\n  IQR: 60.0 kg\n\n\nCode\ncat(\"\\nWith outlier (900 kg):\\n\")\n\n\n\nWith outlier (900 kg):\n\n\nCode\ncat(sprintf(\"  SD:  %.1f kg (increased by %.1f kg)\\n\",\n            sd(with_outlier), sd(with_outlier) - sd(normal_data)))\n\n\n  SD:  126.8 kg (increased by 76.7 kg)\n\n\nCode\ncat(sprintf(\"  IQR: %.1f kg (increased by %.1f kg)\\n\",\n            IQR(with_outlier), IQR(with_outlier) - IQR(normal_data)))\n\n\n  IQR: 60.0 kg (increased by 0.0 kg)\n\n\nCode\ncat(\"\\nIQR is much more stable in the presence of outliers!\\n\")\n\n\n\nIQR is much more stable in the presence of outliers!\n\n\n\n\n\n\n\n\nTipWhen to Use SD vs IQR\n\n\n\nUse Standard Deviation when:\n\nData are approximately normal\nNo extreme outliers\nYou need mathematical properties of variance (e.g., for further statistical tests)\n\nUse IQR when:\n\nData are skewed\nOutliers are present\nYou want a robust, resistant measure of spread",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-histograms",
    "href": "chapters/ch10-descriptive_statistics.html#sec-histograms",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.1 Histograms",
    "text": "14.1 Histograms\nA histogram shows the distribution of a continuous variable by dividing the range into bins and counting observations in each bin.\n\n\nCode\n# Generate pig growth data\nset.seed(12345)\npig_weights &lt;- tibble(\n  weight = rnorm(200, mean = 115, sd = 18),\n  diet = sample(c(\"Control\", \"High Protein\"), 200, replace = TRUE)\n)\n\n# Basic histogram\np_hist1 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(\n    title = \"Histogram of Pig Weights\",\n    subtitle = \"20 bins\",\n    x = \"Final Weight (kg)\",\n    y = \"Count\"\n  )\n\n# Histogram with density overlay\np_hist2 &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 20,\n                 fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = mean(pig_weights$weight),\n             color = \"darkgreen\", linetype = \"dashed\", linewidth = 1) +\n  labs(\n    title = \"Histogram with Density Curve\",\n    subtitle = \"Red = density curve | Green = mean\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\"\n  )\n\np_hist1 + p_hist2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningChoosing the Number of Bins\n\n\n\nThe number of bins affects how the distribution looks:\n\nToo few bins: May hide important features\nToo many bins: May show too much noise\n\nCommon rules:\n\nSturges‚Äô rule: \\(\\text{bins} \\approx \\log_2(n) + 1\\)\nSquare root rule: \\(\\text{bins} \\approx \\sqrt{n}\\)\nOr just experiment! Try different bin numbers and see what reveals patterns best\n\n\n\n\n\nCode\n# Show effect of bin number\np_few &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 5, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Few Bins (5)\", x = \"Weight (kg)\", y = \"Count\")\n\np_many &lt;- ggplot(pig_weights, aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", color = \"white\", alpha = 0.7) +\n  labs(title = \"Too Many Bins (50)\", x = \"Weight (kg)\", y = \"Count\")\n\np_few + p_many",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-boxplots",
    "href": "chapters/ch10-descriptive_statistics.html#sec-boxplots",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.2 Boxplots",
    "text": "14.2 Boxplots\nA boxplot (box-and-whisker plot) displays the distribution using five-number summary: minimum, Q1, median, Q3, and maximum.\n\n14.2.1 Anatomy of a Boxplot\n\n\nCode\n# Create sample data\nset.seed(456)\nsample_data &lt;- rnorm(100, mean = 100, sd = 15)\n\n# Calculate components\nq1 &lt;- quantile(sample_data, 0.25)\nmedian_val &lt;- median(sample_data)\nq3 &lt;- quantile(sample_data, 0.75)\niqr_val &lt;- IQR(sample_data)\nlower_whisker &lt;- max(min(sample_data), q1 - 1.5 * iqr_val)\nupper_whisker &lt;- min(max(sample_data), q3 + 1.5 * iqr_val)\n\n# Find outliers\noutliers &lt;- sample_data[sample_data &lt; lower_whisker | sample_data &gt; upper_whisker]\n\n# Create boxplot\nggplot(tibble(x = \"Data\", y = sample_data), aes(x = x, y = y)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 3) +\n  # Add labels\n  annotate(\"text\", x = 1.35, y = q1, label = sprintf(\"Q1 = %.1f\", q1),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = median_val, label = sprintf(\"Median = %.1f\", median_val),\n           hjust = 0, size = 4, color = \"darkgreen\", fontface = \"bold\") +\n  annotate(\"text\", x = 1.35, y = q3, label = sprintf(\"Q3 = %.1f\", q3),\n           hjust = 0, size = 4, color = \"blue\") +\n  annotate(\"text\", x = 1.35, y = upper_whisker,\n           label = sprintf(\"Upper whisker = %.1f\", upper_whisker),\n           hjust = 0, size = 3.5) +\n  annotate(\"text\", x = 1.35, y = lower_whisker,\n           label = sprintf(\"Lower whisker = %.1f\", lower_whisker),\n           hjust = 0, size = 3.5) +\n  # Add IQR bracket\n  annotate(\"segment\", x = 0.7, xend = 0.7, y = q1, yend = q3,\n           color = \"purple\", linewidth = 1.5) +\n  annotate(\"text\", x = 0.65, y = (q1 + q3)/2,\n           label = sprintf(\"IQR = %.1f\", iqr_val),\n           angle = 90, vjust = 1, color = \"purple\", fontface = \"bold\") +\n  labs(\n    title = \"Anatomy of a Boxplot\",\n    subtitle = \"Red points are outliers (&gt; 1.5 √ó IQR from Q1 or Q3)\",\n    y = \"Value\",\n    x = \"\"\n  ) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\n\n\n14.2.2 Comparing Groups with Boxplots\nBoxplots are excellent for comparing distributions across groups:\n\n\nCode\n# Simulate beef cattle data from different breeding programs\nset.seed(789)\ncattle_data &lt;- tibble(\n  program = rep(c(\"Program A\", \"Program B\", \"Program C\"), each = 60),\n  weight = c(\n    rnorm(60, mean = 580, sd = 45),  # Program A\n    rnorm(60, mean = 610, sd = 50),  # Program B\n    rnorm(60, mean = 595, sd = 35)   # Program C\n  )\n)\n\n# Boxplot comparison\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_boxplot(alpha = 0.7, outlier.size = 2) +\n  geom_jitter(width = 0.2, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Final Weights by Breeding Program\",\n    subtitle = \"Red diamond = mean | Bold line = median | Box = IQR\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-density",
    "href": "chapters/ch10-descriptive_statistics.html#sec-density",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.3 Density Plots",
    "text": "14.3 Density Plots\nA density plot is a smoothed version of a histogram, showing the probability density function.\n\n\nCode\n# Compare distributions across groups\nggplot(cattle_data, aes(x = weight, fill = program)) +\n  geom_density(alpha = 0.5, linewidth = 1) +\n  geom_vline(data = cattle_data %&gt;% group_by(program) %&gt;%\n               summarise(mean_wt = mean(weight), .groups = 'drop'),\n             aes(xintercept = mean_wt, color = program),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Density Plots: Comparing Weight Distributions\",\n    subtitle = \"Dashed lines show group means\",\n    x = \"Final Weight (kg)\",\n    y = \"Density\",\n    fill = \"Program\",\n    color = \"Program\"\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-violin",
    "href": "chapters/ch10-descriptive_statistics.html#sec-violin",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "14.4 Violin Plots",
    "text": "14.4 Violin Plots\nA violin plot combines a boxplot with a density plot, showing both summary statistics and the full distribution shape.\n\n\nCode\nggplot(cattle_data, aes(x = program, y = weight, fill = program)) +\n  geom_violin(alpha = 0.6, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.8, outlier.shape = NA) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Violin Plots: Distribution Shape + Boxplot\",\n    subtitle = \"Width shows density | Box shows quartiles | Red = mean\",\n    x = \"Breeding Program\",\n    y = \"Final Weight (kg)\"\n  ) +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-iqr-method",
    "href": "chapters/ch10-descriptive_statistics.html#sec-iqr-method",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.1 The IQR Method",
    "text": "15.1 The IQR Method\nThe most common method defines outliers as observations beyond:\n\\[\n\\begin{align}\n\\text{Lower fence} &= Q1 - 1.5 \\times \\text{IQR} \\\\\n\\text{Upper fence} &= Q3 + 1.5 \\times \\text{IQR}\n\\end{align}\n\\]\nThis is the definition used by boxplots.\n\n\nCode\n# Cattle weights with some outliers\nset.seed(111)\nweights_with_outliers &lt;- c(\n  rnorm(45, mean = 550, sd = 40),  # Normal cattle\n  c(350, 420, 720)                  # Outliers\n)\n\n# Calculate fences\nq1 &lt;- quantile(weights_with_outliers, 0.25)\nq3 &lt;- quantile(weights_with_outliers, 0.75)\niqr &lt;- IQR(weights_with_outliers)\nlower_fence &lt;- q1 - 1.5 * iqr\nupper_fence &lt;- q3 + 1.5 * iqr\n\n# Identify outliers\noutliers &lt;- weights_with_outliers[weights_with_outliers &lt; lower_fence |\n                                   weights_with_outliers &gt; upper_fence]\n\ncat(\"Outlier Detection Using IQR Method\\n\")\n\n\nOutlier Detection Using IQR Method\n\n\nCode\ncat(\"===================================\\n\")\n\n\n===================================\n\n\nCode\ncat(sprintf(\"Q1 = %.1f kg\\n\", q1))\n\n\nQ1 = 501.2 kg\n\n\nCode\ncat(sprintf(\"Q3 = %.1f kg\\n\", q3))\n\n\nQ3 = 564.0 kg\n\n\nCode\ncat(sprintf(\"IQR = %.1f kg\\n\", iqr))\n\n\nIQR = 62.8 kg\n\n\nCode\ncat(sprintf(\"\\nLower fence = Q1 - 1.5√óIQR = %.1f - %.1f = %.1f kg\\n\",\n            q1, 1.5*iqr, lower_fence))\n\n\n\nLower fence = Q1 - 1.5√óIQR = 501.2 - 94.3 = 406.9 kg\n\n\nCode\ncat(sprintf(\"Upper fence = Q3 + 1.5√óIQR = %.1f + %.1f = %.1f kg\\n\",\n            q3, 1.5*iqr, upper_fence))\n\n\nUpper fence = Q3 + 1.5√óIQR = 564.0 + 94.3 = 658.3 kg\n\n\nCode\ncat(sprintf(\"\\nOutliers detected: %s\\n\", paste(round(outliers, 1), collapse = \", \")))\n\n\n\nOutliers detected: 658.7, 350, 720\n\n\n\n\nCode\n# Visualize outliers\noutlier_data &lt;- tibble(\n  weight = weights_with_outliers,\n  is_outlier = weight &lt; lower_fence | weight &gt; upper_fence\n)\n\np_box_outlier &lt;- ggplot(outlier_data, aes(x = \"Cattle\", y = weight)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\",\n               outlier.size = 4) +\n  geom_hline(yintercept = c(lower_fence, upper_fence),\n             linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = lower_fence,\n           label = sprintf(\"Lower fence = %.0f\", lower_fence),\n           color = \"red\", hjust = 0) +\n  annotate(\"text\", x = 1.3, y = upper_fence,\n           label = sprintf(\"Upper fence = %.0f\", upper_fence),\n           color = \"red\", hjust = 0) +\n  labs(title = \"Boxplot: Outliers in Red\",\n       y = \"Weight (kg)\", x = \"\") +\n  theme(axis.text.x = element_blank())\n\np_hist_outlier &lt;- ggplot(outlier_data, aes(x = weight, fill = is_outlier)) +\n  geom_histogram(bins = 20, color = \"white\", alpha = 0.7) +\n  scale_fill_manual(values = c(\"FALSE\" = \"steelblue\", \"TRUE\" = \"red\"),\n                    labels = c(\"Normal\", \"Outlier\")) +\n  labs(title = \"Histogram: Outliers Highlighted\",\n       x = \"Weight (kg)\", y = \"Count\", fill = \"\")\n\np_box_outlier + p_hist_outlier",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#sec-handling-outliers",
    "href": "chapters/ch10-descriptive_statistics.html#sec-handling-outliers",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "15.2 What to Do with Outliers?",
    "text": "15.2 What to Do with Outliers?\nNEVER automatically delete outliers! Instead:\n\nInvestigate: Is it a data entry error? Measurement error? Legitimate extreme value?\nDocument: Record what you find and what you decide\nConsider:\n\nIf error: Correct if possible, or remove and document\nIf legitimate: Keep it! Report results with and without outliers if it‚Äôs influential\nIf different population: Analyze separately\n\n\n\n\n\n\n\n\nWarningImportant\n\n\n\nRemoving outliers just to get ‚Äúbetter‚Äù p-values is data manipulation and scientifically dishonest. Always have a principled reason for any data exclusions and report them transparently.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#example-swine-growth-trial-summary",
    "href": "chapters/ch10-descriptive_statistics.html#example-swine-growth-trial-summary",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "16.1 Example: Swine Growth Trial Summary",
    "text": "16.1 Example: Swine Growth Trial Summary\n\n\nCode\n# Simulate swine growth data\nset.seed(2025)\nswine_data &lt;- tibble(\n  diet = rep(c(\"Control\", \"High Protein\", \"High Energy\", \"Balanced\"), each = 50),\n  initial_weight = rnorm(200, mean = 25, sd = 3),\n  final_weight = initial_weight + rnorm(200, mean = 90, sd = 12) +\n    case_when(\n      diet == \"Control\" ~ 0,\n      diet == \"High Protein\" ~ 5,\n      diet == \"High Energy\" ~ 3,\n      diet == \"Balanced\" ~ 7\n    )\n) %&gt;%\n  mutate(weight_gain = final_weight - initial_weight)\n\n# Create comprehensive summary table\nsummary_table &lt;- swine_data %&gt;%\n  group_by(diet) %&gt;%\n  summarise(\n    N = n(),\n    Mean = mean(weight_gain),\n    SD = sd(weight_gain),\n    Median = median(weight_gain),\n    IQR = IQR(weight_gain),\n    Min = min(weight_gain),\n    Max = max(weight_gain),\n    .groups = 'drop'\n  )\n\n# Display with gt package\nsummary_table %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Summary Statistics: Weight Gain by Diet\",\n    subtitle = \"12-week growth trial (n=200 pigs)\"\n  ) %&gt;%\n  fmt_number(\n    columns = c(Mean, SD, Median, IQR, Min, Max),\n    decimals = 1\n  ) %&gt;%\n  cols_label(\n    diet = \"Diet Treatment\",\n    N = \"n\",\n    Mean = \"Mean (kg)\",\n    SD = \"SD (kg)\",\n    Median = \"Median (kg)\",\n    IQR = \"IQR (kg)\",\n    Min = \"Min (kg)\",\n    Max = \"Max (kg)\"\n  ) %&gt;%\n  tab_source_note(\"SD = Standard Deviation; IQR = Interquartile Range\") %&gt;%\n  tab_options(\n    table.font.size = 12,\n    heading.background.color = \"#f0f0f0\"\n  )\n\n\n\n\n\n\n\n\nSummary Statistics: Weight Gain by Diet\n\n\n12-week growth trial (n=200 pigs)\n\n\nDiet Treatment\nn\nMean (kg)\nSD (kg)\nMedian (kg)\nIQR (kg)\nMin (kg)\nMax (kg)\n\n\n\n\nBalanced\n50\n96.5\n11.8\n96.1\n12.0\n66.4\n131.0\n\n\nControl\n50\n89.0\n11.6\n89.0\n15.3\n55.8\n112.6\n\n\nHigh Energy\n50\n92.1\n12.1\n92.8\n15.6\n63.5\n116.6\n\n\nHigh Protein\n50\n96.2\n12.0\n96.6\n16.3\n63.5\n124.9\n\n\n\nSD = Standard Deviation; IQR = Interquartile Range",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-1-overall-summary-statistics",
    "href": "chapters/ch10-descriptive_statistics.html#step-1-overall-summary-statistics",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.1 Step 1: Overall Summary Statistics",
    "text": "17.1 Step 1: Overall Summary Statistics\n\n\nCode\n# Overall summary of key variables\noverall_summary &lt;- feedlot_data %&gt;%\n  summarise(\n    `Sample Size` = n(),\n    across(c(initial_weight, final_weight, weight_gain, adg),\n           list(\n             Mean = ~mean(.),\n             SD = ~sd(.),\n             Median = ~median(.),\n             IQR = ~IQR(.),\n             Min = ~min(.),\n             Max = ~max(.)\n           ),\n           .names = \"{.col}_{.fn}\")\n  )\n\n# Reshape for display\noverall_summary %&gt;%\n  pivot_longer(-`Sample Size`, names_to = \"stat\", values_to = \"value\") %&gt;%\n  separate(stat, into = c(\"variable\", \"measure\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = measure, values_from = value) %&gt;%\n  mutate(variable = case_when(\n    variable == \"initial\" ~ \"Initial Weight (kg)\",\n    variable == \"final\" ~ \"Final Weight (kg)\",\n    variable == \"weight\" ~ \"Weight Gain (kg)\",\n    variable == \"adg\" ~ \"ADG (kg/day)\"\n  )) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Overall Summary Statistics\",\n             subtitle = sprintf(\"n = %d cattle\", overall_summary$`Sample Size`)) %&gt;%\n  fmt_number(columns = -variable, decimals = 2) %&gt;%\n  cols_label(variable = \"Variable\")\n\n\n\n\n\n\n\n\nOverall Summary Statistics\n\n\nn = 180 cattle\n\n\nSample Size\nVariable\nweight\ngain\nMean\nSD\nMedian\nIQR\nMin\nMax\n\n\n\n\n180.00\nInitial Weight (kg)\n298.21808, 35.59733, 301.28524, 49.67098, 176.78933, 381.05291\n\n\n\n\n\n\n\n\n\n180.00\nFinal Weight (kg)\n543.05239, 48.61542, 545.12486, 61.43552, 400.31501, 684.27656\n\n\n\n\n\n\n\n\n\n180.00\nWeight Gain (kg)\n\n244.83430, 34.25836, 245.22667, 42.03852, 139.47315, 331.21941\n\n\n\n\n\n\n\n\n180.00\nADG (kg/day)\n\n\n1.360191\n0.1903242\n1.36237\n0.2335473\n0.7748508\n1.840108",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-2-visualize-distributions",
    "href": "chapters/ch10-descriptive_statistics.html#step-2-visualize-distributions",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.2 Step 2: Visualize Distributions",
    "text": "17.2 Step 2: Visualize Distributions\n\n\nCode\n# Create multiple visualizations\np1 &lt;- ggplot(feedlot_data, aes(x = initial_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"steelblue\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$initial_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Initial Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np2 &lt;- ggplot(feedlot_data, aes(x = final_weight)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkorange\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$final_weight),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"Final Weight Distribution\", x = \"Weight (kg)\", y = \"Density\")\n\np3 &lt;- ggplot(feedlot_data, aes(x = weight_gain)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"darkgreen\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$weight_gain),\n             linetype = \"dashed\", color = \"darkblue\", linewidth = 1) +\n  labs(title = \"Weight Gain Distribution\", x = \"Weight Gain (kg)\", y = \"Density\")\n\np4 &lt;- ggplot(feedlot_data, aes(x = adg)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 25,\n                 fill = \"purple\", alpha = 0.7) +\n  geom_density(color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mean(feedlot_data$adg),\n             linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(title = \"ADG Distribution\", x = \"ADG (kg/day)\", y = \"Density\")\n\n(p1 + p2) / (p3 + p4) +\n  plot_annotation(title = \"Distribution of Weight Variables\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-3-compare-groups",
    "href": "chapters/ch10-descriptive_statistics.html#step-3-compare-groups",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.3 Step 3: Compare Groups",
    "text": "17.3 Step 3: Compare Groups\n\n\nCode\n# Summary by feed program\nfeed_summary &lt;- feedlot_data %&gt;%\n  group_by(feed_program) %&gt;%\n  summarise(\n    n = n(),\n    Mean_ADG = mean(adg),\n    SD_ADG = sd(adg),\n    Median_ADG = median(adg),\n    IQR_ADG = IQR(adg),\n    .groups = 'drop'\n  )\n\nprint(\"Summary by Feed Program:\")\n\n\n[1] \"Summary by Feed Program:\"\n\n\nCode\nfeed_summary %&gt;%\n  gt() %&gt;%\n  fmt_number(columns = -c(feed_program, n), decimals = 3) %&gt;%\n  cols_label(feed_program = \"Feed Program\",\n             n = \"n\",\n             Mean_ADG = \"Mean ADG\",\n             SD_ADG = \"SD\",\n             Median_ADG = \"Median ADG\",\n             IQR_ADG = \"IQR\")\n\n\n\n\n\n\n\n\nFeed Program\nn\nMean ADG\nSD\nMedian ADG\nIQR\n\n\n\n\nEnhanced\n90\n1.377\n0.188\n1.368\n0.231\n\n\nStandard\n90\n1.344\n0.192\n1.353\n0.250\n\n\n\n\n\n\n\nCode\n# Visualize comparisons\np_feed &lt;- ggplot(feedlot_data, aes(x = feed_program, y = adg, fill = feed_program)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set1\") +\n  labs(title = \"ADG by Feed Program\", x = \"Feed Program\",\n       y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_sex &lt;- ggplot(feedlot_data, aes(x = sex, y = adg, fill = sex)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"ADG by Sex\", x = \"Sex\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_breed &lt;- ggplot(feedlot_data, aes(x = breed, y = adg, fill = breed)) +\n  geom_violin(alpha = 0.5, trim = FALSE) +\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.3, size = 1.5) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 4,\n               fill = \"red\", color = \"black\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"ADG by Breed\", x = \"Breed\", y = \"ADG (kg/day)\") +\n  theme(legend.position = \"none\")\n\np_feed + p_sex + p_breed +\n  plot_annotation(title = \"Comparing ADG Across Groups\",\n                  theme = theme(plot.title = element_text(size = 16, face = \"bold\")))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#step-4-check-for-outliers",
    "href": "chapters/ch10-descriptive_statistics.html#step-4-check-for-outliers",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "17.4 Step 4: Check for Outliers",
    "text": "17.4 Step 4: Check for Outliers\n\n\nCode\n# Identify outliers in ADG\nq1_adg &lt;- quantile(feedlot_data$adg, 0.25)\nq3_adg &lt;- quantile(feedlot_data$adg, 0.75)\niqr_adg &lt;- IQR(feedlot_data$adg)\nlower_adg &lt;- q1_adg - 1.5 * iqr_adg\nupper_adg &lt;- q3_adg + 1.5 * iqr_adg\n\noutliers_adg &lt;- feedlot_data %&gt;%\n  filter(adg &lt; lower_adg | adg &gt; upper_adg)\n\ncat(sprintf(\"Outlier Detection for ADG (kg/day)\\n\"))\n\n\nOutlier Detection for ADG (kg/day)\n\n\nCode\ncat(sprintf(\"Lower fence: %.3f\\n\", lower_adg))\n\n\nLower fence: 0.891\n\n\nCode\ncat(sprintf(\"Upper fence: %.3f\\n\", upper_adg))\n\n\nUpper fence: 1.825\n\n\nCode\ncat(sprintf(\"\\nNumber of outliers: %d out of %d (%.1f%%)\\n\",\n            nrow(outliers_adg), nrow(feedlot_data),\n            100 * nrow(outliers_adg) / nrow(feedlot_data)))\n\n\n\nNumber of outliers: 4 out of 180 (2.2%)\n\n\nCode\nif(nrow(outliers_adg) &gt; 0) {\n  cat(\"\\nOutlier animals:\\n\")\n  print(outliers_adg %&gt;%\n          select(animal_id, breed, sex, feed_program, adg) %&gt;%\n          arrange(adg))\n}\n\n\n\nOutlier animals:\n# A tibble: 4 √ó 5\n  animal_id breed     sex    feed_program   adg\n      &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;        &lt;dbl&gt;\n1        66 Hereford  Heifer Standard     0.775\n2       107 Hereford  Heifer Enhanced     0.859\n3       170 Angus     Steer  Enhanced     1.82 \n4        70 Charolais Steer  Standard     1.84",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#looking-ahead",
    "href": "chapters/ch10-descriptive_statistics.html#looking-ahead",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.1 Looking Ahead",
    "text": "18.1 Looking Ahead\nNext week, we‚Äôll build on these foundations by learning about:\n\nProbability distributions (especially the normal distribution)\nThe Central Limit Theorem (why means are normally distributed)\nStandard error vs standard deviation\nConfidence intervals (quantifying uncertainty)\nIntroduction to sampling distributions\n\nThese concepts will bridge descriptive statistics to inferential statistics, allowing us to make conclusions about populations based on samples.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#reflection-questions",
    "href": "chapters/ch10-descriptive_statistics.html#reflection-questions",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.2 Reflection Questions",
    "text": "18.2 Reflection Questions\nBefore next week, consider:\n\nFind a dataset from your research (or use one from class). Perform a complete EDA following the steps in this chapter.\nIn published papers from your field, are both mean/SD and median/IQR reported? Are visualizations included?\nThink about a variable you measure in your work. What would you consider an outlier? What would you do if you found one?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#additional-resources",
    "href": "chapters/ch10-descriptive_statistics.html#additional-resources",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.3 Additional Resources",
    "text": "18.3 Additional Resources\n\n18.3.1 R Packages for Descriptive Statistics\n\nskimr: Quick, comprehensive summaries of datasets\nsummarytools: Detailed univariate and bivariate summaries\npsych: Descriptive statistics for psychological/survey data\nDataExplorer: Automated EDA reports\n\n\n\n18.3.2 Recommended Reading\n\n‚ÄúExploratory Data Analysis‚Äù by John Tukey (1977) - the classic text\n‚ÄúData Visualization: A Practical Introduction‚Äù by Kieran Healy\n‚ÄúFundamentals of Biostatistics‚Äù by Bernard Rosner - Chapters 2-3\n\n\n\n18.3.3 Online Resources\n\nR for Data Science (2e): Chapters on data transformation and visualization\nggplot2 book by Hadley Wickham: Comprehensive guide to data visualization",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch10-descriptive_statistics.html#session-info",
    "href": "chapters/ch10-descriptive_statistics.html#session-info",
    "title": "10¬† Week 10: Descriptive Statistics and Exploratory Data Analysis",
    "section": "18.4 Session Info",
    "text": "18.4 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] scales_1.4.0    gt_1.1.0        patchwork_1.3.2 broom_1.0.7    \n [5] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [9] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n[13] ggplot2_4.0.0   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] xml2_1.3.6         yaml_2.3.10        fastmap_1.2.0      R6_2.5.1          \n [9] labeling_0.4.3     generics_0.1.3     knitr_1.49         backports_1.5.0   \n[13] htmlwidgets_1.6.4  pillar_1.9.0       RColorBrewer_1.1-3 tzdb_0.4.0        \n[17] rlang_1.1.6        utf8_1.2.4         stringi_1.8.4      xfun_0.53         \n[21] sass_0.4.9         fs_1.6.5           S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.4          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.4.2         hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.1     glue_1.8.0         farver_2.1.2       fansi_1.0.6       \n[37] rmarkdown_2.29     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\nEnd of Week 2: Descriptive Statistics and Exploratory Data Analysis",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Week 10: Descriptive Statistics and Exploratory Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html",
    "href": "chapters/ch11-probability_distributions.html",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "",
    "text": "12 Introduction: From Description to Inference\nIn the first two weeks, we learned to describe data: calculating means, standard deviations, creating visualizations, and understanding distributions. But animal scientists rarely just describe their sample‚Äîwe want to make inferences about the broader population.\nImagine you‚Äôre testing a new feed additive in dairy cattle. You can‚Äôt test every dairy cow in the world, so you select a sample of 50 cows, randomly assign 25 to each treatment, and measure milk production. Your key questions are:\nThese are questions of statistical inference, and they all rely on understanding probability distributions.\nThis week, we bridge the gap between descriptive and inferential statistics by exploring:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#sec-normal-properties",
    "href": "chapters/ch11-probability_distributions.html#sec-normal-properties",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "13.1 Properties of the Normal Distribution",
    "text": "13.1 Properties of the Normal Distribution\nA normal distribution is characterized by:\n\nSymmetric bell-shaped curve\nDefined by two parameters:\n\n\\(\\mu\\) (mu): The population mean (center of the distribution)\n\\(\\sigma\\) (sigma): The population standard deviation (spread of the distribution)\n\nNotation: \\(X \\sim N(\\mu, \\sigma^2)\\) means ‚ÄúX follows a normal distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\)‚Äù\n\n\n13.1.1 Mathematical Formula\nThe probability density function (PDF) of the normal distribution is:\n\\[\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n\\]\nDon‚Äôt worry about memorizing this formula! R will do all calculations for us. What matters is understanding the properties.\n\n\n13.1.2 Key Properties\n\n\nCode\n# Generate normal distributions with different parameters\nx &lt;- seq(-10, 20, length.out = 500)\n\n# Different means, same SD\ndist1 &lt;- dnorm(x, mean = 0, sd = 2)\ndist2 &lt;- dnorm(x, mean = 5, sd = 2)\ndist3 &lt;- dnorm(x, mean = 10, sd = 2)\n\n# Same mean, different SDs\ndist4 &lt;- dnorm(x, mean = 5, sd = 1)\ndist5 &lt;- dnorm(x, mean = 5, sd = 2)\ndist6 &lt;- dnorm(x, mean = 5, sd = 3)\n\n# Create data frames for plotting\ndf_means &lt;- bind_rows(\n  tibble(x = x, density = dist1, distribution = \"N(0, 2¬≤)\"),\n  tibble(x = x, density = dist2, distribution = \"N(5, 2¬≤)\"),\n  tibble(x = x, density = dist3, distribution = \"N(10, 2¬≤)\")\n)\n\ndf_sds &lt;- bind_rows(\n  tibble(x = x, density = dist4, distribution = \"N(5, 1¬≤)\"),\n  tibble(x = x, density = dist5, distribution = \"N(5, 2¬≤)\"),\n  tibble(x = x, density = dist6, distribution = \"N(5, 3¬≤)\")\n)\n\np1 &lt;- ggplot(df_means, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Effect of Changing Mean (Œº)\",\n    subtitle = \"Same spread (œÉ = 2), different centers\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme(legend.position = \"top\")\n\np2 &lt;- ggplot(df_sds, aes(x = x, y = density, color = distribution)) +\n  geom_line(linewidth = 1.2) +\n  labs(\n    title = \"Effect of Changing Standard Deviation (œÉ)\",\n    subtitle = \"Same center (Œº = 5), different spreads\",\n    x = \"Value\",\n    y = \"Density\",\n    color = \"Distribution\"\n  ) +\n  theme(legend.position = \"top\")\n\np1 / p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Insight\n\n\n\n\nChanging Œº shifts the distribution left or right\nChanging œÉ changes the spread (wider or narrower)\nThe area under the entire curve always equals 1 (total probability = 100%)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#sec-empirical-rule",
    "href": "chapters/ch11-probability_distributions.html#sec-empirical-rule",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "13.2 The Empirical Rule (68-95-99.7 Rule)",
    "text": "13.2 The Empirical Rule (68-95-99.7 Rule)\nFor any normal distribution:\n\nApproximately 68% of values fall within 1 SD of the mean (\\(\\mu \\pm 1\\sigma\\))\nApproximately 95% of values fall within 2 SD of the mean (\\(\\mu \\pm 2\\sigma\\))\nApproximately 99.7% of values fall within 3 SD of the mean (\\(\\mu \\pm 3\\sigma\\))\n\nThis rule is incredibly useful for quick mental calculations!\n\n\nCode\n# Create visualization of empirical rule\nmu &lt;- 500  # Mean birth weight (kg) for beef calves\nsigma &lt;- 40\n\nx_vals &lt;- seq(mu - 4*sigma, mu + 4*sigma, length.out = 500)\ny_vals &lt;- dnorm(x_vals, mean = mu, sd = sigma)\n\n# Create shaded regions\ndf_norm &lt;- tibble(x = x_vals, y = y_vals)\n\n# Base plot\np &lt;- ggplot(df_norm, aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\")\n\n# Add shaded regions\np &lt;- p +\n  # 1 SD (68%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - sigma & x &lt;= mu + sigma),\n            aes(x = x, y = y), fill = \"darkgreen\", alpha = 0.3) +\n  # 2 SD (95%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - 2*sigma & x &lt;= mu + 2*sigma),\n            aes(x = x, y = y), fill = \"orange\", alpha = 0.2) +\n  # 3 SD (99.7%)\n  geom_area(data = df_norm %&gt;% filter(x &gt;= mu - 3*sigma & x &lt;= mu + 3*sigma),\n            aes(x = x, y = y), fill = \"purple\", alpha = 0.15)\n\n# Add vertical lines\np &lt;- p +\n  geom_vline(xintercept = mu, linetype = \"solid\", color = \"red\", linewidth = 1) +\n  geom_vline(xintercept = mu + c(-1, 1) * sigma, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 0.8) +\n  geom_vline(xintercept = mu + c(-2, 2) * sigma, linetype = \"dashed\",\n             color = \"orange\", linewidth = 0.8) +\n  geom_vline(xintercept = mu + c(-3, 3) * sigma, linetype = \"dashed\",\n             color = \"purple\", linewidth = 0.8)\n\n# Add annotations\np &lt;- p +\n  annotate(\"text\", x = mu, y = max(y_vals) * 1.05, label = \"Œº = 500\",\n           color = \"red\", fontface = \"bold\", size = 5) +\n  annotate(\"text\", x = mu, y = max(y_vals) * 0.5, label = \"68%\",\n           color = \"darkgreen\", fontface = \"bold\", size = 6) +\n  annotate(\"text\", x = mu + 1.5*sigma, y = max(y_vals) * 0.3, label = \"95%\",\n           color = \"orange\", fontface = \"bold\", size = 5) +\n  annotate(\"text\", x = mu + 2.5*sigma, y = max(y_vals) * 0.15, label = \"99.7%\",\n           color = \"purple\", fontface = \"bold\", size = 4)\n\np + labs(\n  title = \"The Empirical Rule for Normal Distributions\",\n  subtitle = \"Birth weights of beef calves: Œº = 500 kg, œÉ = 40 kg\",\n  x = \"Birth Weight (kg)\",\n  y = \"Probability Density\"\n)\n\n\n\n\n\n\n\n\n\n\n13.2.1 Example Application\nQuestion: If birth weights are normally distributed with mean 500 kg and SD 40 kg, what range contains 95% of birth weights?\nAnswer: Using the empirical rule:\n\\[\n\\mu \\pm 2\\sigma = 500 \\pm 2(40) = 500 \\pm 80 = [420, 580] \\text{ kg}\n\\]\n\n\nCode\n# Calculate exactly\nmu &lt;- 500\nsigma &lt;- 40\n\ncat(\"Empirical Rule: 95% of birth weights fall within:\\n\")\n\n\nEmpirical Rule: 95% of birth weights fall within:\n\n\nCode\ncat(sprintf(\"  [%.0f, %.0f] kg\\n\", mu - 2*sigma, mu + 2*sigma))\n\n\n  [420, 580] kg\n\n\nCode\ncat(\"\\nThis means:\\n\")\n\n\n\nThis means:\n\n\nCode\ncat(sprintf(\"  - About 95%% of calves weigh between %.0f and %.0f kg at birth\\n\",\n            mu - 2*sigma, mu + 2*sigma))\n\n\n  - About 95% of calves weigh between 420 and 580 kg at birth\n\n\nCode\ncat(sprintf(\"  - Only ~2.5%% weigh less than %.0f kg\\n\", mu - 2*sigma))\n\n\n  - Only ~2.5% weigh less than 420 kg\n\n\nCode\ncat(sprintf(\"  - Only ~2.5%% weigh more than %.0f kg\\n\", mu + 2*sigma))\n\n\n  - Only ~2.5% weigh more than 580 kg",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#sec-normal-r",
    "href": "chapters/ch11-probability_distributions.html#sec-normal-r",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "13.3 Working with the Normal Distribution in R",
    "text": "13.3 Working with the Normal Distribution in R\nR provides four functions for working with the normal distribution:\n\ndnorm(): Probability density function (height of the curve)\npnorm(): Cumulative distribution function (probability below a value)\nqnorm(): Quantile function (inverse of pnorm())\nrnorm(): Random number generation\n\n\n13.3.1 rnorm(): Generate Random Normal Values\n\n\nCode\n# Generate 10 random values from N(100, 15¬≤)\nset.seed(123)\nrandom_values &lt;- rnorm(n = 10, mean = 100, sd = 15)\n\ncat(\"10 random values from N(100, 15¬≤):\\n\")\n\n\n10 random values from N(100, 15¬≤):\n\n\nCode\nprint(round(random_values, 1))\n\n\n [1]  91.6  96.5 123.4 101.1 101.9 125.7 106.9  81.0  89.7  93.3\n\n\nCode\n# Generate 1000 values and visualize\nlarge_sample &lt;- rnorm(n = 1000, mean = 100, sd = 15)\n\nggplot(tibble(x = large_sample), aes(x = x)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = 100, linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  labs(\n    title = \"1000 Random Values from N(100, 15¬≤)\",\n    subtitle = \"Histogram + density overlay | Green line = mean\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n13.3.2 pnorm(): Calculate Probabilities (Area Under Curve)\nQuestion: What proportion of values in N(100, 15) are less than 110?\n\n\nCode\n# Calculate probability\nprob &lt;- pnorm(q = 110, mean = 100, sd = 15)\n\ncat(sprintf(\"P(X ‚â§ 110) = %.4f (%.2f%%)\\n\", prob, prob * 100))\n\n\nP(X ‚â§ 110) = 0.7475 (74.75%)\n\n\nCode\n# Visualize\nx &lt;- seq(50, 150, length.out = 500)\ny &lt;- dnorm(x, mean = 100, sd = 15)\n\ndf_viz &lt;- tibble(x = x, y = y)\n\nggplot(df_viz, aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_area(data = df_viz %&gt;% filter(x &lt;= 110),\n            aes(x = x, y = y), fill = \"red\", alpha = 0.4) +\n  geom_vline(xintercept = 110, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  annotate(\"text\", x = 110, y = max(y) * 0.8,\n           label = sprintf(\"P(X ‚â§ 110) = %.2f%%\", prob * 100),\n           hjust = -0.1, size = 5, color = \"red\") +\n  labs(\n    title = \"Using pnorm(): Calculate Area to the Left\",\n    subtitle = \"N(100, 15) distribution\",\n    x = \"Value\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n13.3.3 More pnorm() Examples\n\n\nCode\n# Example 1: P(X &gt; 115)\nprob_above &lt;- 1 - pnorm(115, mean = 100, sd = 15)\n# OR use lower.tail = FALSE\nprob_above2 &lt;- pnorm(115, mean = 100, sd = 15, lower.tail = FALSE)\n\ncat(\"Example 1: What proportion of values are ABOVE 115?\\n\")\n\n\nExample 1: What proportion of values are ABOVE 115?\n\n\nCode\ncat(sprintf(\"  P(X &gt; 115) = %.4f (%.2f%%)\\n\\n\", prob_above, prob_above * 100))\n\n\n  P(X &gt; 115) = 0.1587 (15.87%)\n\n\nCode\n# Example 2: P(90 &lt; X &lt; 110)\nprob_between &lt;- pnorm(110, mean = 100, sd = 15) - pnorm(90, mean = 100, sd = 15)\n\ncat(\"Example 2: What proportion fall BETWEEN 90 and 110?\\n\")\n\n\nExample 2: What proportion fall BETWEEN 90 and 110?\n\n\nCode\ncat(sprintf(\"  P(90 &lt; X &lt; 110) = %.4f (%.2f%%)\\n\\n\", prob_between, prob_between * 100))\n\n\n  P(90 &lt; X &lt; 110) = 0.4950 (49.50%)\n\n\nCode\n# Example 3: Application to pig weights\n# Pig market weights: N(120, 12¬≤) kg\n# Pigs below 100 kg get a price penalty\n\nprob_penalty &lt;- pnorm(100, mean = 120, sd = 12)\n\ncat(\"Example 3: Pig market weights N(120, 12¬≤)\\n\")\n\n\nExample 3: Pig market weights N(120, 12¬≤)\n\n\nCode\ncat(sprintf(\"  Proportion below 100 kg (penalty): %.4f (%.2f%%)\\n\",\n            prob_penalty, prob_penalty * 100))\n\n\n  Proportion below 100 kg (penalty): 0.0478 (4.78%)\n\n\n\n\n13.3.4 qnorm(): Find Values from Probabilities\nInverse question: What weight cuts off the bottom 10% of pigs?\n\n\nCode\n# Find the 10th percentile (bottom 10%)\ncutoff_10 &lt;- qnorm(p = 0.10, mean = 120, sd = 12)\n\ncat(\"Question: What weight cuts off the bottom 10% of pigs?\\n\")\n\n\nQuestion: What weight cuts off the bottom 10% of pigs?\n\n\nCode\ncat(sprintf(\"Answer: %.2f kg\\n\\n\", cutoff_10))\n\n\nAnswer: 104.62 kg\n\n\nCode\ncat(\"Interpretation: 10% of pigs weigh less than this value\\n\")\n\n\nInterpretation: 10% of pigs weigh less than this value\n\n\nCode\n# Other useful quantiles\nq25 &lt;- qnorm(0.25, mean = 120, sd = 12)  # Q1\nq50 &lt;- qnorm(0.50, mean = 120, sd = 12)  # Median\nq75 &lt;- qnorm(0.75, mean = 120, sd = 12)  # Q3\n\ncat(\"\\nQuartiles of pig weights N(120, 12¬≤):\\n\")\n\n\n\nQuartiles of pig weights N(120, 12¬≤):\n\n\nCode\ncat(sprintf(\"  Q1 (25th percentile): %.2f kg\\n\", q25))\n\n\n  Q1 (25th percentile): 111.91 kg\n\n\nCode\ncat(sprintf(\"  Q2 (50th percentile/median): %.2f kg\\n\", q50))\n\n\n  Q2 (50th percentile/median): 120.00 kg\n\n\nCode\ncat(sprintf(\"  Q3 (75th percentile): %.2f kg\\n\", q75))\n\n\n  Q3 (75th percentile): 128.09 kg",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#the-z-score-formula",
    "href": "chapters/ch11-probability_distributions.html#the-z-score-formula",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "14.1 The Z-Score Formula",
    "text": "14.1 The Z-Score Formula\nFor a value \\(x\\) from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\):\n\\[\nz = \\frac{x - \\mu}{\\sigma}\n\\]\nInterpretation:\n\n\\(z = 0\\): The value equals the mean\n\\(z = 1\\): The value is 1 SD above the mean\n\\(z = -2\\): The value is 2 SD below the mean\n\\(z &gt; 3\\) or \\(z &lt; -3\\): Unusual/outlier (beyond 99.7% of values)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#example-birth-weights-in-beef-cattle",
    "href": "chapters/ch11-probability_distributions.html#example-birth-weights-in-beef-cattle",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "14.2 Example: Birth Weights in Beef Cattle",
    "text": "14.2 Example: Birth Weights in Beef Cattle\n\n\nCode\n# Population parameters\nmu_birth &lt;- 40  # kg\nsigma_birth &lt;- 5\n\n# Individual calf weights\ncalf_weights &lt;- c(35, 40, 42, 48, 52, 30)\n\n# Calculate z-scores\nz_scores &lt;- (calf_weights - mu_birth) / sigma_birth\n\n# Create summary table\ntibble(\n  `Calf ID` = 1:6,\n  `Weight (kg)` = calf_weights,\n  `Z-score` = round(z_scores, 2),\n  Interpretation = case_when(\n    z_scores &lt; -2 ~ \"Unusually light (&lt; -2 SD)\",\n    z_scores &lt; -1 ~ \"Below average\",\n    z_scores &lt; 1 ~ \"Near average\",\n    z_scores &lt; 2 ~ \"Above average\",\n    TRUE ~ \"Unusually heavy (&gt; 2 SD)\"\n  )\n) %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Birth Weights and Z-scores\",\n    subtitle = sprintf(\"Population: Œº = %.0f kg, œÉ = %.0f kg\", mu_birth, sigma_birth)\n  ) %&gt;%\n  data_color(\n    columns = `Z-score`,\n    colors = scales::col_numeric(\n      palette = c(\"red\", \"yellow\", \"lightgreen\", \"yellow\", \"red\"),\n      domain = c(-3, 3)\n    )\n  )\n\n\n\n\n\n\n\n\nBirth Weights and Z-scores\n\n\nPopulation: Œº = 40 kg, œÉ = 5 kg\n\n\nCalf ID\nWeight (kg)\nZ-score\nInterpretation\n\n\n\n\n1\n35\n-1.0\nNear average\n\n\n2\n40\n0.0\nNear average\n\n\n3\n42\n0.4\nNear average\n\n\n4\n48\n1.6\nAbove average\n\n\n5\n52\n2.4\nUnusually heavy (&gt; 2 SD)\n\n\n6\n30\n-2.0\nBelow average",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#sec-standard-normal",
    "href": "chapters/ch11-probability_distributions.html#sec-standard-normal",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "14.3 The Standard Normal Distribution",
    "text": "14.3 The Standard Normal Distribution\nWhen we calculate z-scores, we‚Äôre standardizing the distribution to have:\n\nMean = 0\nStandard deviation = 1\n\nThis is called the standard normal distribution, denoted \\(N(0, 1)\\) or \\(Z \\sim N(0, 1)\\).\n\n\nCode\n# Compare original and standardized distributions\nx_original &lt;- seq(25, 55, length.out = 500)\ny_original &lt;- dnorm(x_original, mean = 40, sd = 5)\n\nz_values &lt;- seq(-3, 3, length.out = 500)\ny_standard &lt;- dnorm(z_values, mean = 0, sd = 1)\n\np_orig &lt;- ggplot(tibble(x = x_original, y = y_original), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_vline(xintercept = 40, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Original: N(40, 5¬≤)\",\n    subtitle = \"Birth weights in kg\",\n    x = \"Weight (kg)\",\n    y = \"Density\"\n  )\n\np_std &lt;- ggplot(tibble(x = z_values, y = y_standard), aes(x = x, y = y)) +\n  geom_line(linewidth = 1.2, color = \"darkgreen\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"Standardized: N(0, 1)\",\n    subtitle = \"Z-scores (standard normal)\",\n    x = \"Z-score\",\n    y = \"Density\"\n  )\n\np_orig + p_std\n\n\n\n\n\n\n\n\n\n\n14.3.1 Why Standardize?\nStandardization allows us to:\n\nCompare values from different distributions (e.g., compare a pig‚Äôs weight to a cow‚Äôs weight)\nUse standard normal tables (historically important, less so now with computers)\nIdentify outliers consistently (|z| &gt; 2 or 3)\nCalculate probabilities easily\n\n\n\nCode\n# Compare animals from different species\npig_weight &lt;- 110  # kg\npig_mean &lt;- 120\npig_sd &lt;- 12\n\ncow_weight &lt;- 550  # kg\ncow_mean &lt;- 600\ncow_sd &lt;- 50\n\n# Calculate z-scores\nz_pig &lt;- (pig_weight - pig_mean) / pig_sd\nz_cow &lt;- (cow_weight - cow_mean) / cow_sd\n\ncat(\"Question: Which animal is further below average for its species?\\n\\n\")\n\n\nQuestion: Which animal is further below average for its species?\n\n\nCode\ncat(\"Pig:\\n\")\n\n\nPig:\n\n\nCode\ncat(sprintf(\"  Weight: %.0f kg (population mean: %.0f kg, SD: %.0f kg)\\n\",\n            pig_weight, pig_mean, pig_sd))\n\n\n  Weight: 110 kg (population mean: 120 kg, SD: 12 kg)\n\n\nCode\ncat(sprintf(\"  Z-score: %.2f\\n\\n\", z_pig))\n\n\n  Z-score: -0.83\n\n\nCode\ncat(\"Cow:\\n\")\n\n\nCow:\n\n\nCode\ncat(sprintf(\"  Weight: %.0f kg (population mean: %.0f kg, SD: %.0f kg)\\n\",\n            cow_weight, cow_mean, cow_sd))\n\n\n  Weight: 550 kg (population mean: 600 kg, SD: 50 kg)\n\n\nCode\ncat(sprintf(\"  Z-score: %.2f\\n\\n\", z_cow))\n\n\n  Z-score: -1.00\n\n\nCode\nif (abs(z_pig) &gt; abs(z_cow)) {\n  cat(\"Answer: The pig is further below average (more unusual) for its species.\\n\")\n} else {\n  cat(\"Answer: The cow is further below average (more unusual) for its species.\\n\")\n}\n\n\nAnswer: The cow is further below average (more unusual) for its species.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#statement-of-the-central-limit-theorem",
    "href": "chapters/ch11-probability_distributions.html#statement-of-the-central-limit-theorem",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "15.1 Statement of the Central Limit Theorem",
    "text": "15.1 Statement of the Central Limit Theorem\n\n\n\n\n\n\nImportantCentral Limit Theorem\n\n\n\nFor a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), if we:\n\nTake random samples of size \\(n\\) from the population\nCalculate the sample mean \\(\\bar{x}\\) for each sample\nRepeat this process many times\n\nThen the distribution of sample means (\\(\\bar{x}\\)) will be approximately normal with:\n\nMean: \\(\\mu_{\\bar{x}} = \\mu\\) (same as population mean)\nStandard deviation: \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) (called the standard error)\n\nThe amazing part: This is true regardless of the shape of the original population distribution, as long as \\(n\\) is ‚Äúlarge enough‚Äù (typically \\(n \\geq 30\\)).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#sec-clt-simulation",
    "href": "chapters/ch11-probability_distributions.html#sec-clt-simulation",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "15.2 Demonstrating the CLT with Simulation",
    "text": "15.2 Demonstrating the CLT with Simulation\nLet‚Äôs prove the CLT works using simulation. We‚Äôll start with a decidedly non-normal population (uniform distribution) and show that sample means become normal.\n\n15.2.1 Population: Uniform Distribution\n\n\nCode\n# Population: uniformly distributed pig birth weights between 20 and 60 kg\n# This is NOT normal (flat distribution)\nset.seed(42)\n\npopulation_size &lt;- 100000\npopulation &lt;- runif(population_size, min = 20, max = 60)\n\npop_mean &lt;- mean(population)\npop_sd &lt;- sd(population)\n\nggplot(tibble(weight = population), aes(x = weight)) +\n  geom_histogram(bins = 50, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = pop_mean, linetype = \"dashed\",\n             color = \"red\", linewidth = 1.2) +\n  annotate(\"text\", x = pop_mean + 2, y = 4000,\n           label = sprintf(\"Œº = %.1f\\nœÉ = %.1f\", pop_mean, pop_sd),\n           hjust = 0, size = 5, color = \"red\") +\n  labs(\n    title = \"Population Distribution: Uniform (NOT Normal)\",\n    subtitle = \"Birth weights uniformly distributed between 20 and 60 kg\",\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n15.2.2 Sampling Distribution for Different Sample Sizes\nNow let‚Äôs draw many samples of different sizes and plot the distribution of sample means:\n\n\nCode\n# Function to simulate sampling distribution\nsimulate_sampling_dist &lt;- function(pop, n_samples, sample_size) {\n  replicate(n_samples, mean(sample(pop, size = sample_size, replace = TRUE)))\n}\n\n# Simulate for different sample sizes\nn_samples &lt;- 1000\n\nsample_means_n5 &lt;- simulate_sampling_dist(population, n_samples, 5)\nsample_means_n10 &lt;- simulate_sampling_dist(population, n_samples, 10)\nsample_means_n30 &lt;- simulate_sampling_dist(population, n_samples, 30)\nsample_means_n100 &lt;- simulate_sampling_dist(population, n_samples, 100)\n\n# Expected standard error\nse_n5 &lt;- pop_sd / sqrt(5)\nse_n10 &lt;- pop_sd / sqrt(10)\nse_n30 &lt;- pop_sd / sqrt(30)\nse_n100 &lt;- pop_sd / sqrt(100)\n\n# Create plots\ndf_n5 &lt;- tibble(mean = sample_means_n5, n = \"n = 5\")\ndf_n10 &lt;- tibble(mean = sample_means_n10, n = \"n = 10\")\ndf_n30 &lt;- tibble(mean = sample_means_n30, n = \"n = 30\")\ndf_n100 &lt;- tibble(mean = sample_means_n100, n = \"n = 100\")\n\ndf_all &lt;- bind_rows(df_n5, df_n10, df_n30, df_n100) %&gt;%\n  mutate(n = factor(n, levels = c(\"n = 5\", \"n = 10\", \"n = 30\", \"n = 100\")))\n\nggplot(df_all, aes(x = mean)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 30,\n                 fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = pop_mean, linetype = \"dashed\",\n             color = \"blue\", linewidth = 1) +\n  facet_wrap(~n, ncol = 2, scales = \"free_y\") +\n  labs(\n    title = \"Central Limit Theorem in Action\",\n    subtitle = \"Distribution of sample means for different sample sizes (1000 samples each)\",\n    x = \"Sample Mean Weight (kg)\",\n    y = \"Density\"\n  ) +\n  theme(strip.text = element_text(size = 12, face = \"bold\"))\n\n\n\n\n\n\n\n\n\n\n\n15.2.3 Summary Statistics of Sampling Distributions\n\n\nCode\n# Calculate statistics for each sampling distribution\nsummary_clt &lt;- tibble(\n  `Sample Size (n)` = c(5, 10, 30, 100),\n  `Mean of Sample Means` = c(mean(sample_means_n5), mean(sample_means_n10),\n                             mean(sample_means_n30), mean(sample_means_n100)),\n  `SD of Sample Means (observed)` = c(sd(sample_means_n5), sd(sample_means_n10),\n                                      sd(sample_means_n30), sd(sample_means_n100)),\n  `Standard Error (theoretical)` = c(se_n5, se_n10, se_n30, se_n100)\n)\n\nsummary_clt %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Central Limit Theorem: Observed vs Theoretical\",\n    subtitle = sprintf(\"Population: Œº = %.2f, œÉ = %.2f\", pop_mean, pop_sd)\n  ) %&gt;%\n  fmt_number(columns = -`Sample Size (n)`, decimals = 3) %&gt;%\n  tab_source_note(\"Standard Error (SE) = œÉ / ‚àön\") %&gt;%\n  tab_source_note(\"Notice: Observed SD ‚âà Theoretical SE\") %&gt;%\n  tab_style(\n    style = cell_fill(color = \"lightyellow\"),\n    locations = cells_body(columns = `Standard Error (theoretical)`)\n  )\n\n\n\n\n\n\n\n\nCentral Limit Theorem: Observed vs Theoretical\n\n\nPopulation: Œº = 40.03, œÉ = 11.58\n\n\nSample Size (n)\nMean of Sample Means\nSD of Sample Means (observed)\nStandard Error (theoretical)\n\n\n\n\n5\n40.027\n5.249\n5.179\n\n\n10\n40.008\n3.610\n3.662\n\n\n30\n39.986\n2.106\n2.114\n\n\n100\n40.082\n1.143\n1.158\n\n\n\nStandard Error (SE) = œÉ / ‚àön\n\n\nNotice: Observed SD ‚âà Theoretical SE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipKey Observations from the CLT\n\n\n\n\nMean stays the same: The mean of sample means equals the population mean (40 kg)\nSpread decreases: As \\(n\\) increases, the spread of sample means decreases (SE = \\(\\sigma/\\sqrt{n}\\))\nShape becomes normal: Even though the population was uniform, sample means become approximately normal, especially for \\(n \\geq 30\\)\nObserved ‚âà Theoretical: The observed SD of sample means matches the theoretical standard error",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#why-the-clt-matters",
    "href": "chapters/ch11-probability_distributions.html#why-the-clt-matters",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "15.3 Why the CLT Matters",
    "text": "15.3 Why the CLT Matters\nThe Central Limit Theorem is profound because it:\n\nJustifies using normal distributions for inference, even when data aren‚Äôt perfectly normal\nExplains why sample means are reliable: Larger samples ‚Üí smaller standard error ‚Üí more precise estimates\nUnderpins t-tests, ANOVA, regression: All assume sampling distributions are approximately normal\nGuides sample size decisions: Tells us how much precision improves with larger \\(n\\)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#three-types-of-distributions-dont-confuse-them",
    "href": "chapters/ch11-probability_distributions.html#three-types-of-distributions-dont-confuse-them",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "16.1 Three Types of Distributions: Don‚Äôt Confuse Them!",
    "text": "16.1 Three Types of Distributions: Don‚Äôt Confuse Them!\n\n\n\n\n\n\nWarningThree Different Distributions\n\n\n\n\nPopulation distribution: Distribution of all values in the entire population\nSample distribution: Distribution of values in one sample\nSampling distribution: Distribution of a statistic (e.g., mean) calculated from many repeated samples\n\nMost common confusion: Mixing up the sample distribution with the sampling distribution!\n\n\n\n16.1.1 Visual Comparison\n\n\nCode\n# Population\nset.seed(999)\npopulation &lt;- rnorm(100000, mean = 550, sd = 60)\n\n# One sample\none_sample &lt;- sample(population, size = 40)\n\n# Many sample means\nn_samples &lt;- 1000\nsample_means &lt;- replicate(n_samples, mean(sample(population, size = 40)))\n\n# Create plots\np_pop &lt;- ggplot(tibble(x = sample(population, 10000)), aes(x = x)) +\n  geom_histogram(bins = 50, fill = \"purple\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(population), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"1. Population Distribution\",\n    subtitle = sprintf(\"All animals: Œº = 550, œÉ = 60\\n(showing 10,000 for visualization)\"),\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\np_sample &lt;- ggplot(tibble(x = one_sample), aes(x = x)) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(one_sample), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"2. One Sample Distribution\",\n    subtitle = sprintf(\"One sample: n = 40, xÃÑ = %.1f, s = %.1f\",\n                      mean(one_sample), sd(one_sample)),\n    x = \"Weight (kg)\",\n    y = \"Count\"\n  )\n\np_sampling &lt;- ggplot(tibble(x = sample_means), aes(x = x)) +\n  geom_histogram(bins = 30, fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_means), color = \"red\",\n             linetype = \"dashed\", linewidth = 1.2) +\n  labs(\n    title = \"3. Sampling Distribution of the Mean\",\n    subtitle = sprintf(\"1000 samples of n=40 each:\\nMean of xÃÑ's = %.1f, SD(xÃÑ) = %.2f\",\n                      mean(sample_means), sd(sample_means)),\n    x = \"Sample Mean Weight (kg)\",\n    y = \"Count\"\n  )\n\np_pop / p_sample / p_sampling +\n  plot_annotation(\n    title = \"Three Different Distributions: Know the Difference!\",\n    theme = theme(plot.title = element_text(size = 16, face = \"bold\"))\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#definitions",
    "href": "chapters/ch11-probability_distributions.html#definitions",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "17.1 Definitions",
    "text": "17.1 Definitions\n\n\n\n\n\n\nImportantStandard Deviation vs Standard Error\n\n\n\nStandard Deviation (SD or \\(s\\)):\n\nMeasures variability in the data itself\nDescribes the spread of individual observations\nFormula: \\(s = \\sqrt{\\frac{\\sum(x_i - \\bar{x})^2}{n-1}}\\)\nDoes not decrease with larger sample size (it estimates the population SD)\n\nStandard Error (SE):\n\nMeasures variability in the sample mean (or other statistic)\nDescribes how much the sample mean varies from sample to sample\nFormula: \\(SE = \\frac{s}{\\sqrt{n}}\\) (for the mean)\nDecreases with larger sample size: More data ‚Üí more precise estimate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#visualizing-the-difference",
    "href": "chapters/ch11-probability_distributions.html#visualizing-the-difference",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "17.2 Visualizing the Difference",
    "text": "17.2 Visualizing the Difference\n\n\nCode\n# Simulate samples of different sizes\nset.seed(123)\npop_mean &lt;- 600\npop_sd &lt;- 50\n\n# Small sample\nn_small &lt;- 10\nsample_small &lt;- rnorm(n_small, mean = pop_mean, sd = pop_sd)\nse_small &lt;- sd(sample_small) / sqrt(n_small)\n\n# Large sample\nn_large &lt;- 100\nsample_large &lt;- rnorm(n_large, mean = pop_mean, sd = pop_sd)\nse_large &lt;- sd(sample_large) / sqrt(n_large)\n\n# Print results\ncat(\"Small Sample (n = 10):\\n\")\n\n\nSmall Sample (n = 10):\n\n\nCode\ncat(sprintf(\"  Mean: %.2f kg\\n\", mean(sample_small)))\n\n\n  Mean: 603.73 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg (measures spread of individual weights)\\n\", sd(sample_small)))\n\n\n  SD: 47.69 kg (measures spread of individual weights)\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg (measures uncertainty in the mean)\\n\\n\", se_small))\n\n\n  SE: 15.08 kg (measures uncertainty in the mean)\n\n\nCode\ncat(\"Large Sample (n = 100):\\n\")\n\n\nLarge Sample (n = 100):\n\n\nCode\ncat(sprintf(\"  Mean: %.2f kg\\n\", mean(sample_large)))\n\n\n  Mean: 602.17 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg (similar to small sample - estimates population SD)\\n\", sd(sample_large)))\n\n\n  SD: 45.21 kg (similar to small sample - estimates population SD)\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg (much smaller - mean is more precise!)\\n\", se_large))\n\n\n  SE: 4.52 kg (much smaller - mean is more precise!)\n\n\nCode\n# Visualize\np1 &lt;- ggplot(tibble(x = sample_small), aes(x = x)) +\n  geom_histogram(bins = 8, fill = \"steelblue\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_small), color = \"red\", linewidth = 1.2) +\n  geom_segment(aes(x = mean(sample_small) - sd(sample_small),\n                   xend = mean(sample_small) + sd(sample_small),\n                   y = 0, yend = 0),\n               color = \"purple\", linewidth = 2, arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = mean(sample_small), y = 1.5,\n           label = sprintf(\"SD = %.1f\", sd(sample_small)),\n           color = \"purple\", fontface = \"bold\") +\n  labs(title = \"Small Sample (n=10)\",\n       subtitle = sprintf(\"Mean = %.1f, SE = %.2f\", mean(sample_small), se_small),\n       x = \"Weight (kg)\", y = \"Count\")\n\np2 &lt;- ggplot(tibble(x = sample_large), aes(x = x)) +\n  geom_histogram(bins = 20, fill = \"darkgreen\", alpha = 0.6, color = \"white\") +\n  geom_vline(xintercept = mean(sample_large), color = \"red\", linewidth = 1.2) +\n  geom_segment(aes(x = mean(sample_large) - sd(sample_large),\n                   xend = mean(sample_large) + sd(sample_large),\n                   y = 0, yend = 0),\n               color = \"purple\", linewidth = 2, arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = mean(sample_large), y = 8,\n           label = sprintf(\"SD = %.1f\", sd(sample_large)),\n           color = \"purple\", fontface = \"bold\") +\n  labs(title = \"Large Sample (n=100)\",\n       subtitle = sprintf(\"Mean = %.1f, SE = %.2f\", mean(sample_large), se_large),\n       x = \"Weight (kg)\", y = \"Count\")\n\np1 + p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#how-se-decreases-with-sample-size",
    "href": "chapters/ch11-probability_distributions.html#how-se-decreases-with-sample-size",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "17.3 How SE Decreases with Sample Size",
    "text": "17.3 How SE Decreases with Sample Size\n\n\nCode\n# Show how SE decreases as n increases\npop_sd &lt;- 50\nsample_sizes &lt;- c(5, 10, 20, 30, 50, 100, 200, 500)\nstandard_errors &lt;- pop_sd / sqrt(sample_sizes)\n\ndf_se &lt;- tibble(\n  n = sample_sizes,\n  SE = standard_errors\n)\n\nggplot(df_se, aes(x = n, y = SE)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_point(size = 4, color = \"red\") +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray50\") +\n  annotate(\"text\", x = 250, y = pop_sd / sqrt(30),\n           label = \"SE = œÉ / ‚àön\",\n           size = 6, color = \"darkblue\", fontface = \"bold\") +\n  labs(\n    title = \"Standard Error Decreases with Sample Size\",\n    subtitle = \"Population SD = 50 kg\",\n    x = \"Sample Size (n)\",\n    y = \"Standard Error (kg)\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n\n\nCode\n# Show specific values\ndf_se %&gt;%\n  gt() %&gt;%\n  tab_header(\n    title = \"Standard Error by Sample Size\",\n    subtitle = \"Population œÉ = 50 kg\"\n  ) %&gt;%\n  fmt_number(columns = SE, decimals = 2) %&gt;%\n  cols_label(n = \"Sample Size\", SE = \"Standard Error (kg)\")\n\n\n\n\n\n\n\n\nStandard Error by Sample Size\n\n\nPopulation œÉ = 50 kg\n\n\nSample Size\nStandard Error (kg)\n\n\n\n\n5\n22.36\n\n\n10\n15.81\n\n\n20\n11.18\n\n\n30\n9.13\n\n\n50\n7.07\n\n\n100\n5.00\n\n\n200\n3.54\n\n\n500\n2.24\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipWhen to Report SD vs SE\n\n\n\nReport SD when:\n\nDescribing the variability in your sample or population\nExample: ‚ÄúPig weights ranged from 100 to 140 kg, with mean 120 kg (SD = 12 kg)‚Äù\n\nReport SE when:\n\nDescribing uncertainty in an estimate (usually the mean)\nExample: ‚ÄúMean pig weight was 120 kg (SE = 1.7 kg)‚Äù\nOr better yet, report a confidence interval (next section!)\n\nBest practice: Report both! E.g., ‚ÄúMean = 120 kg (SD = 12, SE = 1.7, n = 50)‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#concept-and-interpretation",
    "href": "chapters/ch11-probability_distributions.html#concept-and-interpretation",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "18.1 Concept and Interpretation",
    "text": "18.1 Concept and Interpretation\nA 95% confidence interval for the population mean is calculated as:\n\\[\n\\bar{x} \\pm 1.96 \\times SE\n\\]\nWhere:\n\n\\(\\bar{x}\\) = sample mean\n\\(SE = s / \\sqrt{n}\\) = standard error of the mean\n1.96 = critical value from the standard normal distribution (for 95% confidence)\n\n\n18.1.1 What Does ‚Äú95% Confidence‚Äù Mean?\n\n\n\n\n\n\nImportantCorrect Interpretation\n\n\n\n‚ÄúIf we repeated this study many times and calculated a 95% CI each time, about 95% of those intervals would contain the true population mean.‚Äù\nNOT: ‚ÄúThere‚Äôs a 95% probability that the true mean is in this specific interval.‚Äù\nThe true mean either is or isn‚Äôt in the interval‚Äîwe just don‚Äôt know which. The 95% refers to the procedure, not a specific interval.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#calculating-a-confidence-interval",
    "href": "chapters/ch11-probability_distributions.html#calculating-a-confidence-interval",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "18.2 Calculating a Confidence Interval",
    "text": "18.2 Calculating a Confidence Interval\n\n18.2.1 Example: Beef Cattle Finishing Weights\n\n\nCode\n# Sample data: finishing weights of 35 beef cattle\nset.seed(456)\nn &lt;- 35\nweights &lt;- rnorm(n, mean = 580, sd = 60)\n\n# Calculate statistics\nxbar &lt;- mean(weights)\ns &lt;- sd(weights)\nse &lt;- s / sqrt(n)\n\n# 95% CI using z = 1.96 (we'll use t-distribution properly in Week 4)\nci_lower &lt;- xbar - 1.96 * se\nci_upper &lt;- xbar + 1.96 * se\n\ncat(\"Sample Statistics:\\n\")\n\n\nSample Statistics:\n\n\nCode\ncat(sprintf(\"  n = %d cattle\\n\", n))\n\n\n  n = 35 cattle\n\n\nCode\ncat(sprintf(\"  Mean weight: %.2f kg\\n\", xbar))\n\n\n  Mean weight: 587.37 kg\n\n\nCode\ncat(sprintf(\"  SD: %.2f kg\\n\", s))\n\n\n  SD: 68.72 kg\n\n\nCode\ncat(sprintf(\"  SE: %.2f kg\\n\\n\", se))\n\n\n  SE: 11.62 kg\n\n\nCode\ncat(\"95% Confidence Interval:\\n\")\n\n\n95% Confidence Interval:\n\n\nCode\ncat(sprintf(\"  [%.2f, %.2f] kg\\n\\n\", ci_lower, ci_upper))\n\n\n  [564.60, 610.13] kg\n\n\nCode\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"We are 95% confident that the true mean finishing weight\\n\")\n\n\nWe are 95% confident that the true mean finishing weight\n\n\nCode\ncat(sprintf(\"of the population is between %.1f and %.1f kg.\\n\", ci_lower, ci_upper))\n\n\nof the population is between 564.6 and 610.1 kg.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#visualizing-confidence-intervals",
    "href": "chapters/ch11-probability_distributions.html#visualizing-confidence-intervals",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "18.3 Visualizing Confidence Intervals",
    "text": "18.3 Visualizing Confidence Intervals\n\n\nCode\n# Create visualization\nci_data &lt;- tibble(\n  estimate = xbar,\n  ci_lower = ci_lower,\n  ci_upper = ci_upper\n)\n\nggplot(ci_data, aes(x = 1, y = estimate)) +\n  geom_point(size = 5, color = \"red\") +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, linewidth = 1.2, color = \"blue\") +\n  geom_hline(yintercept = 580, linetype = \"dashed\", color = \"darkgreen\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = xbar,\n           label = sprintf(\"xÃÑ = %.1f kg\", xbar),\n           color = \"red\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 1.3, y = ci_lower,\n           label = sprintf(\"Lower: %.1f\", ci_lower),\n           color = \"blue\", size = 4) +\n  annotate(\"text\", x = 1.3, y = ci_upper,\n           label = sprintf(\"Upper: %.1f\", ci_upper),\n           color = \"blue\", size = 4) +\n  annotate(\"text\", x = 1.3, y = 580,\n           label = \"True mean = 580\",\n           color = \"darkgreen\", size = 4, hjust = 0) +\n  labs(\n    title = \"95% Confidence Interval for Mean Weight\",\n    subtitle = \"Blue bars show range of plausible values for population mean\",\n    y = \"Weight (kg)\",\n    x = \"\"\n  ) +\n  coord_cartesian(ylim = c(550, 610)) +\n  theme(axis.text.x = element_blank(),\n        axis.ticks.x = element_blank())",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#what-does-95-mean-a-simulation",
    "href": "chapters/ch11-probability_distributions.html#what-does-95-mean-a-simulation",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "18.4 What Does ‚Äú95%‚Äù Mean? A Simulation",
    "text": "18.4 What Does ‚Äú95%‚Äù Mean? A Simulation\nLet‚Äôs demonstrate what ‚Äú95% confidence‚Äù actually means by simulating 100 studies:\n\n\nCode\n# Simulate 100 studies\nset.seed(789)\nn_studies &lt;- 100\nn_per_study &lt;- 35\ntrue_mean &lt;- 580\ntrue_sd &lt;- 60\n\n# Function to calculate CI for one study\ncalc_ci &lt;- function(study_id) {\n  sample_data &lt;- rnorm(n_per_study, mean = true_mean, sd = true_sd)\n  xbar &lt;- mean(sample_data)\n  se &lt;- sd(sample_data) / sqrt(n_per_study)\n\n  tibble(\n    study = study_id,\n    estimate = xbar,\n    ci_lower = xbar - 1.96 * se,\n    ci_upper = xbar + 1.96 * se,\n    captures_true = ci_lower &lt;= true_mean & ci_upper &gt;= true_mean\n  )\n}\n\n# Run simulation\nci_results &lt;- map_df(1:n_studies, calc_ci)\n\n# Count how many capture the true mean\nn_captured &lt;- sum(ci_results$captures_true)\ncapture_rate &lt;- mean(ci_results$captures_true)\n\ncat(sprintf(\"Out of %d studies:\\n\", n_studies))\n\n\nOut of 100 studies:\n\n\nCode\ncat(sprintf(\"  %d CIs (%.1f%%) captured the true mean\\n\", n_captured, capture_rate * 100))\n\n\n  92 CIs (92.0%) captured the true mean\n\n\nCode\ncat(sprintf(\"  %d CIs (%.1f%%) did NOT capture the true mean\\n\",\n            n_studies - n_captured, (1 - capture_rate) * 100))\n\n\n  8 CIs (8.0%) did NOT capture the true mean\n\n\nCode\ncat(\"\\nThis is very close to the expected 95%!\\n\")\n\n\n\nThis is very close to the expected 95%!\n\n\nCode\n# Visualize (show first 50 for clarity)\nggplot(ci_results %&gt;% slice(1:50),\n       aes(x = study, y = estimate, color = captures_true)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0) +\n  geom_hline(yintercept = true_mean, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1.2) +\n  scale_color_manual(values = c(\"TRUE\" = \"blue\", \"FALSE\" = \"red\"),\n                     labels = c(\"TRUE\" = \"Captures true mean\", \"FALSE\" = \"Misses true mean\")) +\n  annotate(\"text\", x = 45, y = true_mean + 15,\n           label = sprintf(\"True mean = %.0f kg\", true_mean),\n           color = \"darkgreen\", fontface = \"bold\", size = 4) +\n  labs(\n    title = \"95% Confidence Intervals from 50 Simulated Studies\",\n    subtitle = sprintf(\"Red intervals (%.0f%%) miss the true mean - this is expected!\",\n                      (1 - capture_rate) * 100),\n    x = \"Study Number\",\n    y = \"Mean Weight (kg)\",\n    color = \"\"\n  ) +\n  theme(legend.position = \"top\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#factors-affecting-ci-width",
    "href": "chapters/ch11-probability_distributions.html#factors-affecting-ci-width",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "18.5 Factors Affecting CI Width",
    "text": "18.5 Factors Affecting CI Width\nThree factors determine how wide a confidence interval is:\n\nSample size (\\(n\\)): Larger \\(n\\) ‚Üí narrower CI (more precision)\nVariability (\\(\\sigma\\)): More variable data ‚Üí wider CI (less precision)\nConfidence level: Higher confidence (e.g., 99% vs 95%) ‚Üí wider CI (trade precision for confidence)\n\n\n\nCode\n# Demonstrate effect of sample size\ntrue_mean &lt;- 120\ntrue_sd &lt;- 15\nsample_sizes &lt;- c(10, 30, 50, 100, 200)\n\nci_by_n &lt;- map_df(sample_sizes, function(n) {\n  set.seed(123)\n  sample_data &lt;- rnorm(n, mean = true_mean, sd = true_sd)\n  xbar &lt;- mean(sample_data)\n  se &lt;- sd(sample_data) / sqrt(n)\n\n  tibble(\n    n = n,\n    estimate = xbar,\n    ci_lower = xbar - 1.96 * se,\n    ci_upper = xbar + 1.96 * se,\n    width = ci_upper - ci_lower\n  )\n})\n\n# Plot\nggplot(ci_by_n, aes(x = factor(n), y = estimate)) +\n  geom_point(size = 4, color = \"red\") +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper),\n                width = 0.2, linewidth = 1.2, color = \"blue\") +\n  geom_hline(yintercept = true_mean, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1) +\n  geom_text(aes(label = sprintf(\"Width: %.1f\", width)),\n            vjust = -1, size = 3.5) +\n  labs(\n    title = \"Effect of Sample Size on Confidence Interval Width\",\n    subtitle = \"Larger samples ‚Üí narrower CIs ‚Üí more precise estimates\",\n    x = \"Sample Size (n)\",\n    y = \"Weight (kg)\"\n  )",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#scenario",
    "href": "chapters/ch11-probability_distributions.html#scenario",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.1 Scenario",
    "text": "19.1 Scenario\nA dairy researcher wants to estimate the average daily milk production of a new Holstein strain. She samples 40 cows and records their daily production.\n\n\nCode\n# Generate realistic data\nset.seed(2025)\nn_cows &lt;- 40\nmilk_production &lt;- rnorm(n_cows, mean = 35, sd = 6)  # liters per day\n\n# Calculate summary statistics\nxbar &lt;- mean(milk_production)\ns &lt;- sd(milk_production)\nse &lt;- s / sqrt(n_cows)\n\n# Show first 10 observations\nhead(milk_production, 10) %&gt;%\n  round(1) %&gt;%\n  matrix(nrow = 2, byrow = TRUE) %&gt;%\n  as_tibble(.name_repair = \"minimal\") %&gt;%\n  setNames(paste(\"Cow\", 1:5)) %&gt;%\n  mutate(Row = c(\"1-5\", \"6-10\"), .before = 1) %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Daily Milk Production (liters)\",\n             subtitle = \"First 10 cows (n = 40 total)\")\n\n\n\n\n\n\n\n\nDaily Milk Production (liters)\n\n\nFirst 10 cows (n = 40 total)\n\n\nRow\nCow 1\nCow 2\nCow 3\nCow 4\nCow 5\n\n\n\n\n1-5\n38.7\n35.2\n39.6\n42.6\n37.2\n\n\n6-10\n34.0\n37.4\n34.5\n32.9\n39.2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#step-1-descriptive-statistics",
    "href": "chapters/ch11-probability_distributions.html#step-1-descriptive-statistics",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.2 Step 1: Descriptive Statistics",
    "text": "19.2 Step 1: Descriptive Statistics\n\n\nCode\n# Comprehensive summary\nsummary_stats &lt;- tibble(\n  Statistic = c(\"Sample size\", \"Mean\", \"Standard Deviation\", \"Standard Error\",\n                \"Minimum\", \"Q1\", \"Median\", \"Q3\", \"Maximum\", \"Range\"),\n  Value = c(n_cows, xbar, s, se, min(milk_production),\n            quantile(milk_production, 0.25), median(milk_production),\n            quantile(milk_production, 0.75), max(milk_production),\n            max(milk_production) - min(milk_production))\n)\n\nsummary_stats %&gt;%\n  gt() %&gt;%\n  tab_header(title = \"Summary Statistics: Daily Milk Production\") %&gt;%\n  fmt_number(columns = Value, rows = 2:10, decimals = 2) %&gt;%\n  fmt_number(columns = Value, rows = 1, decimals = 0) %&gt;%\n  tab_source_note(\"All values in liters per day (except n)\")\n\n\n\n\n\n\n\n\nSummary Statistics: Daily Milk Production\n\n\nStatistic\nValue\n\n\n\n\nSample size\n40\n\n\nMean\n35.77\n\n\nStandard Deviation\n6.13\n\n\nStandard Error\n0.97\n\n\nMinimum\n24.47\n\n\nQ1\n32.59\n\n\nMedian\n35.26\n\n\nQ3\n39.35\n\n\nMaximum\n52.12\n\n\nRange\n27.65\n\n\n\nAll values in liters per day (except n)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#step-2-visualize-the-data",
    "href": "chapters/ch11-probability_distributions.html#step-2-visualize-the-data",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.3 Step 2: Visualize the Data",
    "text": "19.3 Step 2: Visualize the Data\n\n\nCode\np1 &lt;- ggplot(tibble(production = milk_production), aes(x = production)) +\n  geom_histogram(aes(y = after_stat(density)), bins = 12,\n                 fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_density(color = \"red\", linewidth = 1.2) +\n  geom_vline(xintercept = xbar, linetype = \"dashed\",\n             color = \"darkgreen\", linewidth = 1.2) +\n  annotate(\"text\", x = xbar + 1, y = 0.06,\n           label = sprintf(\"Mean = %.1f L\", xbar),\n           color = \"darkgreen\", hjust = 0, size = 4) +\n  labs(title = \"Distribution of Daily Milk Production\",\n       x = \"Liters per Day\", y = \"Density\")\n\np2 &lt;- ggplot(tibble(production = milk_production), aes(x = \"\", y = production)) +\n  geom_boxplot(fill = \"lightblue\", alpha = 0.6, outlier.color = \"red\", outlier.size = 3) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2, color = \"darkblue\") +\n  stat_summary(fun = mean, geom = \"point\", shape = 23,\n               size = 4, fill = \"red\", color = \"black\") +\n  labs(title = \"Boxplot with Individual Points\",\n       y = \"Liters per Day\", x = \"\") +\n  theme(axis.text.x = element_blank())\n\np1 + p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#step-3-check-normality-for-inference",
    "href": "chapters/ch11-probability_distributions.html#step-3-check-normality-for-inference",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.4 Step 3: Check Normality (for inference)",
    "text": "19.4 Step 3: Check Normality (for inference)\n\n\nCode\n# Q-Q plot to assess normality\nqqnorm(milk_production, main = \"Q-Q Plot: Assessing Normality\",\n       pch = 19, col = \"steelblue\", cex = 1.5)\nqqline(milk_production, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\nCode\ncat(\"\\nAssessment: Points fall approximately along the line,\\n\")\n\n\n\nAssessment: Points fall approximately along the line,\n\n\nCode\ncat(\"suggesting the data are reasonably normal.\\n\")\n\n\nsuggesting the data are reasonably normal.\n\n\nCode\ncat(\"This supports using normal-based inference methods.\\n\")\n\n\nThis supports using normal-based inference methods.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#step-4-calculate-confidence-interval",
    "href": "chapters/ch11-probability_distributions.html#step-4-calculate-confidence-interval",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.5 Step 4: Calculate Confidence Interval",
    "text": "19.5 Step 4: Calculate Confidence Interval\n\n\nCode\n# 95% Confidence Interval\nci_lower &lt;- xbar - 1.96 * se\nci_upper &lt;- xbar + 1.96 * se\n\ncat(\"95% Confidence Interval for Mean Daily Production:\\n\")\n\n\n95% Confidence Interval for Mean Daily Production:\n\n\nCode\ncat(sprintf(\"  [%.2f, %.2f] liters per day\\n\\n\", ci_lower, ci_upper))\n\n\n  [33.87, 37.66] liters per day\n\n\nCode\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"We are 95% confident that the true mean daily milk production\\n\")\n\n\nWe are 95% confident that the true mean daily milk production\n\n\nCode\ncat(sprintf(\"for this Holstein strain is between %.1f and %.1f liters.\\n\\n\", ci_lower, ci_upper))\n\n\nfor this Holstein strain is between 33.9 and 37.7 liters.\n\n\nCode\ncat(\"In practical terms:\\n\")\n\n\nIn practical terms:\n\n\nCode\ncat(sprintf(\"  Best estimate (mean): %.1f liters/day\\n\", xbar))\n\n\n  Best estimate (mean): 35.8 liters/day\n\n\nCode\ncat(sprintf(\"  Margin of error: ¬±%.1f liters (1.96 √ó SE)\\n\", 1.96 * se))\n\n\n  Margin of error: ¬±1.9 liters (1.96 √ó SE)\n\n\nCode\ncat(sprintf(\"  Variability among cows (SD): %.1f liters\\n\", s))\n\n\n  Variability among cows (SD): 6.1 liters",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#step-5-standardize-observations-z-scores",
    "href": "chapters/ch11-probability_distributions.html#step-5-standardize-observations-z-scores",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "19.6 Step 5: Standardize Observations (Z-scores)",
    "text": "19.6 Step 5: Standardize Observations (Z-scores)\n\n\nCode\n# Calculate z-scores\nz_scores &lt;- (milk_production - xbar) / s\n\n# Identify unusual observations\nunusual &lt;- abs(z_scores) &gt; 2\n\n# Create table of extreme cows\nextreme_cows &lt;- tibble(\n  Cow_ID = which(unusual),\n  Production = milk_production[unusual],\n  Z_score = z_scores[unusual],\n  Classification = ifelse(z_scores[unusual] &gt; 0, \"High producer\", \"Low producer\")\n) %&gt;%\n  arrange(desc(abs(Z_score)))\n\nif (nrow(extreme_cows) &gt; 0) {\n  cat(\"Cows with unusual production (|z| &gt; 2):\\n\")\n  extreme_cows %&gt;%\n    gt() %&gt;%\n    fmt_number(columns = c(Production, Z_score), decimals = 2) %&gt;%\n    tab_header(title = \"Outlier Analysis\") %&gt;%\n    data_color(\n      columns = Z_score,\n      colors = scales::col_numeric(\n        palette = c(\"blue\", \"white\", \"red\"),\n        domain = c(-3, 3)\n      )\n    )\n} else {\n  cat(\"No cows with unusual production (all |z| &lt; 2)\\n\")\n}\n\n\nCows with unusual production (|z| &gt; 2):\n\n\n\n\n\n\n\n\nOutlier Analysis\n\n\nCow_ID\nProduction\nZ_score\nClassification\n\n\n\n\n23\n52.12\n2.67\nHigh producer\n\n\n20\n49.57\n2.25\nHigh producer",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#looking-ahead",
    "href": "chapters/ch11-probability_distributions.html#looking-ahead",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "20.1 Looking Ahead",
    "text": "20.1 Looking Ahead\nNext week, we‚Äôll use these foundational concepts to perform hypothesis testing:\n\nNull and alternative hypotheses\nType I and Type II errors\nOne-sample and two-sample t-tests\nP-values in the context of sampling distributions\nMaking decisions with confidence\n\nWith your understanding of sampling distributions and confidence intervals, hypothesis testing will make much more sense!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#reflection-questions",
    "href": "chapters/ch11-probability_distributions.html#reflection-questions",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "20.2 Reflection Questions",
    "text": "20.2 Reflection Questions\nBefore next week, consider:\n\nFind a paper in your field. Do the authors report confidence intervals? If so, how do they interpret them?\nThink about a measurement you collect (e.g., animal weights, feed intake, milk yield). What would you estimate the population mean and SD to be? How large a sample would you need for SE &lt; 5% of the mean?\nSimulate your own CLT demonstration: Start with a non-normal distribution (e.g., exponential or uniform) and verify that sample means become normal.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#additional-resources",
    "href": "chapters/ch11-probability_distributions.html#additional-resources",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "20.3 Additional Resources",
    "text": "20.3 Additional Resources\n\n20.3.1 R Packages\n\ndistributional: Modern tools for working with probability distributions\nggdist: Visualizing distributions and uncertainty\ninfer: Tidy framework for statistical inference\n\n\n\n20.3.2 Recommended Reading\n\n‚ÄúOpenIntro Statistics‚Äù (free online) - Chapters 3-4 on probability and distributions\n‚ÄúThe Lady Tasting Tea‚Äù by David Salsburg - History of the CLT and its importance\n‚ÄúSeeing Theory‚Äù: Interactive visualizations of probability and statistics (https://seeing-theory.brown.edu/)\n\n\n\n20.3.3 Videos\n\nStatQuest: ‚ÄúNormal Distribution, Clearly Explained‚Äù and ‚ÄúCentral Limit Theorem‚Äù\n3Blue1Brown: ‚ÄúBut what is the Central Limit Theorem?‚Äù (YouTube)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#practice-problems",
    "href": "chapters/ch11-probability_distributions.html#practice-problems",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "20.4 Practice Problems",
    "text": "20.4 Practice Problems\nTry these on your own:\n\nCattle weights are N(650, 80¬≤) kg. What proportion weigh:\n\nLess than 600 kg?\nBetween 640 and 700 kg?\nMore than 750 kg?\n\nSample means: If you take samples of n=25 from the population in #1, what are the mean and SE of the sampling distribution?\nConfidence intervals: A sample of 50 pigs has mean weight 110 kg and SD 14 kg. Calculate and interpret a 95% CI for the population mean.\nZ-scores: A pig weighs 125 kg. The population mean is 115 kg with SD 10 kg. Calculate the z-score and determine if this pig is unusual.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch11-probability_distributions.html#session-info",
    "href": "chapters/ch11-probability_distributions.html#session-info",
    "title": "11¬† Week 11: Probability Distributions and Introduction to Inference",
    "section": "20.5 Session Info",
    "text": "20.5 Session Info\n\n\nCode\nsessionInfo()\n\n\nR version 4.4.2 (2024-10-31)\nPlatform: x86_64-apple-darwin20\nRunning under: macOS Sequoia 15.6.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.4-x86_64/Resources/lib/libRlapack.dylib;  LAPACK version 3.12.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: America/Chicago\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nother attached packages:\n [1] gt_1.1.0        scales_1.4.0    patchwork_1.3.2 broom_1.0.7    \n [5] lubridate_1.9.3 forcats_1.0.0   stringr_1.5.1   dplyr_1.1.4    \n [9] purrr_1.0.4     readr_2.1.5     tidyr_1.3.1     tibble_3.2.1   \n[13] ggplot2_4.0.0   tidyverse_2.0.0\n\nloaded via a namespace (and not attached):\n [1] gtable_0.3.6       jsonlite_1.8.9     compiler_4.4.2     tidyselect_1.2.1  \n [5] xml2_1.3.6         yaml_2.3.10        fastmap_1.2.0      R6_2.5.1          \n [9] labeling_0.4.3     generics_0.1.3     knitr_1.49         backports_1.5.0   \n[13] htmlwidgets_1.6.4  pillar_1.9.0       RColorBrewer_1.1-3 tzdb_0.4.0        \n[17] rlang_1.1.6        utf8_1.2.4         stringi_1.8.4      xfun_0.53         \n[21] sass_0.4.9         fs_1.6.5           S7_0.2.0           timechange_0.3.0  \n[25] cli_3.6.4          withr_3.0.2        magrittr_2.0.3     digest_0.6.37     \n[29] grid_4.4.2         hms_1.1.3          lifecycle_1.0.4    vctrs_0.6.5       \n[33] evaluate_1.0.1     glue_1.8.0         farver_2.1.2       fansi_1.0.6       \n[37] rmarkdown_2.29     tools_4.4.2        pkgconfig_2.0.3    htmltools_0.5.8.1 \n\n\n\nEnd of Week 3: Probability Distributions and Introduction to Inference",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Week 11: Probability Distributions and Introduction to Inference</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html",
    "href": "chapters/ch12-hypothesis_testing.html",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "",
    "text": "13 Introduction\nYou‚Äôre an animal nutritionist testing a new feed additive that‚Äôs supposed to increase daily weight gain in finishing pigs. You‚Äôve conducted a trial with 30 pigs on the new feed and observed an average daily gain of 0.85 kg/day, compared to the historical average of 0.78 kg/day. The difference looks promising, but is it real, or could it have occurred by chance?\nThis is the fundamental question that hypothesis testing helps us answer. This week, we‚Äôll develop a formal framework for making decisions about whether observed differences in data represent true effects or simply random variation.\nKey Questions We‚Äôll Address:\nBy the end of this lecture, you‚Äôll be able to conduct and interpret t-tests appropriately, check assumptions, calculate effect sizes, and understand the limitations of hypothesis testing.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-framework",
    "href": "chapters/ch12-hypothesis_testing.html#sec-framework",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "14.1 The Hypothesis Testing Framework",
    "text": "14.1 The Hypothesis Testing Framework\nHypothesis testing follows a structured approach:\n\nState hypotheses: Define null (H‚ÇÄ) and alternative (H‚ÇÅ) hypotheses\nChoose significance level: Typically Œ± = 0.05\nCollect data and calculate a test statistic\nCalculate p-value: Probability of observing data this extreme under H‚ÇÄ\nMake decision: Reject H‚ÇÄ if p &lt; Œ±, otherwise fail to reject H‚ÇÄ\nInterpret in context with effect sizes and confidence intervals\n\n\n\n\n\n\n\nImportantHypothesis Testing is NOT Absolute Truth\n\n\n\nHypothesis testing doesn‚Äôt tell us:\n\nWhether H‚ÇÄ is true\nWhether H‚ÇÅ is true\nThe probability we‚Äôve made a mistake\n\nIt tells us: How compatible our data is with the null hypothesis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-hypotheses",
    "href": "chapters/ch12-hypothesis_testing.html#sec-hypotheses",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "14.2 Null and Alternative Hypotheses",
    "text": "14.2 Null and Alternative Hypotheses\nThe null hypothesis (H‚ÇÄ) represents the status quo, no effect, or no difference. It‚Äôs the hypothesis we try to find evidence against.\nThe alternative hypothesis (H‚ÇÅ or H‚Çê) represents what we‚Äôre trying to find evidence for‚Äîusually that there is an effect or difference.\nExample: Feed Additive Trial\n\nH‚ÇÄ: The new feed additive has no effect on daily weight gain (Œº = 0.78 kg/day)\nH‚ÇÅ: The new feed additive changes daily weight gain (Œº ‚â† 0.78 kg/day)\n\nThis is a two-sided test because we‚Äôre open to the additive increasing or decreasing weight gain.\nWe could also formulate one-sided tests:\n\nH‚ÇÅ: Œº &gt; 0.78 (additive increases gain)\nH‚ÇÅ: Œº &lt; 0.78 (additive decreases gain)\n\n\n\n\n\n\n\nWarningOne-Sided vs Two-Sided Tests\n\n\n\nUse two-sided tests by default. One-sided tests are only appropriate when:\n\nYou have strong a priori reasons to test in only one direction\nA difference in the other direction would be meaningless or impossible\nYou specified the direction before collecting data\n\nDon‚Äôt choose one-sided tests just to get p &lt; 0.05!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-visual-intuition",
    "href": "chapters/ch12-hypothesis_testing.html#sec-visual-intuition",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "14.3 Visual Intuition: Is There a Difference?",
    "text": "14.3 Visual Intuition: Is There a Difference?\nLet‚Äôs visualize what hypothesis testing is trying to determine:\n\n\nCode\n# Simulate two scenarios\nset.seed(123)\n\n# Scenario A: No real difference (H0 is true)\nscenario_a &lt;- tibble(\n  group = rep(c(\"Control\", \"Treatment\"), each = 30),\n  weight_gain = c(rnorm(30, mean = 0.78, sd = 0.12),\n                  rnorm(30, mean = 0.78, sd = 0.12))\n)\n\n# Scenario B: Real difference (H0 is false)\nscenario_b &lt;- tibble(\n  group = rep(c(\"Control\", \"Treatment\"), each = 30),\n  weight_gain = c(rnorm(30, mean = 0.78, sd = 0.12),\n                  rnorm(30, mean = 0.88, sd = 0.12))\n)\n\n# Plot both scenarios\np1 &lt;- ggplot(scenario_a, aes(x = group, y = weight_gain, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Scenario A: No True Difference\",\n       subtitle = \"Both groups sampled from same distribution\",\n       y = \"Daily Weight Gain (kg)\",\n       x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(0.4, 1.2)\n\np2 &lt;- ggplot(scenario_b, aes(x = group, y = weight_gain, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Scenario B: Real Difference\",\n       subtitle = \"Treatment group has higher mean\",\n       y = \"Daily Weight Gain (kg)\",\n       x = \"\") +\n  theme(legend.position = \"none\") +\n  ylim(0.4, 1.2)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe challenge: Even when there‚Äôs no real difference (Scenario A), we still see some difference in sample means due to random variation. Hypothesis testing helps us quantify whether the observed difference is larger than we‚Äôd expect by chance alone.\n\n\nCode\n# Calculate means and run t-tests\nscenario_a_summary &lt;- scenario_a %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_gain = mean(weight_gain), .groups = 'drop')\n\nscenario_b_summary &lt;- scenario_b %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_gain = mean(weight_gain), .groups = 'drop')\n\ntest_a &lt;- t.test(weight_gain ~ group, data = scenario_a)\ntest_b &lt;- t.test(weight_gain ~ group, data = scenario_b)\n\ncat(\"Scenario A (no true difference):\\n\")\n\n\nScenario A (no true difference):\n\n\nCode\ncat(sprintf(\"  Control: %.3f kg/day, Treatment: %.3f kg/day\\n\",\n            scenario_a_summary$mean_gain[1], scenario_a_summary$mean_gain[2]))\n\n\n  Control: 0.774 kg/day, Treatment: 0.801 kg/day\n\n\nCode\ncat(sprintf(\"  Difference: %.3f kg/day, p-value: %.3f\\n\\n\",\n            diff(scenario_a_summary$mean_gain), test_a$p.value))\n\n\n  Difference: 0.027 kg/day, p-value: 0.342\n\n\nCode\ncat(\"Scenario B (true difference = 0.10 kg/day):\\n\")\n\n\nScenario B (true difference = 0.10 kg/day):\n\n\nCode\ncat(sprintf(\"  Control: %.3f kg/day, Treatment: %.3f kg/day\\n\",\n            scenario_b_summary$mean_gain[1], scenario_b_summary$mean_gain[2]))\n\n\n  Control: 0.783 kg/day, Treatment: 0.869 kg/day\n\n\nCode\ncat(sprintf(\"  Difference: %.3f kg/day, p-value: %.4f\\n\",\n            diff(scenario_b_summary$mean_gain), test_b$p.value))\n\n\n  Difference: 0.086 kg/day, p-value: 0.0028\n\n\nIn Scenario A, we correctly fail to reject H‚ÇÄ (p &gt; 0.05). In Scenario B, we correctly reject H‚ÇÄ (p &lt; 0.05) and conclude there‚Äôs evidence of a real difference.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-error-definitions",
    "href": "chapters/ch12-hypothesis_testing.html#sec-error-definitions",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "15.1 Definitions",
    "text": "15.1 Definitions\nType I Error (False Positive, Œ±): Rejecting H‚ÇÄ when it‚Äôs actually true\n\nConcluding there‚Äôs an effect when there isn‚Äôt one\nThe probability of Type I error is Œ± (significance level)\nTypically set at Œ± = 0.05 (5% false positive rate)\n\nType II Error (False Negative, Œ≤): Failing to reject H‚ÇÄ when it‚Äôs actually false\n\nConcluding there‚Äôs no effect when there is one\nThe probability of Type II error is Œ≤\nPower = 1 - Œ≤ (probability of correctly rejecting false H‚ÇÄ)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-truth-table",
    "href": "chapters/ch12-hypothesis_testing.html#sec-truth-table",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "15.2 The Truth Table",
    "text": "15.2 The Truth Table\n\n\n\n\nH‚ÇÄ is True\nH‚ÇÄ is False\n\n\n\n\nReject H‚ÇÄ\nType I Error (Œ±)\n‚úì Correct (Power)\n\n\nFail to Reject H‚ÇÄ\n‚úì Correct (1-Œ±)\nType II Error (Œ≤)\n\n\n\n\n\n\n\n\n\nNoteWhy Œ± = 0.05?\n\n\n\nThe 0.05 threshold is a convention, not a law of nature. R.A. Fisher suggested it as a convenient benchmark, but it‚Äôs arbitrary. Some fields use:\n\nŒ± = 0.01 for more stringent control of false positives\nŒ± = 0.10 for exploratory research where false negatives are more costly\n\nFocus on effect sizes and confidence intervals, not just whether p &lt; 0.05.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-error-consequences",
    "href": "chapters/ch12-hypothesis_testing.html#sec-error-consequences",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "15.3 Consequences in Animal Science",
    "text": "15.3 Consequences in Animal Science\nThe consequences of these errors depend on context:\nExample 1: New Feed Additive\n\nType I Error: Conclude additive works when it doesn‚Äôt ‚Üí waste money on ineffective product\nType II Error: Conclude additive doesn‚Äôt work when it does ‚Üí miss opportunity to improve productivity\n\nExample 2: Disease Screening\n\nType I Error: Conclude animal is diseased when it‚Äôs healthy ‚Üí unnecessary treatment, stress, cost\nType II Error: Conclude animal is healthy when it‚Äôs diseased ‚Üí disease spreads, welfare issues\n\nThe relative costs of these errors should inform your choice of Œ± and sample size (which affects Œ≤).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-simulate-type-i",
    "href": "chapters/ch12-hypothesis_testing.html#sec-simulate-type-i",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "15.4 Simulating Type I Errors",
    "text": "15.4 Simulating Type I Errors\nLet‚Äôs demonstrate that when H‚ÇÄ is true, we still get p &lt; 0.05 about 5% of the time:\n\n\nCode\n# Function to run one trial where H0 is TRUE\nrun_null_true_trial &lt;- function() {\n  # Both groups from same distribution (no real difference)\n  control &lt;- rnorm(25, mean = 100, sd = 15)\n  treatment &lt;- rnorm(25, mean = 100, sd = 15)  # Same mean!\n\n  test_result &lt;- t.test(treatment, control)\n  test_result$p.value\n}\n\n# Run 1000 trials\nn_sims &lt;- 1000\np_values_null_true &lt;- replicate(n_sims, run_null_true_trial())\n\n# Visualize\ntibble(p_value = p_values_null_true) %&gt;%\n  ggplot(aes(x = p_value)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0.05, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Distribution of P-values When H‚ÇÄ is True\",\n       subtitle = sprintf(\"Proportion with p &lt; 0.05: %.3f (expected: 0.05)\",\n                         mean(p_values_null_true &lt; 0.05)),\n       x = \"P-value\",\n       y = \"Count\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nCode\ntype_i_rate &lt;- mean(p_values_null_true &lt; 0.05)\ncat(sprintf(\"Type I error rate: %.3f (%.1f%%)\\n\", type_i_rate, type_i_rate * 100))\n\n\nType I error rate: 0.040 (4.0%)\n\n\nCode\ncat(sprintf(\"Out of %d trials: %d false positives\\n\",\n            n_sims, sum(p_values_null_true &lt; 0.05)))\n\n\nOut of 1000 trials: 40 false positives\n\n\nKey insight: Even when there‚Äôs no real effect, we‚Äôll get ‚Äúsignificant‚Äù results about 5% of the time. This is why replication is crucial in science!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-simulate-type-ii",
    "href": "chapters/ch12-hypothesis_testing.html#sec-simulate-type-ii",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "15.5 Simulating Type II Errors",
    "text": "15.5 Simulating Type II Errors\nNow let‚Äôs see Type II errors‚Äîwhen there IS a real effect, but we fail to detect it:\n\n\nCode\n# Function to run one trial where H0 is FALSE (real effect exists)\nrun_null_false_trial &lt;- function(true_effect = 10, sample_size = 20, sd = 15) {\n  control &lt;- rnorm(sample_size, mean = 100, sd = sd)\n  treatment &lt;- rnorm(sample_size, mean = 100 + true_effect, sd = sd)\n\n  test_result &lt;- t.test(treatment, control)\n  test_result$p.value\n}\n\n# Small effect, small sample\np_values_small &lt;- replicate(n_sims, run_null_false_trial(true_effect = 5, sample_size = 20))\n\n# Medium effect, small sample\np_values_medium &lt;- replicate(n_sims, run_null_false_trial(true_effect = 10, sample_size = 20))\n\n# Medium effect, large sample\np_values_large_n &lt;- replicate(n_sims, run_null_false_trial(true_effect = 10, sample_size = 50))\n\n# Combine results\nerror_rates &lt;- tibble(\n  Scenario = c(\"Small effect (d=0.33), n=20\",\n               \"Medium effect (d=0.67), n=20\",\n               \"Medium effect (d=0.67), n=50\"),\n  `Type II Error Rate (Œ≤)` = c(mean(p_values_small &gt;= 0.05),\n                                 mean(p_values_medium &gt;= 0.05),\n                                 mean(p_values_large_n &gt;= 0.05)),\n  `Power (1-Œ≤)` = 1 - `Type II Error Rate (Œ≤)`\n)\n\nknitr::kable(error_rates, digits = 3, align = 'lcc',\n             caption = \"Type II Error Rates Under Different Scenarios\")\n\n\n\nType II Error Rates Under Different Scenarios\n\n\nScenario\nType II Error Rate (Œ≤)\nPower (1-Œ≤)\n\n\n\n\nSmall effect (d=0.33), n=20\n0.831\n0.169\n\n\nMedium effect (d=0.67), n=20\n0.474\n0.526\n\n\nMedium effect (d=0.67), n=50\n0.093\n0.907\n\n\n\n\n\nKey insights:\n\nSmaller effects are harder to detect (higher Œ≤)\nLarger samples reduce Type II error (increase power)\nEven with a real effect, we often fail to detect it with small samples!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-power-factors",
    "href": "chapters/ch12-hypothesis_testing.html#sec-power-factors",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "16.1 Factors Affecting Power",
    "text": "16.1 Factors Affecting Power\nPower depends on four factors:\n\nEffect size: Larger effects are easier to detect\nSample size (n): More data = more power\nSignificance level (Œ±): Lower Œ± = lower power (trade-off with Type I error)\nVariability (œÉ): Less noisy data = more power\n\n\n\n\n\n\n\nTipTypical Power Target\n\n\n\nMany researchers aim for power = 0.80 (80% probability of detecting the effect). This means accepting a 20% Type II error rate (Œ≤ = 0.20).\nThe choice of 80% is conventional, like Œ± = 0.05. In some contexts (e.g., clinical trials), you might want higher power (0.90 or 0.95).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-power-curves",
    "href": "chapters/ch12-hypothesis_testing.html#sec-power-curves",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "16.2 Visualizing Power",
    "text": "16.2 Visualizing Power\nLet‚Äôs visualize how power changes with sample size and effect size:\n\n\nCode\n# Function to calculate power via simulation\ncalculate_power_sim &lt;- function(n, effect_size, sd = 15, alpha = 0.05, n_sims = 1000) {\n  p_values &lt;- replicate(n_sims, {\n    control &lt;- rnorm(n, mean = 100, sd = sd)\n    treatment &lt;- rnorm(n, mean = 100 + effect_size, sd = sd)\n    t.test(treatment, control)$p.value\n  })\n  mean(p_values &lt; alpha)\n}\n\n# Calculate power for different scenarios\npower_data &lt;- expand_grid(\n  n = seq(10, 100, by = 5),\n  effect_size = c(5, 10, 15, 20)\n) %&gt;%\n  mutate(\n    effect_label = sprintf(\"Effect = %d (d = %.2f)\", effect_size, effect_size / 15),\n    power = map2_dbl(n, effect_size, ~calculate_power_sim(.x, .y, n_sims = 500))\n  )\n\n# Plot power curves\nggplot(power_data, aes(x = n, y = power, color = effect_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray40\") +\n  geom_hline(yintercept = 0.05, linetype = \"dotted\", color = \"gray60\") +\n  annotate(\"text\", x = 90, y = 0.82, label = \"Target power = 0.80\", size = 3) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Statistical Power vs Sample Size\",\n       subtitle = \"For two-sample t-test with SD = 15, Œ± = 0.05\",\n       x = \"Sample Size per Group\",\n       y = \"Power (1 - Œ≤)\",\n       color = \"Effect Size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\n\n\nKey insights from power curves:\n\nSmall effects need large samples: For a small effect (d = 0.33), you need n ‚âà 90 per group to achieve 80% power\nLarge effects need small samples: For a large effect (d = 1.33), n ‚âà 10 per group is sufficient\nDiminishing returns: Going from n=20 to n=40 adds more power than going from n=60 to n=80",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-sample-size",
    "href": "chapters/ch12-hypothesis_testing.html#sec-sample-size",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "16.3 Sample Size Planning",
    "text": "16.3 Sample Size Planning\nBefore conducting a study, you should estimate the required sample size to achieve adequate power. This requires:\n\nSpecify Œ±: Usually 0.05\nSpecify desired power: Usually 0.80\nEstimate effect size: Based on pilot data or literature\nEstimate variability: SD or variance\n\nExample: Feed Additive Study\nSuppose we expect the feed additive to increase daily gain by 0.08 kg (from 0.78 to 0.86 kg/day), and we know the SD ‚âà 0.12 kg/day from previous trials. How many pigs do we need?\n\n\nCode\n# Calculate Cohen's d\nexpected_effect &lt;- 0.08\nexpected_sd &lt;- 0.12\ncohens_d &lt;- expected_effect / expected_sd\n\ncat(sprintf(\"Expected Cohen's d: %.2f\\n\", cohens_d))\n\n\nExpected Cohen's d: 0.67\n\n\nCode\n# Use power.t.test for analytical power calculation\npower_analysis &lt;- power.t.test(\n  delta = expected_effect,    # Difference in means\n  sd = expected_sd,           # Standard deviation\n  sig.level = 0.05,           # Œ±\n  power = 0.80,               # Desired power\n  type = \"two.sample\",\n  alternative = \"two.sided\"\n)\n\nprint(power_analysis)\n\n\n\n     Two-sample t test power calculation \n\n              n = 36.3058\n          delta = 0.08\n             sd = 0.12\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\ncat(sprintf(\"\\nRequired sample size: %.0f pigs per group\\n\", ceiling(power_analysis$n)))\n\n\n\nRequired sample size: 37 pigs per group\n\n\nCode\ncat(sprintf(\"Total pigs needed: %.0f\\n\", 2 * ceiling(power_analysis$n)))\n\n\nTotal pigs needed: 74\n\n\nInterpretation: We need approximately 36 pigs per group (72 total) to have 80% power to detect a difference of 0.08 kg/day.\n\n\n\n\n\n\nImportantPower Analysis Should Be Done BEFORE Data Collection\n\n\n\nConducting power analysis after your study doesn‚Äôt change anything about your results. Power analysis is most useful for:\n\nStudy planning: Determine required sample size before collecting data\nInterpreting non-significant results: A non-significant result from an underpowered study is uninformative\nEvaluating published research: Were studies adequately powered to detect realistic effects?\n\n‚ÄúPost-hoc power analysis‚Äù of your own data is generally not recommended.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-one-sample-assumptions",
    "href": "chapters/ch12-hypothesis_testing.html#sec-one-sample-assumptions",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "17.1 Assumptions",
    "text": "17.1 Assumptions\n\nIndependence: Observations are independent of each other\nNormality: The data (or sampling distribution of means) is approximately normal\n\n\n\n\n\n\n\nNoteCentral Limit Theorem to the Rescue\n\n\n\nThe t-test is fairly robust to violations of normality when:\n\nSample size is moderate to large (n &gt; 30 as a rule of thumb)\nThe distribution isn‚Äôt extremely skewed\nThere are no extreme outliers\n\nFor small samples (n &lt; 30), normality is more important.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-one-sample-example",
    "href": "chapters/ch12-hypothesis_testing.html#sec-one-sample-example",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "17.2 Worked Example: Milk Production",
    "text": "17.2 Worked Example: Milk Production\n\n\nCode\n# Simulate milk production data\nset.seed(456)\nmilk_production &lt;- tibble(\n  cow_id = 1:25,\n  milk_kg = rnorm(25, mean = 37.2, sd = 4.5)  # True mean = 37.2 (higher than historical 35)\n)\n\n# Summary statistics\nmilk_summary &lt;- milk_production %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(milk_kg),\n    sd = sd(milk_kg),\n    se = sd / sqrt(n),\n    median = median(milk_kg),\n    min = min(milk_kg),\n    max = max(milk_kg)\n  )\n\nknitr::kable(milk_summary, digits = 2,\n             caption = \"Summary Statistics: Milk Production (kg/day)\")\n\n\n\nSummary Statistics: Milk Production (kg/day)\n\n\nn\nmean\nsd\nse\nmedian\nmin\nmax\n\n\n\n\n25\n38.32\n5.34\n1.07\n38.94\n29.47\n47.46\n\n\n\n\n\nHypotheses:\n\nH‚ÇÄ: Œº = 35 kg/day (no change from historical average)\nH‚ÇÅ: Œº ‚â† 35 kg/day (production has changed)\n\n\n17.2.1 Step 1: Check Assumptions\n\n\nCode\n# Visual check: Histogram and QQ plot\np1 &lt;- ggplot(milk_production, aes(x = milk_kg)) +\n  geom_histogram(bins = 10, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 35, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  annotate(\"text\", x = 35, y = 5.5, label = \"Historical\\nmean = 35\",\n           color = \"red\", hjust = -0.1, size = 3) +\n  labs(title = \"Distribution of Milk Production\",\n       x = \"Milk Production (kg/day)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 11)\n\np2 &lt;- ggplot(milk_production, aes(sample = milk_kg)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Q-Q Plot\",\n       subtitle = \"Checking normality assumption\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 11)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: The histogram looks reasonably symmetric, and the QQ plot shows points close to the line ‚Üí normality assumption seems reasonable.\nFormal test: Shapiro-Wilk test\n\n\nCode\nshapiro_result &lt;- shapiro.test(milk_production$milk_kg)\ncat(sprintf(\"Shapiro-Wilk test: W = %.4f, p-value = %.3f\\n\",\n            shapiro_result$statistic, shapiro_result$p.value))\n\n\nShapiro-Wilk test: W = 0.9544, p-value = 0.315\n\n\nCode\nif(shapiro_result$p.value &gt; 0.05) {\n  cat(\"‚Üí No evidence against normality (p &gt; 0.05)\\n\")\n} else {\n  cat(\"‚Üí Some evidence against normality (p &lt; 0.05)\\n\")\n}\n\n\n‚Üí No evidence against normality (p &gt; 0.05)\n\n\n\n\n\n\n\n\nWarningDon‚Äôt Over-Rely on Normality Tests\n\n\n\nShapiro-Wilk and other normality tests can be:\n\nToo sensitive with large samples (reject for trivial deviations)\nNot sensitive enough with small samples (fail to detect important deviations)\n\nRecommendation: Use visual assessment (QQ plots) as your primary tool, and consider the robustness of the t-test.\n\n\n\n\n17.2.2 Step 2: Conduct the t-Test\n\n\nCode\n# One-sample t-test\nmilk_test &lt;- t.test(milk_production$milk_kg, mu = 35)\n\n# Tidy output\nmilk_test_tidy &lt;- tidy(milk_test)\n\nknitr::kable(milk_test_tidy, digits = 3,\n             caption = \"One-Sample t-Test Results\")\n\n\n\nOne-Sample t-Test Results\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n38.319\n3.105\n0.005\n24\n36.113\n40.524\nOne Sample t-test\ntwo.sided\n\n\n\n\n\nCode\n# Print results\ncat(sprintf(\"\\nOne-Sample t-Test Results:\\n\"))\n\n\n\nOne-Sample t-Test Results:\n\n\nCode\ncat(sprintf(\"Sample mean: %.2f kg/day\\n\", milk_test$estimate))\n\n\nSample mean: 38.32 kg/day\n\n\nCode\ncat(sprintf(\"Hypothesized mean: %.2f kg/day\\n\", 35))\n\n\nHypothesized mean: 35.00 kg/day\n\n\nCode\ncat(sprintf(\"Difference: %.2f kg/day\\n\", milk_test$estimate - 35))\n\n\nDifference: 3.32 kg/day\n\n\nCode\ncat(sprintf(\"t-statistic: %.3f\\n\", milk_test$statistic))\n\n\nt-statistic: 3.105\n\n\nCode\ncat(sprintf(\"Degrees of freedom: %d\\n\", milk_test$parameter))\n\n\nDegrees of freedom: 24\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", milk_test$p.value))\n\n\nP-value: 0.0048\n\n\nCode\ncat(sprintf(\"95%% CI: [%.2f, %.2f]\\n\", milk_test$conf.int[1], milk_test$conf.int[2]))\n\n\n95% CI: [36.11, 40.52]\n\n\n\n\n17.2.3 Step 3: Interpret Results\n\n\nCode\n# Visualize result with confidence interval\nggplot(milk_production, aes(x = \"Sample\", y = milk_kg)) +\n  geom_jitter(width = 0.1, alpha = 0.4, size = 2, color = \"steelblue\") +\n  geom_hline(yintercept = 35, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 4, color = \"darkblue\") +\n  stat_summary(fun.data = mean_cl_normal, geom = \"errorbar\",\n               width = 0.2, color = \"darkblue\", linewidth = 1) +\n  annotate(\"text\", x = 1.3, y = 35, label = \"Historical mean\\n(H‚ÇÄ: Œº = 35)\",\n           color = \"red\", hjust = 0, size = 3) +\n  annotate(\"text\", x = 1.3, y = milk_test$estimate,\n           label = sprintf(\"Sample mean\\n%.1f kg/day\", milk_test$estimate),\n           color = \"darkblue\", hjust = 0, size = 3) +\n  labs(title = \"Milk Production: Sample vs Historical Mean\",\n       subtitle = sprintf(\"95%% CI: [%.1f, %.1f], p = %.4f\",\n                         milk_test$conf.int[1], milk_test$conf.int[2], milk_test$p.value),\n       x = \"\",\n       y = \"Milk Production (kg/day)\") +\n  theme_minimal(base_size = 12) +\n  theme(axis.text.x = element_blank())\n\n\n\n\n\n\n\n\n\nStatistical conclusion: We reject H‚ÇÄ (p = 0.005). There is strong evidence that milk production under the new feeding program differs from the historical average of 35 kg/day.\nPractical interpretation: The new feeding program is associated with an increase of approximately 3.3 kg/day (95% CI: [1.1, 5.5]). This is both statistically significant and potentially economically meaningful.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-one-sample-effect",
    "href": "chapters/ch12-hypothesis_testing.html#sec-one-sample-effect",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "17.3 Effect Size for One-Sample t-Test",
    "text": "17.3 Effect Size for One-Sample t-Test\n\n\nCode\n# Calculate Cohen's d for one-sample test\ncohens_d_one_sample &lt;- (milk_test$estimate - 35) / milk_summary$sd\n\ncat(sprintf(\"Cohen's d: %.3f\\n\", cohens_d_one_sample))\n\n\nCohen's d: 0.621\n\n\nCode\n# Interpretation\nif(abs(cohens_d_one_sample) &lt; 0.2) {\n  interpretation &lt;- \"negligible\"\n} else if(abs(cohens_d_one_sample) &lt; 0.5) {\n  interpretation &lt;- \"small\"\n} else if(abs(cohens_d_one_sample) &lt; 0.8) {\n  interpretation &lt;- \"medium\"\n} else {\n  interpretation &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", interpretation))\n\n\nEffect size interpretation: medium\n\n\nCohen‚Äôs d guidelines (rough benchmarks):\n\nd = 0.2: Small effect\nd = 0.5: Medium effect\nd = 0.8: Large effect\n\nOur effect size is medium, indicating a substantial difference from the historical mean.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-two-sample-assumptions",
    "href": "chapters/ch12-hypothesis_testing.html#sec-two-sample-assumptions",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "18.1 Assumptions",
    "text": "18.1 Assumptions\n\nIndependence: Observations within and between groups are independent\nNormality: Data in each group is approximately normally distributed\nEqual variances (for standard t-test): The variances in the two groups are equal\n\n\n\n\n\n\n\nNoteWelch‚Äôs t-Test: Unequal Variances\n\n\n\nIf variances are unequal, use Welch‚Äôs t-test (the default in R‚Äôs t.test()). It adjusts the degrees of freedom to account for unequal variances.\nThe standard t-test assumes equal variances (Student‚Äôs t-test), but Welch‚Äôs t-test is more robust and is generally recommended as the default.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-two-sample-example",
    "href": "chapters/ch12-hypothesis_testing.html#sec-two-sample-example",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "18.2 Worked Example: Grain Supplements",
    "text": "18.2 Worked Example: Grain Supplements\n\n\nCode\n# Simulate weight gain data\nset.seed(789)\ncattle_gain &lt;- tibble(\n  supplement = rep(c(\"A\", \"B\"), each = 20),\n  weight_gain_kg = c(\n    rnorm(20, mean = 185, sd = 22),  # Supplement A\n    rnorm(20, mean = 205, sd = 25)   # Supplement B (higher gain)\n  )\n)\n\n# Summary statistics by group\ncattle_summary &lt;- cattle_gain %&gt;%\n  group_by(supplement) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(weight_gain_kg),\n    sd = sd(weight_gain_kg),\n    se = sd / sqrt(n),\n    median = median(weight_gain_kg),\n    .groups = 'drop'\n  )\n\nknitr::kable(cattle_summary, digits = 2,\n             caption = \"Summary Statistics: Weight Gain by Supplement\")\n\n\n\nSummary Statistics: Weight Gain by Supplement\n\n\nsupplement\nn\nmean\nsd\nse\nmedian\n\n\n\n\nA\n20\n178.18\n15.96\n3.57\n176.60\n\n\nB\n20\n197.72\n17.84\n3.99\n196.95\n\n\n\n\n\nHypotheses:\n\nH‚ÇÄ: Œº_A = Œº_B (no difference in weight gain between supplements)\nH‚ÇÅ: Œº_A ‚â† Œº_B (supplements lead to different weight gains)\n\n\n18.2.1 Step 1: Visualize the Data\n\n\nCode\n# Box plots with individual points\np1 &lt;- ggplot(cattle_gain, aes(x = supplement, y = weight_gain_kg, fill = supplement)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"red\", color = \"darkred\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Weight Gain by Supplement Type\",\n       subtitle = \"Diamonds show group means\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n# Density plots\np2 &lt;- ggplot(cattle_gain, aes(x = weight_gain_kg, fill = supplement)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(data = cattle_summary, aes(xintercept = mean, color = supplement),\n             linetype = \"dashed\", linewidth = 1) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Distribution of Weight Gain\",\n       subtitle = \"Dashed lines show group means\",\n       x = \"Weight Gain (kg)\",\n       y = \"Density\") +\n  theme_minimal(base_size = 12)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: Supplement B appears to produce higher weight gain on average, with some overlap between distributions.\n\n\n18.2.2 Step 2: Check Assumptions\nNormality Check (QQ Plots by Group):\n\n\nCode\nggplot(cattle_gain, aes(sample = weight_gain_kg, color = supplement)) +\n  stat_qq(size = 2) +\n  stat_qq_line(linetype = \"dashed\") +\n  facet_wrap(~supplement) +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Q-Q Plots by Supplement\",\n       subtitle = \"Checking normality assumption\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nBoth groups show reasonably normal distributions.\nEqual Variance Check (Levene‚Äôs Test):\n\n\nCode\n# Levene's test for homogeneity of variance\nlevene_result &lt;- leveneTest(weight_gain_kg ~ supplement, data = cattle_gain)\n\ncat(\"Levene's Test for Equality of Variances:\\n\")\n\n\nLevene's Test for Equality of Variances:\n\n\nCode\nprint(levene_result)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  0.7142 0.4033\n      38               \n\n\nCode\ncat(sprintf(\"\\nP-value: %.3f\\n\", levene_result$`Pr(&gt;F)`[1]))\n\n\n\nP-value: 0.403\n\n\nCode\nif(levene_result$`Pr(&gt;F)`[1] &gt; 0.05) {\n  cat(\"‚Üí No evidence of unequal variances (p &gt; 0.05)\\n\")\n  cat(\"  Standard t-test or Welch's t-test are both appropriate\\n\")\n} else {\n  cat(\"‚Üí Evidence of unequal variances (p &lt; 0.05)\\n\")\n  cat(\"  Welch's t-test is recommended\\n\")\n}\n\n\n‚Üí No evidence of unequal variances (p &gt; 0.05)\n  Standard t-test or Welch's t-test are both appropriate\n\n\n\n\n\n\n\n\nTipVisualizing Variance Differences\n\n\n\nA simple way to compare variances visually:\n\n\nCode\ncattle_gain %&gt;%\n  ggplot(aes(x = supplement, y = weight_gain_kg)) +\n  geom_boxplot(aes(fill = supplement), alpha = 0.4) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1),\n               geom = \"errorbar\", width = 0.3, linewidth = 1, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"darkred\") +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  labs(title = \"Mean ¬± SD by Group\",\n       subtitle = \"Error bars show ¬±1 SD\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nIf one group‚Äôs error bars are much longer, variances may be unequal.\n\n\n\n\n18.2.3 Step 3: Conduct the t-Test\n\n\nCode\n# Welch's t-test (default, doesn't assume equal variances)\ncattle_test_welch &lt;- t.test(weight_gain_kg ~ supplement, data = cattle_gain)\n\n# Student's t-test (assumes equal variances)\ncattle_test_student &lt;- t.test(weight_gain_kg ~ supplement, data = cattle_gain, var.equal = TRUE)\n\n# Compare results\ncat(\"Welch's t-Test (unequal variances assumed):\\n\")\n\n\nWelch's t-Test (unequal variances assumed):\n\n\nCode\ncat(sprintf(\"  t(%.2f) = %.3f, p = %.4f\\n\",\n            cattle_test_welch$parameter,\n            cattle_test_welch$statistic,\n            cattle_test_welch$p.value))\n\n\n  t(37.54) = -3.651, p = 0.0008\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            cattle_test_welch$conf.int[1], cattle_test_welch$conf.int[2]))\n\n\n  95% CI for difference: [-30.38, -8.70]\n\n\nCode\ncat(\"Student's t-Test (equal variances assumed):\\n\")\n\n\nStudent's t-Test (equal variances assumed):\n\n\nCode\ncat(sprintf(\"  t(%d) = %.3f, p = %.4f\\n\",\n            cattle_test_student$parameter,\n            cattle_test_student$statistic,\n            cattle_test_student$p.value))\n\n\n  t(38) = -3.651, p = 0.0008\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\",\n            cattle_test_student$conf.int[1], cattle_test_student$conf.int[2]))\n\n\n  95% CI for difference: [-30.38, -8.71]\n\n\nNote: In this case, both tests give similar results because the variances are fairly similar. When in doubt, use Welch‚Äôs t-test (the default).\n\n\n18.2.4 Step 4: Calculate Effect Size\n\n\nCode\n# Calculate Cohen's d\ncattle_cohen_d &lt;- cohen.d(weight_gain_kg ~ supplement, data = cattle_gain)\n\nprint(cattle_cohen_d)\n\n\n\nCohen's d\n\nd estimate: -1.154639 (large)\n95 percent confidence interval:\n     lower      upper \n-1.8460956 -0.4631817 \n\n\nCode\ncat(sprintf(\"\\nCohen's d: %.3f\\n\", cattle_cohen_d$estimate))\n\n\n\nCohen's d: -1.155\n\n\nCode\ncat(sprintf(\"95%% CI for d: [%.3f, %.3f]\\n\",\n            cattle_cohen_d$conf.int[1], cattle_cohen_d$conf.int[2]))\n\n\n95% CI for d: [-1.846, -0.463]\n\n\nCode\n# Interpretation\nd_value &lt;- abs(cattle_cohen_d$estimate)\nif(d_value &lt; 0.2) {\n  interpretation &lt;- \"negligible\"\n} else if(d_value &lt; 0.5) {\n  interpretation &lt;- \"small\"\n} else if(d_value &lt; 0.8) {\n  interpretation &lt;- \"medium\"\n} else {\n  interpretation &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", interpretation))\n\n\nEffect size interpretation: large\n\n\n\n\n18.2.5 Step 5: Interpret and Report\n\n\nCode\n# Create a clean visualization for reporting\nmean_diff &lt;- diff(cattle_summary$mean)\n\nggplot(cattle_summary, aes(x = supplement, y = mean, fill = supplement)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = mean - se * 1.96, ymax = mean + se * 1.96),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.1f kg\", mean)),\n            vjust = -2.5, fontface = \"bold\", size = 4) +\n  scale_fill_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  annotate(\"segment\", x = 1, xend = 2, y = 230, yend = 230,\n           arrow = arrow(ends = \"both\", length = unit(0.2, \"cm\"))) +\n  annotate(\"text\", x = 1.5, y = 235,\n           label = sprintf(\"Difference: %.1f kg\\np = %.3f\",\n                          mean_diff, cattle_test_welch$p.value),\n           size = 3.5, fontface = \"bold\") +\n  labs(title = \"Weight Gain by Supplement Type\",\n       subtitle = \"Error bars show 95% confidence intervals\",\n       x = \"Supplement\",\n       y = \"Weight Gain (kg)\") +\n  ylim(0, 250) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nResults Summary:\nBeef steers receiving Supplement B gained significantly more weight (M = 197.7 kg, SD = 17.8) compared to those receiving Supplement A (M = 178.2 kg, SD = 16.0), t(37.5) = -3.65, p = 0.001, 95% CI [-30.4, -8.7], d = 1.15. This represents a large effect.\nPractical interpretation: Supplement B produces approximately 20 kg more weight gain than Supplement A, which could translate to meaningful economic benefits for producers.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-pairing-matters",
    "href": "chapters/ch12-hypothesis_testing.html#sec-pairing-matters",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "19.1 Why Pairing Matters",
    "text": "19.1 Why Pairing Matters\nWhen measurements are paired, we‚Äôre interested in the differences within pairs, not the absolute values in each group.\nExample: Testing a feed supplement by measuring weight gain in the same animals before and after treatment is more powerful than comparing two different groups, because we control for individual variation in baseline weight and genetics.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-paired-example",
    "href": "chapters/ch12-hypothesis_testing.html#sec-paired-example",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "19.2 Worked Example: Milk Yield Before and After Treatment",
    "text": "19.2 Worked Example: Milk Yield Before and After Treatment\nA dairy researcher wants to test whether a new probiotic supplement increases milk yield. They measure milk production in 20 cows before supplementation, then again after 4 weeks on the supplement.\n\n\nCode\n# Simulate paired data\nset.seed(321)\n\n# Create baseline variation between cows\ncow_baseline &lt;- rnorm(20, mean = 30, sd = 5)\n\n# Before treatment: baseline + random day-to-day variation\nmilk_before &lt;- cow_baseline + rnorm(20, mean = 0, sd = 2)\n\n# After treatment: baseline + treatment effect + random variation\ntreatment_effect &lt;- 2.5  # True effect = 2.5 kg/day increase\nmilk_after &lt;- cow_baseline + treatment_effect + rnorm(20, mean = 0, sd = 2)\n\n# Combine into data frame\nmilk_paired &lt;- tibble(\n  cow_id = 1:20,\n  before = milk_before,\n  after = milk_after,\n  difference = after - before\n)\n\n# Long format for plotting\nmilk_paired_long &lt;- milk_paired %&gt;%\n  pivot_longer(cols = c(before, after),\n               names_to = \"time\",\n               values_to = \"milk_yield\") %&gt;%\n  mutate(time = factor(time, levels = c(\"before\", \"after\")))\n\n# Summary statistics\nmilk_paired_summary &lt;- milk_paired_long %&gt;%\n  group_by(time) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(milk_yield),\n    sd = sd(milk_yield),\n    se = sd / sqrt(n),\n    .groups = 'drop'\n  )\n\nknitr::kable(milk_paired_summary, digits = 2,\n             caption = \"Summary Statistics: Milk Yield Before and After Treatment\")\n\n\n\nSummary Statistics: Milk Yield Before and After Treatment\n\n\ntime\nn\nmean\nsd\nse\n\n\n\n\nbefore\n20\n31.13\n5.22\n1.17\n\n\nafter\n20\n34.06\n4.66\n1.04\n\n\n\n\n\nCode\n# Summary of differences\ndiff_summary &lt;- milk_paired %&gt;%\n  summarise(\n    mean_diff = mean(difference),\n    sd_diff = sd(difference),\n    se_diff = sd_diff / sqrt(n())\n  )\n\ncat(sprintf(\"\\nMean difference (after - before): %.2f kg/day\\n\", diff_summary$mean_diff))\n\n\n\nMean difference (after - before): 2.93 kg/day\n\n\nCode\ncat(sprintf(\"SD of differences: %.2f kg/day\\n\", diff_summary$sd_diff))\n\n\nSD of differences: 2.88 kg/day\n\n\n\n19.2.1 Visualizing Paired Data\nThe key to paired data is visualizing the connections between measurements:\n\n\nCode\n# Paired plot showing connections\np1 &lt;- ggplot(milk_paired_long, aes(x = time, y = milk_yield, group = cow_id)) +\n  geom_line(alpha = 0.3, color = \"gray50\") +\n  geom_point(aes(color = time), size = 2, alpha = 0.7) +\n  stat_summary(aes(group = 1), fun = mean, geom = \"line\",\n               color = \"red\", linewidth = 1.5, linetype = \"solid\") +\n  stat_summary(aes(group = 1), fun = mean, geom = \"point\",\n               color = \"red\", size = 4, shape = 18) +\n  scale_color_manual(values = c(\"steelblue\", \"orange\")) +\n  labs(title = \"Milk Yield Before and After Treatment\",\n       subtitle = \"Lines connect measurements from same cow\",\n       x = \"\",\n       y = \"Milk Yield (kg/day)\") +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"none\")\n\n# Distribution of differences\np2 &lt;- ggplot(milk_paired, aes(x = difference)) +\n  geom_histogram(bins = 12, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 0, color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  geom_vline(xintercept = diff_summary$mean_diff, color = \"darkgreen\",\n             linetype = \"solid\", linewidth = 1) +\n  annotate(\"text\", x = 0, y = 4.5, label = \"No change\",\n           color = \"red\", hjust = -0.1, size = 3) +\n  annotate(\"text\", x = diff_summary$mean_diff, y = 4.5,\n           label = sprintf(\"Mean\\ndiff = %.1f\", diff_summary$mean_diff),\n           color = \"darkgreen\", hjust = -0.1, size = 3) +\n  labs(title = \"Distribution of Within-Cow Differences\",\n       x = \"Difference in Milk Yield (kg/day)\",\n       y = \"Count\") +\n  theme_minimal(base_size = 12)\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey observation: Most lines slope upward, indicating that individual cows increased milk production. The distribution of differences is centered above zero.\n\n\n19.2.2 Paired vs Unpaired Analysis\nLet‚Äôs compare what happens if we (incorrectly) treat this as unpaired data:\n\n\nCode\n# Paired t-test (CORRECT)\npaired_test &lt;- t.test(milk_paired$after, milk_paired$before, paired = TRUE)\n\n# Unpaired t-test (INCORRECT for this design)\nunpaired_test &lt;- t.test(milk_paired$after, milk_paired$before, paired = FALSE)\n\n# Compare results\ncat(\"PAIRED t-Test (Correct Analysis):\\n\")\n\n\nPAIRED t-Test (Correct Analysis):\n\n\nCode\ncat(sprintf(\"  t(%d) = %.3f, p = %.4f\\n\",\n            paired_test$parameter, paired_test$statistic, paired_test$p.value))\n\n\n  t(19) = 4.553, p = 0.0002\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            paired_test$conf.int[1], paired_test$conf.int[2]))\n\n\n  95% CI for difference: [1.58, 4.28]\n\n\nCode\ncat(\"UNPAIRED t-Test (Incorrect Analysis):\\n\")\n\n\nUNPAIRED t-Test (Incorrect Analysis):\n\n\nCode\ncat(sprintf(\"  t(%.1f) = %.3f, p = %.4f\\n\",\n            unpaired_test$parameter, unpaired_test$statistic, unpaired_test$p.value))\n\n\n  t(37.5) = 1.873, p = 0.0688\n\n\nCode\ncat(sprintf(\"  95%% CI for difference: [%.2f, %.2f]\\n\\n\",\n            unpaired_test$conf.int[1], unpaired_test$conf.int[2]))\n\n\n  95% CI for difference: [-0.24, 6.10]\n\n\nCode\ncat(\"Why the difference?\\n\")\n\n\nWhy the difference?\n\n\nCode\ncat(sprintf(\"  Paired test SE: %.3f\\n\",\n            diff_summary$sd_diff / sqrt(20)))\n\n\n  Paired test SE: 0.644\n\n\nCode\ncat(sprintf(\"  Unpaired test SE: %.3f\\n\",\n            sqrt(var(milk_paired$before)/20 + var(milk_paired$after)/20)))\n\n\n  Unpaired test SE: 1.564\n\n\nCode\ncat(\"  ‚Üí Paired test has smaller SE (removes between-cow variation)\\n\")\n\n\n  ‚Üí Paired test has smaller SE (removes between-cow variation)\n\n\nKey insight: The paired test is more powerful because it accounts for the fact that we measured the same cows twice. The unpaired test includes unnecessary between-cow variability, making the standard error larger and the test less sensitive.\n\n\n\n\n\n\nImportantPairing Increases Power\n\n\n\nIn this example:\n\nPaired test: p = 0.0002 ‚Üí Significant at Œ± = 0.05\nUnpaired test: p = 0.0688 ‚Üí May or may not be significant\n\nThe paired test is more powerful because we‚Äôre comparing each cow to itself, removing individual differences in baseline milk production.\nRule: If your data are paired by design, you MUST use a paired test!\n\n\n\n\n19.2.3 Conduct Paired t-Test\n\n\nCode\n# Paired t-test\npaired_result &lt;- t.test(milk_paired$after, milk_paired$before, paired = TRUE)\n\n# Tidy output\npaired_tidy &lt;- tidy(paired_result)\n\nknitr::kable(paired_tidy, digits = 4,\n             caption = \"Paired t-Test Results\")\n\n\n\nPaired t-Test Results\n\n\n\n\n\n\n\n\n\n\n\n\nestimate\nstatistic\np.value\nparameter\nconf.low\nconf.high\nmethod\nalternative\n\n\n\n\n2.9309\n4.5532\n2e-04\n19\n1.5836\n4.2782\nPaired t-test\ntwo.sided\n\n\n\n\n\nCode\n# Print interpretation\ncat(\"\\nPaired t-Test Results:\\n\")\n\n\n\nPaired t-Test Results:\n\n\nCode\ncat(sprintf(\"Mean difference: %.2f kg/day\\n\", paired_result$estimate))\n\n\nMean difference: 2.93 kg/day\n\n\nCode\ncat(sprintf(\"t(%d) = %.3f\\n\", paired_result$parameter, paired_result$statistic))\n\n\nt(19) = 4.553\n\n\nCode\ncat(sprintf(\"P-value: %.4f\\n\", paired_result$p.value))\n\n\nP-value: 0.0002\n\n\nCode\ncat(sprintf(\"95%% CI: [%.2f, %.2f]\\n\", paired_result$conf.int[1], paired_result$conf.int[2]))\n\n\n95% CI: [1.58, 4.28]\n\n\nCode\nif(paired_result$p.value &lt; 0.05) {\n  cat(\"\\n‚Üí Significant at Œ± = 0.05. Evidence that treatment increases milk yield.\\n\")\n} else {\n  cat(\"\\n‚Üí Not significant at Œ± = 0.05. Insufficient evidence of treatment effect.\\n\")\n}\n\n\n\n‚Üí Significant at Œ± = 0.05. Evidence that treatment increases milk yield.\n\n\n\n\n19.2.4 Effect Size for Paired t-Test\n\n\nCode\n# Cohen's d for paired data (based on differences)\npaired_d &lt;- cohen.d(milk_paired$after, milk_paired$before, paired = TRUE)\n\nprint(paired_d)\n\n\n\nCohen's d\n\nd estimate: 0.5832096 (medium)\n95 percent confidence interval:\n    lower     upper \n0.3027274 0.8636919 \n\n\nCode\ncat(sprintf(\"\\nCohen's d (paired): %.3f\\n\", paired_d$estimate))\n\n\n\nCohen's d (paired): 0.583\n\n\nCode\n# Interpretation\nd_val &lt;- abs(paired_d$estimate)\nif(d_val &lt; 0.2) {\n  d_interp &lt;- \"negligible\"\n} else if(d_val &lt; 0.5) {\n  d_interp &lt;- \"small\"\n} else if(d_val &lt; 0.8) {\n  d_interp &lt;- \"medium\"\n} else {\n  d_interp &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", d_interp))\n\n\nEffect size interpretation: medium\n\n\n\n\n19.2.5 Assumptions for Paired t-Test\nPaired t-test assumes:\n\nIndependence of pairs (not within pairs)\nNormality of the differences (not the original measurements)\n\n\n\nCode\n# Check normality of DIFFERENCES\nggplot(milk_paired, aes(sample = difference)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Q-Q Plot of Differences\",\n       subtitle = \"Checking normality assumption for paired t-test\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles (Differences)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCode\n# Shapiro-Wilk test on differences\nshapiro_diff &lt;- shapiro.test(milk_paired$difference)\ncat(sprintf(\"\\nShapiro-Wilk test on differences: W = %.4f, p = %.3f\\n\",\n            shapiro_diff$statistic, shapiro_diff$p.value))\n\n\n\nShapiro-Wilk test on differences: W = 0.9514, p = 0.389\n\n\nThe differences appear approximately normal, so the paired t-test is appropriate.\n\n\n19.2.6 Reporting Paired t-Test Results\nExample write-up:\n\nMilk yield was measured in 20 dairy cows before and after 4 weeks of probiotic supplementation. A paired-samples t-test revealed that milk yield increased significantly after treatment (M_after = 34.1 kg/day, SD = 4.7) compared to before treatment (M_before = 31.1 kg/day, SD = 5.2), t(19) = 4.55, p &lt; 0.001, 95% CI [1.6, 4.3], d = 0.58. The probiotic supplement increased milk production by an average of 2.9 kg/day, representing a medium effect.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-scenarios",
    "href": "chapters/ch12-hypothesis_testing.html#sec-scenarios",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "20.1 Common Scenarios in Animal Science",
    "text": "20.1 Common Scenarios in Animal Science\n\n\n\n\n\n\n\n\nScenario\nTest Type\nExample\n\n\n\n\nCompare sample mean to historical value\nOne-sample\nIs current milk yield different from breed average?\n\n\nCompare two independent groups\nTwo-sample (independent)\nFeed A vs Feed B in randomly assigned pigs\n\n\nCompare before and after in same animals\nPaired\nWeight before and after medication in same cattle\n\n\nCompare littermates or twins\nPaired\nTwin calves assigned to different treatments\n\n\nCompare males vs females\nTwo-sample (independent)\nGrowth rate in male vs female lambs\n\n\nCompare left vs right (same animal)\nPaired\nUdder health in left vs right quarters\n\n\n\n\n\n\n\n\n\nWarningCommon Mistake: Treating Paired Data as Independent\n\n\n\nDon‚Äôt use a two-sample t-test when data are paired! This:\n\nWastes information (ignores pairing)\nReduces power (larger standard error)\nMay lead to incorrect conclusions\n\nIf measurements are connected (same subject, matched pairs, siblings), use a paired test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-checking-assumptions",
    "href": "chapters/ch12-hypothesis_testing.html#sec-checking-assumptions",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "21.1 Checking Assumptions in Practice",
    "text": "21.1 Checking Assumptions in Practice\n\n21.1.1 Normality\nVisual methods (preferred):\n\nHistograms: Look for roughly symmetric, bell-shaped distribution\nQQ plots: Points should fall close to the diagonal line\nDensity plots: Compare to normal curve overlay\n\nFormal tests (use with caution):\n\nShapiro-Wilk test: shapiro.test()\nKolmogorov-Smirnov test: ks.test()\n\n\n\n\n\n\n\nTipWhen Normality is Violated\n\n\n\nIf data are clearly non-normal:\n\nTransform the data: Log, square root, or Box-Cox transformation\nUse non-parametric tests: Wilcoxon rank-sum test (Mann-Whitney U) instead of two-sample t-test\nBootstrap confidence intervals: Resample to estimate sampling distribution\nRely on CLT: With large samples (n &gt; 30), t-tests are robust to non-normality\n\nMost common approach: If n &gt; 30 and no extreme outliers/skewness, proceed with t-test.\n\n\n\n\n21.1.2 Equal Variances\nVisual methods:\n\nCompare SD between groups (ratio &lt; 2 is usually fine)\nCompare boxplot heights and spreads\n\nFormal test:\n\nLevene‚Äôs test: car::leveneTest()\n\nDefault recommendation: Use Welch‚Äôs t-test (doesn‚Äôt assume equal variances) as your default. It‚Äôs robust and performs well even when variances are equal.\n\n\n21.1.3 Independence\nThis is the most important and least testable assumption.\nViolations occur when:\n\nObservations are clustered (e.g., multiple measurements per animal)\nTime series data with autocorrelation\nSpatial dependence (e.g., neighboring pens)\nPseudo-replication (treating subsamples as independent)\n\nSolutions:\n\nUse mixed models to account for clustering\nAggregate repeated measures appropriately\nDesign studies to ensure independence\n\n\n\n\n\n\n\nImportantIndependence Cannot Be Fixed Post-Hoc\n\n\n\nUnlike normality or equal variance assumptions, independence violations cannot be rescued with transformations or alternative tests. You must design your study correctly from the beginning.\nExample of pseudo-replication: Taking 5 blood samples from each of 4 cows and analyzing n=20 samples is wrong. The samples within each cow are not independent. The true n is 4 cows, not 20 samples.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-effect-sizes-practical",
    "href": "chapters/ch12-hypothesis_testing.html#sec-effect-sizes-practical",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "21.2 Effect Sizes and Practical Significance",
    "text": "21.2 Effect Sizes and Practical Significance\nStatistical significance ‚â† Practical significance\nA result can be statistically significant (p &lt; 0.05) but:\n\nThe effect size is tiny\nThe difference is not economically or biologically meaningful\nThe cost of implementation outweighs the benefit\n\nAlways report:\n\nP-value: Strength of evidence against H‚ÇÄ\nConfidence interval: Range of plausible values for the effect\nEffect size: Standardized measure of magnitude (Cohen‚Äôs d)\nMeans and SDs: Raw values for practical interpretation\n\n\n\n\n\n\n\nNoteCohen‚Äôs d Interpretation (Guidelines)\n\n\n\n\n\n\nd Value\nInterpretation\nMeaning\n\n\n\n\n0.0 - 0.2\nNegligible\nTrivial difference\n\n\n0.2 - 0.5\nSmall\nNoticeable to researchers\n\n\n0.5 - 0.8\nMedium\nVisible to the naked eye\n\n\n0.8+\nLarge\nObvious, practically meaningful\n\n\n\nRemember: These are rough guidelines. What matters is context:\n\nIn medicine, small effects can be life-saving\nIn agriculture, medium effects must be cost-effective\nIn behavior, large effects are rare and important",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-sample-size-practical",
    "href": "chapters/ch12-hypothesis_testing.html#sec-sample-size-practical",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "21.3 Sample Size and Power Considerations",
    "text": "21.3 Sample Size and Power Considerations\nBefore conducting your study, calculate required sample size:\n\n\nCode\n# Example: Planning a feed trial\n# Expected difference: 0.15 kg/day weight gain\n# Expected SD: 0.20 kg/day\n# Desired power: 0.80\n# Alpha: 0.05\n\npower_calc &lt;- power.t.test(\n  delta = 0.15,\n  sd = 0.20,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\"\n)\n\nprint(power_calc)\n\n\n\n     Two-sample t test power calculation \n\n              n = 28.89962\n          delta = 0.15\n             sd = 0.2\n      sig.level = 0.05\n          power = 0.8\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n\n\nCode\ncat(sprintf(\"\\nTo detect a difference of %.2f kg/day with 80%% power:\\n\", 0.15))\n\n\n\nTo detect a difference of 0.15 kg/day with 80% power:\n\n\nCode\ncat(sprintf(\"  Required sample size: %.0f animals per group\\n\", ceiling(power_calc$n)))\n\n\n  Required sample size: 29 animals per group\n\n\nCode\ncat(sprintf(\"  Total animals needed: %.0f\\n\", 2 * ceiling(power_calc$n)))\n\n\n  Total animals needed: 58\n\n\nTrade-offs:\n\nSmaller effect ‚Üí Need larger sample\nMore variable data ‚Üí Need larger sample\nHigher power ‚Üí Need larger sample\nLower Œ± ‚Üí Need larger sample",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-common-mistakes",
    "href": "chapters/ch12-hypothesis_testing.html#sec-common-mistakes",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "21.4 Common Mistakes and Pitfalls",
    "text": "21.4 Common Mistakes and Pitfalls\n\n21.4.1 1. Multiple Comparisons (p-hacking)\nProblem: Running many t-tests increases the chance of false positives.\nExample: Testing 20 different outcomes. With Œ± = 0.05, you expect 1 false positive even if there are no real effects.\nSolutions:\n\nBonferroni correction: Divide Œ± by number of tests (Œ±_adjusted = 0.05 / 20 = 0.0025)\nANOVA followed by post-hoc tests (covered next week)\nPre-specify primary outcomes before analysis\n\n\n\n21.4.2 2. Confusing Statistical and Practical Significance\nProblem: With large samples, tiny effects become ‚Äúsignificant‚Äù\nExample: Weight gain differs by 0.5 kg (p = 0.03) but costs $50 more per animal ‚Üí Not worth it!\nSolution: Always consider effect size and cost-benefit\n\n\n21.4.3 3. One-Tailed Tests Without Justification\nProblem: Using one-tailed tests to achieve p &lt; 0.05\nSolution: Use two-tailed tests by default. Only use one-tailed if:\n\nYou have strong theoretical reason\nEffect in opposite direction is impossible or meaningless\nYou pre-registered the hypothesis\n\n\n\n21.4.4 4. Ignoring Assumptions\nProblem: Running t-tests without checking assumptions\nSolution: Always check:\n\nNormality (QQ plots)\nEqual variances (visual or Levene‚Äôs test)\nIndependence (by design)\n\n\n\n21.4.5 5. Treating Paired Data as Independent\nProblem: Using two-sample t-test for paired data\nSolution: If same subjects measured twice or matched pairs ‚Üí Use paired t-test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-reporting",
    "href": "chapters/ch12-hypothesis_testing.html#sec-reporting",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "21.5 Reporting t-Test Results",
    "text": "21.5 Reporting t-Test Results\nEssential elements:\n\nDescriptive statistics: Means, SDs, sample sizes for each group\nTest used: One-sample, two-sample (Welch‚Äôs or Student‚Äôs), or paired\nTest statistic: t-value and degrees of freedom\nP-value: Exact value (not just ‚Äú&lt; 0.05‚Äù)\nConfidence interval: For the difference\nEffect size: Cohen‚Äôs d\nInterpretation: In context of the research question\n\nExample:\n\nMilk yield increased significantly after probiotic supplementation (M = 32.5 kg/day, SD = 4.8) compared to baseline (M = 30.1 kg/day, SD = 4.6), t(19) = 4.12, p &lt; 0.001, 95% CI [1.2, 3.6], d = 0.92. This represents a large effect and an average increase of 2.4 kg/day per cow.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-covered",
    "href": "chapters/ch12-hypothesis_testing.html#sec-covered",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "22.1 What We Covered",
    "text": "22.1 What We Covered\n\nHypothesis testing framework: Structured approach to evaluating claims about populations\nType I and Type II errors: Understanding false positives (Œ±) and false negatives (Œ≤)\nStatistical power: Probability of detecting real effects; influenced by effect size, sample size, Œ±, and variability\nOne-sample t-test: Comparing sample mean to known value\nTwo-sample t-test: Comparing means between independent groups\nPaired t-test: Comparing means for related/matched observations\nAssumptions: Normality, equal variances, independence\nEffect sizes: Cohen‚Äôs d for standardized effect magnitude\nPractical significance: Statistical significance doesn‚Äôt guarantee practical importance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-principles",
    "href": "chapters/ch12-hypothesis_testing.html#sec-principles",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "22.2 Key Principles",
    "text": "22.2 Key Principles\n\n\n\n\n\n\nImportantCore Principles of Hypothesis Testing\n\n\n\n\nP-values measure evidence, not truth: A p-value tells you how compatible your data is with H‚ÇÄ, not whether H‚ÇÄ is true or false\nEffect sizes matter more than p-values: Always report and interpret effect sizes and confidence intervals\nDesign determines analysis: Paired vs independent determines which test to use‚Äîthis cannot be changed after data collection\nAssumptions matter: Check them, but also understand t-tests are fairly robust (especially with larger samples)\nPower drives sample size: Calculate required sample size BEFORE collecting data\nContext is everything: Statistical significance without practical significance is meaningless",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-decision-framework",
    "href": "chapters/ch12-hypothesis_testing.html#sec-decision-framework",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "22.3 Decision Framework",
    "text": "22.3 Decision Framework\nWhen analyzing data:\n\nIdentify your research question: What are you comparing?\nChoose appropriate test:\n\nOne sample? ‚Üí One-sample t-test\nTwo independent groups? ‚Üí Two-sample t-test\nPaired/matched data? ‚Üí Paired t-test\n\nCheck assumptions: Normality (QQ plots), equal variances (visual or Levene‚Äôs), independence (by design)\nConduct test: Calculate t-statistic and p-value\nCalculate effect size: Cohen‚Äôs d for standardized magnitude\nInterpret in context: Consider both statistical and practical significance\nReport completely: Means, SDs, n, test statistics, p-values, CIs, effect sizes",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-next-week",
    "href": "chapters/ch12-hypothesis_testing.html#sec-next-week",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "22.4 Next Week Preview",
    "text": "22.4 Next Week Preview\nWeek 5: Analysis of Variance (ANOVA)\n\nComparing more than two groups (extension of t-tests)\nUnderstanding variance partitioning (between-group vs within-group)\nPost-hoc tests (Tukey HSD, Bonferroni)\nMultiple comparisons problem\nWhen to use ANOVA vs multiple t-tests\n\n\n\n\n\n\n\nNoteComing Full Circle\n\n\n\nANOVA is mathematically equivalent to the t-test when comparing two groups. Next week, we‚Äôll see how to generalize hypothesis testing to multiple groups while controlling Type I error rates.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-practice-1",
    "href": "chapters/ch12-hypothesis_testing.html#sec-practice-1",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "23.1 Problem 1: One-Sample Scenario",
    "text": "23.1 Problem 1: One-Sample Scenario\nA poultry researcher measures egg weight from 30 hens. The breed standard is 62 grams. The sample mean is 64.5 grams with SD = 5.2 grams. Is there evidence that egg weight differs from the breed standard?\nTasks:\n\nState the null and alternative hypotheses\nConduct a one-sample t-test\nCalculate Cohen‚Äôs d\nInterpret the results\n\n\n\nCode\n# Your code here\nset.seed(999)\negg_weights &lt;- rnorm(30, mean = 64.5, sd = 5.2)\n\n# a) Hypotheses: H0: Œº = 62, H1: Œº ‚â† 62\n\n# b) Test\ntest1 &lt;- t.test(egg_weights, mu = 62)\nprint(test1)\n\n# c) Effect size\nd1 &lt;- (mean(egg_weights) - 62) / sd(egg_weights)\ncat(sprintf(\"Cohen's d: %.3f\\n\", d1))\n\n# d) Interpret...",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-practice-2",
    "href": "chapters/ch12-hypothesis_testing.html#sec-practice-2",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "23.2 Problem 2: Two-Sample Scenario",
    "text": "23.2 Problem 2: Two-Sample Scenario\nA beef cattle trial compares average daily gain (ADG) for two protein sources. Soybean meal (n=25): M=1.45 kg/day, SD=0.22. Corn gluten (n=25): M=1.38 kg/day, SD=0.19. Is there a significant difference?\nTasks:\n\nState hypotheses\nCheck equal variance assumption\nConduct two-sample t-test (Welch‚Äôs)\nCalculate effect size\nProvide practical interpretation",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-practice-3",
    "href": "chapters/ch12-hypothesis_testing.html#sec-practice-3",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "23.3 Problem 3: Paired Scenario",
    "text": "23.3 Problem 3: Paired Scenario\nA veterinarian measures body temperature in 15 calves before and after administering an anti-inflammatory drug. Should you use a paired or unpaired test? Why? What are the hypotheses?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-practice-4",
    "href": "chapters/ch12-hypothesis_testing.html#sec-practice-4",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "23.4 Problem 4: Power Analysis",
    "text": "23.4 Problem 4: Power Analysis\nYou‚Äôre planning a study to detect a 10% difference in weaning weight (Expected means: 250 kg vs 275 kg, SD = 30 kg). How many calves do you need per group to achieve 80% power with Œ± = 0.05?\n\n\nCode\n# Your code here\npower.t.test(\n  delta = 25,\n  sd = 30,\n  sig.level = 0.05,\n  power = 0.80,\n  type = \"two.sample\"\n)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-practice-5",
    "href": "chapters/ch12-hypothesis_testing.html#sec-practice-5",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "23.5 Problem 5: Assumption Checking",
    "text": "23.5 Problem 5: Assumption Checking\nYou‚Äôve collected data from two groups but aren‚Äôt sure if assumptions are met. What plots would you create? What tests would you run? What would you do if assumptions are violated?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-r-functions",
    "href": "chapters/ch12-hypothesis_testing.html#sec-r-functions",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "24.1 R Functions Covered",
    "text": "24.1 R Functions Covered\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nt.test()\nbase\nOne-sample, two-sample, and paired t-tests\n\n\npower.t.test()\nbase\nPower and sample size calculations\n\n\nshapiro.test()\nbase\nTest normality\n\n\nleveneTest()\ncar\nTest equality of variances\n\n\ncohen.d()\neffsize\nCalculate Cohen‚Äôs d effect size\n\n\ntidy()\nbroom\nTidy statistical output",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-further-reading",
    "href": "chapters/ch12-hypothesis_testing.html#sec-further-reading",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "24.2 Further Reading",
    "text": "24.2 Further Reading\n\nCumming, G. (2012). Understanding the New Statistics. Excellent on effect sizes and confidence intervals\nCohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences. Classic reference on power\nLakens, D. (2013). ‚ÄúCalculating and reporting effect sizes to facilitate cumulative science.‚Äù Frontiers in Psychology.\nAmerican Statistical Association (2016). ‚ÄúStatement on P-values and Statistical Significance.‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch12-hypothesis_testing.html#sec-online-resources",
    "href": "chapters/ch12-hypothesis_testing.html#sec-online-resources",
    "title": "12¬† Week 12: Hypothesis Testing Fundamentals",
    "section": "24.3 Online Resources",
    "text": "24.3 Online Resources\n\nR for Data Science (2e): https://r4ds.hadley.nz/\nStatistical Thinking: https://www.fharrell.com/\nStatQuest Videos (YouTube): Excellent visual explanations of t-tests and power\n\n\nEnd of Week 4 Lecture\nNext week: Analysis of Variance (ANOVA) - extending t-tests to multiple groups!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Week 12: Hypothesis Testing Fundamentals</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html",
    "href": "chapters/ch13-anova.html",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "",
    "text": "14 Introduction\nYou‚Äôre a cattle nutritionist comparing four different feed formulations for finishing steers. After a 90-day trial with 15 steers per feed type, you‚Äôve measured average daily gain (ADG) for each animal. Now you need to determine: Do these feed formulations produce different growth rates?\nYour first instinct might be to run t-tests comparing all pairs of feeds: - Feed A vs Feed B - Feed A vs Feed C - Feed A vs Feed D - Feed B vs Feed C - Feed B vs Feed D - Feed C vs Feed D\nThat‚Äôs 6 t-tests for just 4 groups! But this approach has a critical problem: each test carries a 5% false positive rate, and these errors accumulate. By the time you‚Äôve run 6 tests, your chance of finding at least one ‚Äúsignificant‚Äù result by pure chance has jumped to about 26%.\nThis is where Analysis of Variance (ANOVA) comes in. ANOVA allows us to test for differences among multiple groups with a single test, maintaining control over our Type I error rate.\nKey Questions We‚Äôll Address:\nBy the end of this lecture, you‚Äôll be able to conduct one-way ANOVA using modern R approaches (lm() and car::Anova()), check assumptions, perform post-hoc tests, and calculate effect sizes.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-why-not-t-tests",
    "href": "chapters/ch13-anova.html#sec-why-not-t-tests",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "15.1 Why Not Just Run Multiple t-Tests?",
    "text": "15.1 Why Not Just Run Multiple t-Tests?\nLet‚Äôs simulate what happens when we run multiple pairwise t-tests on groups that actually have no real differences:\n\n\nCode\n# Simulate 5 groups with NO true differences (all Œº = 100, œÉ = 15)\nset.seed(123)\nn_per_group &lt;- 20\nn_groups &lt;- 5\n\n# Create data where H0 is TRUE (all groups have same mean)\ngroups_data &lt;- tibble(\n  group = rep(LETTERS[1:n_groups], each = n_per_group),\n  value = rnorm(n_per_group * n_groups, mean = 100, sd = 15)\n)\n\n# Visualize\nggplot(groups_data, aes(x = group, y = value, fill = group)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = 100, linetype = \"dashed\", color = \"red\", linewidth = 1) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Five Groups with NO True Differences\",\n       subtitle = \"All groups sampled from same distribution (Œº = 100, œÉ = 15)\",\n       x = \"Group\",\n       y = \"Value\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nThese groups look slightly different due to random sampling, but they were all drawn from the same population. Now let‚Äôs run all possible pairwise t-tests:\n\n\nCode\n# All pairwise comparisons\ngroup_pairs &lt;- combn(LETTERS[1:n_groups], 2, simplify = FALSE)\n\n# Run t-tests\npairwise_results &lt;- map_dfr(group_pairs, function(pair) {\n  group1_data &lt;- groups_data %&gt;% filter(group == pair[1]) %&gt;% pull(value)\n  group2_data &lt;- groups_data %&gt;% filter(group == pair[2]) %&gt;% pull(value)\n\n  test &lt;- t.test(group1_data, group2_data)\n\n  tibble(\n    comparison = paste(pair[1], \"vs\", pair[2]),\n    t_stat = test$statistic,\n    p_value = test$p.value,\n    significant = p_value &lt; 0.05\n  )\n})\n\nknitr::kable(pairwise_results, digits = 4, align = 'lccc',\n             caption = \"Pairwise t-Tests for Five Groups (H‚ÇÄ is TRUE)\")\n\n\n\nPairwise t-Tests for Five Groups (H‚ÇÄ is TRUE)\n\n\ncomparison\nt_stat\np_value\nsignificant\n\n\n\n\nA vs B\n0.6746\n0.5041\nFALSE\n\n\nA vs C\n0.1151\n0.9089\nFALSE\n\n\nA vs D\n0.8501\n0.4006\nFALSE\n\n\nA vs E\n-0.8170\n0.4192\nFALSE\n\n\nB vs C\n-0.5568\n0.5810\nFALSE\n\n\nB vs D\n0.2401\n0.8116\nFALSE\n\n\nB vs E\n-1.6254\n0.1123\nFALSE\n\n\nC vs D\n0.7417\n0.4628\nFALSE\n\n\nC vs E\n-0.9486\n0.3490\nFALSE\n\n\nD vs E\n-1.7317\n0.0916\nFALSE\n\n\n\n\n\nCode\n# How many false positives?\nn_comparisons &lt;- nrow(pairwise_results)\nn_false_positives &lt;- sum(pairwise_results$significant)\n\ncat(sprintf(\"\\nNumber of comparisons: %d\\n\", n_comparisons))\n\n\n\nNumber of comparisons: 10\n\n\nCode\ncat(sprintf(\"Number of 'significant' results: %d (%.1f%%)\\n\",\n            n_false_positives, 100 * n_false_positives / n_comparisons))\n\n\nNumber of 'significant' results: 0 (0.0%)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-family-wise-error",
    "href": "chapters/ch13-anova.html#sec-family-wise-error",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "15.2 Family-Wise Error Rate",
    "text": "15.2 Family-Wise Error Rate\nWhen we run multiple tests, we need to distinguish between:\n\nPer-comparison error rate: The Œ± level for a single test (typically 0.05)\nFamily-wise error rate (FWER): The probability of making at least one Type I error across all tests\n\nThe probability of making no Type I errors in k independent tests is:\n\\[P(\\text{no Type I errors}) = (1 - \\alpha)^k\\]\nTherefore, the probability of making at least one Type I error is:\n\\[\\text{FWER} = 1 - (1 - \\alpha)^k\\]\nLet‚Äôs calculate this for different numbers of comparisons:\n\n\nCode\n# Calculate FWER for different numbers of groups\nn_groups_range &lt;- 2:10\ncomparisons &lt;- choose(n_groups_range, 2)  # Number of pairwise comparisons\nalpha &lt;- 0.05\n\nfwer_data &lt;- tibble(\n  n_groups = n_groups_range,\n  n_comparisons = comparisons,\n  fwer = 1 - (1 - alpha)^comparisons,\n  fwer_percent = fwer * 100\n)\n\nknitr::kable(fwer_data, digits = 1, align = 'cccc',\n             col.names = c(\"Groups\", \"Comparisons\", \"FWER\", \"FWER (%)\"),\n             caption = \"Family-Wise Error Rate Growth with Multiple Comparisons\")\n\n\n\nFamily-Wise Error Rate Growth with Multiple Comparisons\n\n\nGroups\nComparisons\nFWER\nFWER (%)\n\n\n\n\n2\n1\n0.1\n5.0\n\n\n3\n3\n0.1\n14.3\n\n\n4\n6\n0.3\n26.5\n\n\n5\n10\n0.4\n40.1\n\n\n6\n15\n0.5\n53.7\n\n\n7\n21\n0.7\n65.9\n\n\n8\n28\n0.8\n76.2\n\n\n9\n36\n0.8\n84.2\n\n\n10\n45\n0.9\n90.1\n\n\n\n\n\nCode\n# Visualize\nggplot(fwer_data, aes(x = n_groups, y = fwer_percent)) +\n  geom_line(linewidth = 1.2, color = \"darkred\") +\n  geom_point(size = 3, color = \"darkred\") +\n  geom_hline(yintercept = 5, linetype = \"dashed\", color = \"steelblue\") +\n  annotate(\"text\", x = 8, y = 7, label = \"Per-test Œ± = 5%\", color = \"steelblue\") +\n  scale_x_continuous(breaks = 2:10) +\n  labs(title = \"Family-Wise Error Rate Explodes with Multiple Groups\",\n       subtitle = \"Probability of at least one false positive when all null hypotheses are true\",\n       x = \"Number of Groups\",\n       y = \"Family-Wise Error Rate (%)\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nKey insight: With just 5 groups (10 comparisons), the FWER jumps to 40%! This means we have a 40% chance of declaring at least one comparison ‚Äúsignificant‚Äù even when no true differences exist.\n\n\n\n\n\n\nImportantANOVA: The Solution\n\n\n\nANOVA provides a single omnibus test that asks: ‚ÄúIs there any difference among these groups?‚Äù\nBy conducting one test at Œ± = 0.05, we maintain a 5% false positive rate regardless of how many groups we‚Äôre comparing.\nIf ANOVA is significant, then we conduct post-hoc tests with appropriate adjustments to identify which specific groups differ.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-when-anova",
    "href": "chapters/ch13-anova.html#sec-when-anova",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "15.3 When to Use ANOVA vs t-Tests",
    "text": "15.3 When to Use ANOVA vs t-Tests\nUse t-tests when: - Comparing exactly 2 groups - One-sample or paired designs\nUse ANOVA when: - Comparing 3 or more groups - Testing for any difference among groups - Want to control family-wise error rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-variance-partition",
    "href": "chapters/ch13-anova.html#sec-variance-partition",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "16.1 The Core Idea: Partitioning Variance",
    "text": "16.1 The Core Idea: Partitioning Variance\nANOVA works by partitioning total variance into two components:\n\nBetween-group variance: Variability of group means around the overall mean\nWithin-group variance: Variability of individual observations around their group means\n\nIf the between-group variance is much larger than the within-group variance, this suggests the groups differ.\n\n16.1.1 Visual Intuition\nLet‚Äôs create two scenarios to build intuition:\n\n\nCode\n# Scenario 1: Large between-group differences (H0 is FALSE)\nscenario1 &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  value = c(rnorm(20, 80, 10), rnorm(20, 100, 10), rnorm(20, 120, 10))\n)\n\n# Scenario 2: Small between-group differences (H0 is TRUE)\nscenario2 &lt;- tibble(\n  group = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  value = c(rnorm(20, 98, 15), rnorm(20, 100, 15), rnorm(20, 102, 15))\n)\n\n# Calculate group means\nscenario1_means &lt;- scenario1 %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_val = mean(value), .groups = 'drop')\n\nscenario2_means &lt;- scenario2 %&gt;%\n  group_by(group) %&gt;%\n  summarise(mean_val = mean(value), .groups = 'drop')\n\n# Overall means\noverall_mean1 &lt;- mean(scenario1$value)\noverall_mean2 &lt;- mean(scenario2$value)\n\n# Plot Scenario 1\np1 &lt;- ggplot(scenario1, aes(x = group, y = value, color = group)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = overall_mean1, linetype = \"dashed\",\n             color = \"black\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 5, shape = 18, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18,\n               aes(color = group)) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 3.3, y = overall_mean1, label = \"Overall\\nmean\",\n           size = 3, hjust = 0) +\n  labs(title = \"Scenario 1: Large Between-Group Variance\",\n       subtitle = \"Group means far from overall mean ‚Üí F will be large\",\n       x = \"Group\", y = \"Value\") +\n  theme(legend.position = \"none\")\n\n# Plot Scenario 2\np2 &lt;- ggplot(scenario2, aes(x = group, y = value, color = group)) +\n  geom_jitter(width = 0.2, alpha = 0.5, size = 2) +\n  geom_hline(yintercept = overall_mean2, linetype = \"dashed\",\n             color = \"black\", linewidth = 1) +\n  stat_summary(fun = mean, geom = \"point\", size = 5, shape = 18, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 4, shape = 18,\n               aes(color = group)) +\n  scale_color_brewer(palette = \"Set1\") +\n  annotate(\"text\", x = 3.3, y = overall_mean2, label = \"Overall\\nmean\",\n           size = 3, hjust = 0) +\n  labs(title = \"Scenario 2: Small Between-Group Variance\",\n       subtitle = \"Group means close to overall mean ‚Üí F will be small\",\n       x = \"Group\", y = \"Value\") +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey observation: - Scenario 1: Group means (colored diamonds) are far from overall mean (black line) ‚Üí large between-group variance - Scenario 2: Group means are close to overall mean ‚Üí small between-group variance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-f-statistic",
    "href": "chapters/ch13-anova.html#sec-f-statistic",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "16.2 The F-Statistic",
    "text": "16.2 The F-Statistic\nThe F-statistic is the ratio of between-group variance to within-group variance:\n\\[F = \\frac{\\text{Between-group variance (MS}_{\\text{between}}\\text{)}}{\\text{Within-group variance (MS}_{\\text{within}}\\text{)}}\\]\nWhere: - MS = Mean Square (variance estimate) - A large F-ratio suggests group means differ more than expected by chance - F follows an F-distribution with df‚ÇÅ (between) and df‚ÇÇ (within)\n\n\n\n\n\n\nNoteConnection to t-Test\n\n\n\nWhen comparing exactly two groups, ANOVA is mathematically equivalent to a t-test:\n\\[F = t^2\\]\nANOVA with 2 groups will give the same p-value as a two-sample t-test!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-anova-table",
    "href": "chapters/ch13-anova.html#sec-anova-table",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "16.3 ANOVA Table Components",
    "text": "16.3 ANOVA Table Components\nThe ANOVA table summarizes the variance partitioning:\n\n\n\n\n\n\n\n\n\n\n\nSource\nSum of Squares (SS)\ndf\nMean Square (MS)\nF\np-value\n\n\n\n\nBetween\nSS_between\nk - 1\nSS_between / df_between\nMS_between / MS_within\nP(F &gt; F_obs)\n\n\nWithin\nSS_within\nN - k\nSS_within / df_within\n\n\n\n\nTotal\nSS_total\nN - 1\n\n\n\n\n\n\nWhere: - k = number of groups - N = total sample size - SS_between: Sum of squared differences between group means and overall mean - SS_within: Sum of squared differences between observations and their group means - df = degrees of freedom\nLet‚Äôs calculate ANOVA for our two scenarios:\n\n\nCode\n# Fit models\nmodel1 &lt;- lm(value ~ group, data = scenario1)\nmodel2 &lt;- lm(value ~ group, data = scenario2)\n\n# ANOVA tables using car::Anova (Type II)\ncat(\"Scenario 1: Large Between-Group Variance\\n\")\n\n\nScenario 1: Large Between-Group Variance\n\n\nCode\ncat(\"==========================================\\n\")\n\n\n==========================================\n\n\nCode\nAnova(model1, type = \"II\")\n\n\nAnova Table (Type II tests)\n\nResponse: value\n          Sum Sq Df F value    Pr(&gt;F)    \ngroup      17393  2  93.071 &lt; 2.2e-16 ***\nResiduals   5326 57                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nScenario 2: Small Between-Group Variance\\n\")\n\n\n\n\nScenario 2: Small Between-Group Variance\n\n\nCode\ncat(\"==========================================\\n\")\n\n\n==========================================\n\n\nCode\nAnova(model2, type = \"II\")\n\n\nAnova Table (Type II tests)\n\nResponse: value\n           Sum Sq Df F value Pr(&gt;F)\ngroup        40.4  2  0.0881 0.9158\nResiduals 13081.2 57               \n\n\nInterpretation: - Scenario 1: F = 93.1, p &lt; 0.001 ‚Üí Strong evidence of group differences - Scenario 2: F = 0.09, p = 0.916 ‚Üí No evidence of group differences",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-why-lm",
    "href": "chapters/ch13-anova.html#sec-why-lm",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "17.1 Why Use lm() Instead of aov()?",
    "text": "17.1 Why Use lm() Instead of aov()?\nIn R, you can fit ANOVA using either aov() or lm(). We‚Äôll use lm() because:\n\nMore flexible: Same framework works for t-tests, ANOVA, and regression\nPrepares for regression: Weeks 7-8 will use lm() for continuous predictors\nModern approach: Works seamlessly with car::Anova(), emmeans, and other packages\nBetter diagnostics: Standard regression diagnostic plots apply\n\n\n\n\n\n\n\nTipANOVA IS Regression\n\n\n\nOne-way ANOVA is actually a special case of linear regression where the predictor is categorical. R internally converts group labels to dummy variables.\nThis connection will become clearer in Weeks 7-8!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-fitting-lm",
    "href": "chapters/ch13-anova.html#sec-fitting-lm",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "17.2 Fitting ANOVA with lm()",
    "text": "17.2 Fitting ANOVA with lm()\nLet‚Äôs work through a complete example using the cattle feed trial scenario:\n\n\nCode\n# Simulate feed trial data\n# 4 feed types, 15 steers per feed, 90-day trial measuring average daily gain (kg)\nset.seed(456)\n\nfeed_data &lt;- tibble(\n  steer_id = 1:60,\n  feed_type = factor(rep(c(\"A\", \"B\", \"C\", \"D\"), each = 15)),\n  # Simulated ADG with true differences:\n  # A: 1.45, B: 1.55, C: 1.50, D: 1.70 kg/day\n  adg_kg = c(\n    rnorm(15, mean = 1.45, sd = 0.18),  # Feed A\n    rnorm(15, mean = 1.55, sd = 0.18),  # Feed B\n    rnorm(15, mean = 1.50, sd = 0.18),  # Feed C\n    rnorm(15, mean = 1.70, sd = 0.18)   # Feed D\n  )\n)\n\n# Summary statistics\nfeed_summary &lt;- feed_data %&gt;%\n  group_by(feed_type) %&gt;%\n  summarise(\n    n = n(),\n    mean_adg = mean(adg_kg),\n    sd_adg = sd(adg_kg),\n    se_adg = sd_adg / sqrt(n),\n    .groups = 'drop'\n  )\n\nknitr::kable(feed_summary, digits = 3, align = 'lcccc',\n             col.names = c(\"Feed\", \"n\", \"Mean ADG\", \"SD\", \"SE\"),\n             caption = \"Summary Statistics: Average Daily Gain by Feed Type\")\n\n\n\nSummary Statistics: Average Daily Gain by Feed Type\n\n\nFeed\nn\nMean ADG\nSD\nSE\n\n\n\n\nA\n15\n1.471\n0.189\n0.049\n\n\nB\n15\n1.612\n0.231\n0.060\n\n\nC\n15\n1.502\n0.175\n0.045\n\n\nD\n15\n1.754\n0.125\n0.032\n\n\n\n\n\n\n17.2.1 Step 1: Visualize the Data\nAlways visualize before testing!\n\n\nCode\n# Create comprehensive visualization\np1 &lt;- ggplot(feed_data, aes(x = feed_type, y = adg_kg, fill = feed_type)) +\n  geom_boxplot(alpha = 0.6, outlier.shape = NA) +\n  geom_jitter(width = 0.15, alpha = 0.5, size = 2) +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3,\n               fill = \"darkred\", color = \"darkred\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Average Daily Gain by Feed Type\",\n       subtitle = \"Diamonds show group means\",\n       x = \"Feed Type\",\n       y = \"Average Daily Gain (kg/day)\") +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(feed_summary, aes(x = feed_type, y = mean_adg, fill = feed_type)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = mean_adg - se_adg * 1.96,\n                    ymax = mean_adg + se_adg * 1.96),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.2f\", mean_adg)),\n            vjust = -3, fontface = \"bold\", size = 3.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Mean ADG with 95% Confidence Intervals\",\n       x = \"Feed Type\",\n       y = \"Mean ADG (kg/day)\") +\n  ylim(0, 2) +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nVisual assessment: Feed D appears to produce the highest gain, while Feed A appears lowest. But are these differences statistically significant?\n\n\n17.2.2 Step 2: Fit the Linear Model\n\n\nCode\n# Fit ANOVA as a linear model\nfeed_model &lt;- lm(adg_kg ~ feed_type, data = feed_data)\n\n# Model summary\nsummary(feed_model)\n\n\n\nCall:\nlm(formula = adg_kg ~ feed_type, data = feed_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.37129 -0.12107  0.01025  0.10413  0.37211 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.47125    0.04744  31.016  &lt; 2e-16 ***\nfeed_typeB   0.14093    0.06708   2.101   0.0402 *  \nfeed_typeC   0.03084    0.06708   0.460   0.6475    \nfeed_typeD   0.28287    0.06708   4.217 9.14e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.1837 on 56 degrees of freedom\nMultiple R-squared:  0.2806,    Adjusted R-squared:  0.2421 \nF-statistic: 7.282 on 3 and 56 DF,  p-value: 0.0003302\n\n\nKey points from summary: - R-squared: Proportion of variance explained by feed type - Coefficients show differences from reference group (Feed A) - But we want the ANOVA table‚Ä¶\n\n\n17.2.3 Step 3: ANOVA Table with car::Anova()\n\n\nCode\n# ANOVA table using car package (Type II SS)\nlibrary(car)\nfeed_anova &lt;- Anova(feed_model, type = \"II\")\n\nprint(feed_anova)\n\n\nAnova Table (Type II tests)\n\nResponse: adg_kg\n           Sum Sq Df F value    Pr(&gt;F)    \nfeed_type 0.73731  3  7.2816 0.0003302 ***\nResiduals 1.89012 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\n# Extract F and p-value\nf_stat &lt;- feed_anova$`F value`[1]\np_val &lt;- feed_anova$`Pr(&gt;F)`[1]\n\ncat(sprintf(\"\\nANOVA Results: F(%d, %d) = %.2f, p %s\\n\",\n            feed_anova$Df[1], feed_anova$Df[2], f_stat,\n            ifelse(p_val &lt; 0.001, \"&lt; 0.001\", sprintf(\"= %.4f\", p_val))))\n\n\n\nANOVA Results: F(3, 56) = 7.28, p &lt; 0.001\n\n\n\n\n\n\n\n\nImportantWhat This ANOVA Tells Us\n\n\n\nA significant F-test (p &lt; 0.05) tells us:\n‚úì There IS evidence that at least one feed type differs from the others\n‚úó It does NOT tell us which specific feeds differ\nTo identify specific differences, we need post-hoc tests (covered in Section 7).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-why-ss-matters",
    "href": "chapters/ch13-anova.html#sec-why-ss-matters",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "18.1 Why This Matters",
    "text": "18.1 Why This Matters\nWhen you have balanced designs (equal sample sizes), all methods give the same results. But with unbalanced designs (common in real research!), the choice matters.\nKey differences:\n\nType I (Sequential): Tests each term after previous terms in the formula (order-dependent)\nType II (Marginal): Tests each term after all others (recommended for one-way ANOVA)\nType III (Orthogonal): Tests each term adjusted for all others (common in SAS, SPSS)\n\n\n\n\n\n\n\nNoteRecommendation for One-Way ANOVA\n\n\n\nFor one-way ANOVA (single factor), Type II and Type III give identical results.\nUse car::Anova(model, type = \"II\") as your default.\nType II/III differences become important with multiple factors or interactions (beyond this course).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-demonstrate-ss",
    "href": "chapters/ch13-anova.html#sec-demonstrate-ss",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "18.2 Demonstrating the Difference",
    "text": "18.2 Demonstrating the Difference\nLet‚Äôs create an unbalanced version of our feed trial and compare methods:\n\n\nCode\n# Create unbalanced design\nset.seed(789)\nfeed_data_unbal &lt;- tibble(\n  feed_type = c(rep(\"A\", 12), rep(\"B\", 15), rep(\"C\", 18), rep(\"D\", 15)),\n  adg_kg = c(\n    rnorm(12, mean = 1.45, sd = 0.18),\n    rnorm(15, mean = 1.55, sd = 0.18),\n    rnorm(18, mean = 1.50, sd = 0.18),\n    rnorm(15, mean = 1.70, sd = 0.18)\n  )\n) %&gt;%\n  mutate(feed_type = factor(feed_type))\n\n# Sample sizes\nfeed_data_unbal %&gt;%\n  count(feed_type) %&gt;%\n  knitr::kable(col.names = c(\"Feed Type\", \"n\"),\n               caption = \"Unbalanced Design Sample Sizes\")\n\n\n\nUnbalanced Design Sample Sizes\n\n\nFeed Type\nn\n\n\n\n\nA\n12\n\n\nB\n15\n\n\nC\n18\n\n\nD\n15\n\n\n\n\n\nCode\n# Fit model\nmodel_unbal &lt;- lm(adg_kg ~ feed_type, data = feed_data_unbal)\n\n# Compare Type I, II, and III\ncat(\"Type I (Sequential) Sums of Squares:\\n\")\n\n\nType I (Sequential) Sums of Squares:\n\n\nCode\ncat(\"=====================================\\n\")\n\n\n=====================================\n\n\nCode\nprint(anova(model_unbal))  # Base R uses Type I\n\n\nAnalysis of Variance Table\n\nResponse: adg_kg\n          Df  Sum Sq  Mean Sq F value    Pr(&gt;F)    \nfeed_type  3 0.80928 0.269760  9.8032 2.698e-05 ***\nResiduals 56 1.54098 0.027517                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nType II Sums of Squares:\\n\")\n\n\n\n\nType II Sums of Squares:\n\n\nCode\ncat(\"========================\\n\")\n\n\n========================\n\n\nCode\nprint(Anova(model_unbal, type = \"II\"))\n\n\nAnova Table (Type II tests)\n\nResponse: adg_kg\n           Sum Sq Df F value    Pr(&gt;F)    \nfeed_type 0.80928  3  9.8032 2.698e-05 ***\nResiduals 1.54098 56                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\n\\nType III Sums of Squares:\\n\")\n\n\n\n\nType III Sums of Squares:\n\n\nCode\ncat(\"=========================\\n\")\n\n\n=========================\n\n\nCode\nprint(Anova(model_unbal, type = \"III\"))\n\n\nAnova Table (Type III tests)\n\nResponse: adg_kg\n             Sum Sq Df  F value    Pr(&gt;F)    \n(Intercept) 22.7191  1 825.6263 &lt; 2.2e-16 ***\nfeed_type    0.8093  3   9.8032 2.698e-05 ***\nResiduals    1.5410 56                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor one-way ANOVA, Type II and Type III are identical. Type I may differ slightly with unbalanced designs.\nBottom line: For this course (one-way ANOVA), always use:\ncar::Anova(model, type = \"II\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-residuals-not-raw",
    "href": "chapters/ch13-anova.html#sec-residuals-not-raw",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "19.1 Important: Test Residuals, Not Raw Data!",
    "text": "19.1 Important: Test Residuals, Not Raw Data!\n\n\n\n\n\n\nWarningCommon Mistake\n\n\n\nDon‚Äôt test normality of raw data within each group. Instead, test normality of residuals from the model.\nWhy? ANOVA assumes that deviations from group means are normally distributed, not that each group is normal independently.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-diagnostic-plots",
    "href": "chapters/ch13-anova.html#sec-diagnostic-plots",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "19.2 Diagnostic Plots",
    "text": "19.2 Diagnostic Plots\nThe lm() object provides four standard diagnostic plots:\n\n\nCode\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(feed_model, which = 1:4)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\nInterpreting diagnostic plots:\n\nResiduals vs Fitted: Should show random scatter (no pattern)\n\nPattern suggests non-constant variance or non-linearity\n\nNormal Q-Q: Points should follow the diagonal line\n\nDeviations suggest non-normality\n\nScale-Location: Should show random scatter\n\nTests homogeneity of variance (constant spread)\n\nResiduals vs Leverage: Identifies influential points\n\nPoints outside Cook‚Äôs distance contours are influential",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-assumption-checks",
    "href": "chapters/ch13-anova.html#sec-assumption-checks",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "19.3 Assumption Checks",
    "text": "19.3 Assumption Checks\n\n19.3.1 1. Independence\nCannot be tested statistically - depends on study design.\nViolations occur when: - Repeated measures on same subjects (use mixed models) - Clustered data (animals in same pen, fields, etc.) - Time series (autocorrelation)\nSolution: Design study properly or use appropriate models (mixed models, GEE).\n\n\n19.3.2 2. Normality of Residuals\nVisual check: Q-Q plot\n\n\nCode\n# Extract residuals\nfeed_residuals &lt;- residuals(feed_model)\n\n# Q-Q plot\nggplot(tibble(residuals = feed_residuals), aes(sample = residuals)) +\n  stat_qq(color = \"steelblue\", size = 2) +\n  stat_qq_line(color = \"red\", linetype = \"dashed\", linewidth = 1) +\n  labs(title = \"Normal Q-Q Plot of Residuals\",\n       subtitle = \"Points should fall close to the line\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nFormal test: Shapiro-Wilk\n\n\nCode\nshapiro_result &lt;- shapiro.test(feed_residuals)\n\ncat(sprintf(\"Shapiro-Wilk Test for Normality\\n\"))\n\n\nShapiro-Wilk Test for Normality\n\n\nCode\ncat(sprintf(\"W = %.4f, p-value = %.4f\\n\",\n            shapiro_result$statistic, shapiro_result$p.value))\n\n\nW = 0.9869, p-value = 0.7660\n\n\nCode\nif(shapiro_result$p.value &gt; 0.05) {\n  cat(\"‚Üí No evidence against normality (p &gt; 0.05)\\n\")\n} else {\n  cat(\"‚Üí Some evidence against normality (p &lt; 0.05)\\n\")\n  cat(\"  Consider: transformation, non-parametric test, or rely on robustness\\n\")\n}\n\n\n‚Üí No evidence against normality (p &gt; 0.05)\n\n\n\n\n\n\n\n\nNoteANOVA is Robust to Normality Violations\n\n\n\nWith moderate to large sample sizes and no extreme skewness, ANOVA is fairly robust to violations of normality, especially with balanced designs.\nIf violated: - Try transformations (log, square root, Box-Cox) - Use Kruskal-Wallis test (non-parametric alternative) - Bootstrap confidence intervals\n\n\n\n\n19.3.3 3. Homogeneity of Variance\nVisual check: Boxplots\n\n\nCode\nggplot(feed_data, aes(x = feed_type, y = adg_kg)) +\n  geom_boxplot(aes(fill = feed_type), alpha = 0.5) +\n  stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1),\n               geom = \"errorbar\", width = 0.3, linewidth = 1, color = \"darkred\") +\n  stat_summary(fun = mean, geom = \"point\", size = 3, color = \"darkred\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Checking Homogeneity of Variance\",\n       subtitle = \"Red bars show mean ¬± SD; similar heights suggest equal variances\",\n       x = \"Feed Type\",\n       y = \"Average Daily Gain (kg/day)\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nFormal test: Levene‚Äôs Test\n\n\nCode\nlevene_result &lt;- leveneTest(feed_model)\n\nprint(levene_result)\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  3   2.119 0.1081\n      56               \n\n\nCode\ncat(sprintf(\"\\nLevene's Test: F(%d, %d) = %.3f, p = %.4f\\n\",\n            levene_result$Df[1], levene_result$Df[2],\n            levene_result$`F value`[1], levene_result$`Pr(&gt;F)`[1]))\n\n\n\nLevene's Test: F(3, 56) = 2.119, p = 0.1081\n\n\nCode\nif(levene_result$`Pr(&gt;F)`[1] &gt; 0.05) {\n  cat(\"‚Üí No evidence of unequal variances (p &gt; 0.05)\\n\")\n} else {\n  cat(\"‚Üí Evidence of unequal variances (p &lt; 0.05)\\n\")\n  cat(\"  Consider: Welch's ANOVA (oneway.test) or transformation\\n\")\n}\n\n\n‚Üí No evidence of unequal variances (p &gt; 0.05)\n\n\nRule of thumb: If the ratio of largest to smallest SD is &lt; 2, you‚Äôre usually fine.\n\n\nCode\nsd_ratio &lt;- max(feed_summary$sd_adg) / min(feed_summary$sd_adg)\ncat(sprintf(\"Ratio of largest to smallest SD: %.2f\\n\", sd_ratio))\n\n\nRatio of largest to smallest SD: 1.85\n\n\nCode\nif(sd_ratio &lt; 2) {\n  cat(\"‚Üí Ratio &lt; 2, variances similar enough for standard ANOVA\\n\")\n} else {\n  cat(\"‚Üí Ratio ‚â• 2, consider Welch's ANOVA\\n\")\n}\n\n\n‚Üí Ratio &lt; 2, variances similar enough for standard ANOVA",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-violations",
    "href": "chapters/ch13-anova.html#sec-violations",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "19.4 What to Do If Assumptions Are Violated",
    "text": "19.4 What to Do If Assumptions Are Violated\n\n19.4.1 Unequal Variances ‚Üí Welch‚Äôs ANOVA\n\n\nCode\n# Welch's one-way test (doesn't assume equal variances)\nwelch_result &lt;- oneway.test(adg_kg ~ feed_type, data = feed_data)\n\nprint(welch_result)\n\n\n\n    One-way analysis of means (not assuming equal variances)\n\ndata:  adg_kg and feed_type\nF = 10.682, num df = 3.00, denom df = 30.35, p-value = 5.907e-05\n\n\nCode\ncat(sprintf(\"\\nWelch's ANOVA: F(%.2f, %.2f) = %.3f, p = %.4f\\n\",\n            welch_result$parameter[1], welch_result$parameter[2],\n            welch_result$statistic, welch_result$p.value))\n\n\n\nWelch's ANOVA: F(3.00, 30.35) = 10.682, p = 0.0001\n\n\n\n\n19.4.2 Non-Normality ‚Üí Kruskal-Wallis Test\n\n\nCode\n# Kruskal-Wallis (non-parametric alternative to ANOVA)\nkruskal_result &lt;- kruskal.test(adg_kg ~ feed_type, data = feed_data)\n\nprint(kruskal_result)\n\n\n\n    Kruskal-Wallis rank sum test\n\ndata:  adg_kg by feed_type\nKruskal-Wallis chi-squared = 15.964, df = 3, p-value = 0.001153\n\n\nCode\ncat(sprintf(\"\\nKruskal-Wallis: œá¬≤(%d) = %.3f, p = %.4f\\n\",\n            kruskal_result$parameter,\n            kruskal_result$statistic,\n            kruskal_result$p.value))\n\n\n\nKruskal-Wallis: œá¬≤(3) = 15.964, p = 0.0012\n\n\nNote: Kruskal-Wallis tests for differences in distributions, not just means. If significant, use Dunn‚Äôs test for post-hoc comparisons.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-post-hoc-problem",
    "href": "chapters/ch13-anova.html#sec-post-hoc-problem",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.1 The Multiple Comparisons Problem Returns",
    "text": "20.1 The Multiple Comparisons Problem Returns\nIf ANOVA is significant, you might be tempted to run all pairwise t-tests. But this brings back the multiple comparisons problem!\nSolution: Post-hoc tests that adjust p-values to control family-wise error rate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-post-hoc-methods",
    "href": "chapters/ch13-anova.html#sec-post-hoc-methods",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.2 Common Post-Hoc Methods",
    "text": "20.2 Common Post-Hoc Methods\n\n\n\n\n\n\n\n\nMethod\nUse When\nControl Level\n\n\n\n\nTukey HSD\nAll pairwise comparisons\nBalanced or unbalanced\n\n\nBonferroni\nSmall number of comparisons\nConservative, any design\n\n\nDunnett‚Äôs\nComparing to a control group\nSpecific control comparison\n\n\nScheffe\nComplex contrasts\nMost conservative\n\n\nNo adjustment\nDON‚ÄôT DO THIS\nInflates Type I error\n\n\n\n\n\n\n\n\n\nTipRecommended: Tukey HSD\n\n\n\nTukey‚Äôs Honestly Significant Difference is the most commonly used post-hoc test. It: - Controls family-wise error rate at Œ± - Works well with balanced and unbalanced designs - Tests all pairwise comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-emmeans-package",
    "href": "chapters/ch13-anova.html#sec-emmeans-package",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.3 Post-Hoc Tests Using emmeans",
    "text": "20.3 Post-Hoc Tests Using emmeans\nThe emmeans (estimated marginal means) package provides a modern, flexible framework for post-hoc tests:\n\n\nCode\nlibrary(emmeans)\n\n# Step 1: Compute estimated marginal means\nfeed_emm &lt;- emmeans(feed_model, ~ feed_type)\n\nprint(feed_emm)\n\n\n feed_type emmean     SE df lower.CL upper.CL\n A           1.47 0.0474 56     1.38     1.57\n B           1.61 0.0474 56     1.52     1.71\n C           1.50 0.0474 56     1.41     1.60\n D           1.75 0.0474 56     1.66     1.85\n\nConfidence level used: 0.95 \n\n\nCode\n# Step 2: Pairwise comparisons with Tukey adjustment\nfeed_pairs_tukey &lt;- pairs(feed_emm, adjust = \"tukey\")\n\nprint(feed_pairs_tukey)\n\n\n contrast estimate     SE df t.ratio p.value\n A - B     -0.1409 0.0671 56  -2.101  0.1654\n A - C     -0.0308 0.0671 56  -0.460  0.9674\n A - D     -0.2829 0.0671 56  -4.217  0.0005\n B - C      0.1101 0.0671 56   1.641  0.3644\n B - D     -0.1419 0.0671 56  -2.116  0.1606\n C - D     -0.2520 0.0671 56  -3.757  0.0023\n\nP value adjustment: tukey method for comparing a family of 4 estimates \n\n\nCode\n# Clean summary\nfeed_pairs_tukey_df &lt;- as.data.frame(feed_pairs_tukey)\n\nknitr::kable(feed_pairs_tukey_df, digits = 4, align = 'lcccc',\n             caption = \"Tukey HSD Post-Hoc Comparisons\")\n\n\n\nTukey HSD Post-Hoc Comparisons\n\n\ncontrast\nestimate\nSE\ndf\nt.ratio\np.value\n\n\n\n\nA - B\n-0.1409\n0.0671\n56\n-2.1008\n0.1654\n\n\nA - C\n-0.0308\n0.0671\n56\n-0.4597\n0.9674\n\n\nA - D\n-0.2829\n0.0671\n56\n-4.2167\n0.0005\n\n\nB - C\n0.1101\n0.0671\n56\n1.6411\n0.3644\n\n\nB - D\n-0.1419\n0.0671\n56\n-2.1159\n0.1606\n\n\nC - D\n-0.2520\n0.0671\n56\n-3.7569\n0.0023\n\n\n\n\n\nInterpretation: - estimate: Difference in means (Feed 1 - Feed 2) - SE: Standard error of the difference - t.ratio: Test statistic - p.value: Adjusted p-value (controls FWER)\nWhich comparisons are significant (p &lt; 0.05)?\n\n\nCode\nsig_pairs &lt;- feed_pairs_tukey_df %&gt;%\n  filter(p.value &lt; 0.05) %&gt;%\n  dplyr::select(contrast, estimate, p.value)\n\nif(nrow(sig_pairs) &gt; 0) {\n  cat(\"Significant pairwise differences:\\n\")\n  knitr::kable(sig_pairs, digits = 4,\n               caption = \"Significant Pairs (Tukey-adjusted p &lt; 0.05)\")\n} else {\n  cat(\"No significant pairwise differences after Tukey adjustment.\\n\")\n}\n\n\nSignificant pairwise differences:\n\n\n\nSignificant Pairs (Tukey-adjusted p &lt; 0.05)\n\n\ncontrast\nestimate\np.value\n\n\n\n\nA - D\n-0.2829\n0.0005\n\n\nC - D\n-0.2520\n0.0023",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-compare-adjustments",
    "href": "chapters/ch13-anova.html#sec-compare-adjustments",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.4 Comparing Adjustment Methods",
    "text": "20.4 Comparing Adjustment Methods\nLet‚Äôs compare Tukey vs Bonferroni vs no adjustment:\n\n\nCode\n# Different adjustments\npairs_none &lt;- pairs(feed_emm, adjust = \"none\")\npairs_bonf &lt;- pairs(feed_emm, adjust = \"bonferroni\")\npairs_tukey &lt;- pairs(feed_emm, adjust = \"tukey\")\n\n# Extract p-values\ncomparison_df &lt;- tibble(\n  Comparison = as.character(feed_pairs_tukey_df$contrast),\n  `No Adjust` = summary(pairs_none)$p.value,\n  Bonferroni = summary(pairs_bonf)$p.value,\n  Tukey = summary(pairs_tukey)$p.value\n)\n\nknitr::kable(comparison_df, digits = 4, align = 'lccc',\n             caption = \"P-values Under Different Adjustment Methods\")\n\n\n\nP-values Under Different Adjustment Methods\n\n\nComparison\nNo Adjust\nBonferroni\nTukey\n\n\n\n\nA - B\n0.0402\n0.2410\n0.1654\n\n\nA - C\n0.6475\n1.0000\n0.9674\n\n\nA - D\n0.0001\n0.0005\n0.0005\n\n\nB - C\n0.1064\n0.6383\n0.3644\n\n\nB - D\n0.0388\n0.2329\n0.1606\n\n\nC - D\n0.0004\n0.0025\n0.0023\n\n\n\n\n\nObservation: - No adjustment gives smallest p-values (most ‚Äúdiscoveries‚Äù) but inflates Type I error - Bonferroni is most conservative (largest p-values) - Tukey is in between and recommended for pairwise comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-visualize-post-hoc",
    "href": "chapters/ch13-anova.html#sec-visualize-post-hoc",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.5 Visualizing Post-Hoc Results",
    "text": "20.5 Visualizing Post-Hoc Results\n\n20.5.1 Compact Letter Display\nA compact letter display shows which groups differ using letters:\n\n\nCode\n# Compact letter display\nfeed_cld &lt;- cld(feed_emm, Letters = letters, adjust = \"tukey\")\n\nprint(feed_cld)\n\n\n feed_type emmean     SE df lower.CL upper.CL .group\n A           1.47 0.0474 56     1.35     1.59  a    \n C           1.50 0.0474 56     1.38     1.62  a    \n B           1.61 0.0474 56     1.49     1.73  ab   \n D           1.75 0.0474 56     1.63     1.88   b   \n\nConfidence level used: 0.95 \nConf-level adjustment: sidak method for 4 estimates \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n\nCode\n# Visualize with letters\nfeed_cld_df &lt;- as.data.frame(feed_cld)\n\nggplot(feed_cld_df, aes(x = feed_type, y = emmean, fill = feed_type)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = lower.CL, ymax = upper.CL),\n                width = 0.2, linewidth = 1) +\n  geom_text(aes(label = sprintf(\"%.2f\", emmean)),\n            vjust = -2.5, fontface = \"bold\", size = 4) +\n  geom_text(aes(label = .group), vjust = -4.5, size = 5, fontface = \"bold\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Post-Hoc Results: Tukey HSD\",\n       subtitle = \"Groups sharing a letter are not significantly different\",\n       x = \"Feed Type\",\n       y = \"Estimated Marginal Mean ADG (kg/day)\") +\n  ylim(0, 2) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nReading the plot: - Groups with different letters are significantly different - Groups with same letter are not significantly different\n\n\n20.5.2 Pairwise Comparison Plot\n\n\nCode\n# Pairwise comparison plot\nplot(feed_pairs_tukey) +\n  geom_vline(xintercept = 0, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Pairwise Comparisons with 95% Confidence Intervals\",\n       subtitle = \"Intervals not crossing zero indicate significant differences\",\n       x = \"Difference in Mean ADG (kg/day)\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-tukeyhsd-function",
    "href": "chapters/ch13-anova.html#sec-tukeyhsd-function",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "20.6 Alternative: Using TukeyHSD()",
    "text": "20.6 Alternative: Using TukeyHSD()\nFor comparison, here‚Äôs the traditional approach using aov() and TukeyHSD():\n\n\nCode\n# Fit with aov() (alternative to lm)\nfeed_aov &lt;- aov(adg_kg ~ feed_type, data = feed_data)\n\n# Tukey HSD\ntukey_traditional &lt;- TukeyHSD(feed_aov)\n\nprint(tukey_traditional)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = adg_kg ~ feed_type, data = feed_data)\n\n$feed_type\n           diff         lwr        upr     p adj\nB-A  0.14092989 -0.03670144 0.31856122 0.1654298\nC-A  0.03084118 -0.14679015 0.20847251 0.9674384\nD-A  0.28287289  0.10524156 0.46050422 0.0005176\nC-B -0.11008871 -0.28772004 0.06754262 0.3644403\nD-B  0.14194300 -0.03568833 0.31957433 0.1606251\nD-C  0.25203172  0.07440039 0.42966305 0.0022679\n\n\nCode\n# Visualize\nplot(tukey_traditional, las = 1, col = \"steelblue\")\n\n\n\n\n\n\n\n\n\nNote: Both approaches (emmeans and TukeyHSD) give equivalent results. We recommend emmeans for its flexibility and integration with lm().",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-eta-squared",
    "href": "chapters/ch13-anova.html#sec-eta-squared",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "21.1 Eta-Squared (Œ∑¬≤)",
    "text": "21.1 Eta-Squared (Œ∑¬≤)\nEta-squared is the proportion of total variance explained by the grouping variable:\n\\[\\eta^2 = \\frac{SS_{between}}{SS_{total}}\\]\n\n\nCode\nlibrary(effectsize)\n\n# Calculate eta-squared\neta_sq &lt;- eta_squared(feed_model)\n\nprint(eta_sq)\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nfeed_type | 0.28 | [0.10, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nCode\ncat(sprintf(\"\\nEta-squared: %.3f (%.1f%% of variance explained)\\n\",\n            eta_sq$Eta2[1], eta_sq$Eta2[1] * 100))\n\n\n\nEta-squared: 0.281 (28.1% of variance explained)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-omega-squared",
    "href": "chapters/ch13-anova.html#sec-omega-squared",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "21.2 Omega-Squared (œâ¬≤)",
    "text": "21.2 Omega-Squared (œâ¬≤)\nOmega-squared is a less biased estimate of effect size (adjusts for sample size):\n\\[\\omega^2 = \\frac{SS_{between} - (k-1)MS_{within}}{SS_{total} + MS_{within}}\\]\n\n\nCode\n# Calculate omega-squared\nomega_sq &lt;- omega_squared(feed_model)\n\nprint(omega_sq)\n\n\n# Effect Size for ANOVA\n\nParameter | Omega2 |       95% CI\n---------------------------------\nfeed_type |   0.24 | [0.07, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nCode\ncat(sprintf(\"\\nOmega-squared: %.3f\\n\", omega_sq$Omega2[1]))\n\n\n\nOmega-squared: 0.239\n\n\nInterpretation: œâ¬≤ is typically slightly smaller than Œ∑¬≤ and is preferred for reporting.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-effect-size-guidelines",
    "href": "chapters/ch13-anova.html#sec-effect-size-guidelines",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "21.3 Effect Size Guidelines",
    "text": "21.3 Effect Size Guidelines\nCohen‚Äôs (1988) benchmarks for Œ∑¬≤ and œâ¬≤:\n\n\n\nEffect Size\nŒ∑¬≤ / œâ¬≤\nInterpretation\n\n\n\n\nSmall\n0.01\n1% of variance explained\n\n\nMedium\n0.06\n6% of variance explained\n\n\nLarge\n0.14\n14% of variance explained\n\n\n\n\n\nCode\nomega_val &lt;- omega_sq$Omega2[1]\n\nif(omega_val &lt; 0.01) {\n  effect_interp &lt;- \"negligible\"\n} else if(omega_val &lt; 0.06) {\n  effect_interp &lt;- \"small\"\n} else if(omega_val &lt; 0.14) {\n  effect_interp &lt;- \"medium\"\n} else {\n  effect_interp &lt;- \"large\"\n}\n\ncat(sprintf(\"Effect size interpretation: %s\\n\", effect_interp))\n\n\nEffect size interpretation: large\n\n\n\n\n\n\n\n\nImportantAlways Report Effect Sizes\n\n\n\nWhen reporting ANOVA results, include:\n\nF-statistic and p-value\nEffect size (Œ∑¬≤ or œâ¬≤)\nDescriptive statistics (means, SDs)\nPost-hoc results if significant\n\nExample: Feed type significantly affected ADG, F(3, 56) = 12.45, p &lt; 0.001, œâ¬≤ = 0.35. Post-hoc tests revealed Feed D produced significantly higher gain than all other feeds.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-unbalanced",
    "href": "chapters/ch13-anova.html#sec-unbalanced",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "22.1 Unbalanced Designs",
    "text": "22.1 Unbalanced Designs\nUnbalanced designs (unequal sample sizes across groups) are common in practice due to: - Attrition (animals get sick, die, removed) - Unequal recruitment - Practical constraints\nConsequences: - Type I, II, III SS can differ - Slightly reduced power - Post-hoc tests still work\nRecommendation: Use Type II SS with car::Anova(model, type = \"II\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-power-sample-size",
    "href": "chapters/ch13-anova.html#sec-power-sample-size",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "22.2 Power and Sample Size",
    "text": "22.2 Power and Sample Size\n\n22.2.1 Power Analysis for ANOVA\nBefore conducting a study, calculate required sample size:\n\n\nCode\nlibrary(pwr)\n\n# Power analysis for one-way ANOVA\n# Effect size f (Cohen's f):\n# f = 0.10 (small), 0.25 (medium), 0.40 (large)\n\n# Scenario: Want to detect medium effect (f = 0.25)\n# 4 groups, Œ± = 0.05, power = 0.80\n\npower_result &lt;- pwr.anova.test(\n  k = 4,              # Number of groups\n  f = 0.25,           # Effect size (medium)\n  sig.level = 0.05,   # Alpha\n  power = 0.80        # Desired power\n)\n\nprint(power_result)\n\n\n\n     Balanced one-way analysis of variance power calculation \n\n              k = 4\n              n = 44.59927\n              f = 0.25\n      sig.level = 0.05\n          power = 0.8\n\nNOTE: n is number in each group\n\n\nCode\ncat(sprintf(\"\\nRequired sample size: %.0f per group\\n\", ceiling(power_result$n)))\n\n\n\nRequired sample size: 45 per group\n\n\nCode\ncat(sprintf(\"Total sample size needed: %.0f\\n\", 4 * ceiling(power_result$n)))\n\n\nTotal sample size needed: 180\n\n\nNote: Cohen‚Äôs f is related to Œ∑¬≤ by:\n\\[f = \\sqrt{\\frac{\\eta^2}{1 - \\eta^2}}\\]\n\n\n22.2.2 Power Curves\n\n\nCode\n# Calculate power for different sample sizes and effect sizes\npower_data &lt;- expand_grid(\n  n_per_group = seq(10, 50, by = 5),\n  effect_size = c(0.10, 0.25, 0.40)\n) %&gt;%\n  mutate(\n    effect_label = case_when(\n      effect_size == 0.10 ~ \"Small (f = 0.10)\",\n      effect_size == 0.25 ~ \"Medium (f = 0.25)\",\n      effect_size == 0.40 ~ \"Large (f = 0.40)\"\n    ),\n    power = map2_dbl(n_per_group, effect_size, ~ {\n      pwr.anova.test(k = 4, n = .x, f = .y, sig.level = 0.05)$power\n    })\n  )\n\nggplot(power_data, aes(x = n_per_group, y = power, color = effect_label)) +\n  geom_line(linewidth = 1.2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"gray40\") +\n  annotate(\"text\", x = 45, y = 0.82, label = \"Target: 80% power\", size = 3) +\n  scale_color_brewer(palette = \"Set1\") +\n  labs(title = \"Statistical Power for One-Way ANOVA\",\n       subtitle = \"4 groups, Œ± = 0.05\",\n       x = \"Sample Size per Group\",\n       y = \"Power\",\n       color = \"Effect Size\") +\n  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.2)) +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-common-mistakes",
    "href": "chapters/ch13-anova.html#sec-common-mistakes",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "22.3 Common Mistakes and Pitfalls",
    "text": "22.3 Common Mistakes and Pitfalls\n\n22.3.1 1. Running Multiple t-Tests Instead of ANOVA\nProblem: Inflates Type I error rate\nSolution: Use ANOVA first, then post-hoc tests if significant\n\n\n22.3.2 2. Testing Groups Within Raw Data\nProblem: Normality/variance tests on each group separately\nSolution: Check assumptions on residuals from the model\n\n\n22.3.3 3. Ignoring Post-Hoc Adjustments\nProblem: Using adjust = \"none\" in post-hoc tests\nSolution: Always use adjustment (Tukey, Bonferroni, etc.)\n\n\n22.3.4 4. Reporting Only P-values\nProblem: No effect sizes or descriptive statistics\nSolution: Report means, SDs, effect sizes, and confidence intervals\n\n\n22.3.5 5. Treating ANOVA as Proof of Causation\nProblem: Correlation vs causation confusion\nSolution: Only randomized experiments support causal claims",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-reporting-results",
    "href": "chapters/ch13-anova.html#sec-reporting-results",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "22.4 Reporting ANOVA Results",
    "text": "22.4 Reporting ANOVA Results\n\n22.4.1 Example Write-Up\n\nA one-way ANOVA was conducted to compare average daily gain across four feed formulations (A, B, C, D) in finishing steers (n = 15 per group). Assumptions of normality and homogeneity of variance were met (Levene‚Äôs test: p = 0.68).\nFeed type significantly affected ADG, F(3, 56) = 12.45, p &lt; 0.001, œâ¬≤ = 0.35, indicating a large effect. Mean ADG was 1.45 kg/day (SD = 0.18) for Feed A, 1.55 kg/day (SD = 0.17) for Feed B, 1.50 kg/day (SD = 0.19) for Feed C, and 1.70 kg/day (SD = 0.16) for Feed D.\nTukey HSD post-hoc tests revealed that Feed D produced significantly higher ADG than all other feeds (all p &lt; 0.01). No significant differences were found among Feeds A, B, and C (all p &gt; 0.20).\n\n\n\n22.4.2 APA-Style Table\n\n\nCode\n# Create publication-ready table\nanova_table &lt;- tibble(\n  Source = c(\"Feed Type\", \"Residual\"),\n  SS = c(feed_anova$`Sum Sq`[1], feed_anova$`Sum Sq`[2]),\n  df = c(feed_anova$Df[1], feed_anova$Df[2]),\n  MS = c(SS[1]/df[1], SS[2]/df[2]),\n  F = c(feed_anova$`F value`[1], NA),\n  p = c(feed_anova$`Pr(&gt;F)`[1], NA)\n)\n\nknitr::kable(anova_table, digits = c(0, 3, 0, 3, 2, 4),\n             col.names = c(\"Source\", \"SS\", \"df\", \"MS\", \"F\", \"p\"),\n             caption = \"ANOVA Table for Feed Type Effect on Average Daily Gain\",\n             align = 'lccccc')\n\n\n\nANOVA Table for Feed Type Effect on Average Daily Gain\n\n\nSource\nSS\ndf\nMS\nF\np\n\n\n\n\nFeed Type\n0.737\n3\n0.246\n7.28\n3e-04\n\n\nResidual\n1.890\n56\n0.034\nNA\nNA",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-covered",
    "href": "chapters/ch13-anova.html#sec-covered",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.1 What We Covered",
    "text": "23.1 What We Covered\n\nMultiple comparisons problem: Running multiple t-tests inflates Type I error rate\nANOVA logic: Partitions variance into between-group and within-group components\nF-statistic: Ratio of between to within variance; large F suggests group differences\nLinear model approach: Using lm() and car::Anova() provides flexibility and connects to regression\nType II/III SS: Important for unbalanced designs; use Type II for one-way ANOVA\nAssumptions: Independence, normality of residuals, homogeneity of variance\nPost-hoc tests: Tukey HSD, Bonferroni, and others for identifying specific group differences\nEffect sizes: Œ∑¬≤ and œâ¬≤ quantify proportion of variance explained\nPower analysis: Plan sample sizes before data collection",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-principles",
    "href": "chapters/ch13-anova.html#sec-principles",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.2 Key Principles",
    "text": "23.2 Key Principles\n\n\n\n\n\n\nImportantCore Principles of ANOVA\n\n\n\n\nANOVA controls FWER: Single test for multiple groups maintains Œ± = 0.05\nSignificant F means ‚Äúat least one difference‚Äù: Doesn‚Äôt tell you which groups differ\nAlways use post-hoc tests: If ANOVA is significant, conduct adjusted pairwise comparisons\nCheck residuals, not groups: Assumptions apply to model residuals\nReport effect sizes: Statistical significance ‚â† practical importance\nUse modern tools: lm() + car::Anova() + emmeans is the recommended workflow",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-decision-framework",
    "href": "chapters/ch13-anova.html#sec-decision-framework",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.3 ANOVA Decision Framework",
    "text": "23.3 ANOVA Decision Framework\nStep-by-step workflow:\n\nVisualize data: Boxplots, violin plots, means with error bars\nFit model: model &lt;- lm(outcome ~ group, data = df)\nRun ANOVA: car::Anova(model, type = \"II\")\nCheck assumptions: Diagnostic plots, Levene‚Äôs test, Q-Q plot\nIf significant: Conduct post-hoc tests with emmeans\nCalculate effect size: omega_squared(model)\nReport completely: F-statistic, p-value, effect size, means, SDs, post-hoc results",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-r-functions",
    "href": "chapters/ch13-anova.html#sec-r-functions",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.4 R Functions Summary",
    "text": "23.4 R Functions Summary\n\n\n\nFunction\nPackage\nPurpose\n\n\n\n\nlm()\nbase\nFit linear model (ANOVA)\n\n\nAnova()\ncar\nANOVA table with Type II/III SS\n\n\nemmeans()\nemmeans\nEstimated marginal means\n\n\npairs()\nemmeans\nPost-hoc pairwise comparisons\n\n\ncld()\nemmeans\nCompact letter display\n\n\nleveneTest()\ncar\nTest homogeneity of variance\n\n\neta_squared()\neffectsize\nEffect size (Œ∑¬≤)\n\n\nomega_squared()\neffectsize\nEffect size (œâ¬≤)\n\n\npwr.anova.test()\npwr\nPower analysis\n\n\noneway.test()\nbase\nWelch‚Äôs ANOVA\n\n\nkruskal.test()\nbase\nNon-parametric alternative",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-regression-preview",
    "href": "chapters/ch13-anova.html#sec-regression-preview",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.5 Connection to Regression (Preview)",
    "text": "23.5 Connection to Regression (Preview)\n\n\n\n\n\n\nNoteANOVA is Regression!\n\n\n\nOne-way ANOVA is actually linear regression with a categorical predictor. R converts your factor into dummy variables behind the scenes.\nComing in Weeks 7-8: We‚Äôll see how ANOVA and regression are unified under the general linear model framework. Everything you learned this week applies directly to regression!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-next-week",
    "href": "chapters/ch13-anova.html#sec-next-week",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "23.6 Next Week Preview",
    "text": "23.6 Next Week Preview\nWeek 6: Categorical Data Analysis\n\nChi-square tests (goodness-of-fit, independence)\nFisher‚Äôs exact test\nRisk ratios and odds ratios\n2√ó2 contingency tables\nMoving from continuous to categorical outcomes",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-practice-1",
    "href": "chapters/ch13-anova.html#sec-practice-1",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "24.1 Problem 1: Beef Breed Comparison",
    "text": "24.1 Problem 1: Beef Breed Comparison\nA rancher wants to compare weight gain across three beef breeds. Here‚Äôs the data:\n\n\nCode\nbreed_data &lt;- tibble(\n  breed = rep(c(\"Angus\", \"Hereford\", \"Simmental\"), each = 12),\n  weight_gain_kg = c(\n    rnorm(12, mean = 320, sd = 35),\n    rnorm(12, mean = 305, sd = 32),\n    rnorm(12, mean = 340, sd = 38)\n  )\n)\n\n\nTasks:\n\nVisualize the data with boxplots\nFit a linear model and run ANOVA\nCheck assumptions (diagnostic plots, Levene‚Äôs test)\nIf significant, conduct Tukey HSD post-hoc tests\nCalculate omega-squared effect size\nWrite a results paragraph",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-practice-2",
    "href": "chapters/ch13-anova.html#sec-practice-2",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "24.2 Problem 2: Unbalanced Design",
    "text": "24.2 Problem 2: Unbalanced Design\nYou have data on milk yield from four dairy management systems, but with unequal sample sizes:\n\nSystem A: n = 15\nSystem B: n = 12\nSystem C: n = 18\nSystem D: n = 10\n\nTasks:\n\nExplain why this is an unbalanced design\nWhich type of sums of squares should you use?\nHow would you handle this in your analysis?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-practice-3",
    "href": "chapters/ch13-anova.html#sec-practice-3",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "24.3 Problem 3: Power Analysis",
    "text": "24.3 Problem 3: Power Analysis\nYou‚Äôre planning a study to compare five feeding regimens. Based on pilot data, you expect a medium effect size (f = 0.25).\nTasks:\n\nCalculate required sample size per group for 80% power\nCalculate required sample size for 90% power\nIf you can only recruit 20 animals per group, what power will you have?\n\n\n\nCode\n# Your code here\nlibrary(pwr)\n\n# Part a\npwr.anova.test(k = 5, f = 0.25, sig.level = 0.05, power = 0.80)\n\n# Part b\npwr.anova.test(k = 5, f = 0.25, sig.level = 0.05, power = 0.90)\n\n# Part c\npwr.anova.test(k = 5, n = 20, f = 0.25, sig.level = 0.05)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-practice-4",
    "href": "chapters/ch13-anova.html#sec-practice-4",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "24.4 Problem 4: Assumption Violations",
    "text": "24.4 Problem 4: Assumption Violations\nYour ANOVA shows: - Levene‚Äôs test: p = 0.02 (unequal variances) - Shapiro-Wilk on residuals: p = 0.35 (normality OK)\nQuestions:\n\nWhich assumption is violated?\nWhat are two ways to address this?\nWrite the R code for an alternative analysis",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-practice-5",
    "href": "chapters/ch13-anova.html#sec-practice-5",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "24.5 Problem 5: Interpretation",
    "text": "24.5 Problem 5: Interpretation\nYou conduct ANOVA on 4 groups (n = 20 each) and get: - F(3, 76) = 2.15, p = 0.10 - Omega-squared = 0.04\nQuestions:\n\nIs the overall ANOVA significant?\nShould you conduct post-hoc tests? Why or why not?\nWhat is the effect size interpretation?\nIf you had gotten p = 0.03, would post-hoc tests be guaranteed to find significant pairs?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-reading",
    "href": "chapters/ch13-anova.html#sec-reading",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "25.1 Recommended Reading",
    "text": "25.1 Recommended Reading\n\nMaxwell & Delaney (2004). Designing Experiments and Analyzing Data. Comprehensive ANOVA coverage\nGelman & Hill (2007). Data Analysis Using Regression and Multilevel/Hierarchical Models. Modern perspective\nKeppel & Wickens (2004). Design and Analysis: A Researcher‚Äôs Handbook. Classic experimental design text",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-online",
    "href": "chapters/ch13-anova.html#sec-online",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "25.2 Online Resources",
    "text": "25.2 Online Resources\n\nUCLA Statistical Consulting: https://stats.oarc.ucla.edu/r/\nR Graphics Cookbook: https://r-graphics.org/\nemmeans package vignette: Excellent guide to post-hoc tests",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch13-anova.html#sec-papers-ss",
    "href": "chapters/ch13-anova.html#sec-papers-ss",
    "title": "13¬† Week 13: Analysis of Variance (ANOVA)",
    "section": "25.3 Papers on Type I/II/III SS",
    "text": "25.3 Papers on Type I/II/III SS\n\nLangsrud (2003). ‚ÄúANOVA for unbalanced data: Use Type II instead of Type III sums of squares‚Äù\nShaw & Mitchell-Olds (1993). ‚ÄúANOVA for unbalanced data: An overview‚Äù\n\n\nEnd of Week 5 Lecture\nNext week: Categorical Data Analysis - moving from continuous outcomes to counts and proportions!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Week 13: Analysis of Variance (ANOVA)</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html",
    "href": "chapters/ch14-categorical_data.html",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "",
    "text": "15 Introduction\nUp to this point in the course, we‚Äôve focused primarily on continuous outcomes: cattle weight gains, milk production, feed efficiency, and similar numeric measurements. But many important questions in animal science involve categorical outcomes:\nWhen our outcome variable is categorical rather than continuous, we need different statistical tools. This week, we‚Äôll explore methods for analyzing categorical data, from simple chi-square tests to logistic regression models.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#a-motivating-example",
    "href": "chapters/ch14-categorical_data.html#a-motivating-example",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "15.1 A Motivating Example",
    "text": "15.1 A Motivating Example\nImagine you‚Äôre investigating bovine respiratory disease (BRD) in feedlot cattle. You‚Äôve implemented a new vaccine protocol and want to know:\n\nDid it work? Are disease rates different between vaccinated and unvaccinated cattle?\nHow much did it help? What‚Äôs the reduction in disease risk?\nWhat factors matter? Do age, weight, or previous health status affect disease risk?\n\nTraditional t-tests and ANOVA won‚Äôt work here‚Äîour outcome is binary (disease: Yes/No), not continuous. We need categorical data analysis methods.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#key-questions-well-address",
    "href": "chapters/ch14-categorical_data.html#key-questions-well-address",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "15.2 Key Questions We‚Äôll Address",
    "text": "15.2 Key Questions We‚Äôll Address\nBy the end of this week, you‚Äôll be able to answer:\n\nWhen should I use chi-square tests vs Fisher‚Äôs exact test?\nHow do I calculate and interpret odds ratios and risk ratios?\nWhat‚Äôs the difference between association and causation with categorical data?\nWhen do I need logistic regression instead of simpler tests?\nHow do I interpret logistic regression coefficients?\n\n\n\n\n\n\n\nNoteBuilding on Previous Weeks\n\n\n\nWe‚Äôve already covered hypothesis testing (Week 4) and ANOVA (Week 5). The logic is the same for categorical data:\n\nState hypotheses (null: no association; alternative: association exists)\nCalculate a test statistic\nDetermine if results are more extreme than expected by chance\nConsider effect sizes and practical significance\n\nThe difference is in how we measure associations with categorical variables.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#types-of-categorical-variables",
    "href": "chapters/ch14-categorical_data.html#types-of-categorical-variables",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "16.1 Types of Categorical Variables",
    "text": "16.1 Types of Categorical Variables\nCategorical variables come in different forms:\nNominal: Categories with no inherent order - Breed (Holstein, Jersey, Angus) - Coat color (Black, White, Brown) - Disease diagnosis (BRD, Pneumonia, Healthy)\nOrdinal: Categories with a natural order - Body condition score (1, 2, 3, 4, 5) - Lameness score (None, Mild, Moderate, Severe) - Treatment response (Poor, Fair, Good, Excellent)\nBinary: Special case with only two categories - Disease status (Yes/No) - Survival (Alive/Dead) - Treatment group (Control/Treated)\n\n\n\n\n\n\nImportantThe methods we cover this week work best for nominal and binary data\n\n\n\nFor ordinal data with many categories, specialized methods (ordinal logistic regression, Wilcoxon tests) may be more appropriate. However, chi-square tests can still be used as a first approximation.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#contingency-tables",
    "href": "chapters/ch14-categorical_data.html#contingency-tables",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "16.2 Contingency Tables",
    "text": "16.2 Contingency Tables\nA contingency table (also called a cross-tabulation) displays the frequency distribution of two or more categorical variables.\nLet‚Äôs create a simple example:\n\n\nCode\n# Simulate vaccine trial data\nvaccine_data &lt;- tibble(\n  animal_id = 1:200,\n  vaccine = rep(c(\"Control\", \"Vaccinated\"), each = 100),\n  disease = c(\n    sample(c(\"Yes\", \"No\"), 100, replace = TRUE, prob = c(0.30, 0.70)),  # Control\n    sample(c(\"Yes\", \"No\"), 100, replace = TRUE, prob = c(0.15, 0.85))   # Vaccinated\n  )\n)\n\n# Create contingency table\ntable_result &lt;- table(vaccine_data$vaccine, vaccine_data$disease)\ntable_result\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\nThis is a 2√ó2 contingency table (2 rows √ó 2 columns). Let‚Äôs make it more informative:\n\n\nCode\n# Add row/column totals\naddmargins(table_result)\n\n\n            \n              No Yes Sum\n  Control     67  33 100\n  Vaccinated  88  12 100\n  Sum        155  45 200\n\n\nWe can also calculate proportions:\n\n\nCode\n# Proportions by row (vaccine status)\nprop.table(table_result, margin = 1) %&gt;%\n  round(3)\n\n\n            \n               No  Yes\n  Control    0.67 0.33\n  Vaccinated 0.88 0.12\n\n\nInterpretation: In this simulated data: - 31% of control animals got disease - 12% of vaccinated animals got disease - Difference: 31% - 12% = 19 percentage points\nBut is this difference statistically significant, or could it have occurred by chance?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#when-to-use-it",
    "href": "chapters/ch14-categorical_data.html#when-to-use-it",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "17.1 When to Use It",
    "text": "17.1 When to Use It\nUse this test when you have: - One categorical variable - A hypothesis about the expected distribution - Want to test if your data fits that distribution",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#classic-example-mendelian-genetics",
    "href": "chapters/ch14-categorical_data.html#classic-example-mendelian-genetics",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "17.2 Classic Example: Mendelian Genetics",
    "text": "17.2 Classic Example: Mendelian Genetics\nSuppose you‚Äôre breeding horses and expect a 3:1 ratio of black to chestnut coat color (simple dominant inheritance). You observe:\n\nBlack: 73\nChestnut: 27\nTotal: 100\n\nDoes this match the expected 3:1 ratio?\n\n\nCode\n# Observed counts\nobserved &lt;- c(Black = 73, Chestnut = 27)\n\n# Expected proportions (3:1 ratio)\nexpected_props &lt;- c(0.75, 0.25)\n\n# Chi-square goodness of fit test\nchisq_result &lt;- chisq.test(observed, p = expected_props)\nchisq_result\n\n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 0.21333, df = 1, p-value = 0.6442\n\n\n\n\nCode\n# Tidy output\ntidy(chisq_result)\n\n\n# A tibble: 1 √ó 4\n  statistic p.value parameter method                                  \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                   \n1     0.213   0.644         1 Chi-squared test for given probabilities\n\n\nInterpretation: - p-value = 0.644 - We fail to reject the null hypothesis - The observed data is consistent with a 3:1 ratio",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#how-it-works",
    "href": "chapters/ch14-categorical_data.html#how-it-works",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "17.3 How It Works",
    "text": "17.3 How It Works\nThe chi-square test compares observed (O) and expected (E) frequencies:\n\\[\n\\chi^2 = \\sum \\frac{(O - E)^2}{E}\n\\]\nLet‚Äôs calculate it manually:\n\n\nCode\n# Expected counts (3:1 ratio with n=100)\nexpected &lt;- c(75, 25)\n\n# Calculate chi-square statistic\nchi_square_stat &lt;- sum((observed - expected)^2 / expected)\nchi_square_stat\n\n\n[1] 0.2133333\n\n\nCode\n# Compare to test result\nchisq_result$statistic\n\n\nX-squared \n0.2133333 \n\n\nThe p-value comes from the chi-square distribution with degrees of freedom = k - 1 (where k = number of categories).\n\n\nCode\n# Visualize chi-square distribution\ntibble(x = seq(0, 10, 0.01)) %&gt;%\n  mutate(density = dchisq(x, df = 1)) %&gt;%\n  ggplot(aes(x = x, y = density)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_vline(xintercept = chi_square_stat, color = \"red\", linetype = \"dashed\", size = 1) +\n  geom_area(data = . %&gt;% filter(x &gt;= chi_square_stat),\n            fill = \"red\", alpha = 0.3) +\n  labs(title = \"Chi-Square Distribution (df = 1)\",\n       subtitle = \"Red area = p-value\",\n       x = \"Chi-square statistic\",\n       y = \"Density\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#assumptions",
    "href": "chapters/ch14-categorical_data.html#assumptions",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "17.4 Assumptions",
    "text": "17.4 Assumptions\n\n\n\n\n\n\nWarningChi-Square Test Assumptions\n\n\n\n\nIndependent observations: Each animal counted once\nExpected cell counts ‚â• 5: Rule of thumb for valid p-values\nRandom sampling: Data should be representative\n\nIf expected counts &lt; 5, the chi-square approximation may be poor. Consider: - Combining categories - Using exact tests - Collecting more data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#more-complex-example-dihybrid-cross",
    "href": "chapters/ch14-categorical_data.html#more-complex-example-dihybrid-cross",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "17.5 More Complex Example: Dihybrid Cross",
    "text": "17.5 More Complex Example: Dihybrid Cross\nIn a dihybrid cross (two genes), we expect a 9:3:3:1 ratio. Let‚Äôs test data from 160 offspring:\n\n\nCode\n# Observed counts\nobserved_dihybrid &lt;- c(95, 28, 26, 11)\nnames(observed_dihybrid) &lt;- c(\"Both_Dominant\", \"First_Dominant\",\n                               \"Second_Dominant\", \"Both_Recessive\")\n\n# Expected proportions (9:3:3:1 ratio)\nexpected_props_dihybrid &lt;- c(9, 3, 3, 1) / 16\n\n# Chi-square test\nchisq_dihybrid &lt;- chisq.test(observed_dihybrid, p = expected_props_dihybrid)\ntidy(chisq_dihybrid)\n\n\n# A tibble: 1 √ó 4\n  statistic p.value parameter method                                  \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                                   \n1      1.04   0.790         3 Chi-squared test for given probabilities\n\n\n\n\nCode\n# Visualize observed vs expected\ntibble(\n  Phenotype = factor(names(observed_dihybrid),\n                     levels = names(observed_dihybrid)),\n  Observed = observed_dihybrid,\n  Expected = expected_props_dihybrid * sum(observed_dihybrid)\n) %&gt;%\n  pivot_longer(cols = c(Observed, Expected),\n               names_to = \"Type\", values_to = \"Count\") %&gt;%\n  ggplot(aes(x = Phenotype, y = Count, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Observed\" = \"steelblue\",\n                                \"Expected\" = \"coral\")) +\n  labs(title = \"Dihybrid Cross: Observed vs Expected Counts\",\n       subtitle = paste0(\"œá¬≤ = \", round(chisq_dihybrid$statistic, 2),\n                        \", p = \", round(chisq_dihybrid$p.value, 3)),\n       x = \"Phenotype\", y = \"Count\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nInterpretation: The data fits the expected 9:3:3:1 ratio well (p = 0.79).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#the-null-hypothesis",
    "href": "chapters/ch14-categorical_data.html#the-null-hypothesis",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.1 The Null Hypothesis",
    "text": "18.1 The Null Hypothesis\n\nH‚ÇÄ: The two variables are independent (no association)\nH‚ÇÅ: The two variables are associated",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#example-vaccine-efficacy",
    "href": "chapters/ch14-categorical_data.html#example-vaccine-efficacy",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.2 Example: Vaccine Efficacy",
    "text": "18.2 Example: Vaccine Efficacy\nLet‚Äôs return to our vaccine trial. Are vaccine status and disease outcome independent?\n\n\nCode\n# Recreate table for clarity\nvaccine_table &lt;- table(vaccine_data$vaccine, vaccine_data$disease)\nvaccine_table\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\nCode\n# Chi-square test of independence\nchi_vaccine &lt;- chisq.test(vaccine_table)\nchi_vaccine\n\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  vaccine_table\nX-squared = 11.47, df = 1, p-value = 0.0007075\n\n\n\n\nCode\ntidy(chi_vaccine)\n\n\n# A tibble: 1 √ó 4\n  statistic  p.value parameter method                                           \n      &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                                            \n1      11.5 0.000707         1 Pearson's Chi-squared test with Yates' continuit‚Ä¶\n\n\nInterpretation: - œá¬≤ = 11.47 - p-value = 7^{-4} - We reject the null hypothesis - There is an association between vaccination and disease status",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#what-does-the-test-do",
    "href": "chapters/ch14-categorical_data.html#what-does-the-test-do",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.3 What Does the Test Do?",
    "text": "18.3 What Does the Test Do?\nUnder independence, we can calculate expected counts for each cell:\n\\[\nE_{ij} = \\frac{(\\text{row } i \\text{ total}) \\times (\\text{column } j \\text{ total})}{\\text{grand total}}\n\\]\n\n\nCode\n# Expected counts under independence\nchi_vaccine$expected\n\n\n            \n               No  Yes\n  Control    77.5 22.5\n  Vaccinated 77.5 22.5\n\n\nThe chi-square statistic measures how different observed counts are from expected:\n\\[\n\\chi^2 = \\sum_{all\\ cells} \\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\n\\]",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#degrees-of-freedom",
    "href": "chapters/ch14-categorical_data.html#degrees-of-freedom",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.4 Degrees of Freedom",
    "text": "18.4 Degrees of Freedom\nFor a contingency table: \\[\ndf = (r - 1) \\times (c - 1)\n\\]\nwhere r = number of rows, c = number of columns.\nFor our 2√ó2 table: df = (2-1) √ó (2-1) = 1",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#visualizing-associations",
    "href": "chapters/ch14-categorical_data.html#visualizing-associations",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.5 Visualizing Associations",
    "text": "18.5 Visualizing Associations\n\n\nCode\n# Create visualization\nvaccine_data %&gt;%\n  count(vaccine, disease) %&gt;%\n  group_by(vaccine) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  ggplot(aes(x = vaccine, y = prop, fill = disease)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = paste0(round(prop * 100, 1), \"%\")),\n            position = position_dodge(width = 0.9),\n            vjust = -0.5, size = 3.5) +\n  scale_fill_manual(values = c(\"Yes\" = \"coral\", \"No\" = \"steelblue\")) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 1)) +\n  labs(title = \"Disease Rates by Vaccine Status\",\n       subtitle = paste0(\"œá¬≤ = \", round(chi_vaccine$statistic, 2),\n                        \", p = \", round(chi_vaccine$p.value, 4)),\n       x = \"Vaccine Status\", y = \"Proportion\", fill = \"Disease\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#effect-size-cram√©rs-v",
    "href": "chapters/ch14-categorical_data.html#effect-size-cram√©rs-v",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.6 Effect Size: Cram√©r‚Äôs V",
    "text": "18.6 Effect Size: Cram√©r‚Äôs V\nChi-square tells us if an association exists, but not how strong it is. For effect size, we use Cram√©r‚Äôs V:\n\\[\nV = \\sqrt{\\frac{\\chi^2}{n \\times (k - 1)}}\n\\]\nwhere n = sample size, k = min(rows, columns)\n\n\nCode\n# Calculate Cram√©r's V\nn &lt;- sum(vaccine_table)\nk &lt;- min(nrow(vaccine_table), ncol(vaccine_table))\ncramers_v &lt;- sqrt(chi_vaccine$statistic / (n * (k - 1)))\ncramers_v\n\n\nX-squared \n0.2394737 \n\n\nCode\n# Interpretation guide\ncat(\"Cram√©r's V =\", round(cramers_v, 3), \"\\n\")\n\n\nCram√©r's V = 0.239 \n\n\nCode\ncat(\"Effect size interpretation (Cohen's guidelines for 2x2 tables):\\n\")\n\n\nEffect size interpretation (Cohen's guidelines for 2x2 tables):\n\n\nCode\ncat(\"  Small:  V = 0.10\\n\")\n\n\n  Small:  V = 0.10\n\n\nCode\ncat(\"  Medium: V = 0.30\\n\")\n\n\n  Medium: V = 0.30\n\n\nCode\ncat(\"  Large:  V = 0.50\\n\")\n\n\n  Large:  V = 0.50\n\n\nOur effect size (V = 0.239) suggests a small to medium association.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#larger-tables",
    "href": "chapters/ch14-categorical_data.html#larger-tables",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "18.7 Larger Tables",
    "text": "18.7 Larger Tables\nChi-square tests work with larger contingency tables too. Example: Three housing systems √ó three health outcomes:\n\n\nCode\n# Simulate housing system study\nset.seed(123)\nhousing_data &lt;- tibble(\n  housing = sample(c(\"Confinement\", \"Pasture\", \"Mixed\"),\n                   300, replace = TRUE)\n) %&gt;%\n  rowwise() %&gt;%\n  mutate(\n    health = case_when(\n      housing == \"Confinement\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                        prob = c(0.60, 0.30, 0.10)),\n      housing == \"Pasture\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                    prob = c(0.75, 0.20, 0.05)),\n      housing == \"Mixed\" ~ sample(c(\"Healthy\", \"Mild\", \"Severe\"), 1,\n                                  prob = c(0.70, 0.25, 0.05))\n    )\n  ) %&gt;%\n  ungroup()\n\n# Contingency table\nhousing_table &lt;- table(housing_data$housing, housing_data$health)\nhousing_table\n\n\n             \n              Healthy Mild Severe\n  Confinement      66   35     11\n  Mixed            68   20      2\n  Pasture          75   22      1\n\n\nCode\n# Chi-square test\nchi_housing &lt;- chisq.test(housing_table)\ntidy(chi_housing)\n\n\n# A tibble: 1 √ó 4\n  statistic p.value parameter method                    \n      &lt;dbl&gt;   &lt;dbl&gt;     &lt;int&gt; &lt;chr&gt;                     \n1      15.5 0.00384         4 Pearson's Chi-squared test\n\n\n\n\nCode\n# Mosaic-style visualization\nhousing_data %&gt;%\n  count(housing, health) %&gt;%\n  ggplot(aes(x = housing, y = n, fill = health)) +\n  geom_col(position = \"fill\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_continuous(labels = percent_format()) +\n  labs(title = \"Health Status by Housing System\",\n       subtitle = paste0(\"œá¬≤ = \", round(chi_housing$statistic, 2),\n                        \", p = \", round(chi_housing$p.value, 4)),\n       x = \"Housing System\", y = \"Proportion\", fill = \"Health Status\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#when-to-use-fishers-exact-test",
    "href": "chapters/ch14-categorical_data.html#when-to-use-fishers-exact-test",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "19.1 When to Use Fisher‚Äôs Exact Test",
    "text": "19.1 When to Use Fisher‚Äôs Exact Test\n\nAny expected cell count &lt; 5\nSmall sample sizes (n &lt; 20 or so)\nWhen you want an exact p-value (not an approximation)\n\n\n\n\n\n\n\nNoteFisher‚Äôs Exact Test vs Chi-Square\n\n\n\nFisher‚Äôs exact test calculates the exact probability of observing the data (and more extreme tables) under the null hypothesis of independence. Chi-square uses an approximation based on the chi-square distribution.\nFor large samples, results are nearly identical. For small samples, Fisher‚Äôs test is more reliable.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#example-rare-disease-outbreak",
    "href": "chapters/ch14-categorical_data.html#example-rare-disease-outbreak",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "19.2 Example: Rare Disease Outbreak",
    "text": "19.2 Example: Rare Disease Outbreak\nSuppose we‚Äôre investigating a rare disease in a small herd. Only 15 animals total:\n\n\nCode\n# Small sample data\nrare_disease &lt;- matrix(c(7, 1,    # Exposed\n                         4, 3),   # Not exposed\n                       nrow = 2, byrow = TRUE,\n                       dimnames = list(Exposure = c(\"Exposed\", \"Not Exposed\"),\n                                      Disease = c(\"Yes\", \"No\")))\nrare_disease\n\n\n             Disease\nExposure      Yes No\n  Exposed       7  1\n  Not Exposed   4  3\n\n\nCheck expected counts:\n\n\nCode\nchisq.test(rare_disease)$expected\n\n\n             Disease\nExposure           Yes       No\n  Exposed     5.866667 2.133333\n  Not Exposed 5.133333 1.866667\n\n\nExpected count in one cell is 3.2 (close to the threshold). Let‚Äôs use Fisher‚Äôs exact test:\n\n\nCode\nfisher_result &lt;- fisher.test(rare_disease)\nfisher_result\n\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  rare_disease\np-value = 0.2821\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n   0.2680235 313.3252444\nsample estimates:\nodds ratio \n  4.676004 \n\n\n\n\nCode\ntidy(fisher_result)\n\n\n# A tibble: 1 √ó 6\n  estimate p.value conf.low conf.high method                         alternative\n     &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;                          &lt;chr&gt;      \n1     4.68   0.282    0.268      313. Fisher's Exact Test for Count‚Ä¶ two.sided  \n\n\nInterpretation: - Exact p-value = 0.2821 - We reject the null hypothesis at Œ± = 0.05 - There is evidence of an association between exposure and disease\nCompare to chi-square (for demonstration):\n\n\nCode\nchi_rare &lt;- chisq.test(rare_disease)\n\ntibble(\n  Test = c(\"Chi-Square\", \"Fisher's Exact\"),\n  `P-value` = c(chi_rare$p.value, fisher_result$p.value)\n) %&gt;%\n  kable(digits = 4,\n        caption = \"Comparison of Tests on Small Sample Data\")\n\n\n\nComparison of Tests on Small Sample Data\n\n\nTest\nP-value\n\n\n\n\nChi-Square\n0.4586\n\n\nFisher‚Äôs Exact\n0.2821\n\n\n\n\n\nThe warning on chi-square confirms we should use Fisher‚Äôs test here.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#computational-note",
    "href": "chapters/ch14-categorical_data.html#computational-note",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "19.3 Computational Note",
    "text": "19.3 Computational Note\nFisher‚Äôs exact test can be computationally intensive for large tables. Most software (including R) can handle 2√ó2 tables easily, but larger tables may take time or use simulation-based approximations.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#table-notation",
    "href": "chapters/ch14-categorical_data.html#table-notation",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "20.1 2√ó2 Table Notation",
    "text": "20.1 2√ó2 Table Notation\nFor a standard 2√ó2 table:\n\n\n\n\nDisease Yes\nDisease No\nTotal\n\n\n\n\nExposed\na\nb\na+b\n\n\nNot Exposed\nc\nd\nc+d\n\n\nTotal\na+c\nb+d\nn",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#risk-ratio-relative-risk",
    "href": "chapters/ch14-categorical_data.html#risk-ratio-relative-risk",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "20.2 Risk Ratio (Relative Risk)",
    "text": "20.2 Risk Ratio (Relative Risk)\nThe risk ratio (RR) compares the risk of disease in exposed vs unexposed groups:\n\\[\nRR = \\frac{a/(a+b)}{c/(c+d)} = \\frac{\\text{Risk in exposed}}{\\text{Risk in unexposed}}\n\\]\nInterpretation: - RR = 1: No association - RR &gt; 1: Exposure increases risk - RR &lt; 1: Exposure decreases risk (protective)\n\n20.2.1 Example: Vaccine Efficacy\n\n\nCode\n# Our vaccine data in 2x2 format\nvaccine_table\n\n\n            \n             No Yes\n  Control    67  33\n  Vaccinated 88  12\n\n\n\n\nCode\n# Extract counts\na &lt;- vaccine_table[\"Vaccinated\", \"Yes\"]    # Vaccinated & diseased\nb &lt;- vaccine_table[\"Vaccinated\", \"No\"]     # Vaccinated & not diseased\nc &lt;- vaccine_table[\"Control\", \"Yes\"]       # Control & diseased\nd &lt;- vaccine_table[\"Control\", \"No\"]        # Control & not diseased\n\n# Calculate risks\nrisk_vaccinated &lt;- a / (a + b)\nrisk_control &lt;- c / (c + d)\n\n# Risk ratio\nrisk_ratio &lt;- risk_vaccinated / risk_control\nrisk_ratio\n\n\n[1] 0.3636364\n\n\nCode\ncat(\"Risk in vaccinated group:\", round(risk_vaccinated, 3), \"\\n\")\n\n\nRisk in vaccinated group: 0.12 \n\n\nCode\ncat(\"Risk in control group:\", round(risk_control, 3), \"\\n\")\n\n\nRisk in control group: 0.33 \n\n\nCode\ncat(\"Risk Ratio:\", round(risk_ratio, 3), \"\\n\")\n\n\nRisk Ratio: 0.364 \n\n\nInterpretation: The vaccinated group has 0.36 times the risk of disease compared to controls. Since RR &lt; 1, vaccination is protective.\n\n\n20.2.2 Vaccine Efficacy\nVaccine efficacy = 1 - RR = proportion of disease prevented by vaccination\n\n\nCode\nvaccine_efficacy &lt;- (1 - risk_ratio) * 100\ncat(\"Vaccine efficacy:\", round(vaccine_efficacy, 1), \"%\\n\")\n\n\nVaccine efficacy: 63.6 %\n\n\nThe vaccine prevents approximately 64% of disease cases.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#odds-ratio",
    "href": "chapters/ch14-categorical_data.html#odds-ratio",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "20.3 Odds Ratio",
    "text": "20.3 Odds Ratio\nThe odds ratio (OR) compares the odds of disease in exposed vs unexposed:\n\\[\nOR = \\frac{a/b}{c/d} = \\frac{a \\times d}{b \\times c}\n\\]\nOdds are different from risk (probability): - Risk = a / (a+b) - Odds = a / b\n\n\nCode\n# Calculate odds\nodds_vaccinated &lt;- a / b\nodds_control &lt;- c / d\n\n# Odds ratio\nodds_ratio &lt;- odds_vaccinated / odds_control\n# OR: Simplified calculation\nodds_ratio_simple &lt;- (a * d) / (b * c)\n\ncat(\"Odds in vaccinated group:\", round(odds_vaccinated, 3), \"\\n\")\n\n\nOdds in vaccinated group: 0.136 \n\n\nCode\ncat(\"Odds in control group:\", round(odds_control, 3), \"\\n\")\n\n\nOdds in control group: 0.493 \n\n\nCode\ncat(\"Odds Ratio:\", round(odds_ratio, 3), \"\\n\")\n\n\nOdds Ratio: 0.277 \n\n\n\n\n\n\n\n\nImportantRisk Ratio vs Odds Ratio\n\n\n\n\nRisk Ratio: More intuitive, easier to interpret\nOdds Ratio: More commonly used in logistic regression and case-control studies\nWhen the outcome is rare (&lt; 10%), OR ‚âà RR\nWhen the outcome is common, OR and RR can differ substantially\n\nIn our vaccine example (disease rates ~12-31%), RR = 0.36 and OR = 0.28 are somewhat different.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#confidence-intervals",
    "href": "chapters/ch14-categorical_data.html#confidence-intervals",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "20.4 Confidence Intervals",
    "text": "20.4 Confidence Intervals\nPoint estimates are useful, but we also need uncertainty quantification. Let‚Äôs calculate 95% CIs:\n\n\nCode\n# Using prop.test for risk ratio CI\nprop_test &lt;- prop.test(c(a, c), c(a+b, c+d))\n\n# For OR, use logarithmic CI\nlog_or &lt;- log(odds_ratio)\nse_log_or &lt;- sqrt(1/a + 1/b + 1/c + 1/d)\nci_log_or &lt;- log_or + c(-1.96, 1.96) * se_log_or\nci_or &lt;- exp(ci_log_or)\n\n# Display results\ntibble(\n  Measure = c(\"Risk Ratio\", \"Odds Ratio\"),\n  Estimate = c(risk_ratio, odds_ratio),\n  `95% CI Lower` = c(prop_test$estimate[1] / prop_test$estimate[2] *\n                       exp(-1.96 * sqrt(1/(a+b) + 1/(c+d))),\n                     ci_or[1]),\n  `95% CI Upper` = c(prop_test$estimate[1] / prop_test$estimate[2] *\n                       exp(1.96 * sqrt(1/(a+b) + 1/(c+d))),\n                     ci_or[2])\n) %&gt;%\n  kable(digits = 3,\n        caption = \"Risk Ratio and Odds Ratio with 95% Confidence Intervals\")\n\n\n\nRisk Ratio and Odds Ratio with 95% Confidence Intervals\n\n\nMeasure\nEstimate\n95% CI Lower\n95% CI Upper\n\n\n\n\nRisk Ratio\n0.364\n0.276\n0.480\n\n\nOdds Ratio\n0.277\n0.133\n0.576\n\n\n\n\n\nSince both CIs exclude 1.0, we conclude there‚Äôs a statistically significant association.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#why-not-linear-regression",
    "href": "chapters/ch14-categorical_data.html#why-not-linear-regression",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.1 Why Not Linear Regression?",
    "text": "21.1 Why Not Linear Regression?\nWith a binary outcome (0/1, Yes/No), linear regression has problems:\n\n\nCode\n# Create data with binary outcome\ndisease_weight &lt;- tibble(\n  weight = runif(100, 200, 400),\n  disease = rbinom(100, 1, prob = plogis(-5 + 0.015 * weight))\n)\n\n# Try linear regression (wrong!)\nlm_wrong &lt;- lm(disease ~ weight, data = disease_weight)\n\n# Plot\ndisease_weight %&gt;%\n  ggplot(aes(x = weight, y = disease)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Why Linear Regression Fails with Binary Outcomes\",\n       subtitle = \"Predicted values can be &lt; 0 or &gt; 1 (impossible for probabilities!)\",\n       x = \"Weight (kg)\", y = \"Disease (0 = No, 1 = Yes)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nProblems with linear regression for binary outcomes: - Predicted values can be &lt; 0 or &gt; 1 - Residuals are not normally distributed - Variance is not constant (heteroscedasticity)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#the-logistic-function",
    "href": "chapters/ch14-categorical_data.html#the-logistic-function",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.2 The Logistic Function",
    "text": "21.2 The Logistic Function\nLogistic regression uses a special link function that constrains predictions to [0, 1]:\n\\[\nP(Y = 1) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\n\\]\nThis S-shaped curve is called the logistic function or sigmoid function:\n\n\nCode\ntibble(x = seq(-6, 6, 0.1)) %&gt;%\n  mutate(probability = plogis(x)) %&gt;%  # plogis() = logistic function\n  ggplot(aes(x = x, y = probability)) +\n  geom_line(size = 1.5, color = \"steelblue\") +\n  geom_hline(yintercept = c(0, 1), linetype = \"dashed\", alpha = 0.5) +\n  geom_hline(yintercept = 0.5, linetype = \"dotted\", alpha = 0.5) +\n  labs(title = \"The Logistic Function\",\n       subtitle = \"Output is always between 0 and 1\",\n       x = \"Linear Predictor (Œ≤‚ÇÄ + Œ≤‚ÇÅX)\",\n       y = \"Probability P(Y = 1)\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#understanding-log-odds-logit",
    "href": "chapters/ch14-categorical_data.html#understanding-log-odds-logit",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.3 Understanding Log-Odds (Logit)",
    "text": "21.3 Understanding Log-Odds (Logit)\nThe logistic regression equation can be rewritten as:\n\\[\n\\log\\left(\\frac{P}{1-P}\\right) = \\beta_0 + \\beta_1 X\n\\]\nThe left side is the log-odds (also called the logit). This makes the relationship linear in log-odds space.\n\n\n\n\n\n\nNoteThree Ways to Think About Logistic Regression\n\n\n\n\nProbability scale: P(Y=1) is a curve between 0 and 1\nOdds scale: Odds = P/(1-P), ranges from 0 to ‚àû\nLog-odds scale: log(Odds), ranges from -‚àû to +‚àû (linear!)\n\nCoefficients in logistic regression are in log-odds scale.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#simple-logistic-regression-example",
    "href": "chapters/ch14-categorical_data.html#simple-logistic-regression-example",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.4 Simple Logistic Regression Example",
    "text": "21.4 Simple Logistic Regression Example\nLet‚Äôs model disease risk as a function of body weight:\n\n\nCode\n# Fit logistic regression\nlogistic_model &lt;- glm(disease ~ weight,\n                      data = disease_weight,\n                      family = binomial)\n\n# Summary\nsummary(logistic_model)\n\n\n\nCall:\nglm(formula = disease ~ weight, family = binomial, data = disease_weight)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept) -4.015138   1.302445  -3.083  0.00205 **\nweight       0.011715   0.004264   2.747  0.00601 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 131.79  on 99  degrees of freedom\nResidual deviance: 123.55  on 98  degrees of freedom\nAIC: 127.55\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nCode\n# Tidy output\ntidy(logistic_model, conf.int = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Logistic Regression: Disease ~ Weight\")\n\n\n\nLogistic Regression: Disease ~ Weight\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-4.0151\n1.3024\n-3.0828\n0.0021\n-6.7092\n-1.5660\n\n\nweight\n0.0117\n0.0043\n2.7474\n0.0060\n0.0036\n0.0205\n\n\n\n\n\nInterpreting coefficients (in log-odds scale): - Intercept (Œ≤‚ÇÄ) = -4.02: Log-odds of disease when weight = 0 (not meaningful here) - Weight (Œ≤‚ÇÅ) = 0.0117: For each 1 kg increase in weight, log-odds of disease increase by 0.0117",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#converting-to-odds-ratios",
    "href": "chapters/ch14-categorical_data.html#converting-to-odds-ratios",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.5 Converting to Odds Ratios",
    "text": "21.5 Converting to Odds Ratios\nTo get odds ratios, exponentiate the coefficients:\n\n\nCode\n# Odds ratios\ntidy(logistic_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Odds Ratios from Logistic Regression\")\n\n\n\nOdds Ratios from Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0180\n1.3024\n-3.0828\n0.0021\n0.0012\n0.2089\n\n\nweight\n1.0118\n0.0043\n2.7474\n0.0060\n1.0036\n1.0207\n\n\n\n\n\nInterpretation: For each 1 kg increase in weight, the odds of disease are multiplied by 1.012.\nSince OR &gt; 1, higher weight is associated with higher disease risk.\n\n\n\n\n\n\nImportantInterpreting Odds Ratios\n\n\n\n\nOR = 1: No association (predictor doesn‚Äôt affect odds)\nOR &gt; 1: Predictor increases odds of outcome\nOR &lt; 1: Predictor decreases odds of outcome (protective)\n\nExample: OR = 1.02 means odds increase by 2% per unit increase in predictor.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#predicted-probabilities",
    "href": "chapters/ch14-categorical_data.html#predicted-probabilities",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.6 Predicted Probabilities",
    "text": "21.6 Predicted Probabilities\nWe can use the model to predict disease probability for any weight:\n\n\nCode\n# Predict probabilities for a range of weights\npred_data &lt;- tibble(\n  weight = seq(200, 400, by = 10)\n)\n\npred_data &lt;- pred_data %&gt;%\n  mutate(\n    predicted_prob = predict(logistic_model, newdata = pred_data,\n                            type = \"response\")\n  )\n\n# Show some predictions\npred_data %&gt;%\n  filter(weight %in% c(200, 250, 300, 350, 400)) %&gt;%\n  kable(digits = 3,\n        caption = \"Predicted Disease Probabilities by Weight\")\n\n\n\nPredicted Disease Probabilities by Weight\n\n\nweight\npredicted_prob\n\n\n\n\n200\n0.158\n\n\n250\n0.252\n\n\n300\n0.377\n\n\n350\n0.521\n\n\n400\n0.662\n\n\n\n\n\n\n\nCode\n# Visualize predictions\ndisease_weight %&gt;%\n  ggplot(aes(x = weight, y = disease)) +\n  geom_point(alpha = 0.4, size = 2) +\n  geom_line(data = pred_data,\n            aes(x = weight, y = predicted_prob),\n            color = \"blue\", size = 1.5) +\n  labs(title = \"Logistic Regression: Disease Risk by Weight\",\n       subtitle = \"Blue curve shows predicted probabilities\",\n       x = \"Weight (kg)\",\n       y = \"Probability of Disease\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#model-fit-and-diagnostics",
    "href": "chapters/ch14-categorical_data.html#model-fit-and-diagnostics",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.7 Model Fit and Diagnostics",
    "text": "21.7 Model Fit and Diagnostics\n\n21.7.1 Deviance\nSimilar to residual sum of squares in linear regression, logistic regression uses deviance to measure model fit.\n\n\nCode\nglance(logistic_model) %&gt;%\n  kable(digits = 2,\n        caption = \"Logistic Regression Model Fit Statistics\")\n\n\n\nLogistic Regression Model Fit Statistics\n\n\n\n\n\n\n\n\n\n\n\n\nnull.deviance\ndf.null\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n131.79\n99\n-61.78\n127.55\n132.76\n123.55\n98\n100\n\n\n\n\n\n\nNull deviance: Model with intercept only (no predictors)\nResidual deviance: Model with predictors\nLower deviance = better fit\n\n\n\n21.7.2 Likelihood Ratio Test\nCompare models with a likelihood ratio test:\n\n\nCode\n# Null model (intercept only)\nnull_model &lt;- glm(disease ~ 1, data = disease_weight, family = binomial)\n\n# Compare models\nanova(null_model, logistic_model, test = \"Chisq\") %&gt;%\n  kable(digits = 4,\n        caption = \"Likelihood Ratio Test: Does Weight Improve the Model?\")\n\n\n\nLikelihood Ratio Test: Does Weight Improve the Model?\n\n\nResid. Df\nResid. Dev\nDf\nDeviance\nPr(&gt;Chi)\n\n\n\n\n99\n131.7911\nNA\nNA\nNA\n\n\n98\n123.5509\n1\n8.2402\n0.0041\n\n\n\n\n\nWeight significantly improves model fit (p &lt; 0.05).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#multiple-logistic-regression",
    "href": "chapters/ch14-categorical_data.html#multiple-logistic-regression",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.8 Multiple Logistic Regression",
    "text": "21.8 Multiple Logistic Regression\nWe can include multiple predictors, just like multiple linear regression:\n\n\nCode\n# Add more predictors\ndisease_multi &lt;- disease_weight %&gt;%\n  mutate(\n    age = runif(n(), 1, 5),\n    previous_disease = sample(c(0, 1), n(), replace = TRUE, prob = c(0.7, 0.3))\n  )\n\n# Fit multiple logistic regression\nmulti_logistic &lt;- glm(disease ~ weight + age + previous_disease,\n                      data = disease_multi,\n                      family = binomial)\n\n# Odds ratios\ntidy(multi_logistic, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4,\n        caption = \"Multiple Logistic Regression: Odds Ratios\")\n\n\n\nMultiple Logistic Regression: Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0116\n1.3896\n-3.2073\n0.0013\n0.0006\n0.1572\n\n\nweight\n1.0113\n0.0043\n2.6080\n0.0091\n1.0030\n1.0202\n\n\nage\n1.2182\n0.1844\n1.0702\n0.2845\n0.8500\n1.7606\n\n\nprevious_disease\n0.9770\n0.4772\n-0.0487\n0.9611\n0.3760\n2.4755\n\n\n\n\n\nInterpretation (if results were significant): - Each 1 kg increase in weight multiplies odds of disease by 1.011, holding age and previous disease constant - Previous disease history increases odds by a factor of 0.98, adjusting for weight and age",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#practical-example-brd-risk-factors",
    "href": "chapters/ch14-categorical_data.html#practical-example-brd-risk-factors",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.9 Practical Example: BRD Risk Factors",
    "text": "21.9 Practical Example: BRD Risk Factors\nLet‚Äôs analyze a more realistic dataset with bovine respiratory disease (BRD):\n\n\nCode\n# Simulate realistic BRD data\nset.seed(456)\nbrd_data &lt;- tibble(\n  animal_id = 1:300,\n  arrival_weight = rnorm(300, mean = 250, sd = 30),\n  age_months = runif(300, min = 6, max = 10),\n  vaccinated = sample(c(\"Yes\", \"No\"), 300, replace = TRUE),\n  temperature_arrival = rnorm(300, mean = 39.2, sd = 0.8)\n) %&gt;%\n  mutate(\n    # Disease risk depends on multiple factors\n    risk_score = -8 +\n      0.01 * arrival_weight +\n      0.3 * age_months -\n      1.5 * (vaccinated == \"Yes\") +\n      0.5 * temperature_arrival,\n    brd = rbinom(300, 1, prob = plogis(risk_score))\n  )\n\n# Check disease rate\nmean(brd_data$brd)\n\n\n[1] 1\n\n\n\n\nCode\n# Fit comprehensive model\nbrd_model &lt;- glm(brd ~ arrival_weight + age_months + vaccinated + temperature_arrival,\n                 data = brd_data,\n                 family = binomial)\n\n# Get results with Wald CIs\nbrd_results &lt;- tidy(brd_model, exponentiate = TRUE)\nbrd_ci &lt;- confint.default(brd_model) # Wald CIs\nbrd_results$conf.low &lt;- exp(brd_ci[,1])\nbrd_results$conf.high &lt;- exp(brd_ci[,2])\n\n# Display results\nbrd_results %&gt;%\n  select(term, estimate, conf.low, conf.high, p.value) %&gt;%\n  kable(digits = 4,\n        caption = \"BRD Risk Factors: Odds Ratios with 95% Wald CIs\")\n\n\n\nBRD Risk Factors: Odds Ratios with 95% Wald CIs\n\n\nterm\nestimate\nconf.low\nconf.high\np.value\n\n\n\n\n(Intercept)\n344744308613\n0\nInf\n1\n\n\narrival_weight\n1\n0\nInf\n1\n\n\nage_months\n1\n0\nInf\n1\n\n\nvaccinatedYes\n1\n0\nInf\n1\n\n\ntemperature_arrival\n1\n0\nInf\n1\n\n\n\n\n\n\n\nCode\n# Extract key odds ratios\nor_vaccine &lt;- exp(coef(brd_model)[\"vaccinatedYes\"])\nor_temp &lt;- exp(coef(brd_model)[\"temperature_arrival\"])\n\ncat(\"Key findings:\\n\")\n\n\nKey findings:\n\n\nCode\ncat(\"- Vaccination reduces odds of BRD by\",\n    round((1 - or_vaccine) * 100, 1), \"%\\n\")\n\n\n- Vaccination reduces odds of BRD by 0 %\n\n\nCode\ncat(\"- Each 1¬∞C increase in arrival temperature multiplies odds by\",\n    round(or_temp, 2), \"\\n\")\n\n\n- Each 1¬∞C increase in arrival temperature multiplies odds by 1",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#predictions-for-new-animals",
    "href": "chapters/ch14-categorical_data.html#predictions-for-new-animals",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "21.10 Predictions for New Animals",
    "text": "21.10 Predictions for New Animals\n\n\nCode\n# Predict BRD risk for specific scenarios\nnew_animals &lt;- tibble(\n  scenario = c(\"Low risk\", \"Medium risk\", \"High risk\"),\n  arrival_weight = c(280, 250, 220),\n  age_months = c(7, 8, 9),\n  vaccinated = c(\"Yes\", \"No\", \"No\"),\n  temperature_arrival = c(38.5, 39.2, 40.0)\n)\n\nnew_animals &lt;- new_animals %&gt;%\n  mutate(\n    predicted_risk = predict(brd_model, newdata = new_animals, type = \"response\")\n  )\n\nnew_animals %&gt;%\n  select(scenario, vaccinated, temperature_arrival, predicted_risk) %&gt;%\n  kable(digits = 3,\n        caption = \"Predicted BRD Risk for Different Scenarios\")\n\n\n\nPredicted BRD Risk for Different Scenarios\n\n\nscenario\nvaccinated\ntemperature_arrival\npredicted_risk\n\n\n\n\nLow risk\nYes\n38.5\n1\n\n\nMedium risk\nNo\n39.2\n1\n\n\nHigh risk\nNo\n40.0\n1\n\n\n\n\n\n\n\n\n\n\n\nNoteWhen to Use Logistic Regression vs Chi-Square\n\n\n\n\nChi-square test: Quick test of association between two categorical variables\nLogistic regression:\n\nInclude continuous predictors\nControl for confounders\nMake predictions\nModel complex relationships\nGet odds ratios with confidence intervals",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#chi-square-test-assumptions-1",
    "href": "chapters/ch14-categorical_data.html#chi-square-test-assumptions-1",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "22.1 Chi-Square Test Assumptions",
    "text": "22.1 Chi-Square Test Assumptions\n\nIndependence: Each observation must be independent\n\nViolated by: Repeated measures, clustered data (multiple animals per farm)\nSolution: Use mixed models or GEE for clustered data\n\nExpected cell counts ‚â• 5: Rule of thumb for valid chi-square approximation\n\nCheck: Use chisq.test()$expected\nSolution: Fisher‚Äôs exact test, combine categories, or collect more data\n\nRandom sampling: Data should be representative of population\nMutually exclusive categories: Each observation in exactly one category",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#logistic-regression-assumptions",
    "href": "chapters/ch14-categorical_data.html#logistic-regression-assumptions",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "22.2 Logistic Regression Assumptions",
    "text": "22.2 Logistic Regression Assumptions\n\nBinary outcome: DV must be 0/1 (or convertible to binary)\nIndependence: Observations are independent\nLinearity of log-odds: Relationship between continuous predictors and log-odds should be linear\n\nCheck: Plot log-odds vs predictor, or use splines\n\nNo perfect multicollinearity: Predictors shouldn‚Äôt be perfectly correlated\nAdequate sample size: Rule of thumb: 10-15 events per predictor variable\n\n\n\n\n\n\n\nWarningCommon Pitfalls\n\n\n\n\nSmall expected cell counts: Chi-square p-values unreliable ‚Üí Use Fisher‚Äôs exact\nMultiple comparisons: Testing many associations increases false positives ‚Üí Adjust for multiple testing\nConfusing OR and RR: Odds ratios ‚â† Risk ratios (except for rare outcomes)\nCausal language: ‚ÄúAssociation‚Äù ‚â† ‚ÄúCausation‚Äù ‚Üí Observational data can‚Äôt prove causation\nOverfitting logistic models: Too many predictors for sample size ‚Üí Use fewer predictors or collect more data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#sample-size-considerations",
    "href": "chapters/ch14-categorical_data.html#sample-size-considerations",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "22.3 Sample Size Considerations",
    "text": "22.3 Sample Size Considerations\nFor chi-square tests, use power analysis:\n\n\nCode\n# What sample size for 80% power to detect OR = 2.0?\n# Assuming equal groups, alpha = 0.05\n\n# Simulate to demonstrate concept\nsimulate_power &lt;- function(n_per_group, or, n_sims = 1000) {\n  p_control &lt;- 0.20  # 20% baseline risk\n  p_exposed &lt;- p_control * or / (1 - p_control + p_control * or)\n\n  p_values &lt;- replicate(n_sims, {\n    exposed &lt;- sample(c(\"Yes\", \"No\"), n_per_group, replace = TRUE,\n                     prob = c(p_exposed, 1 - p_exposed))\n    control &lt;- sample(c(\"Yes\", \"No\"), n_per_group, replace = TRUE,\n                     prob = c(p_control, 1 - p_control))\n\n    outcome &lt;- c(exposed, control)\n    group &lt;- rep(c(\"Exposed\", \"Control\"), each = n_per_group)\n\n    test_result &lt;- tryCatch(\n      chisq.test(table(group, outcome))$p.value,\n      error = function(e) NA,\n      warning = function(w) NA\n    )\n\n    test_result\n  })\n\n  power &lt;- mean(p_values &lt; 0.05, na.rm = TRUE)\n  return(power)\n}\n\n# Test different sample sizes\nsample_sizes &lt;- seq(50, 200, by = 25)\npower_results &lt;- map_dbl(sample_sizes, ~simulate_power(.x, or = 2.0, n_sims = 500))\n\n# Plot\ntibble(n_per_group = sample_sizes, power = power_results) %&gt;%\n  ggplot(aes(x = n_per_group, y = power)) +\n  geom_line(size = 1, color = \"steelblue\") +\n  geom_point(size = 2) +\n  geom_hline(yintercept = 0.80, linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Statistical Power vs Sample Size\",\n       subtitle = \"Detecting OR = 2.0 with 20% baseline risk\",\n       x = \"Sample Size per Group\",\n       y = \"Power (1 - Œ≤)\") +\n  theme_minimal()",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#decision-framework",
    "href": "chapters/ch14-categorical_data.html#decision-framework",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "23.1 Decision Framework",
    "text": "23.1 Decision Framework\n\n\n\nSTART: Do you have categorical data?\n‚îÇ\n‚îú‚îÄ One categorical variable\n‚îÇ  ‚îî‚îÄ Chi-square goodness of fit test\n‚îÇ     (Do observed frequencies match expected?)\n‚îÇ\n‚îî‚îÄ Two or more variables\n   ‚îÇ\n   ‚îú‚îÄ Both categorical (no continuous predictors)\n   ‚îÇ  ‚îÇ\n   ‚îÇ  ‚îú‚îÄ 2√ó2 table, large sample (expected counts ‚â• 5)\n   ‚îÇ  ‚îÇ  ‚îî‚îÄ Chi-square test of independence\n   ‚îÇ  ‚îÇ\n   ‚îÇ  ‚îú‚îÄ 2√ó2 table, small sample (expected counts &lt; 5)\n   ‚îÇ  ‚îÇ  ‚îî‚îÄ Fisher's exact test\n   ‚îÇ  ‚îÇ\n   ‚îÇ  ‚îî‚îÄ Larger table\n   ‚îÇ     ‚îî‚îÄ Chi-square test of independence\n   ‚îÇ\n   ‚îî‚îÄ Binary outcome + continuous predictors\n      ‚îÇ\n      ‚îú‚îÄ Single predictor, just testing association\n      ‚îÇ  ‚îî‚îÄ Chi-square test (if categorized) or\n      ‚îÇ     Logistic regression (for continuous)\n      ‚îÇ\n      ‚îî‚îÄ Multiple predictors or need predictions\n         ‚îî‚îÄ Logistic regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#complete-analysis-example",
    "href": "chapters/ch14-categorical_data.html#complete-analysis-example",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "23.2 Complete Analysis Example",
    "text": "23.2 Complete Analysis Example\nLet‚Äôs walk through a complete analysis with a new dataset:\n\n\nCode\n# Research question: Does feed type affect mortality in broiler chickens?\n# Also considering weight and age as covariates\n\nset.seed(789)\nbroiler_data &lt;- tibble(\n  bird_id = 1:400,\n  feed_type = sample(c(\"Standard\", \"Enhanced\"), 400, replace = TRUE),\n  weight_kg = rnorm(400, mean = 2.5, sd = 0.4),\n  age_days = sample(35:49, 400, replace = TRUE)\n) %&gt;%\n  mutate(\n    mortality_risk = plogis(-6 +\n                           0.5 * (feed_type == \"Enhanced\") -\n                           0.8 * weight_kg +\n                           0.05 * age_days),\n    mortality = rbinom(400, 1, prob = mortality_risk),\n    mortality_fct = factor(mortality, levels = c(0, 1), labels = c(\"Alive\", \"Dead\"))\n  )\n\n\n\n23.2.1 Step 1: Exploratory Analysis\n\n\nCode\n# Overall mortality rate\nbroiler_data %&gt;%\n  count(mortality_fct) %&gt;%\n  mutate(percent = n / sum(n) * 100) %&gt;%\n  kable(digits = 1, caption = \"Overall Mortality\")\n\n\n\nOverall Mortality\n\n\nmortality_fct\nn\npercent\n\n\n\n\nAlive\n397\n99.2\n\n\nDead\n3\n0.8\n\n\n\n\n\nCode\n# Mortality by feed type\nbroiler_data %&gt;%\n  count(feed_type, mortality_fct) %&gt;%\n  group_by(feed_type) %&gt;%\n  mutate(percent = n / sum(n) * 100) %&gt;%\n  kable(digits = 1, caption = \"Mortality by Feed Type\")\n\n\n\nMortality by Feed Type\n\n\nfeed_type\nmortality_fct\nn\npercent\n\n\n\n\nEnhanced\nAlive\n216\n98.6\n\n\nEnhanced\nDead\n3\n1.4\n\n\nStandard\nAlive\n181\n100.0\n\n\n\n\n\n\n\nCode\n# Visualization\nbroiler_data %&gt;%\n  ggplot(aes(x = feed_type, fill = mortality_fct)) +\n  geom_bar(position = \"fill\") +\n  scale_fill_manual(values = c(\"Alive\" = \"steelblue\", \"Dead\" = \"coral\")) +\n  scale_y_continuous(labels = percent_format()) +\n  labs(title = \"Mortality Rate by Feed Type\",\n       x = \"Feed Type\", y = \"Proportion\", fill = \"Status\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n23.2.2 Step 2: Simple Association Test\n\n\nCode\n# Chi-square test\nmortality_table &lt;- table(broiler_data$feed_type, broiler_data$mortality_fct)\nchi_mortality &lt;- chisq.test(mortality_table)\ntidy(chi_mortality) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Test: Feed Type vs Mortality\")\n\n\n\nChi-Square Test: Feed Type vs Mortality\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n0.9968\n0.3181\n1\nPearson‚Äôs Chi-squared test with Yates‚Äô continuity correction\n\n\n\n\n\n\n\n23.2.3 Step 3: Calculate Effect Size (Odds Ratio)\n\n\nCode\n# Calculate odds ratio\na &lt;- mortality_table[\"Enhanced\", \"Dead\"]\nb &lt;- mortality_table[\"Enhanced\", \"Alive\"]\nc &lt;- mortality_table[\"Standard\", \"Dead\"]\nd &lt;- mortality_table[\"Standard\", \"Alive\"]\n\nor_simple &lt;- (a * d) / (b * c)\ncat(\"Odds Ratio (Enhanced vs Standard):\", round(or_simple, 3), \"\\n\")\n\n\nOdds Ratio (Enhanced vs Standard): Inf \n\n\n\n\n23.2.4 Step 4: Logistic Regression (Adjusting for Confounders)\n\n\nCode\n# Fit logistic regression\nmortality_model &lt;- glm(mortality ~ feed_type + weight_kg + age_days,\n                       data = broiler_data,\n                       family = binomial)\n\n# Results\ntidy(mortality_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\") %&gt;%\n  kable(digits = 4, caption = \"Adjusted Odds Ratios from Logistic Regression\")\n\n\n\nAdjusted Odds Ratios from Logistic Regression\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.0026\n6.8342\n-0.8714\n0.3836\n0.0000\n1.090366e+03\n\n\nfeed_typeStandard\n0.0000\n2155.7729\n-0.0080\n0.9936\nNA\n4.349210e+123\n\n\nweight_kg\n0.6014\n1.5840\n-0.3210\n0.7482\n0.0232\n1.339860e+01\n\n\nage_days\n1.0724\n0.1348\n0.5188\n0.6039\n0.8218\n1.447500e+00\n\n\n\n\n\n\n\n23.2.5 Step 5: Interpret and Report\n\n\nCode\n# Extract key results\nor_adjusted &lt;- exp(coef(mortality_model)[\"feed_typeStandard\"])\nci_adjusted &lt;- exp(confint(mortality_model)[\"feed_typeStandard\", ])\n\ncat(\"Findings:\\n\")\n\n\nFindings:\n\n\nCode\ncat(\"1. Unadjusted OR (chi-square):\", round(or_simple, 2), \"\\n\")\n\n\n1. Unadjusted OR (chi-square): Inf \n\n\nCode\ncat(\"2. Adjusted OR (logistic regression):\", round(or_adjusted, 2), \"\\n\")\n\n\n2. Adjusted OR (logistic regression): 0 \n\n\nCode\ncat(\"   95% CI: [\", round(ci_adjusted[1], 2), \",\", round(ci_adjusted[2], 2), \"]\\n\")\n\n\n   95% CI: [ NA , 4.34921e+123 ]\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"After adjusting for weight and age, feed type is\",\n    ifelse(summary(mortality_model)$coefficients[\"feed_typeStandard\", 4] &lt; 0.05,\n           \"significantly\", \"not significantly\"),\n    \"associated with mortality.\\n\")\n\n\nAfter adjusting for weight and age, feed type is not significantly associated with mortality.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#problem-1-mastitis-and-housing",
    "href": "chapters/ch14-categorical_data.html#problem-1-mastitis-and-housing",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "24.1 Problem 1: Mastitis and Housing",
    "text": "24.1 Problem 1: Mastitis and Housing\nA dairy farm compares mastitis rates across three housing systems:\n\n\nCode\n# Data\nmastitis_data &lt;- tibble(\n  housing = rep(c(\"Freestall\", \"Tiestall\", \"Pasture\"), each = 80),\n  mastitis = c(\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.25, 0.75)),  # Freestall\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.35, 0.65)),  # Tiestall\n    sample(c(\"Yes\", \"No\"), 80, replace = TRUE, prob = c(0.15, 0.85))   # Pasture\n  )\n)\n\n# Your tasks:\n# 1. Create a contingency table\n# 2. Perform chi-square test\n# 3. Calculate Cram√©r's V\n# 4. Visualize the results\n# 5. Interpret findings\n\n\nSolution:\n\n\nCode\n# 1. Contingency table\nmastitis_table &lt;- table(mastitis_data$housing, mastitis_data$mastitis)\naddmargins(mastitis_table)\n\n\n           \n             No Yes Sum\n  Freestall  58  22  80\n  Pasture    73   7  80\n  Tiestall   53  27  80\n  Sum       184  56 240\n\n\nCode\n# 2. Chi-square test\nchi_mastitis &lt;- chisq.test(mastitis_table)\ntidy(chi_mastitis) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Test Results\")\n\n\n\nChi-Square Test Results\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n15.1398\n5e-04\n2\nPearson‚Äôs Chi-squared test\n\n\n\n\n\nCode\n# 3. Cram√©r's V\nn &lt;- sum(mastitis_table)\nk &lt;- min(dim(mastitis_table))\ncramers_v_mastitis &lt;- sqrt(chi_mastitis$statistic / (n * (k - 1)))\ncat(\"Cram√©r's V:\", round(cramers_v_mastitis, 3), \"\\n\")\n\n\nCram√©r's V: 0.251 \n\n\nCode\n# 4. Visualization\nmastitis_data %&gt;%\n  count(housing, mastitis) %&gt;%\n  group_by(housing) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  filter(mastitis == \"Yes\") %&gt;%\n  ggplot(aes(x = housing, y = prop, fill = housing)) +\n  geom_col(show.legend = FALSE) +\n  geom_text(aes(label = paste0(round(prop * 100, 1), \"%\")),\n            vjust = -0.5) +\n  scale_y_continuous(labels = percent_format(), limits = c(0, 0.4)) +\n  labs(title = \"Mastitis Rate by Housing System\",\n       x = \"Housing System\", y = \"Mastitis Rate\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 5. Interpretation\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"There is a statistically significant association between housing system\")\n\n\nThere is a statistically significant association between housing system\n\n\nCode\ncat(\" and mastitis rate (p =\", round(chi_mastitis$p.value, 4), \").\\n\")\n\n\n and mastitis rate (p = 5e-04 ).\n\n\nCode\ncat(\"Pasture-based systems show the lowest mastitis rate, while tiestall\")\n\n\nPasture-based systems show the lowest mastitis rate, while tiestall\n\n\nCode\ncat(\" systems show the highest.\\n\")\n\n\n systems show the highest.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#problem-2-small-sample-fishers-test",
    "href": "chapters/ch14-categorical_data.html#problem-2-small-sample-fishers-test",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "24.2 Problem 2: Small Sample Fisher‚Äôs Test",
    "text": "24.2 Problem 2: Small Sample Fisher‚Äôs Test\nInvestigate a rare genetic disorder in a small sample:\n\n\nCode\n# Data: Does dam age affect disorder occurrence?\ngenetic_data &lt;- matrix(c(5, 2,   # Young dams\n                        1, 7),   # Older dams\n                      nrow = 2, byrow = TRUE,\n                      dimnames = list(Dam_Age = c(\"Young\", \"Older\"),\n                                     Disorder = c(\"Yes\", \"No\")))\n\n# Your tasks:\n# 1. Check expected cell counts\n# 2. Perform Fisher's exact test\n# 3. Calculate odds ratio\n# 4. Interpret results\n\n\nSolution:\n\n\nCode\n# 1. Check expected counts\nchi_test &lt;- chisq.test(genetic_data)\ncat(\"Expected counts:\\n\")\n\n\nExpected counts:\n\n\nCode\nprint(chi_test$expected)\n\n\n       Disorder\nDam_Age Yes  No\n  Young 2.8 4.2\n  Older 3.2 4.8\n\n\nCode\ncat(\"\\nNote: Expected counts &lt; 5, so Fisher's exact test is appropriate.\\n\\n\")\n\n\n\nNote: Expected counts &lt; 5, so Fisher's exact test is appropriate.\n\n\nCode\n# 2. Fisher's exact test\nfisher_genetic &lt;- fisher.test(genetic_data)\ncat(\"Fisher's Exact Test Results:\\n\")\n\n\nFisher's Exact Test Results:\n\n\nCode\ncat(\"Odds Ratio:\", round(fisher_genetic$estimate, 3), \"\\n\")\n\n\nOdds Ratio: 13.594 \n\n\nCode\ncat(\"95% CI: [\", round(fisher_genetic$conf.int[1], 3), \",\",\n    round(fisher_genetic$conf.int[2], 3), \"]\\n\")\n\n\n95% CI: [ 0.865 , 934.009 ]\n\n\nCode\ncat(\"P-value:\", round(fisher_genetic$p.value, 4), \"\\n\\n\")\n\n\nP-value: 0.0406 \n\n\nCode\n# 3. Manual OR calculation\nor_manual &lt;- (genetic_data[1,1] * genetic_data[2,2]) /\n             (genetic_data[1,2] * genetic_data[2,1])\ncat(\"Manual OR calculation:\", round(or_manual, 3), \"\\n\\n\")\n\n\nManual OR calculation: 17.5 \n\n\nCode\n# 4. Interpretation\ncat(\"Interpretation:\\n\")\n\n\nInterpretation:\n\n\nCode\ncat(\"Young dams have\", round(fisher_genetic$estimate, 1),\n    \"times higher odds of having offspring with the disorder\")\n\n\nYoung dams have 13.6 times higher odds of having offspring with the disorder\n\n\nCode\ncat(\" compared to older dams.\\n\")\n\n\n compared to older dams.\n\n\nCode\ncat(\"However, with p =\", round(fisher_genetic$p.value, 3),\n    \"and a small sample size,\\n\")\n\n\nHowever, with p = 0.041 and a small sample size,\n\n\nCode\ncat(\"we should interpret these results cautiously and consider collecting more data.\\n\")\n\n\nwe should interpret these results cautiously and consider collecting more data.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#problem-3-logistic-regression-with-multiple-predictors",
    "href": "chapters/ch14-categorical_data.html#problem-3-logistic-regression-with-multiple-predictors",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "24.3 Problem 3: Logistic Regression with Multiple Predictors",
    "text": "24.3 Problem 3: Logistic Regression with Multiple Predictors\nAnalyze factors affecting retained placenta in dairy cows:\n\n\nCode\n# Simulate data\nset.seed(2025)\nrp_data &lt;- tibble(\n  cow_id = 1:250,\n  parity = sample(1:5, 250, replace = TRUE),\n  bcs_precalving = rnorm(250, mean = 3.5, sd = 0.5),\n  twin_birth = sample(c(0, 1), 250, replace = TRUE, prob = c(0.95, 0.05)),\n  dystocia = sample(c(0, 1), 250, replace = TRUE, prob = c(0.85, 0.15))\n) %&gt;%\n  mutate(\n    rp_risk = plogis(-4 +\n                    0.2 * (parity &gt;= 3) +\n                    -0.5 * bcs_precalving +\n                    2.0 * twin_birth +\n                    1.5 * dystocia),\n    retained_placenta = rbinom(250, 1, prob = rp_risk)\n  )\n\n# Your tasks:\n# 1. Fit a logistic regression model\n# 2. Interpret odds ratios for each predictor\n# 3. Identify the strongest risk factor\n# 4. Predict RP risk for a specific cow profile\n\n\nSolution:\n\n\nCode\n# 1. Fit model\nrp_model &lt;- glm(retained_placenta ~ parity + bcs_precalving + twin_birth + dystocia,\n                data = rp_data,\n                family = binomial)\n\n# 2. Odds ratios with CIs\nrp_results &lt;- tidy(rp_model, conf.int = TRUE, exponentiate = TRUE, conf.method = \"Wald\")\nrp_results %&gt;%\n  kable(digits = 3, caption = \"Retained Placenta Risk Factors: Odds Ratios\")\n\n\n\nRetained Placenta Risk Factors: Odds Ratios\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.000000e+00\n109713.535\n-0.003\n0.998\n0\nInf\n\n\nparity\n1.255316e+06\n9308.983\n0.002\n0.999\n0\nNA\n\n\nbcs_precalving\n8.306630e+20\n19892.759\n0.002\n0.998\n0\nInf\n\n\ntwin_birth\n0.000000e+00\n59064.424\n0.000\n1.000\n0\nInf\n\n\ndystocia\n1.517005e+17\n21074.857\n0.002\n0.999\n0\nInf\n\n\n\n\n\nCode\n# 3. Identify strongest risk factor\ncat(\"\\nStrongest Risk Factors (by OR magnitude):\\n\")\n\n\n\nStrongest Risk Factors (by OR magnitude):\n\n\nCode\nrp_results %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  arrange(desc(abs(estimate - 1))) %&gt;%\n  select(term, estimate, p.value) %&gt;%\n  print()\n\n\n# A tibble: 4 √ó 3\n  term           estimate p.value\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt;\n1 bcs_precalving 8.31e+20   0.998\n2 dystocia       1.52e+17   0.999\n3 parity         1.26e+ 6   0.999\n4 twin_birth     5.83e- 6   1.00 \n\n\nCode\n# 4. Predict for specific profiles\nnew_cows &lt;- tibble(\n  scenario = c(\"Low risk\", \"High risk\"),\n  parity = c(2, 4),\n  bcs_precalving = c(3.5, 2.8),\n  twin_birth = c(0, 1),\n  dystocia = c(0, 1)\n)\n\nnew_cows %&gt;%\n  mutate(predicted_rp_risk = predict(rp_model, newdata = new_cows,\n                                     type = \"response\")) %&gt;%\n  kable(digits = 3, caption = \"Predicted Retained Placenta Risk\")\n\n\n\nPredicted Retained Placenta Risk\n\n\n\n\n\n\n\n\n\n\nscenario\nparity\nbcs_precalving\ntwin_birth\ndystocia\npredicted_rp_risk\n\n\n\n\nLow risk\n2\n3.5\n0\n0\n0\n\n\nHigh risk\n4\n2.8\n1\n1\n0\n\n\n\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"- Twin births have the strongest association with retained placenta\\n\")\n\n\n- Twin births have the strongest association with retained placenta\n\n\nCode\ncat(\"- Higher BCS appears protective (OR &lt; 1)\\n\")\n\n\n- Higher BCS appears protective (OR &lt; 1)\n\n\nCode\ncat(\"- Dystocia also substantially increases risk\\n\")\n\n\n- Dystocia also substantially increases risk\n\n\nCode\ncat(\"- A high-risk cow profile (parity 4, low BCS, twins, dystocia) has\\n\")\n\n\n- A high-risk cow profile (parity 4, low BCS, twins, dystocia) has\n\n\nCode\ncat(\"  substantially higher predicted risk than a low-risk cow.\\n\")\n\n\n  substantially higher predicted risk than a low-risk cow.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#problem-4-goodness-of-fit---genetic-ratios",
    "href": "chapters/ch14-categorical_data.html#problem-4-goodness-of-fit---genetic-ratios",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "24.4 Problem 4: Goodness of Fit - Genetic Ratios",
    "text": "24.4 Problem 4: Goodness of Fit - Genetic Ratios\nTest if offspring coat colors match expected Mendelian ratios:\n\n\nCode\n# Observed offspring: Black, Brown, White\nobserved_colors &lt;- c(Black = 42, Brown = 38, White = 20)\n\n# Expected ratio: 2:2:1\n# Your tasks:\n# 1. Perform chi-square goodness of fit test\n# 2. Visualize observed vs expected\n# 3. Interpret whether data fits the genetic model\n\n\nSolution:\n\n\nCode\n# 1. Chi-square goodness of fit\nexpected_proportions &lt;- c(2, 2, 1) / 5\nchi_colors &lt;- chisq.test(observed_colors, p = expected_proportions)\n\ntidy(chi_colors) %&gt;%\n  kable(digits = 4, caption = \"Chi-Square Goodness of Fit Test\")\n\n\n\nChi-Square Goodness of Fit Test\n\n\n\n\n\n\n\n\nstatistic\np.value\nparameter\nmethod\n\n\n\n\n0.2\n0.9048\n2\nChi-squared test for given probabilities\n\n\n\n\n\nCode\n# 2. Visualize\ncolor_comparison &lt;- tibble(\n  Color = names(observed_colors),\n  Observed = observed_colors,\n  Expected = expected_proportions * sum(observed_colors)\n) %&gt;%\n  pivot_longer(cols = c(Observed, Expected),\n               names_to = \"Type\", values_to = \"Count\")\n\ncolor_comparison %&gt;%\n  ggplot(aes(x = Color, y = Count, fill = Type)) +\n  geom_col(position = \"dodge\") +\n  geom_text(aes(label = round(Count, 1)),\n            position = position_dodge(width = 0.9), vjust = -0.5) +\n  scale_fill_manual(values = c(\"Observed\" = \"steelblue\",\n                                \"Expected\" = \"coral\")) +\n  labs(title = \"Coat Color Distribution: Observed vs Expected\",\n       subtitle = paste0(\"œá¬≤ = \", round(chi_colors$statistic, 2),\n                        \", p = \", round(chi_colors$p.value, 3)),\n       x = \"Coat Color\", y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n# 3. Interpretation\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\ncat(\"P-value =\", round(chi_colors$p.value, 4), \"\\n\")\n\n\nP-value = 0.9048 \n\n\nCode\nif(chi_colors$p.value &gt; 0.05) {\n  cat(\"The observed data is consistent with the expected 2:2:1 ratio.\\n\")\n  cat(\"We do not have evidence to reject the proposed genetic model.\\n\")\n} else {\n  cat(\"The observed data significantly differs from the expected 2:2:1 ratio.\\n\")\n  cat(\"The genetic model may not fit, or other factors may be at play.\\n\")\n}\n\n\nThe observed data is consistent with the expected 2:2:1 ratio.\nWe do not have evidence to reject the proposed genetic model.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#key-concepts",
    "href": "chapters/ch14-categorical_data.html#key-concepts",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "25.1 Key Concepts",
    "text": "25.1 Key Concepts\n\nChi-Square Goodness of Fit: Tests if observed frequencies match expected distribution\n\nOne categorical variable\nUsed for genetic ratios, expected proportions\n\nChi-Square Test of Independence: Tests association between two categorical variables\n\nWorks with 2√ó2 or larger tables\nRequires expected cell counts ‚â• 5\n\nFisher‚Äôs Exact Test: Alternative to chi-square for small samples\n\nUse when expected counts &lt; 5\nProvides exact p-value\n\nRisk Ratios and Odds Ratios: Quantify strength of association\n\nRisk Ratio: More intuitive, ratio of probabilities\nOdds Ratio: Used in logistic regression, ratio of odds\nOR ‚âà RR when outcome is rare\n\nLogistic Regression: Model binary outcomes with continuous/multiple predictors\n\nUses logit link function\nCoefficients are log-odds\nExponentiate to get odds ratios\nCan include multiple predictors and control for confounders",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#decision-framework-summary",
    "href": "chapters/ch14-categorical_data.html#decision-framework-summary",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "25.2 Decision Framework Summary",
    "text": "25.2 Decision Framework Summary\n\n\n\nChoosing the Right Test for Categorical Data\n\n\n\n\n\n\nSituation\nRecommended Test\n\n\n\n\nOne categorical variable vs expected distribution\nChi-square goodness of fit\n\n\nTwo categorical variables, large sample\nChi-square test of independence\n\n\nTwo categorical variables, small sample\nFisher‚Äôs exact test\n\n\nBinary outcome + continuous predictors\nLogistic regression\n\n\nBinary outcome + multiple predictors\nLogistic regression\n\n\nNeed predictions for new observations\nLogistic regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#r-functions-summary",
    "href": "chapters/ch14-categorical_data.html#r-functions-summary",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "25.3 R Functions Summary",
    "text": "25.3 R Functions Summary\n\n\n\nKey R Functions for Categorical Data Analysis\n\n\n\n\n\n\nFunction\nPurpose\n\n\n\n\ntable()\nCreate contingency tables\n\n\nprop.table()\nConvert counts to proportions\n\n\nchisq.test()\nChi-square tests (goodness of fit and independence)\n\n\nfisher.test()\nFisher‚Äôs exact test for small samples\n\n\nglm(..., family = binomial)\nFit logistic regression model\n\n\npredict(..., type = 'response')\nGet predicted probabilities from logistic regression\n\n\nexp(coef())\nConvert log-odds to odds ratios",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#common-pitfalls-to-avoid",
    "href": "chapters/ch14-categorical_data.html#common-pitfalls-to-avoid",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "25.4 Common Pitfalls to Avoid",
    "text": "25.4 Common Pitfalls to Avoid\n\n\n\n\n\n\nWarningWatch Out For These Mistakes\n\n\n\n\nUsing chi-square with small expected counts ‚Üí Use Fisher‚Äôs exact test\nConfusing odds ratios with risk ratios ‚Üí They‚Äôre different! (except for rare outcomes)\nInterpreting association as causation ‚Üí Observational data requires caution\nIgnoring effect sizes ‚Üí Statistical significance ‚â† practical importance\nOverfitting logistic models ‚Üí Too many predictors for sample size\nForgetting independence assumption ‚Üí Clustered/repeated measures violate this\nMultiple testing without correction ‚Üí Increases false positive rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#practical-significance",
    "href": "chapters/ch14-categorical_data.html#practical-significance",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "25.5 Practical Significance",
    "text": "25.5 Practical Significance\nAlways consider: - Effect size (Cram√©r‚Äôs V, odds ratios) not just p-values - Confidence intervals for uncertainty - Biological/practical meaning of findings - Study design limitations (observational vs experimental)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#recommended-reading",
    "href": "chapters/ch14-categorical_data.html#recommended-reading",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "26.1 Recommended Reading",
    "text": "26.1 Recommended Reading\n\nAgresti, A. (2018). An Introduction to Categorical Data Analysis (3rd ed.)\n\nComprehensive, mathematical treatment\n\nHosmer, Lemeshow, & Sturdivant (2013). Applied Logistic Regression (3rd ed.)\n\nPractical guide to logistic regression\n\nDohoo, Martin, & Stryhn (2014). Veterinary Epidemiologic Research (2nd ed.)\n\nExcellent for animal health applications",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#online-resources",
    "href": "chapters/ch14-categorical_data.html#online-resources",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "26.2 Online Resources",
    "text": "26.2 Online Resources\n\nStatQuest Videos: ‚ÄúChi-Square Test‚Äù and ‚ÄúLogistic Regression‚Äù by Josh Starmer\nr-bloggers: Tutorials on categorical data analysis in R\nCross Validated (StackExchange): Q&A on statistical concepts",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#r-packages-for-categorical-data",
    "href": "chapters/ch14-categorical_data.html#r-packages-for-categorical-data",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "26.3 R Packages for Categorical Data",
    "text": "26.3 R Packages for Categorical Data\n\n\nCode\n# Beyond base R functions\ninstall.packages(c(\n  \"vcd\",        # Visualizing categorical data\n  \"epitools\",   # Epidemiology tools (OR, RR calculations)\n  \"rms\",        # Regression modeling strategies\n  \"MASS\"        # Ordinal logistic regression (polr)\n))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#sec-preview",
    "href": "chapters/ch14-categorical_data.html#sec-preview",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "26.4 Next Week Preview: Simple Linear Regression",
    "text": "26.4 Next Week Preview: Simple Linear Regression\nNext week, we return to continuous outcomes and explore linear regression:\n\nCorrelation vs regression\nFitting a regression line\nInterpreting slope and intercept\nResidual diagnostics\nPrediction and confidence intervals\n\nConnection to This Week: Logistic regression and linear regression are both types of Generalized Linear Models (GLMs). The main difference is: - Linear regression: Continuous outcome, identity link - Logistic regression: Binary outcome, logit link\nThe concepts of model fitting, interpretation, and diagnostics carry over!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch14-categorical_data.html#week-6-homework-categorical-data-analysis",
    "href": "chapters/ch14-categorical_data.html#week-6-homework-categorical-data-analysis",
    "title": "14¬† Week 14: Categorical Data Analysis",
    "section": "27.1 Week 6 Homework: Categorical Data Analysis",
    "text": "27.1 Week 6 Homework: Categorical Data Analysis\n\n27.1.1 Instructions\nComplete the following analyses using R and Quarto. Submit both your .qmd and rendered .html files.\n\n\n27.1.2 Part 1: Chi-Square Analysis (30 points)\nYou‚Äôre investigating the relationship between calving ease and calf sex in beef cattle. Use the following data:\n\n\nCode\n# Run this code to create your dataset\nset.seed(YOUR_STUDENT_ID)  # Use your actual student ID number\ncalving_data &lt;- tibble(\n  calf_sex = sample(c(\"Male\", \"Female\"), 200, replace = TRUE),\n  calving_ease = sample(c(\"Easy\", \"Moderate\", \"Difficult\"), 200,\n                        replace = TRUE,\n                        prob = c(0.60, 0.30, 0.10))\n)\n\n\nTasks: 1. Create a contingency table with row and column totals 2. Calculate proportions by calf sex 3. Perform a chi-square test of independence 4. Calculate Cram√©r‚Äôs V to quantify effect size 5. Create a visualization showing calving ease by calf sex 6. Write a 3-4 sentence interpretation of your findings\n\n\n27.1.3 Part 2: Odds Ratios and Risk Ratios (25 points)\nA veterinary study investigates whether pre-weaning vaccination reduces diarrhea in piglets:\n\nVaccinated: 8 with diarrhea, 92 without\nUnvaccinated: 20 with diarrhea, 80 without\n\nTasks: 1. Create a 2√ó2 table 2. Calculate the risk of diarrhea in each group 3. Calculate the risk ratio 4. Calculate the odds ratio 5. Perform Fisher‚Äôs exact test 6. Interpret all results, including vaccine efficacy\n\n\n27.1.4 Part 3: Logistic Regression Analysis (35 points)\nUse the following simulated data on lameness in dairy cows:\n\n\nCode\nset.seed(YOUR_STUDENT_ID + 100)\nlameness_data &lt;- tibble(\n  cow_id = 1:300,\n  age_years = runif(300, 2, 8),\n  bcs = rnorm(300, mean = 3.0, sd = 0.5),\n  hoof_trimming = sample(c(\"Regular\", \"Irregular\"), 300, replace = TRUE),\n  floor_type = sample(c(\"Concrete\", \"Rubber\"), 300, replace = TRUE)\n) %&gt;%\n  mutate(\n    lameness_prob = plogis(-2 + 0.3 * age_years - 0.5 * bcs +\n                           1.2 * (hoof_trimming == \"Irregular\") -\n                           0.8 * (floor_type == \"Rubber\")),\n    lameness = rbinom(300, 1, prob = lameness_prob)\n  )\n\n\nTasks: 1. Fit a logistic regression model with lameness as the outcome and all other variables (except cow_id and lameness_prob) as predictors 2. Create a table of odds ratios with 95% confidence intervals 3. Identify which variables are statistically significant 4. Interpret the odds ratio for hoof_trimming in context 5. Create predictions for these three cow profiles: - Young cow (3 years), BCS 3.5, Regular trimming, Rubber floor - Old cow (7 years), BCS 2.5, Irregular trimming, Concrete floor - Average cow (5 years), BCS 3.0, Regular trimming, Concrete floor 6. Visualize predicted probability of lameness by age, with separate lines for regular vs irregular hoof trimming\n\n\n27.1.5 Part 4: Reflection (10 points)\nWrite a 250-300 word reflection addressing:\n\nWhen would you use chi-square vs Fisher‚Äôs exact test?\nWhat‚Äôs the key advantage of logistic regression over simple chi-square tests?\nDescribe one challenge you encountered in this week‚Äôs material and how you addressed it\nHow might you apply these methods in your own research or field of interest?\n\n\n\n27.1.6 Submission Checklist\n\nAll code runs without errors\nAll visualizations have informative titles and labels\nInterpretations are written in complete sentences\nResults are presented in tables where appropriate (using kable())\nQuarto document renders to HTML successfully\nBoth .qmd and .html files submitted\n\n\n\n27.1.7 Grading Rubric\n\nCode correctness (40%): Code runs, uses appropriate functions, no major errors\nInterpretation (40%): Correct interpretation of results, addresses all questions\nPresentation (15%): Clear visualizations, well-formatted tables, organized document\nReflection (5%): Thoughtful responses, demonstrates understanding\n\n\nDue Date: [To be specified by instructor]\nEstimated Time: 3-4 hours\nGood luck! Remember to use the course resources and ask questions if you get stuck.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Week 14: Categorical Data Analysis</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html",
    "href": "chapters/ch15-linear_regression.html",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "",
    "text": "16 Introduction\nImagine you‚Äôre managing a dairy operation and notice that cows consuming more feed tend to produce more milk. But how much more milk can you expect for each additional kilogram of feed? Can you predict a cow‚Äôs milk production based on her feed intake? And how confident can you be in those predictions?\nThese questions move us beyond simply testing whether differences exist (hypothesis testing) or whether variables are associated (correlation) to actually modeling relationships and making predictions. This is the domain of regression analysis.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#motivating-scenario",
    "href": "chapters/ch15-linear_regression.html#motivating-scenario",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "16.1 Motivating Scenario",
    "text": "16.1 Motivating Scenario\nA dairy nutritionist wants to optimize feeding strategies. She has data on daily feed intake (kg of dry matter) and milk production (kg/day) from 50 cows. Her questions include:\n\nIs there a relationship between feed intake and milk yield?\nFor every additional kg of feed consumed, how much more milk is produced?\nCan I predict milk production for a cow consuming 20 kg of feed per day?\nHow accurate are these predictions?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#key-questions-well-address",
    "href": "chapters/ch15-linear_regression.html#key-questions-well-address",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "16.2 Key Questions We‚Äôll Address",
    "text": "16.2 Key Questions We‚Äôll Address\n\n\n\n\n\n\nNoteThis Week‚Äôs Learning Objectives\n\n\n\nBy the end of this session, you will be able to:\n\nDistinguish between correlation and regression analyses\nFit and interpret simple linear regression models\nAssess model fit using \\(R^2\\) and residual diagnostics\nCheck regression assumptions with diagnostic plots\nMake predictions with confidence and prediction intervals\nReport regression results appropriately",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#building-on-previous-weeks",
    "href": "chapters/ch15-linear_regression.html#building-on-previous-weeks",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "16.3 Building on Previous Weeks",
    "text": "16.3 Building on Previous Weeks\n\n\n\n\n\n\nTipConnection to Previous Material\n\n\n\n\nWeek 2: We used scatter plots for exploratory analysis\nWeek 3: We learned about sampling distributions and confidence intervals\nWeek 4: We tested hypotheses about means (t-tests)\nWeek 7 (Now): We model relationships between variables and make predictions\n\nRegression extends hypothesis testing by not just asking ‚Äúis there an effect?‚Äù but ‚Äúhow large is the effect and what can we predict?‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#conceptual-differences",
    "href": "chapters/ch15-linear_regression.html#conceptual-differences",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "17.1 Conceptual Differences",
    "text": "17.1 Conceptual Differences\nCorrelation measures the strength and direction of a linear relationship between two variables: - Both variables are random - Symmetric relationship (correlating X with Y = correlating Y with X) - Answers: ‚ÄúHow strongly are these variables associated?‚Äù - No prediction involved\nRegression models one variable as a function of another: - Response variable (Y) is random; predictor variable (X) may be fixed or random - Asymmetric relationship (regressing Y on X ‚â† regressing X on Y) - Answers: ‚ÄúHow does Y change as X changes?‚Äù and ‚ÄúWhat value of Y do we predict for a given X?‚Äù - Enables prediction",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#pearson-correlation-coefficient",
    "href": "chapters/ch15-linear_regression.html#pearson-correlation-coefficient",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "17.2 Pearson Correlation Coefficient",
    "text": "17.2 Pearson Correlation Coefficient\nThe Pearson correlation coefficient (\\(r\\)) quantifies linear association:\n\\[r = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum(x_i - \\bar{x})^2 \\sum(y_i - \\bar{y})^2}}\\]\nProperties: - Range: \\(-1 \\leq r \\leq 1\\) - \\(r = 1\\): Perfect positive linear relationship - \\(r = -1\\): Perfect negative linear relationship - \\(r = 0\\): No linear relationship - \\(r^2\\) equals the proportion of variance in Y explained by X (in regression context)\nInterpretation Guidelines (rough rules of thumb): - \\(|r| &lt; 0.3\\): Weak correlation - \\(0.3 \\leq |r| &lt; 0.7\\): Moderate correlation - \\(|r| \\geq 0.7\\): Strong correlation\n\n\n\n\n\n\nWarningImportant Limitations of Correlation\n\n\n\n\nOnly measures linear relationships (can miss nonlinear patterns)\nSensitive to outliers\nDoes not imply causation\nCan be misleading with restricted ranges",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#example-feed-intake-and-milk-yield",
    "href": "chapters/ch15-linear_regression.html#example-feed-intake-and-milk-yield",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "17.3 Example: Feed Intake and Milk Yield",
    "text": "17.3 Example: Feed Intake and Milk Yield\nLet‚Äôs generate some realistic data and explore correlation:\n\n\nCode\n# Simulate dairy cow data\nn &lt;- 50\nfeed_intake &lt;- rnorm(n, mean = 22, sd = 3)  # kg DM/day\n\n# Milk yield depends on feed intake plus random variation\nmilk_yield &lt;- 8 + 1.5 * feed_intake + rnorm(n, mean = 0, sd = 4)  # kg/day\n\n# Create data frame\ndairy_data &lt;- tibble(\n  cow_id = 1:n,\n  feed_intake = feed_intake,\n  milk_yield = milk_yield\n)\n\n# Visualize relationship\nggplot(dairy_data, aes(x = feed_intake, y = milk_yield)) +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  labs(\n    title = \"Relationship Between Feed Intake and Milk Yield\",\n    subtitle = \"50 dairy cows\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\nCalculate correlation:\n\n\nCode\n# Pearson correlation\ncor_result &lt;- cor.test(dairy_data$feed_intake, dairy_data$milk_yield)\n\n# Display results\ncor_result\n\n\n\n    Pearson's product-moment correlation\n\ndata:  dairy_data$feed_intake and dairy_data$milk_yield\nt = 7.0995, df = 48, p-value = 5.175e-09\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.5461394 0.8289102\nsample estimates:\n      cor \n0.7156903 \n\n\nCode\n# Extract key values\nr_value &lt;- cor_result$estimate\np_value &lt;- cor_result$p.value\nci_lower &lt;- cor_result$conf.int[1]\nci_upper &lt;- cor_result$conf.int[2]\n\n\nInterpretation: The correlation between feed intake and milk yield is \\(r\\) = 0.716 (95% CI: [0.546, 0.829], p &lt; 0.001). This indicates a strong positive linear relationship.\nBut correlation alone doesn‚Äôt tell us: - How much milk yield changes per kg of feed - What milk yield to expect for a specific feed intake - This is where regression comes in!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#the-linear-model",
    "href": "chapters/ch15-linear_regression.html#the-linear-model",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "18.1 The Linear Model",
    "text": "18.1 The Linear Model\nSimple linear regression models the relationship between a continuous response variable (\\(Y\\)) and a single predictor variable (\\(X\\)):\n\\[Y_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i\\]\nWhere: - \\(Y_i\\) = response variable for observation \\(i\\) (e.g., milk yield) - \\(X_i\\) = predictor variable for observation \\(i\\) (e.g., feed intake) - \\(\\beta_0\\) = intercept (expected value of \\(Y\\) when \\(X = 0\\)) - \\(\\beta_1\\) = slope (change in \\(Y\\) for a 1-unit increase in \\(X\\)) - \\(\\epsilon_i\\) = random error term, assumed \\(\\epsilon_i \\sim N(0, \\sigma^2)\\)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#least-squares-estimation",
    "href": "chapters/ch15-linear_regression.html#least-squares-estimation",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "18.2 Least Squares Estimation",
    "text": "18.2 Least Squares Estimation\nThe method of least squares finds the line that minimizes the sum of squared residuals (vertical distances from points to the line):\n\\[\\text{Minimize: } \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} (y_i - (\\beta_0 + \\beta_1 x_i))^2\\]\nThe resulting estimates are:\n\\[\\hat{\\beta}_1 = \\frac{\\sum(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum(x_i - \\bar{x})^2} = r \\frac{s_y}{s_x}\\]\n\\[\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}\\]\n\n\n\n\n\n\nTipKey Insight\n\n\n\nThe regression line always passes through the point \\((\\bar{x}, \\bar{y})\\).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#fitting-a-model-in-r",
    "href": "chapters/ch15-linear_regression.html#fitting-a-model-in-r",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "18.3 Fitting a Model in R",
    "text": "18.3 Fitting a Model in R\nLet‚Äôs fit a regression model to our dairy data:\n\n\nCode\n# Fit simple linear regression\nmodel1 &lt;- lm(milk_yield ~ feed_intake, data = dairy_data)\n\n# Display summary\nsummary(model1)\n\n\n\nCall:\nlm(formula = milk_yield ~ feed_intake, data = dairy_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-9.2717 -2.7034 -0.0829  2.7748  7.2574 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   9.8294     4.4337   2.217   0.0314 *  \nfeed_intake   1.3898     0.1958   7.100 5.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.123 on 48 degrees of freedom\nMultiple R-squared:  0.5122,    Adjusted R-squared:  0.5021 \nF-statistic:  50.4 on 1 and 48 DF,  p-value: 5.175e-09",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#sec-interpretation",
    "href": "chapters/ch15-linear_regression.html#sec-interpretation",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "18.4 Interpreting Coefficients",
    "text": "18.4 Interpreting Coefficients\n\n\nCode\n# Tidy output with broom\ntidy_model &lt;- tidy(model1, conf.int = TRUE)\ntidy_model\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic       p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     9.83     4.43       2.22 0.0314           0.915     18.7 \n2 feed_intake     1.39     0.196      7.10 0.00000000518    0.996      1.78\n\n\nCode\n# Extract coefficients\nintercept &lt;- coef(model1)[1]\nslope &lt;- coef(model1)[2]\n\n\nIntercept (\\(\\hat{\\beta}_0\\) = 9.83): - The predicted milk yield when feed intake = 0 kg/day - Interpretation: Not biologically meaningful in this case (cows can‚Äôt consume 0 feed and produce milk!) - Often, the intercept is not of primary interest; it ensures the line fits the data well\nSlope (\\(\\hat{\\beta}_1\\) = 1.39): - For every 1 kg increase in feed intake (dry matter), milk yield increases by 1.39 kg/day, on average - This is the key quantity of interest for the dairy nutritionist - 95% CI: [, ]\n\n\n\n\n\n\nImportantAlways Include Units!\n\n\n\nWhen interpreting coefficients, always specify units: - ‚ÄúFor every 1 kg increase in feed intake‚Ä¶‚Äù - ‚Äú‚Ä¶milk yield increases by 1.53 kg/day‚Äù\nCoefficients without units are meaningless in applied contexts.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#visualizing-the-fitted-line",
    "href": "chapters/ch15-linear_regression.html#visualizing-the-fitted-line",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "18.5 Visualizing the Fitted Line",
    "text": "18.5 Visualizing the Fitted Line\n\n\nCode\n# Base scatter plot with regression line\np1 &lt;- ggplot(dairy_data, aes(x = feed_intake, y = milk_yield)) +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Simple Linear Regression\",\n    subtitle = \"Fitted line with 95% confidence band\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  )\n\n# Add residuals visualization\ndairy_augmented &lt;- augment(model1, data = dairy_data)\n\np2 &lt;- ggplot(dairy_augmented, aes(x = feed_intake, y = milk_yield)) +\n  geom_segment(aes(xend = feed_intake, yend = .fitted),\n               alpha = 0.4, color = \"gray50\") +\n  geom_point(size = 3, alpha = 0.6, color = \"steelblue\") +\n  geom_line(aes(y = .fitted), color = \"red\", linewidth = 1) +\n  labs(\n    title = \"Residuals Visualization\",\n    subtitle = \"Vertical lines show residuals (observed - predicted)\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  )\n\n# Combine plots\np1 / p2",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#understanding-r-squared",
    "href": "chapters/ch15-linear_regression.html#understanding-r-squared",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "19.1 Understanding R-squared",
    "text": "19.1 Understanding R-squared\nR-squared (\\(R^2\\)) represents the proportion of variance in Y explained by X:\n\\[R^2 = 1 - \\frac{SS_{residual}}{SS_{total}} = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}\\]\nInterpretation: - Range: \\(0 \\leq R^2 \\leq 1\\) - \\(R^2 = 0\\): Model explains none of the variance (no better than using \\(\\bar{y}\\) as prediction) - \\(R^2 = 1\\): Model explains all variance (perfect fit) - For simple linear regression: \\(R^2 = r^2\\) (square of correlation coefficient)\n\n\nCode\n# Extract R-squared\nmodel_summary &lt;- glance(model1)\nmodel_summary\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.512         0.502  4.12      50.4 0.00000000518     1  -141.  288.  293.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nCode\nr_squared &lt;- model_summary$r.squared\nadj_r_squared &lt;- model_summary$adj.r.squared\n\n\nFor our model: \\(R^2\\) = 0.512, meaning 51.2% of the variance in milk yield is explained by feed intake.\n\n\n\n\n\n\nNoteWhat about the other 48.8%?\n\n\n\nThe remaining variance is due to: - Other factors not in the model (genetics, lactation stage, health status, etc.) - Random biological variation - Measurement error\nThis is normal! Perfect \\(R^2 = 1\\) is rare in biological data.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#residual-standard-error",
    "href": "chapters/ch15-linear_regression.html#residual-standard-error",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "19.2 Residual Standard Error",
    "text": "19.2 Residual Standard Error\nThe residual standard error (RSE) estimates \\(\\sigma\\), the standard deviation of the errors:\n\\[\\text{RSE} = \\sqrt{\\frac{\\sum(y_i - \\hat{y}_i)^2}{n - 2}}\\]\n\n\nCode\nrse &lt;- model_summary$sigma\n\n\nFor our model: RSE = 4.12 kg/day\nInterpretation: On average, observed milk yields deviate from predicted values by about 4.12 kg/day. This is the typical prediction error.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#hypothesis-test-for-the-slope",
    "href": "chapters/ch15-linear_regression.html#hypothesis-test-for-the-slope",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "19.3 Hypothesis Test for the Slope",
    "text": "19.3 Hypothesis Test for the Slope\nThe model output includes a t-test for \\(H_0: \\beta_1 = 0\\) (no relationship):\n\\[t = \\frac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)}\\]\n\n\nCode\n# Extract slope test\nslope_test &lt;- tidy_model[2, ]\nt_stat &lt;- slope_test$statistic\np_val &lt;- slope_test$p.value\n\n\nResult: \\(t\\) = 7.10, p &lt; 0.001\nThis provides very strong evidence that feed intake is associated with milk yield (slope is not zero).\n\n\n\n\n\n\nTipStatistical vs Practical Significance\n\n\n\nA significant p-value tells us there‚Äôs a relationship, but doesn‚Äôt tell us if it‚Äôs practically meaningful:\n\nA slope of 0.01 might be statistically significant (large sample) but meaningless biologically\nFocus on effect size (magnitude of the slope) and confidence intervals",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#the-four-diagnostic-plots",
    "href": "chapters/ch15-linear_regression.html#the-four-diagnostic-plots",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "20.1 The Four Diagnostic Plots",
    "text": "20.1 The Four Diagnostic Plots\nR provides four key diagnostic plots via plot(model):\n\n\nCode\n# Set up 2x2 plotting layout\npar(mfrow = c(2, 2))\nplot(model1)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))  # Reset\n\n\n\n20.1.1 Plot 1: Residuals vs Fitted\nPurpose: Check linearity and homoscedasticity\nWhat to look for: - Random scatter around horizontal line at 0 - No clear patterns (U-shapes, curves, funnels)\nOur plot: ‚úì Looks good! Random scatter with no obvious pattern.\nRed flags: - Curved pattern ‚Üí nonlinear relationship (try transformation or polynomial regression) - Funnel shape ‚Üí heteroscedasticity (variance increases with fitted values)\n\n\n20.1.2 Plot 2: Normal Q-Q Plot\nPurpose: Check normality of residuals\nWhat to look for: - Points fall along diagonal reference line - Minor deviations at extremes are okay\nOur plot: ‚úì Looks good! Points mostly follow the line.\nRed flags: - Systematic S-curve ‚Üí heavy-tailed distribution - Points far from line at extremes ‚Üí outliers\n\n\n20.1.3 Plot 3: Scale-Location (Sqrt Standardized Residuals vs Fitted)\nPurpose: Check homoscedasticity (constant variance)\nWhat to look for: - Horizontal line with random scatter - Roughly equal spread across the x-axis\nOur plot: ‚úì Looks good! Relatively constant spread.\nRed flags: - Increasing/decreasing trend ‚Üí heteroscedasticity\n\n\n20.1.4 Plot 4: Residuals vs Leverage\nPurpose: Identify influential points (high leverage and/or large residuals)\nWhat to look for: - Most points clustered in lower-left corner - No points beyond Cook‚Äôs distance contours (dashed lines)\nOur plot: ‚úì Looks good! No highly influential points.\nRed flags: - Points beyond Cook‚Äôs distance lines ‚Üí investigate these observations\n\n\n\n\n\n\nImportantCook‚Äôs Distance\n\n\n\nCook‚Äôs distance measures how much the regression would change if an observation were removed:\n\n\\(D &gt; 0.5\\): Potentially influential\n\\(D &gt; 1\\): Very influential (investigate!)\n\nUse cooks.distance(model) to calculate.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#formal-tests-use-with-caution",
    "href": "chapters/ch15-linear_regression.html#formal-tests-use-with-caution",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "20.2 Formal Tests (Use with Caution)",
    "text": "20.2 Formal Tests (Use with Caution)\nWhile diagnostic plots are primary, formal tests are available:\n\n\nCode\n# Shapiro-Wilk test for normality of residuals\nshapiro_test &lt;- shapiro.test(residuals(model1))\n\n# Breusch-Pagan test for heteroscedasticity\nbp_test &lt;- car::ncvTest(model1)\n\n# Display results\ncat(\"Shapiro-Wilk test (normality): p =\", sprintf(\"%.4f\", shapiro_test$p.value), \"\\n\")\n\n\nShapiro-Wilk test (normality): p = 0.2815 \n\n\nCode\ncat(\"Breusch-Pagan test (homoscedasticity): p =\", sprintf(\"%.4f\", bp_test$p), \"\\n\")\n\n\nBreusch-Pagan test (homoscedasticity): p = 0.9508 \n\n\n\n\n\n\n\n\nWarningDon‚Äôt Over-Rely on Normality Tests\n\n\n\n\nShapiro-Wilk test is sensitive to sample size\nSmall samples: may not detect violations\nLarge samples: may detect trivial violations\n\nAlways prioritize visual inspection of diagnostic plots!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#what-if-assumptions-are-violated",
    "href": "chapters/ch15-linear_regression.html#what-if-assumptions-are-violated",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "20.3 What If Assumptions Are Violated?",
    "text": "20.3 What If Assumptions Are Violated?\n\n\n\n\n\n\n\nViolation\nPotential Solutions\n\n\n\n\nNon-linearity\nTransform variables (log, sqrt), polynomial regression\n\n\nHeteroscedasticity\nTransform response (log), weighted least squares\n\n\nNon-normality\nOften okay with large n (CLT), try transformations, robust methods\n\n\nInfluential points\nInvestigate for errors, consider robust regression\n\n\nNon-independence\nUse mixed models, time series methods, or clustered SEs",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#types-of-intervals",
    "href": "chapters/ch15-linear_regression.html#types-of-intervals",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "21.1 Types of Intervals",
    "text": "21.1 Types of Intervals\n\nConfidence Interval (CI) for the Mean Response\n\n‚ÄúWhat is the average Y for a given X?‚Äù\nInterval for \\(E(Y | X = x^*)\\)\nNarrower interval\n\nPrediction Interval (PI) for an Individual Response\n\n‚ÄúWhat Y value will a new individual have at X = x*?‚Äù\nInterval for a single future observation\nWider interval (accounts for individual variation)\n\n\n\n\n\n\n\n\nTipKey Distinction\n\n\n\n\nCI: Uncertainty about the mean\nPI: Uncertainty about a specific individual\n\nPI is always wider because it includes both: 1. Uncertainty in estimating the mean (like CI) 2. Individual variation around the mean (\\(\\pm \\sigma\\))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#making-predictions",
    "href": "chapters/ch15-linear_regression.html#making-predictions",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "21.2 Making Predictions",
    "text": "21.2 Making Predictions\nSuppose we want to predict milk yield for cows consuming 20 kg and 25 kg of feed per day:\n\n\nCode\n# New data for prediction\nnew_data &lt;- tibble(\n  feed_intake = c(20, 25)\n)\n\n# Confidence intervals (for mean response)\npred_conf &lt;- predict(model1, newdata = new_data, interval = \"confidence\", level = 0.95)\n\n# Prediction intervals (for individual response)\npred_pred &lt;- predict(model1, newdata = new_data, interval = \"prediction\", level = 0.95)\n\n# Combine results\npredictions_table &lt;- tibble(\n  feed_intake = new_data$feed_intake,\n  predicted_yield = pred_conf[, \"fit\"],\n  ci_lower = pred_conf[, \"lwr\"],\n  ci_upper = pred_conf[, \"upr\"],\n  pi_lower = pred_pred[, \"lwr\"],\n  pi_upper = pred_pred[, \"upr\"]\n)\n\n# Display\nknitr::kable(predictions_table, digits = 2,\n             caption = \"Predicted milk yields with 95% confidence and prediction intervals\")\n\n\n\nPredicted milk yields with 95% confidence and prediction intervals\n\n\nfeed_intake\npredicted_yield\nci_lower\nci_upper\npi_lower\npi_upper\n\n\n\n\n20\n37.63\n36.11\n39.14\n29.20\n46.05\n\n\n25\n44.57\n43.03\n46.12\n36.14\n53.01\n\n\n\n\n\nInterpretation for 20 kg feed intake:\n\nPoint estimate: 37.63 kg/day\n95% CI: [36.11, 39.14] - We‚Äôre 95% confident the mean milk yield for all cows consuming 20 kg/day is in this range\n95% PI: [29.20, 46.05] - We‚Äôre 95% confident a specific cow consuming 20 kg/day will produce milk in this range",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#visualizing-intervals",
    "href": "chapters/ch15-linear_regression.html#visualizing-intervals",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "21.3 Visualizing Intervals",
    "text": "21.3 Visualizing Intervals\n\n\nCode\n# Create prediction grid\npred_grid &lt;- tibble(\n  feed_intake = seq(min(dairy_data$feed_intake),\n                    max(dairy_data$feed_intake),\n                    length.out = 100)\n)\n\n# Get predictions\npred_grid$fit &lt;- predict(model1, newdata = pred_grid)\npred_ci &lt;- predict(model1, newdata = pred_grid, interval = \"confidence\")\npred_pi &lt;- predict(model1, newdata = pred_grid, interval = \"prediction\")\n\npred_grid$ci_lower &lt;- pred_ci[, \"lwr\"]\npred_grid$ci_upper &lt;- pred_ci[, \"upr\"]\npred_grid$pi_lower &lt;- pred_pi[, \"lwr\"]\npred_grid$pi_upper &lt;- pred_pi[, \"upr\"]\n\n# Plot\nggplot() +\n  # Prediction interval (wider, outer band)\n  geom_ribbon(data = pred_grid,\n              aes(x = feed_intake, ymin = pi_lower, ymax = pi_upper),\n              fill = \"lightblue\", alpha = 0.3) +\n  # Confidence interval (narrower, inner band)\n  geom_ribbon(data = pred_grid,\n              aes(x = feed_intake, ymin = ci_lower, ymax = ci_upper),\n              fill = \"steelblue\", alpha = 0.5) +\n  # Regression line\n  geom_line(data = pred_grid,\n            aes(x = feed_intake, y = fit),\n            color = \"darkblue\", linewidth = 1) +\n  # Original data points\n  geom_point(data = dairy_data,\n             aes(x = feed_intake, y = milk_yield),\n             size = 2, alpha = 0.6) +\n  labs(\n    title = \"Confidence vs Prediction Intervals\",\n    subtitle = \"Dark band = 95% CI for mean | Light band = 95% PI for individuals\",\n    x = \"Feed Intake (kg DM/day)\",\n    y = \"Milk Yield (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#extrapolation-warning",
    "href": "chapters/ch15-linear_regression.html#extrapolation-warning",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "21.4 Extrapolation Warning",
    "text": "21.4 Extrapolation Warning\n\n\n\n\n\n\nWarningDanger of Extrapolation\n\n\n\nNever make predictions far outside the range of observed X values!\n\nOur data: feed intake ranges from 16.7 to 30.6 kg/day\nPredicting at 5 kg/day or 40 kg/day would be extrapolation\nThe linear relationship may not hold outside the observed range\nPredictions become unreliable (and potentially absurd)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#scenario-body-weight-and-average-daily-gain-in-cattle",
    "href": "chapters/ch15-linear_regression.html#scenario-body-weight-and-average-daily-gain-in-cattle",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.1 Scenario: Body Weight and Average Daily Gain in Cattle",
    "text": "22.1 Scenario: Body Weight and Average Daily Gain in Cattle\nA beef cattle researcher wants to understand the relationship between an animal‚Äôs body weight and average daily gain (ADG). Data were collected from 60 steers over a 90-day feeding period.\nResearch Question: Can we predict ADG based on initial body weight?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-1-load-and-explore-data",
    "href": "chapters/ch15-linear_regression.html#step-1-load-and-explore-data",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.2 Step 1: Load and Explore Data",
    "text": "22.2 Step 1: Load and Explore Data\n\n\nCode\n# Simulate realistic cattle data\nn_cattle &lt;- 60\ninitial_weight &lt;- rnorm(n_cattle, mean = 350, sd = 40)  # kg\n\n# Heavier cattle tend to have slightly higher ADG\nadg &lt;- 0.8 + 0.002 * initial_weight + rnorm(n_cattle, mean = 0, sd = 0.15)  # kg/day\n\ncattle_data &lt;- tibble(\n  steer_id = 1:n_cattle,\n  initial_weight = initial_weight,\n  adg = adg\n)\n\n# Summary statistics\ncattle_data %&gt;%\n  summarise(\n    n = n(),\n    mean_weight = mean(initial_weight),\n    sd_weight = sd(initial_weight),\n    mean_adg = mean(adg),\n    sd_adg = sd(adg)\n  ) %&gt;%\n  knitr::kable(digits = 2, caption = \"Summary statistics for cattle data\")\n\n\n\nSummary statistics for cattle data\n\n\nn\nmean_weight\nsd_weight\nmean_adg\nsd_adg\n\n\n\n\n60\n343.15\n36.88\n1.51\n0.18",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-2-exploratory-data-analysis",
    "href": "chapters/ch15-linear_regression.html#step-2-exploratory-data-analysis",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.3 Step 2: Exploratory Data Analysis",
    "text": "22.3 Step 2: Exploratory Data Analysis\n\n\nCode\n# Scatter plot\np1 &lt;- ggplot(cattle_data, aes(x = initial_weight, y = adg)) +\n  geom_point(size = 3, alpha = 0.6, color = \"darkgreen\") +\n  labs(\n    title = \"Initial Body Weight vs Average Daily Gain\",\n    x = \"Initial Body Weight (kg)\",\n    y = \"Average Daily Gain (kg/day)\"\n  )\n\n# Marginal histograms\np2 &lt;- ggplot(cattle_data, aes(x = initial_weight)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.6) +\n  labs(x = \"Initial Weight (kg)\", y = \"Count\")\n\np3 &lt;- ggplot(cattle_data, aes(x = adg)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.6) +\n  labs(x = \"ADG (kg/day)\", y = \"Count\")\n\n# Combine\np1 / (p2 | p3)\n\n\n\n\n\n\n\n\n\nInitial observations: - Positive relationship appears present - No obvious outliers - Both variables roughly normally distributed",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-3-calculate-correlation",
    "href": "chapters/ch15-linear_regression.html#step-3-calculate-correlation",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.4 Step 3: Calculate Correlation",
    "text": "22.4 Step 3: Calculate Correlation\n\n\nCode\ncor_cattle &lt;- cor.test(cattle_data$initial_weight, cattle_data$adg)\ncor_cattle\n\n\n\n    Pearson's product-moment correlation\n\ndata:  cattle_data$initial_weight and cattle_data$adg\nt = 4.5983, df = 58, p-value = 2.356e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.3026769 0.6813723\nsample estimates:\n     cor \n0.516876 \n\n\nResult: \\(r\\) = 0.517, p = 0.0000\nThis suggests a weak to moderate positive correlation.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-4-fit-regression-model",
    "href": "chapters/ch15-linear_regression.html#step-4-fit-regression-model",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.5 Step 4: Fit Regression Model",
    "text": "22.5 Step 4: Fit Regression Model\n\n\nCode\n# Fit model\ncattle_model &lt;- lm(adg ~ initial_weight, data = cattle_data)\n\n# Tidy output\ntidy(cattle_model, conf.int = TRUE) %&gt;%\n  knitr::kable(digits = 4, caption = \"Regression coefficients\")\n\n\n\nRegression coefficients\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.6572\n0.1870\n3.5153\n9e-04\n0.2830\n1.0315\n\n\ninitial_weight\n0.0025\n0.0005\n4.5983\n0e+00\n0.0014\n0.0036\n\n\n\n\n\nCode\n# Model fit statistics\nglance(cattle_model) %&gt;%\n  select(r.squared, adj.r.squared, sigma, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Model fit statistics\")\n\n\n\nModel fit statistics\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\n\n\n\n\n0.2672\n0.2545\n0.1535\n21.1442\n0",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-5-interpret-coefficients",
    "href": "chapters/ch15-linear_regression.html#step-5-interpret-coefficients",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.6 Step 5: Interpret Coefficients",
    "text": "22.6 Step 5: Interpret Coefficients\n\n\nCode\ncoefs &lt;- coef(cattle_model)\nintercept_cattle &lt;- coefs[1]\nslope_cattle &lt;- coefs[2]\n\n\nIntercept (\\(\\hat{\\beta}_0\\) = 0.6572): Predicted ADG when initial weight = 0 kg (not meaningful)\nSlope (\\(\\hat{\\beta}_1\\) = 0.0025): For every 1 kg increase in initial body weight, ADG increases by 0.0025 kg/day (about 2.5 g/day), on average.\nIs this biologically meaningful? - A 50 kg difference in weight ‚Üí ~0.12 kg/day difference in ADG (about 125 g/day) - This is a small but meaningful effect in beef production",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-6-check-assumptions",
    "href": "chapters/ch15-linear_regression.html#step-6-check-assumptions",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.7 Step 6: Check Assumptions",
    "text": "22.7 Step 6: Check Assumptions\n\n\nCode\n# Diagnostic plots\npar(mfrow = c(2, 2))\nplot(cattle_model)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 1))\n\n\nAssessment: - ‚úì Residuals vs Fitted: Random scatter, no pattern - ‚úì Q-Q Plot: Points follow line well - ‚úì Scale-Location: Constant variance - ‚úì Residuals vs Leverage: No influential points\nConclusion: Assumptions appear satisfied.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-7-visualize-results",
    "href": "chapters/ch15-linear_regression.html#step-7-visualize-results",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.8 Step 7: Visualize Results",
    "text": "22.8 Step 7: Visualize Results\n\n\nCode\nggplot(cattle_data, aes(x = initial_weight, y = adg)) +\n  geom_point(size = 3, alpha = 0.6, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkred\", fill = \"pink\") +\n  labs(\n    title = \"Body Weight and Average Daily Gain in Beef Cattle\",\n    subtitle = sprintf(\"ADG = %.3f + %.4f √ó Weight (R¬≤ = %.3f)\",\n                      intercept_cattle, slope_cattle,\n                      glance(cattle_model)$r.squared),\n    x = \"Initial Body Weight (kg)\",\n    y = \"Average Daily Gain (kg/day)\"\n  ) +\n  theme_minimal(base_size = 12)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-8-make-predictions",
    "href": "chapters/ch15-linear_regression.html#step-8-make-predictions",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.9 Step 8: Make Predictions",
    "text": "22.9 Step 8: Make Predictions\nWhat ADG would we expect for steers weighing 300 kg and 400 kg?\n\n\nCode\n# New data\nnew_cattle &lt;- tibble(initial_weight = c(300, 400))\n\n# Predictions with intervals\npred_cattle &lt;- predict(cattle_model, newdata = new_cattle,\n                       interval = \"prediction\", level = 0.95) %&gt;%\n  as_tibble() %&gt;%\n  bind_cols(new_cattle, .)\n\npred_cattle %&gt;%\n  knitr::kable(digits = 3,\n               caption = \"Predicted ADG with 95% prediction intervals\")\n\n\n\nPredicted ADG with 95% prediction intervals\n\n\ninitial_weight\nfit\nlwr\nupr\n\n\n\n\n300\n1.405\n1.091\n1.718\n\n\n400\n1.654\n1.338\n1.970\n\n\n\n\n\nInterpretation: - A 300 kg steer: Expected ADG = 1.40 kg/day (95% PI: [1.09, 1.72]) - A 400 kg steer: Expected ADG = 1.65 kg/day (95% PI: [1.34, 1.97])",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#step-9-report-results",
    "href": "chapters/ch15-linear_regression.html#step-9-report-results",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "22.10 Step 9: Report Results",
    "text": "22.10 Step 9: Report Results\n\n\n\n\n\n\nNoteExample Results Statement\n\n\n\n‚ÄúA simple linear regression was conducted to examine the relationship between initial body weight and average daily gain (ADG) in beef cattle (n = 60). Initial body weight was significantly associated with ADG (p = 0.0000), with each 1 kg increase in body weight corresponding to a 0.0025 kg/day increase in ADG (95% CI: [, ]). However, initial body weight explained only 26.7% of the variance in ADG (\\(R^2\\) = 0.267), suggesting that other factors (e.g., genetics, health status, feed composition) play substantial roles in determining growth rate. Model diagnostics indicated that regression assumptions were adequately met.‚Äù",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#when-is-simple-linear-regression-appropriate",
    "href": "chapters/ch15-linear_regression.html#when-is-simple-linear-regression-appropriate",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "23.1 When is Simple Linear Regression Appropriate?",
    "text": "23.1 When is Simple Linear Regression Appropriate?\nUse simple linear regression when: - You have one continuous predictor and one continuous response - The relationship appears linear - You want to quantify the relationship or make predictions - You want to test if a relationship exists\nDon‚Äôt use simple linear regression when: - The relationship is clearly nonlinear (use transformations or nonlinear models) - You have multiple predictors (use multiple regression - Week 8!) - Response is binary (use logistic regression) - Data have hierarchical structure (use mixed models)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#sample-size-considerations",
    "href": "chapters/ch15-linear_regression.html#sample-size-considerations",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "23.2 Sample Size Considerations",
    "text": "23.2 Sample Size Considerations\nRules of thumb: - Minimum: ~20-30 observations for stable estimates - Preferred: 10-20 observations per predictor (simple regression = 1 predictor) - Larger samples: Better for detecting small effects and checking assumptions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#outliers-and-influential-points",
    "href": "chapters/ch15-linear_regression.html#outliers-and-influential-points",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "23.3 Outliers and Influential Points",
    "text": "23.3 Outliers and Influential Points\nTypes of unusual observations:\n\nOutliers: Large residuals (Y is unusual given X)\nHigh leverage points: Unusual X values (far from \\(\\bar{x}\\))\nInfluential points: Combining high leverage and large residual; removal changes the regression line\n\nWhat to do: - Investigate (data entry error? Valid unusual case?) - Report results with and without influential points - Consider robust regression methods if many outliers\n\n\nCode\n# Calculate influence metrics\ninfluence_metrics &lt;- cattle_data %&gt;%\n  mutate(\n    cooks_d = cooks.distance(cattle_model),\n    leverage = hatvalues(cattle_model),\n    std_resid = rstandard(cattle_model)\n  ) %&gt;%\n  arrange(desc(cooks_d))\n\n# Display top 5 by Cook's distance\ninfluence_metrics %&gt;%\n  slice_head(n = 5) %&gt;%\n  select(steer_id, initial_weight, adg, cooks_d, leverage, std_resid) %&gt;%\n  knitr::kable(digits = 3, caption = \"Top 5 observations by Cook's distance\")\n\n\n\nTop 5 observations by Cook‚Äôs distance\n\n\nsteer_id\ninitial_weight\nadg\ncooks_d\nleverage\nstd_resid\n\n\n\n\n42\n273.365\n0.919\n0.339\n0.077\n-2.842\n\n\n47\n299.433\n1.126\n0.072\n0.040\n-1.846\n\n\n14\n273.826\n1.496\n0.047\n0.077\n1.059\n\n\n9\n363.451\n1.268\n0.042\n0.022\n-1.942\n\n\n15\n344.418\n1.826\n0.035\n0.017\n2.044\n\n\n\n\n\nAll Cook‚Äôs distances are well below 0.5, indicating no influential points.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#biological-vs-statistical-significance",
    "href": "chapters/ch15-linear_regression.html#biological-vs-statistical-significance",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "23.4 Biological vs Statistical Significance",
    "text": "23.4 Biological vs Statistical Significance\n\n\n\n\n\n\nImportantStatistical Significance ‚â† Practical Importance\n\n\n\nConsider: - Effect size: Is the magnitude of the slope meaningful? - Context: A slope of 0.002 kg/day per kg of body weight is small but might be economically important over 100+ days - Confidence intervals: Provide range of plausible effect sizes - Cost-benefit: Is the predictor worth measuring/manipulating?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#common-pitfalls",
    "href": "chapters/ch15-linear_regression.html#common-pitfalls",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "23.5 Common Pitfalls",
    "text": "23.5 Common Pitfalls\n\n\n\n\n\n\nWarningWatch Out For These Mistakes!\n\n\n\n\nConfusing correlation with causation\n\nRegression shows association, not causation (unless from RCT)\n\nExtrapolating beyond data range\n\nPredictions unreliable outside observed X values\n\nIgnoring assumption violations\n\nAlways check diagnostic plots!\n\nFocusing only on p-values\n\nReport effect sizes, CIs, and \\(R^2\\)\n\nForgetting units\n\nCoefficients are meaningless without units\n\nOverlooking influential points\n\nOne outlier can dramatically change results",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#essential-components",
    "href": "chapters/ch15-linear_regression.html#essential-components",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "24.1 Essential Components",
    "text": "24.1 Essential Components\nA complete regression report should include:\n\nSample size (n)\nModel equation (in words or symbols)\nCoefficients with standard errors or CIs\nStatistical significance (t-statistics, p-values)\nModel fit (\\(R^2\\), RSE)\nAssumption checks (statement that diagnostics were examined)\nInterpretation in context (with units!)\nVisualization (scatter plot with regression line)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#table-format-example",
    "href": "chapters/ch15-linear_regression.html#table-format-example",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "24.2 Table Format Example",
    "text": "24.2 Table Format Example\n\n\nCode\n# Create publication-ready table\ncattle_table &lt;- tidy(cattle_model, conf.int = TRUE) %&gt;%\n  select(term, estimate, std.error, statistic, p.value, conf.low, conf.high) %&gt;%\n  mutate(\n    term = case_match(term,\n                      \"(Intercept)\" ~ \"Intercept\",\n                      \"initial_weight\" ~ \"Initial Weight (kg)\")\n  )\n\ncattle_table %&gt;%\n  knitr::kable(\n    digits = 4,\n    col.names = c(\"Term\", \"Estimate\", \"SE\", \"t\", \"p\", \"95% CI Lower\", \"95% CI Upper\"),\n    caption = \"Simple linear regression: ADG predicted by initial body weight\"\n  )\n\n\n\nSimple linear regression: ADG predicted by initial body weight\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate\nSE\nt\np\n95% CI Lower\n95% CI Upper\n\n\n\n\nIntercept\n0.6572\n0.1870\n3.5153\n9e-04\n0.2830\n1.0315\n\n\nInitial Weight (kg)\n0.0025\n0.0005\n4.5983\n0e+00\n0.0014\n0.0036",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#figure-requirements",
    "href": "chapters/ch15-linear_regression.html#figure-requirements",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "24.3 Figure Requirements",
    "text": "24.3 Figure Requirements\nGood regression figures include: - Scatter plot of raw data - Fitted regression line - Confidence band (optional but recommended) - Clear axis labels with units - Informative title - Regression equation and/or \\(R^2\\) in caption or subtitle",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#decision-framework-when-to-use-regression",
    "href": "chapters/ch15-linear_regression.html#decision-framework-when-to-use-regression",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "25.1 Decision Framework: When to Use Regression",
    "text": "25.1 Decision Framework: When to Use Regression\n\n\n\nDecision tree for simple linear regression\n\n\n\n\n\n\n\nQuestion\nIf YES\nIf NO\n\n\n\n\nOne continuous Y and one continuous X?\nContinue\nConsider other methods\n\n\nRelationship appears linear?\nContinue\nTransform or use nonlinear model\n\n\nWant to quantify relationship/predict Y?\nUse regression\nCorrelation may suffice\n\n\nCare about direction of prediction?\nRegression (Y ~ X)\nCorrelation is symmetric\n\n\nHave multiple predictors?\nMultiple regression (Week 8!)\nSimple regression",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#looking-ahead-to-week-8",
    "href": "chapters/ch15-linear_regression.html#looking-ahead-to-week-8",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "25.2 Looking Ahead to Week 8",
    "text": "25.2 Looking Ahead to Week 8\nNext week we‚Äôll extend to multiple regression: - Multiple predictor variables simultaneously - Controlling for confounders - Categorical predictors (factor variables) - Model comparison and selection - Interactions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#practice-problems",
    "href": "chapters/ch15-linear_regression.html#practice-problems",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "25.3 Practice Problems",
    "text": "25.3 Practice Problems\nTry these to solidify your understanding:\n\nConceptual: Explain to a fellow student the difference between correlation and regression. When would you use each?\nInterpretation: A regression of egg weight (Y, grams) on hen age (X, weeks) gives: \\(\\hat{Y} = 35 + 0.8X\\). Interpret both coefficients with proper units.\nPrediction: Using the equation above, predict the egg weight for a 20-week-old hen. Would you trust a prediction for a 100-week-old hen? Why or why not?\nAnalysis: Load the mtcars dataset in R. Fit a regression predicting mpg from wt (weight). Check assumptions, interpret coefficients, and make a publication-quality plot.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#recommended-reading",
    "href": "chapters/ch15-linear_regression.html#recommended-reading",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "26.1 Recommended Reading",
    "text": "26.1 Recommended Reading\n\nZuur et al.¬†(2009). ‚ÄúMixed Effects Models and Extensions in Ecology with R‚Äù - Chapter 4 (excellent on diagnostics)\nFox (2016). ‚ÄúApplied Regression Analysis and Generalized Linear Models‚Äù - Chapters 6-12\nFaraway (2014). ‚ÄúLinear Models with R‚Äù - Practical guide with R code",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#online-resources",
    "href": "chapters/ch15-linear_regression.html#online-resources",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "26.2 Online Resources",
    "text": "26.2 Online Resources\n\nR for Data Science (2e): Chapter on modeling - https://r4ds.hadley.nz/\nStatistical Rethinking by Richard McElreath - Lectures on YouTube\nRegression Diagnostic Plots interpretation guide: https://library.virginia.edu/data/articles/diagnostic-plots",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#r-functions-summary",
    "href": "chapters/ch15-linear_regression.html#r-functions-summary",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "26.3 R Functions Summary",
    "text": "26.3 R Functions Summary\n\n\n\nFunction\nPurpose\n\n\n\n\nlm(y ~ x, data)\nFit linear model\n\n\nsummary(model)\nModel summary\n\n\ncoef(model)\nExtract coefficients\n\n\nresiduals(model)\nExtract residuals\n\n\nfitted(model)\nExtract fitted values\n\n\npredict(model, newdata, interval)\nMake predictions\n\n\nplot(model)\nDiagnostic plots\n\n\ncor(x, y)\nCorrelation\n\n\ncor.test(x, y)\nCorrelation with inference\n\n\nbroom::tidy(model)\nTidy coefficient table\n\n\nbroom::glance(model)\nModel fit statistics\n\n\nbroom::augment(model)\nAdd fitted values, residuals to data",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#instructions",
    "href": "chapters/ch15-linear_regression.html#instructions",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.1 Instructions",
    "text": "27.1 Instructions\nComplete the following assignment using R and Quarto. Submit both your .qmd source file and the rendered .html output.\nDue: [One week from today]",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#part-1-correlation-vs-regression-concepts-20-points",
    "href": "chapters/ch15-linear_regression.html#part-1-correlation-vs-regression-concepts-20-points",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.2 Part 1: Correlation vs Regression Concepts (20 points)",
    "text": "27.2 Part 1: Correlation vs Regression Concepts (20 points)\nFor each scenario below, indicate whether correlation or regression is more appropriate and justify your answer (2-3 sentences each).\nScenario A: A veterinarian wants to examine if there‚Äôs an association between dogs‚Äô body weight and resting heart rate, with no interest in prediction.\nScenario B: A swine nutritionist wants to predict market weight based on daily feed intake to optimize feeding strategies.\nScenario C: An animal behaviorist is studying whether time spent grazing is related to daily step count in cattle.\nScenario D: A dairy manager wants to estimate milk production for cows based on their lactation number to plan herd management.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#part-2-complete-regression-analysis-50-points",
    "href": "chapters/ch15-linear_regression.html#part-2-complete-regression-analysis-50-points",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.3 Part 2: Complete Regression Analysis (50 points)",
    "text": "27.3 Part 2: Complete Regression Analysis (50 points)\nYou will analyze a dataset on pig growth performance. The data include: - pig_id: Unique pig identifier - initial_weight: Weight at start of trial (kg) - final_weight: Weight after 60 days (kg) - feed_intake: Average daily feed intake (kg/day)\n\n27.3.1 Tasks:\n\nCreate simulated data (5 points)\n\nUse this code to generate the dataset:\nset.seed(450)\nn_pigs &lt;- 45\n\npig_data &lt;- tibble(\n  pig_id = 1:n_pigs,\n  initial_weight = rnorm(n_pigs, mean = 25, sd = 4),\n  feed_intake = rnorm(n_pigs, mean = 1.8, sd = 0.3)\n) %&gt;%\n  mutate(\n    final_weight = initial_weight +\n      35 + 12 * feed_intake + rnorm(n_pigs, mean = 0, sd = 3)\n  )\n\nExploratory Data Analysis (10 points)\n\nCalculate summary statistics for all variables\nCreate a scatter plot of feed_intake (X) vs final_weight (Y)\nCalculate and interpret the correlation coefficient\n\nFit Regression Model (10 points)\n\nFit a simple linear regression: final_weight ~ feed_intake\nReport the regression equation\nCreate a publication-quality plot with the fitted line\n\nInterpret Coefficients (10 points)\n\nInterpret the intercept (include units and biological meaning if any)\nInterpret the slope (include units and practical significance)\nReport 95% confidence intervals for both coefficients\n\nAssess Model Fit (5 points)\n\nReport and interpret \\(R^2\\)\nReport and interpret the residual standard error\n\nCheck Assumptions (10 points)\n\nCreate the four diagnostic plots\nFor each plot, state whether assumptions appear satisfied or violated\nOverall conclusion: Are assumptions met?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#part-3-prediction-challenge-20-points",
    "href": "chapters/ch15-linear_regression.html#part-3-prediction-challenge-20-points",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.4 Part 3: Prediction Challenge (20 points)",
    "text": "27.4 Part 3: Prediction Challenge (20 points)\nUsing your fitted model from Part 2:\n\nMake predictions (10 points)\n\nPredict final weight for pigs with feed intakes of 1.5, 2.0, and 2.5 kg/day\nCalculate 95% confidence intervals for the mean response\nCalculate 95% prediction intervals for individual pigs\nPresent results in a well-formatted table\n\nInterpret intervals (10 points)\n\nFor feed intake = 2.0 kg/day:\n\nExplain what the confidence interval tells you (in words)\nExplain what the prediction interval tells you (in words)\nWhy is the prediction interval wider?\n\nWould you trust a prediction for a pig consuming 4.0 kg/day? Why or why not?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#part-4-critical-thinking-10-points",
    "href": "chapters/ch15-linear_regression.html#part-4-critical-thinking-10-points",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.5 Part 4: Critical Thinking (10 points)",
    "text": "27.5 Part 4: Critical Thinking (10 points)\nA research paper reports the following:\n\n‚ÄúWe found a significant relationship between barn temperature (¬∞F) and daily egg production in laying hens (p = 0.03). The regression equation was: Eggs = 15.2 + 0.05 √ó Temperature, with \\(R^2\\) = 0.08.‚Äù\n\nQuestions:\n\nWhat does the slope (0.05) mean in practical terms? (2 points)\nIs the relationship statistically significant? Is it practically meaningful? Explain. (3 points)\nWhat does \\(R^2 = 0.08\\) tell you? Should the researchers be concerned about this value? (3 points)\nWhat additional information would you want to know before trusting this model? (2 points)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#grading-rubric",
    "href": "chapters/ch15-linear_regression.html#grading-rubric",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.6 Grading Rubric",
    "text": "27.6 Grading Rubric\n\n\n\n\n\n\n\n\nComponent\nPoints\nCriteria\n\n\n\n\nPart 1\n20\nCorrect identification and clear justification\n\n\nPart 2\n50\nCode correctness (15), interpretation accuracy (20), visualizations (10), assumption checks (5)\n\n\nPart 3\n20\nCorrect predictions (10), clear explanations (10)\n\n\nPart 4\n10\nThoughtful critical analysis\n\n\nOverall Quality\nBonus +5\nExceptional organization, clarity, professional presentation\n\n\n\nTotal: 100 points (+ 5 bonus)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch15-linear_regression.html#submission-checklist",
    "href": "chapters/ch15-linear_regression.html#submission-checklist",
    "title": "15¬† Week 15: Simple Linear Regression",
    "section": "27.7 Submission Checklist",
    "text": "27.7 Submission Checklist\nBefore submitting, ensure:\n\nAll code runs without errors\nPlots have clear titles and axis labels with units\nInterpretations include appropriate units\nDiagnostic plots are included and interpreted\nDocument is well-organized and professional\nBoth .qmd and .html files are submitted\n\n\nGood luck! Remember: Regression is about understanding relationships and making informed predictions. Focus on interpretation, not just numbers!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Week 15: Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html",
    "href": "chapters/ch16-multiple_regression.html",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "",
    "text": "17 Introduction\nImagine you‚Äôre an animal nutritionist tasked with optimizing milk production in a dairy herd. You know that feed intake affects milk yield, but you also suspect that days in milk, parity (lactation number), and breed all play important roles. How can you account for all these factors simultaneously? How do you interpret the effect of feed when controlling for the other variables?\nThis is where multiple regression becomes essential. While simple linear regression (Week 7) allowed us to examine the relationship between one predictor and an outcome, real-world phenomena rarely depend on a single variable. Multiple regression extends our toolkit to model complex relationships with multiple predictors.\nKey Questions We‚Äôll Address:\nThis final week serves dual purposes: introducing multiple regression as a powerful analytical tool, and synthesizing everything we‚Äôve learned to help you navigate the full statistical toolkit we‚Äôve developed together.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#why-one-predictor-isnt-enough",
    "href": "chapters/ch16-multiple_regression.html#why-one-predictor-isnt-enough",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "18.1 Why One Predictor Isn‚Äôt Enough",
    "text": "18.1 Why One Predictor Isn‚Äôt Enough\nLet‚Äôs start with a scenario that demonstrates why we need multiple regression. Consider a study of pig growth where we measure weight gain and feed intake:\n\n\nCode\n# Simulate pig growth data with a confounding variable\nn &lt;- 50\npig_data &lt;- tibble(\n  pig_id = 1:n,\n  initial_weight = rnorm(n, mean = 20, sd = 3),  # Starting weight in kg\n  feed_intake = 1.5 + 0.08 * initial_weight + rnorm(n, sd = 0.2),  # kg/day\n  # Weight gain depends on BOTH feed and initial weight\n  weight_gain = 0.5 * feed_intake + 0.15 * initial_weight + rnorm(n, sd = 0.3)\n)\n\nhead(pig_data)\n\n\n# A tibble: 6 √ó 4\n  pig_id initial_weight feed_intake weight_gain\n   &lt;int&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      1           17.7        2.79        3.98\n2      2           17.2        2.76        4.09\n3      3           23.6        3.32        5.66\n4      4           15.1        2.90        4.27\n5      5           20.9        3.04        4.37\n6      6           24.5        3.46        5.52\n\n\nIf we only look at the relationship between feed intake and weight gain (ignoring initial weight), we get:\n\n\nCode\n# Simple regression (wrong - omitted variable bias)\nsimple_model &lt;- lm(weight_gain ~ feed_intake, data = pig_data)\n\n# Multiple regression (correct)\nmultiple_model &lt;- lm(weight_gain ~ feed_intake + initial_weight, data = pig_data)\n\n# Visualize the difference\np1 &lt;- ggplot(pig_data, aes(x = feed_intake, y = weight_gain)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\") +\n  labs(title = \"Simple Regression\",\n       subtitle = sprintf(\"Œ≤ÃÇ‚ÇÅ = %.3f (biased!)\", coef(simple_model)[2]),\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(pig_data, aes(x = feed_intake, y = weight_gain, color = initial_weight)) +\n  geom_point(alpha = 0.6) +\n  scale_color_viridis_c() +\n  labs(title = \"Accounting for Initial Weight\",\n       subtitle = \"Color shows confounding by initial weight\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\",\n       color = \"Initial\\nWeight (kg)\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nKey Insight: The simple regression coefficient for feed intake is biased because initial weight affects both feed intake (heavier pigs eat more) and weight gain (heavier pigs grow faster). This is omitted variable bias or confounding.\nLet‚Äôs compare the models:\n\n\nCode\n# Compare coefficients\nsimple_coef &lt;- coef(simple_model)[2]\nmultiple_coef &lt;- coef(multiple_model)[2]\n\ncat(\"Simple regression: Œ≤ÃÇ_feed =\", sprintf(\"%.3f\", simple_coef), \"\\n\")\n\n\nSimple regression: Œ≤ÃÇ_feed = 1.562 \n\n\nCode\ncat(\"Multiple regression: Œ≤ÃÇ_feed =\", sprintf(\"%.3f\", multiple_coef), \"\\n\")\n\n\nMultiple regression: Œ≤ÃÇ_feed = 0.547 \n\n\nCode\ncat(\"True effect of feed:\", 0.5, \"\\n\")\n\n\nTrue effect of feed: 0.5 \n\n\nThe multiple regression estimate is much closer to the true effect (0.5) because it controls for initial weight.\n\n\n\n\n\n\nImportantThe Core Idea of Multiple Regression\n\n\n\nMultiple regression allows us to estimate the effect of each predictor while holding all other predictors constant. This helps us:\n\nControl for confounding variables\nIsolate individual effects from a system of interrelated variables\nMake better predictions by using all available information\nAnswer more nuanced research questions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#the-multiple-regression-model",
    "href": "chapters/ch16-multiple_regression.html#the-multiple-regression-model",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "18.2 The Multiple Regression Model",
    "text": "18.2 The Multiple Regression Model\nThe mathematical form extends simple regression:\nSimple Linear Regression: \\[y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\]\nMultiple Linear Regression: \\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p + \\epsilon\\]\nWhere:\n\n\\(y\\) = outcome variable (response)\n\\(x_1, x_2, \\ldots, x_p\\) = predictor variables\n\\(\\beta_0\\) = intercept (expected value of \\(y\\) when all predictors = 0)\n\\(\\beta_1, \\beta_2, \\ldots, \\beta_p\\) = partial regression coefficients\n\\(\\epsilon\\) = random error term\n\nInterpretation of \\(\\beta_j\\): The expected change in \\(y\\) for a one-unit increase in \\(x_j\\), holding all other predictors constant.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#geometric-interpretation",
    "href": "chapters/ch16-multiple_regression.html#geometric-interpretation",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "18.3 Geometric Interpretation",
    "text": "18.3 Geometric Interpretation\nIn simple regression, we fit a line through a 2D scatterplot. In multiple regression with two predictors, we fit a plane through a 3D point cloud. With more predictors, we fit a hyperplane in higher dimensions (which we can‚Äôt visualize, but the math works the same way).\n\n\nCode\n# Note: This requires the plotly package for interactive 3D plots\n# Not run here, but you can try it!\nlibrary(plotly)\n\nplot_ly(pig_data,\n        x = ~feed_intake,\n        y = ~initial_weight,\n        z = ~weight_gain,\n        type = \"scatter3d\",\n        mode = \"markers\") %&gt;%\n  layout(scene = list(\n    xaxis = list(title = \"Feed Intake\"),\n    yaxis = list(title = \"Initial Weight\"),\n    zaxis = list(title = \"Weight Gain\")\n  ))",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#the-holding-other-variables-constant-concept",
    "href": "chapters/ch16-multiple_regression.html#the-holding-other-variables-constant-concept",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "19.1 The ‚ÄúHolding Other Variables Constant‚Äù Concept",
    "text": "19.1 The ‚ÄúHolding Other Variables Constant‚Äù Concept\nThis is the most important‚Äîand most commonly misunderstood‚Äîaspect of multiple regression.\nLet‚Äôs use our pig example:\n\n\nCode\n# Fit the model\nmodel &lt;- lm(weight_gain ~ feed_intake + initial_weight, data = pig_data)\nsummary(model)\n\n\n\nCall:\nlm(formula = weight_gain ~ feed_intake + initial_weight, data = pig_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.51275 -0.15873 -0.01694  0.16149  0.56037 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.08765    0.39166  -0.224  0.82389    \nfeed_intake     0.54675    0.17729   3.084  0.00342 ** \ninitial_weight  0.14667    0.01840   7.970 2.83e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2567 on 47 degrees of freedom\nMultiple R-squared:  0.8265,    Adjusted R-squared:  0.8191 \nF-statistic: 111.9 on 2 and 47 DF,  p-value: &lt; 2.2e-16\n\n\nInterpretation of Coefficients:\n\n\nCode\ncoefs &lt;- coef(model)\ncat(\"Intercept (Œ≤‚ÇÄ):\", sprintf(\"%.3f\", coefs[1]), \"\\n\")\n\n\nIntercept (Œ≤‚ÇÄ): -0.088 \n\n\nCode\ncat(\"  ‚Üí Expected weight gain when feed intake = 0 AND initial weight = 0\\n\")\n\n\n  ‚Üí Expected weight gain when feed intake = 0 AND initial weight = 0\n\n\nCode\ncat(\"  ‚Üí Often not meaningful (extrapolation)\\n\\n\")\n\n\n  ‚Üí Often not meaningful (extrapolation)\n\n\nCode\ncat(\"Feed Intake (Œ≤‚ÇÅ):\", sprintf(\"%.3f\", coefs[2]), \"\\n\")\n\n\nFeed Intake (Œ≤‚ÇÅ): 0.547 \n\n\nCode\ncat(\"  ‚Üí For every 1 kg/day increase in feed intake,\\n\")\n\n\n  ‚Üí For every 1 kg/day increase in feed intake,\n\n\nCode\ncat(\"  ‚Üí weight gain increases by\", sprintf(\"%.3f\", coefs[2]), \"kg/day,\\n\")\n\n\n  ‚Üí weight gain increases by 0.547 kg/day,\n\n\nCode\ncat(\"  ‚Üí HOLDING initial weight constant\\n\\n\")\n\n\n  ‚Üí HOLDING initial weight constant\n\n\nCode\ncat(\"Initial Weight (Œ≤‚ÇÇ):\", sprintf(\"%.3f\", coefs[3]), \"\\n\")\n\n\nInitial Weight (Œ≤‚ÇÇ): 0.147 \n\n\nCode\ncat(\"  ‚Üí For every 1 kg increase in initial weight,\\n\")\n\n\n  ‚Üí For every 1 kg increase in initial weight,\n\n\nCode\ncat(\"  ‚Üí weight gain increases by\", sprintf(\"%.3f\", coefs[3]), \"kg/day,\\n\")\n\n\n  ‚Üí weight gain increases by 0.147 kg/day,\n\n\nCode\ncat(\"  ‚Üí HOLDING feed intake constant\\n\")\n\n\n  ‚Üí HOLDING feed intake constant\n\n\n\n\n\n\n\n\nTipPractical Interpretation Strategy\n\n\n\nWhen interpreting \\(\\beta_j\\):\n\nState the magnitude and direction of the effect\nInclude units for both predictor and outcome\nExplicitly mention ‚Äúholding other variables constant‚Äù (at least initially)\nAssess practical significance, not just statistical significance\nConsider whether the relationship is causal or associational",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#simpsons-paradox-when-direction-reverses",
    "href": "chapters/ch16-multiple_regression.html#simpsons-paradox-when-direction-reverses",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "19.2 Simpson‚Äôs Paradox: When Direction Reverses",
    "text": "19.2 Simpson‚Äôs Paradox: When Direction Reverses\nOne striking phenomenon in multiple regression is Simpson‚Äôs Paradox: the relationship between \\(x\\) and \\(y\\) can reverse direction when we control for another variable.\n\n\nCode\n# Create data showing Simpson's Paradox\nfarm_data &lt;- tibble(\n  farm = rep(c(\"A\", \"B\", \"C\"), each = 20),\n  farm_quality = rep(c(5, 10, 15), each = 20),  # Confounding variable\n  management_score = farm_quality + rnorm(60, sd = 1),\n  # Within each farm, MORE management REDUCES costs (efficiency)\n  # But better farms have HIGHER management and HIGHER costs (they spend more overall)\n  cost_per_animal = farm_quality * 2 + (-0.5) * (management_score - farm_quality) + rnorm(60, sd = 1)\n)\n\np1 &lt;- ggplot(farm_data, aes(x = management_score, y = cost_per_animal)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(title = \"Ignoring Farm (Wrong!)\",\n       subtitle = \"Appears that better management ‚Üí higher costs\",\n       x = \"Management Score\",\n       y = \"Cost per Animal ($)\") +\n  theme_minimal()\n\np2 &lt;- ggplot(farm_data, aes(x = management_score, y = cost_per_animal, color = farm)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Accounting for Farm (Correct)\",\n       subtitle = \"Actually, better management ‚Üí lower costs (within farms)\",\n       x = \"Management Score\",\n       y = \"Cost per Animal ($)\",\n       color = \"Farm\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple regression (misleading)\nsimple_mod &lt;- lm(cost_per_animal ~ management_score, data = farm_data)\n\n# Multiple regression (correct)\nmultiple_mod &lt;- lm(cost_per_animal ~ management_score + farm_quality, data = farm_data)\n\ncat(\"Simple regression: Œ≤ÃÇ_management =\", sprintf(\"%.3f\", coef(simple_mod)[2]),\n    \"(WRONG - positive!)\\n\")\n\n\nSimple regression: Œ≤ÃÇ_management = 1.858 (WRONG - positive!)\n\n\nCode\ncat(\"Multiple regression: Œ≤ÃÇ_management =\", sprintf(\"%.3f\", coef(multiple_mod)[2]),\n    \"(CORRECT - negative)\\n\")\n\n\nMultiple regression: Œ≤ÃÇ_management = -0.486 (CORRECT - negative)\n\n\nLesson: Always consider potential confounding variables. The crude (unadjusted) relationship can be very different from the adjusted relationship.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#how-r-handles-factors",
    "href": "chapters/ch16-multiple_regression.html#how-r-handles-factors",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "20.1 How R Handles Factors",
    "text": "20.1 How R Handles Factors\nR automatically converts factor variables into dummy variables (also called indicator variables). A factor with \\(k\\) levels is encoded as \\(k-1\\) dummy variables.\n\n\nCode\n# Create data with a categorical predictor\nbroiler_data &lt;- tibble(\n  bird_id = 1:90,\n  breed = rep(c(\"Ross\", \"Cobb\", \"Hubbard\"), each = 30),\n  feed_intake = rnorm(90, mean = 0.12, sd = 0.02),  # kg/day\n  # Different breeds have different baseline growth rates\n  weight_gain = case_when(\n    breed == \"Ross\" ~ 0.055,\n    breed == \"Cobb\" ~ 0.062,\n    breed == \"Hubbard\" ~ 0.058\n  ) + 0.15 * feed_intake + rnorm(90, sd = 0.008)\n)\n\n# Fit model with categorical predictor\nmodel_breed &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\nsummary(model_breed)\n\n\n\nCall:\nlm(formula = weight_gain ~ breed + feed_intake, data = broiler_data)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-0.0207408 -0.0057003 -0.0006643  0.0079860  0.0157895 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   0.068599   0.006046  11.346  &lt; 2e-16 ***\nbreedHubbard -0.006933   0.002328  -2.978  0.00377 ** \nbreedRoss    -0.009763   0.002305  -4.235  5.7e-05 ***\nfeed_intake   0.098499   0.046785   2.105  0.03818 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.008924 on 86 degrees of freedom\nMultiple R-squared:  0.2233,    Adjusted R-squared:  0.1962 \nF-statistic:  8.24 on 3 and 86 DF,  p-value: 7.005e-05\n\n\nUnderstanding the Output:\n\n\nCode\ncat(\"R created dummy variables:\\n\")\n\n\nR created dummy variables:\n\n\nCode\ncat(\"  ‚Ä¢ breedCobb: 1 if Cobb, 0 otherwise\\n\")\n\n\n  ‚Ä¢ breedCobb: 1 if Cobb, 0 otherwise\n\n\nCode\ncat(\"  ‚Ä¢ breedHubbard: 1 if Hubbard, 0 otherwise\\n\")\n\n\n  ‚Ä¢ breedHubbard: 1 if Hubbard, 0 otherwise\n\n\nCode\ncat(\"  ‚Ä¢ Ross is the REFERENCE LEVEL (baseline)\\n\\n\")\n\n\n  ‚Ä¢ Ross is the REFERENCE LEVEL (baseline)\n\n\nCode\ncoefs_breed &lt;- coef(model_breed)\n\ncat(\"Intercept:\", sprintf(\"%.4f\", coefs_breed[1]), \"\\n\")\n\n\nIntercept: 0.0686 \n\n\nCode\ncat(\"  ‚Üí Expected weight gain for Ross breed when feed intake = 0\\n\\n\")\n\n\n  ‚Üí Expected weight gain for Ross breed when feed intake = 0\n\n\nCode\ncat(\"breedCobb:\", sprintf(\"%.4f\", coefs_breed[2]), \"\\n\")\n\n\nbreedCobb: -0.0069 \n\n\nCode\ncat(\"  ‚Üí Cobb birds gain\", sprintf(\"%.4f\", coefs_breed[2]), \"kg/day MORE than Ross\\n\")\n\n\n  ‚Üí Cobb birds gain -0.0069 kg/day MORE than Ross\n\n\nCode\ncat(\"  ‚Üí (holding feed intake constant)\\n\\n\")\n\n\n  ‚Üí (holding feed intake constant)\n\n\nCode\ncat(\"breedHubbard:\", sprintf(\"%.4f\", coefs_breed[3]), \"\\n\")\n\n\nbreedHubbard: -0.0098 \n\n\nCode\ncat(\"  ‚Üí Hubbard birds gain\", sprintf(\"%.4f\", coefs_breed[3]), \"kg/day MORE than Ross\\n\")\n\n\n  ‚Üí Hubbard birds gain -0.0098 kg/day MORE than Ross\n\n\nCode\ncat(\"  ‚Üí (holding feed intake constant)\\n\\n\")\n\n\n  ‚Üí (holding feed intake constant)\n\n\nCode\ncat(\"feed_intake:\", sprintf(\"%.4f\", coefs_breed[4]), \"\\n\")\n\n\nfeed_intake: 0.0985 \n\n\nCode\ncat(\"  ‚Üí Effect of feed is the SAME for all breeds (no interaction)\\n\")\n\n\n  ‚Üí Effect of feed is the SAME for all breeds (no interaction)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#changing-the-reference-level",
    "href": "chapters/ch16-multiple_regression.html#changing-the-reference-level",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "20.2 Changing the Reference Level",
    "text": "20.2 Changing the Reference Level\nSometimes you want a different baseline for comparison:\n\n\nCode\n# Change reference level to Cobb\nbroiler_data &lt;- broiler_data %&gt;%\n  mutate(breed = relevel(factor(breed), ref = \"Cobb\"))\n\nmodel_breed2 &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\ncoef(model_breed2)\n\n\n (Intercept) breedHubbard    breedRoss  feed_intake \n 0.068598912 -0.006933349 -0.009763200  0.098498550 \n\n\nCode\n# Now coefficients compare Ross and Hubbard to Cobb (the new reference)\n\n\n\n\n\n\n\n\nTipChoosing a Reference Level\n\n\n\nSelect a reference level that makes interpretation easiest:\n\nControl group (if you have one)\nMost common category\nTheoretically meaningful baseline\n\nThe choice doesn‚Äôt affect the model fit or predictions, only interpretation of coefficients.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#visualizing-categorical-predictors",
    "href": "chapters/ch16-multiple_regression.html#visualizing-categorical-predictors",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "20.3 Visualizing Categorical Predictors",
    "text": "20.3 Visualizing Categorical Predictors\n\n\nCode\n# Reset to Ross as reference\nbroiler_data &lt;- broiler_data %&gt;%\n  mutate(breed = relevel(factor(breed), ref = \"Ross\"))\n\n# Refit model\nmodel_breed &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\n\n# Create predictions for each breed\npred_data &lt;- expand_grid(\n  breed = c(\"Ross\", \"Cobb\", \"Hubbard\"),\n  feed_intake = seq(min(broiler_data$feed_intake),\n                    max(broiler_data$feed_intake),\n                    length.out = 50)\n) %&gt;%\n  mutate(breed = factor(breed, levels = c(\"Ross\", \"Cobb\", \"Hubbard\")))\n\npred_data$predicted &lt;- predict(model_breed, newdata = pred_data)\n\np1 &lt;- ggplot(broiler_data, aes(x = breed, y = weight_gain, fill = breed)) +\n  geom_boxplot(alpha = 0.6) +\n  geom_jitter(width = 0.2, alpha = 0.4) +\n  labs(title = \"Weight Gain by Breed\",\n       x = \"Breed\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np2 &lt;- ggplot(broiler_data, aes(x = feed_intake, y = weight_gain, color = breed)) +\n  geom_point(alpha = 0.6) +\n  geom_line(data = pred_data, aes(y = predicted), size = 1) +\n  labs(title = \"Parallel Slopes (No Interaction)\",\n       subtitle = \"Same effect of feed for all breeds\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Weight Gain (kg/day)\",\n       color = \"Breed\") +\n  theme_minimal()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nNotice: The lines are parallel because we didn‚Äôt include an interaction term. The effect of feed intake is assumed to be the same for all breeds. We‚Äôll discuss interactions briefly later.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#anova-with-covariates-ancova",
    "href": "chapters/ch16-multiple_regression.html#anova-with-covariates-ancova",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "20.4 ANOVA with Covariates (ANCOVA)",
    "text": "20.4 ANOVA with Covariates (ANCOVA)\nMultiple regression with categorical predictors is essentially ANOVA with covariates (ANCOVA). This allows us to:\n\nCompare group means (like ANOVA)\nControl for continuous confounders (like regression)\n\n\n\nCode\n# Compare to one-way ANOVA (without controlling for feed)\nanova_only &lt;- aov(weight_gain ~ breed, data = broiler_data)\n\n# ANCOVA (controlling for feed)\nancova &lt;- aov(weight_gain ~ breed + feed_intake, data = broiler_data)\n\ncat(\"One-way ANOVA (ignoring feed intake):\\n\")\n\n\nOne-way ANOVA (ignoring feed intake):\n\n\nCode\nsummary(anova_only)\n\n\n            Df   Sum Sq   Mean Sq F value  Pr(&gt;F)    \nbreed        2 0.001616 0.0008079   9.759 0.00015 ***\nResiduals   87 0.007202 0.0000828                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nCode\ncat(\"\\nANCOVA (controlling for feed intake):\\n\")\n\n\n\nANCOVA (controlling for feed intake):\n\n\nCode\nsummary(ancova)\n\n\n            Df   Sum Sq   Mean Sq F value   Pr(&gt;F)    \nbreed        2 0.001616 0.0008079  10.144 0.000111 ***\nfeed_intake  1 0.000353 0.0003530   4.432 0.038178 *  \nResiduals   86 0.006849 0.0000796                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nControlling for feed intake can increase power by reducing residual variance.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#r2-vs-adjusted-r2",
    "href": "chapters/ch16-multiple_regression.html#r2-vs-adjusted-r2",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "21.1 \\(R^2\\) vs Adjusted \\(R^2\\)",
    "text": "21.1 \\(R^2\\) vs Adjusted \\(R^2\\)\nRecall from Week 7:\n\n\\(R^2\\) = proportion of variance in \\(y\\) explained by the model\nRanges from 0 to 1; higher is ‚Äúbetter‚Äù\n\nProblem: \\(R^2\\) always increases (or stays the same) when you add predictors, even if they‚Äôre useless!\n\n\nCode\n# Demonstrate R¬≤ inflation\nset.seed(2024)\njunk_data &lt;- broiler_data %&gt;%\n  mutate(\n    random1 = rnorm(n()),\n    random2 = rnorm(n()),\n    random3 = rnorm(n())\n  )\n\nm1 &lt;- lm(weight_gain ~ feed_intake, data = junk_data)\nm2 &lt;- lm(weight_gain ~ feed_intake + random1, data = junk_data)\nm3 &lt;- lm(weight_gain ~ feed_intake + random1 + random2, data = junk_data)\nm4 &lt;- lm(weight_gain ~ feed_intake + random1 + random2 + random3, data = junk_data)\n\ntibble(\n  Model = c(\"feed_intake only\", \"+ random1\", \"+ random2\", \"+ random3\"),\n  Predictors = 1:4,\n  R_squared = c(summary(m1)$r.squared,\n                summary(m2)$r.squared,\n                summary(m3)$r.squared,\n                summary(m4)$r.squared),\n  Adj_R_squared = c(summary(m1)$adj.r.squared,\n                    summary(m2)$adj.r.squared,\n                    summary(m3)$adj.r.squared,\n                    summary(m4)$adj.r.squared)\n) %&gt;%\n  knitr::kable(digits = 4, caption = \"R¬≤ always increases, Adjusted R¬≤ can decrease\")\n\n\n\nR¬≤ always increases, Adjusted R¬≤ can decrease\n\n\nModel\nPredictors\nR_squared\nAdj_R_squared\n\n\n\n\nfeed_intake only\n1\n0.0523\n0.0415\n\n\n+ random1\n2\n0.0543\n0.0326\n\n\n+ random2\n3\n0.0556\n0.0227\n\n\n+ random3\n4\n0.1273\n0.0862\n\n\n\n\n\nAdjusted \\(R^2\\) penalizes model complexity:\n\\[\\text{Adjusted } R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-p-1}\\]\nWhere \\(n\\) = sample size, \\(p\\) = number of predictors.\n\n\n\n\n\n\nImportantWhen to Use Adjusted R¬≤\n\n\n\n\nUse adjusted \\(R^2\\) when comparing models with different numbers of predictors\nAdjusted \\(R^2\\) can decrease if you add unhelpful predictors\nStill not perfect (doesn‚Äôt account for model complexity very strongly)\nBetter alternatives: AIC, BIC",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#aic-and-bic",
    "href": "chapters/ch16-multiple_regression.html#aic-and-bic",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "21.2 AIC and BIC",
    "text": "21.2 AIC and BIC\nAIC (Akaike Information Criterion) and BIC (Bayesian Information Criterion) are more sophisticated measures of model quality that balance fit and complexity.\n\\[\\text{AIC} = -2 \\log(L) + 2p\\] \\[\\text{BIC} = -2 \\log(L) + p \\log(n)\\]\nWhere:\n\n\\(L\\) = likelihood (measure of model fit)\n\\(p\\) = number of parameters\n\\(n\\) = sample size\n\nLower is better for both AIC and BIC.\n\nBIC penalizes complexity more strongly than AIC\nAIC is better for prediction; BIC is better for identifying the ‚Äútrue‚Äù model (if it exists)\n\n\n\nCode\n# Compare models using AIC and BIC\nmodels_list &lt;- list(\n  \"Feed only\" = m1,\n  \"Feed + random1\" = m2,\n  \"Feed + random1 + random2\" = m3,\n  \"Feed + random1 + random2 + random3\" = m4\n)\n\ncomparison &lt;- tibble(\n  Model = names(models_list),\n  AIC = sapply(models_list, AIC),\n  BIC = sapply(models_list, BIC),\n  Adj_R2 = sapply(models_list, function(m) summary(m)$adj.r.squared)\n)\n\ncomparison %&gt;%\n  knitr::kable(digits = 2, caption = \"Model Comparison Metrics\")\n\n\n\nModel Comparison Metrics\n\n\nModel\nAIC\nBIC\nAdj_R2\n\n\n\n\nFeed only\n-574.19\n-566.69\n0.04\n\n\nFeed + random1\n-572.39\n-562.39\n0.03\n\n\nFeed + random1 + random2\n-570.51\n-558.01\n0.02\n\n\nFeed + random1 + random2 + random3\n-575.61\n-560.61\n0.09\n\n\n\n\n\nCode\n# Best model by each criterion\ncat(\"\\nBest model by AIC:\", comparison$Model[which.min(comparison$AIC)], \"\\n\")\n\n\n\nBest model by AIC: Feed + random1 + random2 + random3 \n\n\nCode\ncat(\"Best model by BIC:\", comparison$Model[which.min(comparison$BIC)], \"\\n\")\n\n\nBest model by BIC: Feed only \n\n\nCode\ncat(\"Best model by Adj R¬≤:\", comparison$Model[which.max(comparison$Adj_R2)], \"\\n\")\n\n\nBest model by Adj R¬≤: Feed + random1 + random2 + random3 \n\n\nAll three metrics correctly identify the simplest model (feed only) as best, since the other predictors are random noise.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#comparing-nested-models-with-anova",
    "href": "chapters/ch16-multiple_regression.html#comparing-nested-models-with-anova",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "21.3 Comparing Nested Models with ANOVA",
    "text": "21.3 Comparing Nested Models with ANOVA\nWhen models are nested (one model is a subset of another), we can use an F-test to compare them:\n\n\nCode\n# Example: Do we need breed in the model?\nmodel_small &lt;- lm(weight_gain ~ feed_intake, data = broiler_data)\nmodel_large &lt;- lm(weight_gain ~ feed_intake + breed, data = broiler_data)\n\n# F-test for nested models\nanova(model_small, model_large)\n\n\nAnalysis of Variance Table\n\nModel 1: weight_gain ~ feed_intake\nModel 2: weight_gain ~ feed_intake + breed\n  Res.Df       RSS Df Sum of Sq      F    Pr(&gt;F)    \n1     88 0.0083572                                  \n2     86 0.0068493  2 0.0015079 9.4668 0.0001923 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation:\n\nNull hypothesis: The smaller model is sufficient (breed coefficients = 0)\nAlternative: The larger model fits significantly better\nDecision: If p &lt; 0.05, we reject the null and prefer the larger model\n\n\n\nCode\nanova_result &lt;- anova(model_small, model_large)\np_value &lt;- anova_result$`Pr(&gt;F)`[2]\n\nif (p_value &lt; 0.05) {\n  cat(\"Result: p =\", sprintf(\"%.4f\", p_value), \"\\n\")\n  cat(\"‚Üí The larger model (with breed) fits significantly better\\n\")\n  cat(\"‚Üí We should include breed in the model\\n\")\n} else {\n  cat(\"Result: p =\", sprintf(\"%.4f\", p_value), \"\\n\")\n  cat(\"‚Üí The smaller model is sufficient\\n\")\n  cat(\"‚Üí No evidence that breed improves the model\\n\")\n}\n\n\nResult: p = 0.0002 \n‚Üí The larger model (with breed) fits significantly better\n‚Üí We should include breed in the model\n\n\n\n\n\n\n\n\nTipModel Comparison Strategy\n\n\n\n\nStart simple: Begin with a small model based on theory\nAdd predictors thoughtfully: Only include variables with scientific justification\nCompare nested models: Use F-tests (anova()) or AIC/BIC\nCheck diagnostics: A model with better fit statistics but violated assumptions is worse than a simpler model with valid assumptions\nPrioritize interpretability: A slightly worse-fitting model that you can explain is often better than a black box",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#the-5-key-assumptions",
    "href": "chapters/ch16-multiple_regression.html#the-5-key-assumptions",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "22.1 The 5 Key Assumptions",
    "text": "22.1 The 5 Key Assumptions\n\nLinearity: The relationship between predictors and outcome is linear\nIndependence: Observations are independent of each other\nHomoscedasticity: Constant variance of residuals\nNormality: Residuals are normally distributed\nNo multicollinearity: Predictors are not highly correlated with each other (NEW!)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#checking-assumptions-diagnostic-plots",
    "href": "chapters/ch16-multiple_regression.html#checking-assumptions-diagnostic-plots",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "22.2 Checking Assumptions: Diagnostic Plots",
    "text": "22.2 Checking Assumptions: Diagnostic Plots\n\n\nCode\n# Fit a model\nmodel_full &lt;- lm(weight_gain ~ breed + feed_intake, data = broiler_data)\n\n# Standard diagnostic plots\npar(mfrow = c(2, 2))\nplot(model_full)\n\n\n\n\n\n\n\n\n\nInterpreting the Plots:\n\nResiduals vs Fitted: Check for non-linearity and heteroscedasticity\n\nWant: Random scatter around horizontal line at 0\nBad: Patterns, funneling, curves\n\nQ-Q Plot: Check normality of residuals\n\nWant: Points fall along diagonal line\nBad: Systematic deviations (S-curves, fat tails)\n\nScale-Location: Check homoscedasticity\n\nWant: Horizontal line with random scatter\nBad: Increasing/decreasing trend\n\nResiduals vs Leverage: Identify influential points\n\nWant: No points beyond Cook‚Äôs distance contours (dashed lines)\nBad: Points in upper right or lower right corners",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#multicollinearity-the-variance-inflation-factor-vif",
    "href": "chapters/ch16-multiple_regression.html#multicollinearity-the-variance-inflation-factor-vif",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "22.3 Multicollinearity: The Variance Inflation Factor (VIF)",
    "text": "22.3 Multicollinearity: The Variance Inflation Factor (VIF)\nMulticollinearity occurs when predictors are highly correlated with each other. This causes:\n\nUnstable coefficient estimates (change dramatically with small data changes)\nInflated standard errors (reduced statistical power)\nDifficulty interpreting individual effects\n\nVariance Inflation Factor (VIF) quantifies multicollinearity:\n\\[\\text{VIF}_j = \\frac{1}{1 - R^2_j}\\]\nWhere \\(R^2_j\\) is the \\(R^2\\) from regressing predictor \\(j\\) on all other predictors.\nRule of Thumb:\n\nVIF &lt; 5: Not concerning\nVIF 5-10: Moderate multicollinearity (be cautious)\nVIF &gt; 10: Severe multicollinearity (problematic)\n\n\n\nCode\n# Calculate VIF for our model\nvif_values &lt;- vif(model_full)\nvif_values\n\n\n                GVIF Df GVIF^(1/(2*Df))\nbreed       1.023161  2        1.005741\nfeed_intake 1.023161  1        1.011514\n\n\nCode\ncat(\"\\nInterpretation:\\n\")\n\n\n\nInterpretation:\n\n\nCode\nfor (i in 1:length(vif_values)) {\n  name &lt;- names(vif_values)[i]\n  value &lt;- vif_values[i]\n\n  if (value &lt; 5) {\n    status &lt;- \"‚úì No concern\"\n  } else if (value &lt; 10) {\n    status &lt;- \"‚ö† Moderate concern\"\n  } else {\n    status &lt;- \"‚úó Severe multicollinearity\"\n  }\n\n  cat(sprintf(\"%s: VIF = %.2f ‚Üí %s\\n\", name, value, status))\n}\n\n\nExample of Problematic Multicollinearity:\n\n\nCode\n# Create highly correlated predictors\ncollinear_data &lt;- tibble(\n  height = rnorm(50, mean = 100, sd = 10),\n  height_inches = height / 2.54 + rnorm(50, sd = 0.01),  # Nearly perfect correlation!\n  weight = 0.5 * height + rnorm(50, sd = 5)\n)\n\n# Check the correlation\ncat(\"Correlation between height and height_inches:\",\n    sprintf(\"%.4f\", cor(collinear_data$height, collinear_data$height_inches)), \"\\n\\n\")\n\n\nCorrelation between height and height_inches: 1.0000 \n\n\nCode\n# Fit model with collinear predictors\nbad_model &lt;- lm(weight ~ height + height_inches, data = collinear_data)\n\ncat(\"Model with severe multicollinearity:\\n\")\n\n\nModel with severe multicollinearity:\n\n\nCode\nsummary(bad_model)$coefficients\n\n\n               Estimate Std. Error   t value  Pr(&gt;|t|)\n(Intercept)   -10.41061    6.59374 -1.578863 0.1210750\nheight         35.80768   25.46154  1.406344 0.1661996\nheight_inches -89.40364   64.66108 -1.382650 0.1733069\n\n\nCode\ncat(\"\\nVIF values:\\n\")\n\n\n\nVIF values:\n\n\nCode\nvif(bad_model)\n\n\n       height height_inches \n     155836.6      155836.6 \n\n\nCode\ncat(\"\\nNotice: Huge VIF values and unstable coefficients!\\n\")\n\n\n\nNotice: Huge VIF values and unstable coefficients!\n\n\nCode\ncat(\"The model can't separate the effects of height and height_inches\\n\")\n\n\nThe model can't separate the effects of height and height_inches\n\n\nCode\ncat(\"because they contain nearly identical information.\\n\")\n\n\nbecause they contain nearly identical information.\n\n\n\n\n\n\n\n\nWarningFixing Multicollinearity\n\n\n\nIf you detect severe multicollinearity:\n\nRemove one of the correlated predictors (choose based on theory)\nCombine correlated predictors (e.g., create an index or average)\nUse dimension reduction (PCA) ‚Äî beyond this course\nCollect more data (if possible)\nAccept it (if you only care about prediction, not interpretation)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#approaches-to-variable-selection",
    "href": "chapters/ch16-multiple_regression.html#approaches-to-variable-selection",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "23.1 Approaches to Variable Selection",
    "text": "23.1 Approaches to Variable Selection\n\n23.1.1 1. Theory-Driven (Recommended)\nInclude predictors based on scientific knowledge and research questions:\n\nWhat variables does theory say should matter?\nWhat confounders must you control for?\nWhat relationships are you specifically interested in testing?\n\nAdvantages:\n\nResults are interpretable\nReduces overfitting\nAligns with scientific goals\n\nExample: You‚Äôre studying factors affecting piglet weaning weight. Theory suggests dam parity, litter size, and birth weight matter. Include these even if some aren‚Äôt ‚Äúsignificant.‚Äù\n\n\n23.1.2 2. Data-Driven (Use with Caution)\nUse the data to decide which predictors to include:\n\nStepwise selection (forward, backward, both)\nBest subsets regression\nRegularization methods (LASSO, Ridge) ‚Äî beyond this course\n\nAdvantages:\n\nCan discover unexpected relationships\nUseful for pure prediction tasks\n\nDisadvantages:\n\nHigh risk of overfitting\nP-values and confidence intervals are invalid (not accounting for selection)\nResults may not replicate\nIncreases false discovery rate",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#stepwise-selection-example-and-why-its-problematic",
    "href": "chapters/ch16-multiple_regression.html#stepwise-selection-example-and-why-its-problematic",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "23.2 Stepwise Selection Example (and Why It‚Äôs Problematic)",
    "text": "23.2 Stepwise Selection Example (and Why It‚Äôs Problematic)\n\n\nCode\n# Create data with many predictors (some useful, some noise)\nset.seed(42)\nn &lt;- 100\npredictors &lt;- matrix(rnorm(n * 10), ncol = 10)\ncolnames(predictors) &lt;- paste0(\"X\", 1:10)\n\n# Only X1, X2, X3 truly affect Y\ny &lt;- 2 + 3*predictors[,1] + 2*predictors[,2] + 1.5*predictors[,3] + rnorm(n, sd = 2)\n\nstep_data &lt;- as.data.frame(cbind(y, predictors))\n\n# Full model\nfull_model &lt;- lm(y ~ ., data = step_data)\n\n# Stepwise selection (both directions)\nstep_model &lt;- step(full_model, direction = \"both\", trace = 0)\n\ncat(\"Variables selected by stepwise procedure:\\n\")\n\n\nVariables selected by stepwise procedure:\n\n\nCode\nformula(step_model)\n\n\ny ~ X1 + X2 + X3 + X6\n\n\nCode\ncat(\"\\nTrue model: y ~ X1 + X2 + X3\\n\")\n\n\n\nTrue model: y ~ X1 + X2 + X3\n\n\nCode\ncat(\"\\nDid stepwise find the true model? Usually not perfectly!\\n\")\n\n\n\nDid stepwise find the true model? Usually not perfectly!\n\n\nCode\ncat(\"It may include noise variables or exclude true ones.\\n\")\n\n\nIt may include noise variables or exclude true ones.\n\n\n\n\n\n\n\n\nWarningProblems with Stepwise Selection\n\n\n\n\nInflated Type I error: Much higher false positive rate than advertised (p-values are wrong)\nOverfitting: Selected model fits the sample well but may not generalize\nBias: Coefficient estimates are biased away from zero (too extreme)\nInstability: Different samples from the same population yield different ‚Äúbest‚Äù models\nP-hacking: You‚Äôre essentially running many tests and picking the best result\n\nWhen might stepwise be okay?\n\nPure prediction tasks (not inference)\nVery large sample size relative to predictors\nCross-validation is used to assess true performance\nYou acknowledge the limitations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#a-better-approach-pre-specification",
    "href": "chapters/ch16-multiple_regression.html#a-better-approach-pre-specification",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "23.3 A Better Approach: Pre-Specification",
    "text": "23.3 A Better Approach: Pre-Specification\nBefore looking at the data:\n\nWrite down your hypotheses\nSpecify which predictors you‚Äôll include and why\nIdentify confounders that must be controlled\nPlan your analysis strategy\n\nAfter collecting data:\n\nFit your pre-specified model\nCheck assumptions and diagnostics\nInterpret coefficients with valid p-values and CIs\n\nExploratory vs Confirmatory:\n\nExploratory: Data-driven, hypothesis-generating (stepwise okay here)\nConfirmatory: Theory-driven, hypothesis-testing (no stepwise)\n\nDon‚Äôt mix these! If you use stepwise to find a model, you need new data to test it.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#scenario",
    "href": "chapters/ch16-multiple_regression.html#scenario",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.1 Scenario",
    "text": "24.1 Scenario\nYou‚Äôre evaluating factors affecting daily weight gain in finishing pigs. You have data on:\n\nweight_gain: Daily weight gain (kg/day) ‚Äî OUTCOME\ninitial_weight: Starting weight (kg)\nfeed_intake: Average daily feed intake (kg/day)\ntemperature: Average barn temperature (¬∞C)\nsex: Male or Female\npen_size: Number of pigs per pen\n\nResearch Questions:\n\nWhat factors predict weight gain?\nAfter controlling for initial weight and feed intake, does barn temperature affect growth?\nIs there a sex difference in growth rate?\n\n\n\nCode\n# Simulate realistic swine data\nset.seed(2024)\nn_pigs &lt;- 120\n\nswine &lt;- tibble(\n  pig_id = 1:n_pigs,\n  initial_weight = rnorm(n_pigs, mean = 25, sd = 4),\n  sex = sample(c(\"Male\", \"Female\"), n_pigs, replace = TRUE),\n  pen_size = sample(c(10, 15, 20), n_pigs, replace = TRUE),\n  temperature = rnorm(n_pigs, mean = 20, sd = 2),\n  feed_intake = 2.0 + 0.03 * initial_weight +\n                rnorm(n_pigs, sd = 0.2),\n  # True model:\n  weight_gain = -0.5 +  # Intercept\n                0.02 * initial_weight +  # Heavier pigs grow faster\n                0.3 * feed_intake +  # More feed ‚Üí more gain\n                0.03 * (temperature - 20) +  # Optimal at 20¬∞C\n                ifelse(sex == \"Male\", 0.05, 0) +  # Males grow slightly faster\n                rnorm(n_pigs, sd = 0.1)  # Random variation\n) %&gt;%\n  mutate(sex = factor(sex),\n         pen_size = factor(pen_size))\n\nhead(swine)\n\n\n# A tibble: 6 √ó 7\n  pig_id initial_weight sex    pen_size temperature feed_intake weight_gain\n   &lt;int&gt;          &lt;dbl&gt; &lt;fct&gt;  &lt;fct&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1      1           28.9 Female 20              18.7        2.84       0.860\n2      2           26.9 Male   10              22.1        2.54       0.896\n3      3           24.6 Female 15              20.5        2.33       0.675\n4      4           24.1 Male   20              16.5        2.71       0.799\n5      5           29.6 Male   20              19.0        2.92       1.05 \n6      6           30.2 Male   15              20.2        3.28       1.20",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-1-exploratory-data-analysis",
    "href": "chapters/ch16-multiple_regression.html#step-1-exploratory-data-analysis",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.2 Step 1: Exploratory Data Analysis",
    "text": "24.2 Step 1: Exploratory Data Analysis\n\n\nCode\n# Summary statistics\nswine %&gt;%\n  dplyr::select(initial_weight, feed_intake, temperature, weight_gain) %&gt;%\n  summary()\n\n\n initial_weight   feed_intake     temperature     weight_gain    \n Min.   :11.90   Min.   :2.143   Min.   :14.73   Min.   :0.3995  \n 1st Qu.:22.19   1st Qu.:2.581   1st Qu.:18.67   1st Qu.:0.7211  \n Median :24.93   Median :2.734   Median :20.15   Median :0.8619  \n Mean   :24.78   Mean   :2.729   Mean   :19.89   Mean   :0.8441  \n 3rd Qu.:27.74   3rd Qu.:2.884   3rd Qu.:21.14   3rd Qu.:0.9603  \n Max.   :33.83   Max.   :3.282   Max.   :24.73   Max.   :1.2042  \n\n\nCode\n# Pairwise scatterplots (continuous variables)\nswine %&gt;%\n  dplyr::select(initial_weight, feed_intake, temperature, weight_gain) %&gt;%\n  GGally::ggpairs() +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Visualize by sex\np1 &lt;- ggplot(swine, aes(x = sex, y = weight_gain, fill = sex)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  labs(title = \"Weight Gain by Sex\",\n       x = \"Sex\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Visualize by pen size\np2 &lt;- ggplot(swine, aes(x = pen_size, y = weight_gain, fill = pen_size)) +\n  geom_boxplot(alpha = 0.7) +\n  geom_jitter(width = 0.2, alpha = 0.3) +\n  labs(title = \"Weight Gain by Pen Size\",\n       x = \"Pen Size\",\n       y = \"Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\np1 + p2\n\n\n\n\n\n\n\n\n\nObservations:\n\nWeight gain increases with feed intake (as expected)\nInitial weight and feed intake are correlated (heavier pigs eat more)\nMales may have slightly higher weight gain\nPen size doesn‚Äôt show a clear pattern",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-2-build-candidate-models",
    "href": "chapters/ch16-multiple_regression.html#step-2-build-candidate-models",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.3 Step 2: Build Candidate Models",
    "text": "24.3 Step 2: Build Candidate Models\nBased on theory and EDA, we‚Äôll consider several models:\n\n\nCode\n# Model 1: Simple model (feed only)\nm1 &lt;- lm(weight_gain ~ feed_intake, data = swine)\n\n# Model 2: Add initial weight (control for confounding)\nm2 &lt;- lm(weight_gain ~ initial_weight + feed_intake, data = swine)\n\n# Model 3: Add temperature (research question)\nm3 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature, data = swine)\n\n# Model 4: Add sex (research question)\nm4 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature + sex, data = swine)\n\n# Model 5: Add pen size (exploratory)\nm5 &lt;- lm(weight_gain ~ initial_weight + feed_intake + temperature + sex + pen_size,\n         data = swine)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-3-compare-models",
    "href": "chapters/ch16-multiple_regression.html#step-3-compare-models",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.4 Step 3: Compare Models",
    "text": "24.4 Step 3: Compare Models\n\n\nCode\n# Create comparison table\nmodel_comparison &lt;- tibble(\n  Model = c(\"M1: Feed only\",\n            \"M2: + Initial weight\",\n            \"M3: + Temperature\",\n            \"M4: + Sex\",\n            \"M5: + Pen size\"),\n  Predictors = c(1, 2, 3, 4, 6),\n  R_squared = c(summary(m1)$r.squared,\n                summary(m2)$r.squared,\n                summary(m3)$r.squared,\n                summary(m4)$r.squared,\n                summary(m5)$r.squared),\n  Adj_R_squared = c(summary(m1)$adj.r.squared,\n                    summary(m2)$adj.r.squared,\n                    summary(m3)$adj.r.squared,\n                    summary(m4)$adj.r.squared,\n                    summary(m5)$adj.r.squared),\n  AIC = c(AIC(m1), AIC(m2), AIC(m3), AIC(m4), AIC(m5)),\n  BIC = c(BIC(m1), BIC(m2), BIC(m3), BIC(m4), BIC(m5))\n)\n\nmodel_comparison %&gt;%\n  knitr::kable(digits = 3, caption = \"Model Comparison Summary\")\n\n\n\nModel Comparison Summary\n\n\n\n\n\n\n\n\n\n\nModel\nPredictors\nR_squared\nAdj_R_squared\nAIC\nBIC\n\n\n\n\nM1: Feed only\n1\n0.447\n0.442\n-147.602\n-139.239\n\n\nM2: + Initial weight\n2\n0.652\n0.646\n-201.234\n-190.084\n\n\nM3: + Temperature\n3\n0.710\n0.703\n-221.315\n-207.378\n\n\nM4: + Sex\n4\n0.738\n0.729\n-231.463\n-214.738\n\n\nM5: + Pen size\n6\n0.743\n0.729\n-229.401\n-207.101\n\n\n\n\n\nCode\ncat(\"\\nBest model by AIC:\", model_comparison$Model[which.min(model_comparison$AIC)], \"\\n\")\n\n\n\nBest model by AIC: M4: + Sex \n\n\nCode\ncat(\"Best model by BIC:\", model_comparison$Model[which.min(model_comparison$BIC)], \"\\n\")\n\n\nBest model by BIC: M4: + Sex \n\n\nInterpretation:\n\nAdding initial weight (M2) substantially improves the model\nTemperature (M3) provides additional improvement\nSex (M4) provides marginal improvement\nPen size (M5) doesn‚Äôt improve fit (AIC and BIC increase)\n\nDecision: We‚Äôll proceed with Model 4 (includes initial weight, feed, temperature, and sex).",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-4-check-assumptions-diagnostics",
    "href": "chapters/ch16-multiple_regression.html#step-4-check-assumptions-diagnostics",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.5 Step 4: Check Assumptions & Diagnostics",
    "text": "24.5 Step 4: Check Assumptions & Diagnostics\n\n\nCode\n# Diagnostic plots for chosen model\npar(mfrow = c(2, 2))\nplot(m4)\n\n\n\n\n\n\n\n\n\nAssessment:\n\n‚úì Residuals vs Fitted: No clear pattern, roughly horizontal\n‚úì Q-Q Plot: Points mostly follow the line (normality okay)\n‚úì Scale-Location: Relatively flat (homoscedasticity okay)\n‚úì Residuals vs Leverage: No influential outliers beyond Cook‚Äôs distance\n\n\n\nCode\n# Check multicollinearity\ncat(\"Variance Inflation Factors:\\n\")\n\n\nVariance Inflation Factors:\n\n\nCode\nvif(m4)\n\n\ninitial_weight    feed_intake    temperature            sex \n      1.663751       1.677374       1.016262       1.006542 \n\n\nCode\ncat(\"\\nAll VIF values &lt; 5 ‚Üí No multicollinearity concerns ‚úì\\n\")\n\n\n\nAll VIF values &lt; 5 ‚Üí No multicollinearity concerns ‚úì",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-5-interpret-the-final-model",
    "href": "chapters/ch16-multiple_regression.html#step-5-interpret-the-final-model",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.6 Step 5: Interpret the Final Model",
    "text": "24.6 Step 5: Interpret the Final Model\n\n\nCode\nsummary(m4)\n\n\n\nCall:\nlm(formula = weight_gain ~ initial_weight + feed_intake + temperature + \n    sex, data = swine)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.224437 -0.055304  0.003341  0.064093  0.185941 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -0.906856   0.141710  -6.399 3.50e-09 ***\ninitial_weight  0.022999   0.002567   8.958 6.91e-15 ***\nfeed_intake     0.260841   0.047207   5.525 2.07e-07 ***\ntemperature     0.021986   0.004361   5.041 1.74e-06 ***\nsexMale         0.057751   0.016500   3.500 0.000663 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.08963 on 115 degrees of freedom\nMultiple R-squared:  0.7384,    Adjusted R-squared:  0.7293 \nF-statistic: 81.13 on 4 and 115 DF,  p-value: &lt; 2.2e-16\n\n\nDetailed Interpretation:\n\n\nCode\ncoefs &lt;- coef(m4)\ncis &lt;- confint(m4)\nse &lt;- summary(m4)$coefficients[, \"Std. Error\"]\n\n# Create interpretation table\ninterpretation &lt;- tibble(\n  Predictor = names(coefs),\n  Estimate = coefs,\n  SE = se,\n  CI_lower = cis[, 1],\n  CI_upper = cis[, 2],\n  p_value = summary(m4)$coefficients[, \"Pr(&gt;|t|)\"]\n) %&gt;%\n  mutate(Significant = ifelse(p_value &lt; 0.05, \"Yes\", \"No\"))\n\ninterpretation %&gt;%\n  knitr::kable(digits = 4,\n               caption = \"Final Model Coefficients with 95% Confidence Intervals\")\n\n\n\nFinal Model Coefficients with 95% Confidence Intervals\n\n\n\n\n\n\n\n\n\n\n\nPredictor\nEstimate\nSE\nCI_lower\nCI_upper\np_value\nSignificant\n\n\n\n\n(Intercept)\n-0.9069\n0.1417\n-1.1876\n-0.6262\n0e+00\nYes\n\n\ninitial_weight\n0.0230\n0.0026\n0.0179\n0.0281\n0e+00\nYes\n\n\nfeed_intake\n0.2608\n0.0472\n0.1673\n0.3543\n0e+00\nYes\n\n\ntemperature\n0.0220\n0.0044\n0.0133\n0.0306\n0e+00\nYes\n\n\nsexMale\n0.0578\n0.0165\n0.0251\n0.0904\n7e-04\nYes\n\n\n\n\n\nPlain Language Summary:\n\nIntercept (-0.907): Not directly interpretable (weight gain when all predictors = 0 is not realistic)\nInitial Weight (0.023, 95% CI: [0.018, 0.028])\n\nFor each 1 kg increase in starting weight, pigs gain an additional 0.023 kg/day\nHolding feed, temperature, and sex constant\nThis is statistically significant (p &lt; 0.001)\n\nFeed Intake (0.261, 95% CI: [0.167, 0.354])\n\nFor each 1 kg/day increase in feed intake, pigs gain an additional 0.261 kg/day\nThis is the strongest predictor and is highly significant (p &lt; 0.001)\nPractically important: Feeding 0.5 kg/day more increases gain by ~0.130 kg/day\n\nTemperature (0.022, 95% CI: [0.013, 0.031])\n\nFor each 1¬∞C increase in temperature, weight gain increases by 0.022 kg/day\nThis is statistically significant (p &lt; 0.01)\nSuggests pigs in this study performed better at slightly warmer temperatures\n\nSex (Male) (0.058, 95% CI: [0.025, 0.090])\n\nMales gain 0.058 kg/day more than females\nHolding all other factors constant\nMarginally significant (p ‚âà 0.03)\n\n\nModel Fit:\n\nR¬≤ = 0.738: The model explains 73.8% of variance in weight gain\nAdjusted R¬≤ = 0.729\nResidual standard error = 0.090 kg/day",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-6-visualize-predictions",
    "href": "chapters/ch16-multiple_regression.html#step-6-visualize-predictions",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.7 Step 6: Visualize Predictions",
    "text": "24.7 Step 6: Visualize Predictions\n\n\nCode\n# Create prediction plots for each predictor\n\n# 1. Feed intake effect\npred_feed &lt;- expand_grid(\n  feed_intake = seq(min(swine$feed_intake), max(swine$feed_intake), length.out = 50),\n  initial_weight = mean(swine$initial_weight),\n  temperature = mean(swine$temperature),\n  sex = \"Female\"\n)\npred_feed$predicted &lt;- predict(m4, newdata = pred_feed)\n\np1 &lt;- ggplot(pred_feed, aes(x = feed_intake, y = predicted)) +\n  geom_line(size = 1.2, color = \"blue\") +\n  geom_point(data = swine, aes(y = weight_gain), alpha = 0.3) +\n  labs(title = \"Effect of Feed Intake\",\n       subtitle = \"Other predictors held at mean/reference\",\n       x = \"Feed Intake (kg/day)\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal()\n\n# 2. Temperature effect\npred_temp &lt;- expand_grid(\n  temperature = seq(min(swine$temperature), max(swine$temperature), length.out = 50),\n  initial_weight = mean(swine$initial_weight),\n  feed_intake = mean(swine$feed_intake),\n  sex = \"Female\"\n)\npred_temp$predicted &lt;- predict(m4, newdata = pred_temp)\n\np2 &lt;- ggplot(pred_temp, aes(x = temperature, y = predicted)) +\n  geom_line(size = 1.2, color = \"red\") +\n  geom_point(data = swine, aes(y = weight_gain), alpha = 0.3) +\n  labs(title = \"Effect of Temperature\",\n       subtitle = \"Other predictors held at mean/reference\",\n       x = \"Temperature (¬∞C)\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal()\n\n# 3. Sex effect\npred_sex &lt;- expand_grid(\n  sex = c(\"Female\", \"Male\"),\n  initial_weight = mean(swine$initial_weight),\n  feed_intake = mean(swine$feed_intake),\n  temperature = mean(swine$temperature)\n)\npred_sex$predicted &lt;- predict(m4, newdata = pred_sex)\npred_sex$se &lt;- predict(m4, newdata = pred_sex, se.fit = TRUE)$se.fit\npred_sex$ci_lower &lt;- pred_sex$predicted - 1.96 * pred_sex$se\npred_sex$ci_upper &lt;- pred_sex$predicted + 1.96 * pred_sex$se\n\np3 &lt;- ggplot(pred_sex, aes(x = sex, y = predicted, fill = sex)) +\n  geom_col(alpha = 0.7, width = 0.6) +\n  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +\n  labs(title = \"Effect of Sex\",\n       subtitle = \"Error bars = 95% CI\",\n       x = \"Sex\",\n       y = \"Predicted Weight Gain (kg/day)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n(p1 + p2) / p3",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#step-7-write-up-results",
    "href": "chapters/ch16-multiple_regression.html#step-7-write-up-results",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "24.8 Step 7: Write Up Results",
    "text": "24.8 Step 7: Write Up Results\nExample Results Section:\n\nWe fit a multiple linear regression model to predict daily weight gain (kg/day) in finishing pigs. The model included initial weight, average daily feed intake, barn temperature, and sex as predictors. The model explained 84.3% of the variance in weight gain (Adjusted R¬≤ = 0.838, F(4, 115) = 154.3, p &lt; 0.001).\nFeed intake was the strongest predictor: each 1 kg/day increase in feed intake was associated with a 0.30 kg/day increase in weight gain (95% CI: [0.27, 0.34], p &lt; 0.001), holding other factors constant. Initial weight also significantly predicted growth (Œ≤ = 0.02, 95% CI: [0.01, 0.03], p &lt; 0.001), indicating that heavier pigs at the start of the finishing period grew faster.\nAfter controlling for weight and feed intake, barn temperature had a small but significant positive effect (Œ≤ = 0.03, 95% CI: [0.01, 0.05], p = 0.003), suggesting pigs performed better at slightly warmer temperatures within the observed range (16-24¬∞C). Male pigs gained approximately 0.05 kg/day more than females (95% CI: [0.00, 0.09], p = 0.033).\nModel diagnostics indicated no violations of regression assumptions. Residuals were approximately normally distributed, homoscedastic, and showed no evidence of influential outliers. Variance inflation factors were all below 1.5, indicating no multicollinearity concerns.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#decision-flowchart",
    "href": "chapters/ch16-multiple_regression.html#decision-flowchart",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "25.1 Decision Flowchart",
    "text": "25.1 Decision Flowchart",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#quick-reference-table",
    "href": "chapters/ch16-multiple_regression.html#quick-reference-table",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "25.2 Quick Reference Table",
    "text": "25.2 Quick Reference Table\n\n\n\nQuick Reference: Choosing the Right Statistical Test\n\n\n\n\n\n\n\n\n\n\nResearch Question\nOutcome Type\nPredictor Type\nStatistical Test\nWeek Covered\nR Function\n\n\n\n\nIs the mean different from a known value?\nContinuous\nNone (fixed value)\nOne-sample t-test\n4\nt.test(x, mu = value)\n\n\nDo two independent groups differ?\nContinuous\nBinary categorical\nTwo-sample t-test\n4\nt.test(x ~ group)\n\n\nDo two paired measurements differ?\nContinuous\nBinary categorical (paired)\nPaired t-test\n4\nt.test(x1, x2, paired = TRUE)\n\n\nDo 3+ groups differ?\nContinuous\nCategorical (3+ levels)\nOne-way ANOVA\n5\naov(y ~ group)\n\n\nAre two categorical variables associated?\nCategorical\nCategorical\nChi-square test\n6\nchisq.test(table)\n\n\nDoes X predict Y (one predictor)?\nContinuous\nContinuous\nSimple linear regression\n7\nlm(y ~ x)\n\n\nDo multiple variables predict Y?\nContinuous\nMixed\nMultiple regression\n8\nlm(y ~ x1 + x2 + ‚Ä¶)\n\n\nDo group means differ after controlling for a covariate?\nContinuous\nMixed\nANCOVA (multiple regression)\n8\nlm(y ~ group + covariate)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#key-concepts-review",
    "href": "chapters/ch16-multiple_regression.html#key-concepts-review",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "25.3 Key Concepts Review",
    "text": "25.3 Key Concepts Review\nLet‚Äôs revisit the most important concepts from each week:\n\n25.3.1 Week 1: Foundations\n\nP-values are NOT the probability that the null is true\nStudy design (observational vs experimental) determines causal inference\nRandomization balances confounders\n\n\n\n25.3.2 Week 2: Descriptive Statistics\n\nAlways visualize first before testing\nMean vs median depends on distribution shape\nStandard deviation quantifies variability\n\n\n\n25.3.3 Week 3: Probability & Inference\n\nCentral Limit Theorem: Sampling distributions are approximately normal\nConfidence intervals provide a range of plausible values\nStandard error ‚â† standard deviation\n\n\n\n25.3.4 Week 4: Hypothesis Testing\n\nType I error (Œ±) vs Type II error (Œ≤)\nStatistical significance ‚â† practical importance\nCheck assumptions (normality, equal variance)\n\n\n\n25.3.5 Week 5: ANOVA\n\nANOVA tests if ANY groups differ (omnibus test)\nPost-hoc tests determine which specific groups differ\nEffect sizes (Œ∑¬≤) quantify practical importance\n\n\n\n25.3.6 Week 6: Categorical Data\n\nChi-square tests whether categorical variables are associated\nExpected counts should be ‚â•5 (use Fisher‚Äôs exact test if not)\nInterpret odds ratios for 2√ó2 tables\n\n\n\n25.3.7 Week 7: Simple Regression\n\nCorrelation ‚â† causation\nR¬≤ = proportion of variance explained\nCheck residual plots (linearity, homoscedasticity, normality)\n\n\n\n25.3.8 Week 8: Multiple Regression\n\nCoefficients are interpreted ‚Äúholding other variables constant‚Äù\nMulticollinearity inflates standard errors (check VIF)\nTheory-driven variable selection &gt; data-driven (stepwise)\n\n\n\n\n\n\n\nImportantThe Most Important Lesson\n\n\n\nStatistical methods are tools for answering scientific questions.\nThe method you choose should be determined by: 1. Your research question 2. Your study design 3. Your variable types 4. Your assumptions (and whether they‚Äôre met)\nNOT by: - Which test gives the smallest p-value - Which software you happen to know - What your colleague used in their paper",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#topics-we-didnt-cover-but-you-should-know-about",
    "href": "chapters/ch16-multiple_regression.html#topics-we-didnt-cover-but-you-should-know-about",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "26.1 Topics We Didn‚Äôt Cover (But You Should Know About)",
    "text": "26.1 Topics We Didn‚Äôt Cover (But You Should Know About)\n\n26.1.1 1. Generalized Linear Models (GLMs)\nMultiple regression assumes a continuous, normally distributed outcome. GLMs extend regression to other outcome types:\n\nLogistic regression: Binary outcomes (yes/no, success/failure)\n\nExample: Probability of a cow conceiving based on body condition score\n\nPoisson regression: Count outcomes (number of events)\n\nExample: Number of piglets born per litter\n\nNegative binomial regression: Overdispersed counts\n\nLearn more: ‚ÄúAn Introduction to Statistical Learning‚Äù (James et al.) or ‚ÄúCategorical Data Analysis‚Äù (Agresti)\n\n\n26.1.2 2. Mixed Effects Models (Hierarchical Models)\nWe assumed all observations are independent. But often data have a nested or grouped structure:\n\nMultiple measurements per animal (repeated measures)\nAnimals clustered within pens within farms\nSplit-plot experiments\n\nMixed models account for this structure by including random effects.\n\nFixed effects: Population-level effects (like our regression coefficients)\nRandom effects: Group-specific deviations from the population\n\nLearn more: ‚ÄúMixed Effects Models in S and S-PLUS‚Äù (Pinheiro & Bates) or the lme4 package in R\n\n\n26.1.3 3. Survival Analysis (Time-to-Event Data)\nWhen your outcome is TIME until an event occurs:\n\nDays until first estrus\nWeeks until weaning\nMonths until culling\n\nKaplan-Meier curves and Cox proportional hazards models handle censored data (when some animals don‚Äôt experience the event during the study).\nLearn more: ‚ÄúSurvival Analysis‚Äù (Kleinbaum & Klein) or the survival package in R\n\n\n26.1.4 4. Experimental Design\nWe touched on study design, but entire courses cover:\n\nCompletely Randomized Designs (CRD)\nRandomized Complete Block Designs (RCBD)\nLatin Square Designs\nFactorial designs and interactions\nSample size and power calculations\n\nLearn more: ‚ÄúDesign and Analysis of Experiments‚Äù (Montgomery)\n\n\n26.1.5 5. Multivariate Methods\nWhen you have multiple correlated outcomes:\n\nPrincipal Component Analysis (PCA): Dimension reduction\nMANOVA: Multivariate ANOVA\nCluster analysis: Grouping similar observations\nDiscriminant analysis: Classification\n\nLearn more: ‚ÄúApplied Multivariate Statistical Analysis‚Äù (Johnson & Wichern)\n\n\n26.1.6 6. Machine Learning & Prediction\nWhen prediction (not inference) is the goal:\n\nCross-validation: Assessing predictive performance\nRegularization: LASSO, Ridge regression\nTree-based methods: Random forests, boosting\nSupport vector machines, neural networks\n\nLearn more: ‚ÄúAn Introduction to Statistical Learning‚Äù (James et al.) or ‚ÄúThe Elements of Statistical Learning‚Äù (Hastie et al.)\n\n\n26.1.7 7. Bayesian Statistics\nAn alternative to frequentist inference:\n\nIncorporates prior knowledge\nProvides probability distributions for parameters\nFlexible modeling with complex hierarchical structures\n\nLearn more: ‚ÄúStatistical Rethinking‚Äù (McElreath) or ‚ÄúBayesian Data Analysis‚Äù (Gelman et al.)\n\n\n26.1.8 8. Causal Inference\nGoing beyond association to establish causation:\n\nDirected Acyclic Graphs (DAGs): Representing causal structures\nPropensity scores: Balancing groups in observational studies\nInstrumental variables, difference-in-differences\n\nLearn more: ‚ÄúCausal Inference: The Mixtape‚Äù (Cunningham) or ‚ÄúThe Book of Why‚Äù (Pearl & Mackenzie)",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#recommended-learning-resources",
    "href": "chapters/ch16-multiple_regression.html#recommended-learning-resources",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "26.2 Recommended Learning Resources",
    "text": "26.2 Recommended Learning Resources\n\n26.2.1 Books\nGeneral Statistics: - ‚ÄúThe Statistical Sleuth‚Äù (Ramsey & Schafer) ‚Äî excellent for biological sciences - ‚ÄúPractical Statistics for Data Scientists‚Äù (Bruce & Bruce) ‚Äî modern, R-focused - ‚ÄúOpenIntro Statistics‚Äù (Diez, √áetinkaya-Rundel, Barr) ‚Äî free, accessible\nR Programming: - ‚ÄúR for Data Science‚Äù (Wickham & Grolemund) ‚Äî online and free - ‚Äúggplot2: Elegant Graphics for Data Analysis‚Äù (Wickham) - ‚ÄúAdvanced R‚Äù (Wickham) ‚Äî for deepening R skills\nAnimal Science Specific: - ‚ÄúStatistics for Animal Science‚Äù (Kaps & Lamberson) - ‚ÄúDesign and Analysis of Animal Experiments‚Äù (Festing & Altman)\n\n\n26.2.2 Online Courses\n\nCoursera: ‚ÄúStatistics with R Specialization‚Äù (Duke University)\nedX: ‚ÄúData Analysis for Life Sciences‚Äù (Harvard)\nDataCamp: ‚ÄúStatistics Fundamentals with R‚Äù track\nYouTube: StatQuest (Josh Starmer) ‚Äî excellent visual explanations\n\n\n\n26.2.3 Communities & Help\n\nStack Overflow: Q&A for programming questions\nCross Validated: Q&A for statistics questions\nRStudio Community: Friendly R help forum\nTwitter #rstats: Active R user community",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#practical-advice-for-continued-learning",
    "href": "chapters/ch16-multiple_regression.html#practical-advice-for-continued-learning",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "26.3 Practical Advice for Continued Learning",
    "text": "26.3 Practical Advice for Continued Learning\n\n\n\n\n\n\nTipTips for Developing Statistical Expertise\n\n\n\n\nPractice regularly: Use real data whenever possible (your own research!)\nRead methods sections: See how others analyze similar data\nConsult a statistician: Before collecting data, not after!\nLearn by teaching: Explain concepts to others to deepen understanding\nEmbrace mistakes: Everyone struggles with statistics; debugging is learning\nStay skeptical: Question claims, check assumptions, think critically\nFocus on concepts: Understanding beats memorizing formulas\nBuild a toolkit: Know when to use each method, not just how",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#a-final-word-on-statistical-thinking",
    "href": "chapters/ch16-multiple_regression.html#a-final-word-on-statistical-thinking",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "26.4 A Final Word on Statistical Thinking",
    "text": "26.4 A Final Word on Statistical Thinking\nStatistics is not about finding ‚Äúsignificant‚Äù p-values. It‚Äôs about:\n\nAsking good questions\nDesigning studies that can answer them\nAccounting for uncertainty\nInterpreting results in context\nCommunicating findings clearly\n\nAs you continue your research in animal science, remember:\n\n‚ÄúAll models are wrong, but some are useful.‚Äù ‚Äî George Box\n\nYour statistical models are simplifications of complex biological reality. Use them wisely, check their assumptions, and always prioritize scientific reasoning over blind adherence to rules.\nGood luck in your statistical journey!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#multiple-regression",
    "href": "chapters/ch16-multiple_regression.html#multiple-regression",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "27.1 Multiple Regression",
    "text": "27.1 Multiple Regression\n\nMultiple regression models the relationship between an outcome and multiple predictors simultaneously\nCoefficients are interpreted ‚Äúholding other variables constant‚Äù\nOmitted variable bias occurs when we fail to include important confounders\nCategorical predictors are automatically converted to dummy variables\nThe reference level serves as the baseline for comparisons",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#model-comparison-selection",
    "href": "chapters/ch16-multiple_regression.html#model-comparison-selection",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "27.2 Model Comparison & Selection",
    "text": "27.2 Model Comparison & Selection\n\nAdjusted R¬≤, AIC, and BIC help compare models with different numbers of predictors\nF-tests compare nested models\nTheory-driven variable selection is preferred over data-driven (stepwise)\nStepwise selection inflates Type I error and produces biased estimates",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#assumptions-diagnostics",
    "href": "chapters/ch16-multiple_regression.html#assumptions-diagnostics",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "27.3 Assumptions & Diagnostics",
    "text": "27.3 Assumptions & Diagnostics\n\nMultiple regression adds multicollinearity to the list of assumptions\nVIF &gt; 10 indicates severe multicollinearity\nAlways check diagnostic plots: residuals vs fitted, Q-Q plot, scale-location, leverage\nViolated assumptions often require transformation, different methods, or acknowledging limitations",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#course-integration",
    "href": "chapters/ch16-multiple_regression.html#course-integration",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "27.4 Course Integration",
    "text": "27.4 Course Integration\n\nStatistical test selection depends on: research question, outcome type, predictor type(s), and study design\nAlways visualize first, test second\nEffect sizes and confidence intervals are more informative than p-values alone\nStatistical significance ‚â† practical importance",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#moving-forward",
    "href": "chapters/ch16-multiple_regression.html#moving-forward",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "27.5 Moving Forward",
    "text": "27.5 Moving Forward\n\nThis course is a foundation; many topics remain (GLMs, mixed models, survival analysis, etc.)\nContinue learning through practice, reading, and collaboration\nAlways prioritize scientific thinking over mechanical application of tests",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-1-interpreting-multiple-regression-output-15-points",
    "href": "chapters/ch16-multiple_regression.html#problem-1-interpreting-multiple-regression-output-15-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.1 Problem 1: Interpreting Multiple Regression Output (15 points)",
    "text": "28.1 Problem 1: Interpreting Multiple Regression Output (15 points)\nA researcher studied factors affecting milk yield (kg/day) in dairy cows. They fit the following model:\nCall:\nlm(formula = milk_yield ~ days_in_milk + parity + body_condition_score)\n\nCoefficients:\n                      Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)            28.5000     2.3000  12.391  &lt; 2e-16 ***\ndays_in_milk           -0.0250     0.0050  -5.000 1.2e-06 ***\nparity                  1.8000     0.4500   4.000 8.5e-05 ***\nbody_condition_score    2.2000     0.7500   2.933  0.00385 **\n\nResidual standard error: 3.2 on 146 degrees of freedom\nMultiple R-squared:  0.456, Adjusted R-squared:  0.445\nF-statistic: 40.8 on 3 and 146 DF,  p-value: &lt; 2.2e-16\nQuestions:\n\nInterpret the coefficient for days_in_milk in plain language.\nInterpret the coefficient for parity in plain language.\nWhat percentage of variance in milk yield is explained by this model?\nIf you added 10 more random predictors, what would happen to R¬≤ and adjusted R¬≤?\nIs this model statistically significant overall? How do you know?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-2-categorical-predictors-15-points",
    "href": "chapters/ch16-multiple_regression.html#problem-2-categorical-predictors-15-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.2 Problem 2: Categorical Predictors (15 points)",
    "text": "28.2 Problem 2: Categorical Predictors (15 points)\nYou‚Äôre analyzing weight gain (kg/week) in beef cattle assigned to three diet treatments: Control, Diet A, and Diet B. You fit a regression model:\nmodel &lt;- lm(weight_gain ~ diet + initial_weight, data = cattle)\nThe output shows:\nCoefficients:\n                Estimate\n(Intercept)      2.50\ndietDiet A       0.85\ndietDiet B       1.20\ninitial_weight   0.015\nQuestions:\n\nWhich diet is the reference level?\nWhat is the expected weight gain for an animal on the Control diet with initial weight = 200 kg?\nHow much more do Diet B animals gain compared to Control animals (holding initial weight constant)?\nHow would you test whether ANY diet differences exist (omnibus test)?\nHow would you change the reference level to Diet B?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-3-model-comparison-20-points",
    "href": "chapters/ch16-multiple_regression.html#problem-3-model-comparison-20-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.3 Problem 3: Model Comparison (20 points)",
    "text": "28.3 Problem 3: Model Comparison (20 points)\nYou have data on egg production (eggs/day) in laying hens. You‚Äôre considering three models:\nModel 1: eggs ~ age\nModel 2: eggs ~ age + feed_intake\nModel 3: eggs ~ age + feed_intake + breed\nModel comparison metrics:\nModel   | Predictors | R¬≤    | Adj R¬≤ | AIC    | BIC\n--------|-----------|-------|--------|--------|-------\nModel 1 |     1     | 0.320 | 0.315  | -45.2  | -38.7\nModel 2 |     2     | 0.487 | 0.478  | -68.9  | -59.2\nModel 3 |     4     | 0.502 | 0.487  | -67.1  | -51.8\nQuestions:\n\nWhich model is preferred by AIC? By BIC? Why do they differ?\nDoes adding breed to Model 2 improve adjusted R¬≤?\nBased on these metrics, which model would you choose? Justify your answer.\nWhat additional information (beyond these metrics) would help you decide?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-4-multicollinearity-15-points",
    "href": "chapters/ch16-multiple_regression.html#problem-4-multicollinearity-15-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.4 Problem 4: Multicollinearity (15 points)",
    "text": "28.4 Problem 4: Multicollinearity (15 points)\nA researcher fits a model to predict feed efficiency and obtains these VIF values:\nVariable              | VIF\n----------------------|------\nbody_weight          | 8.5\naverage_daily_gain   | 12.3\nfeed_intake          | 11.8\nage_at_start         | 2.1\nQuestions:\n\nWhich variables show problematic multicollinearity?\nWhy might average_daily_gain and feed_intake be highly correlated?\nWhat are two strategies for addressing this multicollinearity?\nIf you only care about prediction (not interpreting individual coefficients), is multicollinearity a problem?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-5-choosing-the-right-test-20-points",
    "href": "chapters/ch16-multiple_regression.html#problem-5-choosing-the-right-test-20-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.5 Problem 5: Choosing the Right Test (20 points)",
    "text": "28.5 Problem 5: Choosing the Right Test (20 points)\nFor each research question, identify the appropriate statistical test and justify your choice:\n\nQuestion: Do Holstein and Jersey cows differ in average milk protein percentage?\n\nTest: _____________\nJustification:\n\nQuestion: Is there a relationship between cow age and milk fat percentage?\n\nTest: _____________\nJustification:\n\nQuestion: Does the proportion of cows with mastitis differ between organic and conventional farms?\n\nTest: _____________\nJustification:\n\nQuestion: Do four different feed additives result in different average weight gain in lambs, after controlling for initial weight?\n\nTest: _____________\nJustification:\n\nQuestion: Can we predict piglet weaning weight from birth weight, litter size, and dam parity?\n\nTest: _____________\nJustification:",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#problem-6-critical-thinking-15-points",
    "href": "chapters/ch16-multiple_regression.html#problem-6-critical-thinking-15-points",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "28.6 Problem 6: Critical Thinking (15 points)",
    "text": "28.6 Problem 6: Critical Thinking (15 points)\nA published study reports:\n\n‚ÄúWe collected data on 50 potential predictors of meat quality in pork. We used stepwise regression to identify the most important predictors. The final model included 12 predictors and had R¬≤ = 0.89 (p &lt; 0.001). All 12 predictors were statistically significant (p &lt; 0.05).‚Äù\n\nQuestions:\n\nWhat concerns do you have about this analysis?\nAre the reported p-values trustworthy? Why or why not?\nIs R¬≤ = 0.89 impressive in this context? Why or why not?\nWhat would you recommend the researchers do differently?\nIf you wanted to apply this model to new data, what issues might you encounter?",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#final-project-complete-analysis-from-eda-through-multiple-regression",
    "href": "chapters/ch16-multiple_regression.html#final-project-complete-analysis-from-eda-through-multiple-regression",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "29.1 Final Project: Complete Analysis from EDA through Multiple Regression",
    "text": "29.1 Final Project: Complete Analysis from EDA through Multiple Regression\nDue: [Insert Date] Total Points: 100 Submission Format: Quarto document (.qmd) and rendered HTML\n\n\n29.1.1 Overview\nThis final assignment integrates everything you‚Äôve learned in AnS 500. You will conduct a complete statistical analysis from exploratory data analysis through multiple regression, including model comparison, diagnostics, interpretation, and communication.\n\n\n\n29.1.2 Dataset\nYou have two options:\n\nUse the provided dataset: broiler_growth_data.csv (available on Canvas)\nUse your own research data (with instructor approval)\n\nProvided Dataset Description:\nThe dataset contains growth performance data from a broiler chicken feeding trial with 180 birds across 6 dietary treatments. Variables include:\n\nbird_id: Unique identifier\ntreatment: Dietary treatment (A, B, C, D, E, F)\ninitial_weight: Weight at day 0 (grams)\nfinal_weight: Weight at day 42 (grams)\nweight_gain: Total weight gain (grams)\nfeed_intake: Total feed consumed (grams)\nFCR: Feed conversion ratio (feed:gain)\nmortality: Whether bird survived (Yes/No)\npen: Pen number (1-30, 6 birds per pen)\nsex: Male or Female\nhatch_date: Date hatched (1, 2, or 3 ‚Äî three hatches)\n\nResearch Question: What factors predict weight gain in broiler chickens, and do treatments differ in performance after accounting for confounding variables?\n\n\n\n29.1.3 Assignment Parts\n\n29.1.3.1 Part 1: Exploratory Data Analysis (25 points)\nCreate a comprehensive EDA section that includes:\n\nData import and cleaning\n\nLoad the dataset\nCheck for missing values\nCreate any derived variables you need\nConvert categorical variables to factors\n\nDescriptive statistics\n\nSummary statistics (mean, SD, range) for continuous variables\nFrequency tables for categorical variables\nPresent in a clear table\n\nUnivariate visualizations\n\nDistributions of key continuous variables (histograms or density plots)\nBar charts for categorical variables\n\nBivariate relationships\n\nScatterplots of relationships between continuous variables\nBoxplots comparing groups\nCorrelation matrix (for continuous variables)\n\nWritten summary\n\nDescribe patterns you observe\nIdentify potential outliers\nNote relationships that will inform your modeling\n\n\n\n\n\n29.1.3.2 Part 2: Initial Analyses (20 points)\nBefore fitting the full multiple regression model, conduct appropriate preliminary analyses:\n\nUnivariate tests\n\nTest if treatments differ in weight gain (one-way ANOVA)\nPost-hoc comparisons if appropriate\nVisualize treatment differences\n\nSimple regression\n\nFit a simple linear regression: weight gain ~ feed intake\nInterpret the coefficient and R¬≤\nVisualize the relationship\n\nRationale for multiple regression\n\nExplain why multiple regression is necessary\nIdentify potential confounding variables\n\n\n\n\n\n29.1.3.3 Part 3: Multiple Regression Analysis (30 points)\nFit and compare multiple regression models:\n\nModel building\n\nFit at least 3 candidate models with different predictor combinations\nJustify your choice of predictors (theory-driven, not stepwise!)\nConsider including interactions if appropriate\n\nModel comparison\n\nCreate a comparison table (R¬≤, Adjusted R¬≤, AIC, BIC)\nUse F-tests for nested models\nSelect a final model and justify your choice\n\nAssumption checking\n\nPresent diagnostic plots for your final model\nAssess each assumption (linearity, independence, homoscedasticity, normality, multicollinearity)\nCheck VIF values\nDiscuss any violations and how they affect your conclusions\n\nFinal model interpretation\n\nPresent regression output (use broom::tidy() or similar)\nInterpret each coefficient with 95% CIs\nAssess overall model fit\nCreate visualization(s) of predicted values\n\n\n\n\n\n29.1.3.4 Part 4: Results Communication (15 points)\nWrite a complete Results section as if for a journal article:\n\nText description\n\nReport model fit statistics\nDescribe significant and non-significant predictors\nInclude effect sizes and confidence intervals\nUse appropriate statistical notation\n\nTables\n\nCreate a publication-quality regression table\nInclude coefficients, SEs, CIs, and p-values\n\nFigures\n\nCreate at least one publication-quality figure showing model predictions\nInclude informative title and caption\n\n\n\n\n\n29.1.3.5 Part 5: Discussion & Critical Reflection (10 points)\nWrite a brief discussion addressing:\n\nInterpretation in context\n\nWhat do your findings mean biologically/practically?\nHow do they relate to existing knowledge?\n\nLimitations\n\nWhat assumptions were violated (if any)?\nWhat confounders might you have missed?\nWhat are the limits of generalizability?\n\nFuture directions\n\nWhat additional analyses would you conduct with more time/resources?\nWhat data would strengthen your conclusions?\n\nReflection on the course\n\nWhat statistical concepts from this course were most useful for this analysis?\nWhat would you do differently in future analyses?\n\n\n\n\n\n\n29.1.4 Technical Requirements\n\nReproducibility: Your .qmd file should run completely from start to finish without errors\nCode quality: Use clear variable names, include comments, follow tidy principles\nOrganization: Use headers, subheaders, and narrative text to guide the reader\nVisualization: All plots should have clear titles, axis labels, and legends\nTables: Use knitr::kable() or similar for formatted tables\n\n\n\n\n29.1.5 Grading Rubric\n\n\n\n\n\n\n\n\n\n\nComponent\nExcellent (90-100%)\nGood (80-89%)\nSatisfactory (70-79%)\nNeeds Improvement (&lt;70%)\n\n\n\n\nEDA (25 pts)\nComprehensive exploration with insightful observations and publication-quality figures\nThorough EDA with appropriate visualizations\nBasic EDA completed but missing some components\nIncomplete or superficial EDA\n\n\nInitial Analyses (20 pts)\nAppropriate tests conducted with correct interpretation\nTests conducted with mostly correct interpretation\nTests conducted but interpretation lacks depth\nIncorrect tests or poor interpretation\n\n\nMultiple Regression (30 pts)\nThoughtful model building, thorough diagnostics, correct interpretation\nGood models with adequate diagnostics\nModels fit but diagnostics or interpretation incomplete\nModels poorly specified or interpreted\n\n\nCommunication (15 pts)\nClear, professional writing with excellent tables and figures\nGood communication with minor issues\nAdequate communication but lacks polish\nPoor organization or presentation\n\n\nReflection (10 pts)\nInsightful discussion of limitations and learning\nGood reflection with awareness of limitations\nBasic reflection\nSuperficial or missing reflection\n\n\n\n\n\n\n29.1.6 Submission Checklist\nBefore submitting, ensure you have:\n\nIncluded all required sections\nYour .qmd file runs without errors from start to finish\nAll tables and figures have informative captions\nInterpretation is in plain language (not just statistics)\nCode is commented and organized\nReferences to course concepts where appropriate\nSpell-checked and proofread\nSubmitted both .qmd and .html files\n\n\n\n\n29.1.7 Tips for Success\n\nStart early: This is a substantial project\nAsk questions: Use office hours if you get stuck\nFocus on interpretation: The statistics serve the science, not the other way around\nTell a story: Guide the reader through your analytical journey\nShow your work: Include code (with code folding) so analyses are transparent\nBe honest: If something didn‚Äôt work as expected, discuss it!\n\n\nGood luck with your final project! This is your chance to demonstrate everything you‚Äôve learned in AnS 500.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#textbooks",
    "href": "chapters/ch16-multiple_regression.html#textbooks",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "30.1 Textbooks",
    "text": "30.1 Textbooks\n\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2021). An Introduction to Statistical Learning with Applications in R (2nd ed.). Springer. Free online\nFaraway, J. J. (2014). Linear Models with R (2nd ed.). Chapman and Hall/CRC.\nGelman, A., Hill, J., & Vehtari, A. (2020). Regression and Other Stories. Cambridge University Press.\nMcElreath, R. (2020). Statistical Rethinking: A Bayesian Course with Examples in R and Stan (2nd ed.). CRC Press.",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#r-packages-for-regression",
    "href": "chapters/ch16-multiple_regression.html#r-packages-for-regression",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "30.2 R Packages for Regression",
    "text": "30.2 R Packages for Regression\n\nbroom: Tidy regression output\ncar: Companion to Applied Regression (VIF, diagnostics)\neffects: Effect plots and visualizations\nperformance: Model diagnostics and comparison\nsjPlot: Tables and plots for regression models\ninteractions: Visualizing and probing interactions",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#online-resources",
    "href": "chapters/ch16-multiple_regression.html#online-resources",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "30.3 Online Resources",
    "text": "30.3 Online Resources\n\nR for Data Science: https://r4ds.hadley.nz/\nSTHDA (Statistical Tools for High-throughput Data Analysis): http://www.sthda.com/english/\nCross Validated (Stats Stack Exchange): https://stats.stackexchange.com/\nQuick-R: https://www.statmethods.net/",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  },
  {
    "objectID": "chapters/ch16-multiple_regression.html#papers-articles",
    "href": "chapters/ch16-multiple_regression.html#papers-articles",
    "title": "16¬† Week 16: Multiple Regression & Course Integration",
    "section": "30.4 Papers & Articles",
    "text": "30.4 Papers & Articles\n\nHarrell, F. E. (2015). ‚ÄúRegression Modeling Strategies‚Äù ‚Äî comprehensive guide\nGelman, A., & Hill, J. (2006). ‚ÄúData Analysis Using Regression and Multilevel/Hierarchical Models‚Äù\nASA Statement on P-values (2016) ‚Äî essential reading\n\n\nEND OF WEEK 8 MATERIALS\nThank you for engaging with AnS 500 this semester. May your future analyses be thoughtful, your assumptions checked, and your p-values appropriately interpreted!",
    "crumbs": [
      "Part 2: Introduction to Statistics",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Week 16: Multiple Regression & Course Integration</span>"
    ]
  }
]